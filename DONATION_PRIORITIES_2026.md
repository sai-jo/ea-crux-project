# Top 3 Organizations to Prioritize for Donations (2026)

**Analysis Date:** January 2026
**Based on:** AI Transition Model Master Graph + Organization Strategic Analysis

## Executive Summary

After analyzing the causal pathways in the AI Transition Model and evaluating organizational funding gaps, I recommend prioritizing donations to these three organizations for **highest marginal impact per dollar**:

1. **Apollo Research** - Deceptive alignment detection (misalignment pathway)
2. **GovAI** - Compute governance policy (power concentration pathway)
3. **METR** - Dangerous capability evaluation (misuse pathway)

---

## 1. Apollo Research (Highest Priority)

**Website:** https://www.apolloresearch.ai
**Estimated Funding:** $3-10M/year
**Team Size:** ~15-20 researchers

### Why Critical

Apollo Research operates at the **most critical bottleneck** in AI safety: detecting whether advanced AI systems are capable of deceptive alignment (hiding misaligned goals). This addresses the core uncertainty in the strongest causal pathway to existential catastrophe:

```
Misalignment Potential → AI Takeover → Existential Catastrophe
```

### Strategic Position

- **Direct influence on deployment decisions**: Research directly informs OpenAI Preparedness Framework, Anthropic Responsible Scaling Policy (ASL-3/4), DeepMind Frontier Safety Framework
- **Gatekeeper role**: Operates as de facto "nuclear safety inspector" for frontier AI labs
- **Policy integration**: Advises UK AISI, EU AI Act, US NIST standards
- **Unique expertise**: Only organization systematically researching scheming, sandbagging, and strategic deception at scale

### Key Work Areas

- **Scheming evaluations**: Testing whether AI systems can strategically hide capabilities or goals
- **Sandbagging detection**: Identifying when models deliberately underperform on safety evaluations
- **Situational awareness**: Understanding when models know they're being tested vs. deployed
- **Model organisms**: Creating controlled examples of deceptive behavior for study

### Funding Gap & Marginal Impact

**Critical constraints:**
- Small team (15-20) relative to scope (evaluating all frontier models)
- Compute budget limitations restrict depth of testing
- Model access depends on maintaining lab relationships

**Impact of $1M donation:**
- Fund 2-3 additional PhD-level researchers (50% capacity increase)
- 5-10x compute budget expansion for deeper evaluations
- Enable systematic testing across more model architectures
- Accelerate development of standardized deception detection methods

### Why Most Underfunded

- **Specialization barrier**: Deception evaluation requires unique expertise blend (ML, cognitive science, adversarial security)
- **Field recognition lag**: "Worst-case safety" and deception research identified as underfunded in funding landscape
- **Scaling difficulty**: Unlike interpretability (hundreds of researchers), deception evaluation hasn't scaled due to methodological complexity

**Donation Value: EXCEPTIONAL** - Addresses highest-risk failure mode with smallest team and clearest funding gap

---

## 2. GovAI (Centre for the Governance of AI)

**Website:** https://governance.ai
**Estimated Funding:** $3-8M/year
**Team Size:** ~8-12 researchers

### Why Critical

GovAI addresses the **power concentration and lock-in pathway** through policy research on AI governance architecture. Their work has multiplicative leverage: one policy paper can influence billions in governance decisions and shape international coordination structures.

### Strategic Position

- **Compute governance leadership**: Published foundational work on using compute as policy lever for AI development
- **International influence**: Advises UK AISI, US AISI, EU AI Office, international AI safety summits
- **Policy multiplication**: Research scales globally with minimal additional resources
- **Chokepoint identification**: Maps governance interventions to AI transition model factors

### Key Work Areas

- **Compute governance**: Using computational requirements as regulatory lever (chip export controls, datacenter monitoring, FLOP-based thresholds)
- **International coordination**: Designing governance structures for multi-stakeholder AI development
- **Regulatory frameworks**: Informing Bletchley Declaration, EU AI Act, US Executive Order on AI
- **Power concentration mitigation**: Policies preventing AI ownership lock-in

### Funding Gap & Marginal Impact

**Critical constraints:**
- Policy research heavily underfunded vs. technical research (~10:1 ratio)
- Non-US governance work specifically identified as funding gap
- Small team limits breadth of policy areas covered

**Impact of $1M donation:**
- Enable 1-2 new research streams (e.g., international compute governance architecture)
- Fund policy fellows for targeted government engagement
- Support rapid-response policy analysis during critical decision windows
- Potentially influence $10B+ in governance spending through well-timed research

### Why High Leverage

- **Policy as force multiplier**: Technical safety research requires deployment; governance shapes *whether* and *how* deployment happens
- **Timing criticality**: Governance windows close as AI capabilities advance; current moment (2025-2027) is critical for setting precedents
- **Institutional void**: Very few organizations doing rigorous AI governance research outside US/UK

**Donation Value: EXCEPTIONAL (policy leverage)** - Highest influence-per-dollar through shaping international governance architecture

---

## 3. METR (Model Evaluation & Threat Research)

**Website:** https://metr.org
**Formerly:** ARC Evals
**Estimated Funding:** $5-15M/year
**Team Size:** ~20-30 people

### Why Critical

METR operates as the **pre-deployment gateway** for misuse-potential risks, evaluating frontier models for dangerous capabilities before release. This directly addresses the human-caused catastrophe pathway:

```
AI Capabilities → Human-Caused Catastrophe → Existential Catastrophe
```

### Strategic Position

- **Deployment gate-keeping**: Integrated into OpenAI Preparedness Framework and Anthropic Responsible Scaling Policy
- **Threat vector coverage**: Only organization systematically testing for autonomous replication, bio-risk, cyber-offense capabilities
- **Lab cooperation**: Works across OpenAI, Anthropic, DeepMind, UK AISI
- **Standards development**: Defining what "dangerous capability" means operationally

### Key Work Areas

- **Autonomous replication testing**: Can models self-replicate, acquire resources, evade shutdown?
- **Bio-risk evaluation**: Do models increase bioweapons accessibility for non-experts?
- **Cyber capability assessment**: Offensive cybersecurity capabilities (vulnerability discovery, exploit development)
- **Evaluation methodology**: Developing standardized dangerous capability test suites

### Funding Gap & Marginal Impact

**Critical constraints:**
- Each new model generation requires methodological innovation (GPT-4 → GPT-5 evaluation complexity increases)
- Limited evaluation depth due to budget constraints
- Dependent on maintaining lab access and cooperation
- Threat vector coverage incomplete (e.g., chemical weapons, persuasion/manipulation less tested)

**Impact of $1M donation:**
- Enable systematic testing of 2-3 additional threat vectors per model
- Fund deeper evaluation of existing threat categories
- Hire 1-2 additional evaluators per threat domain
- Support compute costs for more comprehensive red-teaming

### Why Moderately Well-Funded But Still High-Value

- Larger than Apollo/GovAI but scope is also larger (must evaluate *every* frontier model *before* deployment)
- Identified as "well-funded area" (evaluations) but dangerous capability evals specifically less funded than alignment evals
- Scaling demands increase with model capability (more ways to cause harm = more things to test)

**Donation Value: HIGH** - Direct risk reduction through comprehensive pre-deployment testing; clear path from resources to impact

---

## Comparative Analysis

| Organization | Pathway Addressed | Funding Gap | Team Size | Marginal ROI | Leverage Type |
|--------------|-------------------|-------------|-----------|--------------|---------------|
| **Apollo Research** | Misalignment → AI Takeover | Very High | Small (15-20) | Exceptional | Detection bottleneck |
| **GovAI** | Power Concentration → Lock-in | Very High | Small (8-12) | Exceptional | Policy multiplier |
| **METR** | Misuse → Human Catastrophe | High | Medium (20-30) | High | Deployment gate |

---

## Alternative Organizations (Tier 2 Priority)

If the above three are already well-supported or not accepting donations:

### 4. Redwood Research
- **Focus:** AI control (worst-case safety), mechanistic interpretability
- **Funding:** ~$3-8M/year
- **Why:** Addresses "what if alignment fails" scenario with practical defense mechanisms

### 5. ARC (Alignment Research Center)
- **Focus:** Eliciting Latent Knowledge (ELK), scalable oversight
- **Funding:** ~$5-12M/year
- **Why:** Foundational theoretical research enabling other safety approaches

---

## Funding Context

**Total AI Safety Field:**
- ~$100-500M/year across all organizations
- Open Philanthropy largest funder (~$50-100M/year)
- AI capabilities funding: ~$50-100B/year (1000:1 ratio)

**Identified Funding Gaps:**
- Deception evaluation (Apollo): Very underfunded
- Compute governance (GovAI): Very underfunded
- Worst-case safety (Redwood): Underfunded
- Dangerous capability evals (METR): Moderately funded but scaling constrained

---

## Donation Strategies

### For Large Donors ($500K+)

**Recommended:** Split across all three priorities
- 40% Apollo Research (highest gap)
- 30% GovAI (policy leverage)
- 30% METR (deployment safety)

**Rationale:** Addresses all three critical pathways (misalignment, lock-in, misuse) while concentrating on highest-gap organizations

### For Medium Donors ($50K-$500K)

**Recommended:** Apollo Research or GovAI (pick one based on theory of change)
- Choose **Apollo** if you believe misalignment is the highest x-risk pathway
- Choose **GovAI** if you believe governance failures and power concentration are highest risk

### For Small Donors (<$50K)

**Recommended:** Apollo Research
- Smallest organization with clearest marginal value
- Most acute funding constraints
- Easiest to demonstrate impact of smaller donations

---

## How to Donate

### Apollo Research
- Website: https://www.apolloresearch.ai
- Contact: Research director (check website for current contact)
- Note: May require reaching out directly; not all safety orgs have public donation pages

### GovAI
- Website: https://governance.ai
- Fiscal sponsor: May operate through academic institution (Oxford) or fiscal sponsor
- Contact through website for donation information

### METR
- Website: https://metr.org
- Contact: Check website for donation/support information
- May operate through fiscal sponsor

### Alternative Route: Open Philanthropy / Long-Term Future Fund
If direct donation is complex, consider:
- **LTFF (Long-Term Future Fund):** https://funds.effectivealtruism.org/funds/far-future
  - Rolling applications, small-medium grants
  - Funds organizations like the above based on evaluation
- **Open Philanthropy:** (Large donors only, relationship-driven)
  - Can direct preferences toward specific organizations

---

## Assessment Methodology

This analysis synthesized:

1. **AI Transition Model Master Graph** - Causal pathways from factors → scenarios → outcomes
2. **Category Edge Analysis** - Strength of causal connections (strong/medium/weak)
3. **Organization Strategic Positioning** - Which factors each org addresses
4. **Funding Landscape Data** - Current funding levels and identified gaps
5. **Marginal Value Analysis** - Impact of additional resources per organization

**Key Insight:** Focused on organizations operating at *bottlenecks* (Apollo), *force multipliers* (GovAI), and *deployment gates* (METR) rather than larger, better-funded organizations working on well-covered areas.

---

## Caveats & Updates

- Funding estimates are approximate (organizations don't always publish exact budgets)
- Organizational priorities shift; verify current focus areas before donating
- This analysis focuses on x-risk reduction, not broader AI safety/ethics
- Assumes transformative AI timeline of 2026-2035 (affects urgency calculations)
- Update recommended every 6-12 months as field evolves rapidly

**Last Updated:** January 2026
**Next Review:** July 2026

---
title: "Existential Catastrophe"
description: "The probability and severity of catastrophic AI-related events—loss of control, weaponization, large-scale accidents, or irreversible lock-in to harmful power structures."
sidebar:
  order: 1
  label: Existential Catastrophe
pageType: stub
lastEdited: "2025-12-29"
---
import {Backlinks, DataInfoBox, FactorRelationshipDiagram, ImpactList} from '../../../../components/wiki';

<DataInfoBox entityId="existential-catastrophe" />

## Overview

Existential Catastrophe measures the probability and potential severity of catastrophic AI-related events. This is about the **tail risks**—the scenarios we most urgently want to avoid because they could cause irreversible harm at civilizational scale.

Unlike [Transition Smoothness](/ai-transition-model/factors/transition-turbulence/) (which concerns the journey) or [Steady State Quality](/ai-transition-model/outcomes/long-term-trajectory/) (which concerns the destination), Existential Catastrophe is about avoiding catastrophe entirely. A world with high existential catastrophe might navigate a smooth transition to a good steady state—or might not make it there at all.

---

## Sub-dimensions

| Dimension | Description | Key Parameters |
|-----------|-------------|----------------|
| **Loss of Control** | AI systems pursuing goals misaligned with humanity; inability to correct or shut down advanced systems | Alignment Robustness, Human Oversight Quality |
| **Misuse Catastrophe** | Deliberate weaponization of AI for mass harm—bioweapons, autonomous weapons, critical infrastructure attacks | Biological Threat Exposure, Cyber Threat Exposure |
| **Accident at Scale** | Unintended large-scale harms from deployed systems; cascading failures across interconnected AI | Safety-Capability Gap, Safety Culture Strength |
| **Lock-in Risk** | Irreversible commitment to bad values, goals, or power structures | AI Control Concentration, Institutional Quality |
| **Concentration Catastrophe** | Single actor gains decisive AI advantage and uses it harmfully | AI Control Concentration, Racing Intensity |

---

## What Contributes to Existential Catastrophe

<FactorRelationshipDiagram nodeId="existential-catastrophe" direction="incoming" client:load />

### Scenario Impact Scores

<ImpactList nodeId="existential-catastrophe" direction="to" client:load />

### Primary Contributing Aggregates

| Aggregate | Relationship | Mechanism |
|-----------|--------------|-----------|
| [Misalignment Potential](/ai-transition-model/factors/misalignment-potential/) | ↓↓↓ Decreases risk | Aligned, interpretable, overseen systems are less likely to cause catastrophe |
| [Misuse Potential](/ai-transition-model/factors/misuse-potential/) | ↑↑↑ Increases risk | Higher bio/cyber exposure, concentration, and racing all elevate existential catastrophe |
| [Civilizational Competence](/ai-transition-model/factors/civilizational-competence/) | ↓↓ Decreases risk | Effective governance can slow racing, enforce safety standards, coordinate responses |

### Key Individual Parameters

| Parameter | Effect | Strength |
|-----------|--------|----------|
| [Alignment Robustness](/ai-transition-model/parameters/alignment-robustness/) | ↓ Reduces | ↓↓↓ Critical |
| [Safety-Capability Gap](/ai-transition-model/parameters/safety-capability-gap/) | ↑ Increases | ↑↑↑ Critical |
| [Racing Intensity](/ai-transition-model/parameters/racing-intensity/) | ↑ Increases | ↑↑↑ Strong |
| [Human Oversight Quality](/ai-transition-model/parameters/human-oversight-quality/) | ↓ Reduces | ↓↓ Strong |
| [Interpretability Coverage](/ai-transition-model/parameters/interpretability-coverage/) | ↓ Reduces | ↓↓ Strong |
| [AI Control Concentration](/ai-transition-model/parameters/ai-control-concentration/) | ↑/↓ Depends | ↑↑ Context-dependent |
| [Biological Threat Exposure](/ai-transition-model/parameters/biological-threat-exposure/) | ↑ Increases | ↑↑ Direct |
| [Cyber Threat Exposure](/ai-transition-model/parameters/cyber-threat-exposure/) | ↑ Increases | ↑↑ Direct |

---

## Why This Matters

Existential catastrophe is the most time-sensitive outcome dimension:
- **Irreversibility**: Many catastrophic scenarios cannot be undone
- **Path dependence**: High existential catastrophe can foreclose good steady states entirely
- **Limited recovery**: Unlike transition disruption, catastrophe may preclude recovery
- **Urgency**: Near-term capability advances increase near-term existential catastrophe

This is why much AI safety work focuses on existential catastrophe reduction—it's the outcome where failure is most permanent.

---

## Related Outcomes

- [Long-term Steady State Quality](/ai-transition-model/outcomes/long-term-trajectory/) — The destination (if we avoid catastrophe)
- [Transition Smoothness](/ai-transition-model/factors/transition-turbulence/) — The journey quality

---

## Related Factors

- [Misalignment Potential](/ai-transition-model/factors/misalignment-potential/)
- [Misuse Potential](/ai-transition-model/factors/misuse-potential/)
- [Civilizational Competence](/ai-transition-model/factors/civilizational-competence/)

<Backlinks entityId="existential-catastrophe" />

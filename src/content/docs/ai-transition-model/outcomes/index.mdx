---
title: "Ultimate Outcomes"
description: "The two ultimate outcomes of the AI transition: avoiding existential catastrophe and ensuring a positive long-term trajectory."
sidebar:
  order: 0
  label: Overview
lastEdited: "2026-01-03"
---
import {Mermaid, OutcomesTable, FullModelDiagram} from '../../../../components/wiki';

## Overview

Ultimate Outcomes represent what we fundamentally care about when thinking about AI's impact on humanity. Unlike [Scenarios](/ai-transition-model/scenarios/) (which describe intermediate scenarios) or [parameters](/ai-transition-model/) (which measure specific factors), Ultimate Outcomes describe the **final states** we're trying to achieve or avoid.

There are two Ultimate Outcomes:

1. **[Existential Catastrophe](/ai-transition-model/outcomes/existential-catastrophe/)** — Does catastrophe occur?
2. **[Long-term Trajectory](/ai-transition-model/outcomes/long-term-trajectory/)** — What's the expected value of the future?

---

## The Two Outcomes

<Mermaid client:load chart={`
flowchart LR
    subgraph Outcomes["What We Ultimately Care About"]
        ACUTE[Existential Catastrophe]
        VALUE[Long-term Trajectory]
    end

    ACUTE -.->|"must avoid to reach"| VALUE

    style ACUTE fill:#ff6b6b
    style VALUE fill:#4ecdc4
`} />

<OutcomesTable client:load />

---

## Why Two Outcomes?

Previous versions of this framework had three outcomes (including "Transition Smoothness"). We moved to two because:

1. **Transition Turbulence is a pathway, not endpoint**: How rough the transition is affects *both* existential catastrophe and long-term trajectory. It belongs in [Root Factors](/ai-transition-model/factors/transition-turbulence/).

2. **Cleaner analytical structure**: Two outcomes are genuinely orthogonal:
   - You can have low existential catastrophe but poor long-term trajectory (safe dystopia)
   - You can have high existential catastrophe but good conditional value (high-stakes gamble)

3. **Temporal clarity**: Existential Catastrophe is primarily about the transition period; Long-term Trajectory is about what comes after.

---

## How They Relate

These outcomes are **partially independent**—you can have different combinations:

| Scenario | Existential Catastrophe | Long-term Trajectory | Example |
|----------|------------|----------------|---------|
| Best case | Low | High | Aligned AI, smooth transition, flourishing |
| Safe dystopia | Low | Low | No catastrophe but authoritarian lock-in |
| High-stakes success | High (survived) | High | Near-misses but good outcome |
| Extinction | Very High | N/A | Catastrophe occurs |

This independence means:
- **Different Ultimate Scenarios affect different Ultimate Outcomes**
- **Trade-offs exist**: Some approaches that reduce existential catastrophe might worsen long-term trajectory (e.g., authoritarian control)
- **Both matter**: We shouldn't sacrifice one entirely for the other

---

## How Ultimate Scenarios Flow to Ultimate Outcomes

<FullModelDiagram client:load />

Each ultimate scenario has sub-variants with different probability estimates. See the [Ultimate Scenarios](/ai-transition-model/scenarios/) section for details.

---

## Temporal Structure

These outcomes map to different phases of the AI transition:

| Phase | Primary Concern | Relevant Outcome |
|-------|-----------------|------------------|
| **Pre-transformative AI** (now) | Building capacity, avoiding racing | Existential Catastrophe (preparation) |
| **Existential Catastrophe Period** | Surviving the transition | Existential Catastrophe |
| **Resolution** | How it resolves | Both |
| **Long-run Trajectory** | Quality of the future | Long-term Trajectory |

---

## Related Pages

- [Scenarios](/ai-transition-model/scenarios/) — The intermediate scenarios
- [AI Transition Model](/ai-transition-model/) — All parameters
- [Aggregate Parameters](/ai-transition-model/factors/) — How parameters group together

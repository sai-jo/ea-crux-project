---
title: "AI Control Concentration"
description: "How concentrated or distributed power over AI development and deployment is across actors—a structural parameter where neither extreme is optimal. Currently concentrating: fewer than 20 organizations can train frontier models, with costs projected to reach $1-10B by 2030."
parentFactor: misuse-potential
sidebar:
  order: 3
quality: 91
llmSummary: "Analyzes AI power concentration trends showing fewer than 20 organizations can train frontier models (costs: $100M+ now, projected $1-10B by 2030), with 3 cloud firms controlling 68% of infrastructure. Argues power distribution has an optimal range between authoritarian concentration and coordination-impossible diffusion, with current trajectory moving toward 5-10 frontier-capable orgs by 2030."
lastEdited: "2025-12-29"
importance: 75
tractability: 30
neglectedness: 60
uncertainty: 65
---
import {DataInfoBox, Backlinks, Mermaid, R, PageCauseEffectGraph} from '../../../../components/wiki';


<DataInfoBox entityId="ai-control-concentration" />

AI Control Concentration measures how concentrated or distributed power over AI development and deployment is across actors—including corporations, governments, and individuals. Unlike most parameters where "higher is better," power distribution has an **optimal range**: extreme concentration enables authoritarianism and regulatory capture, while extreme diffusion prevents coordination and creates race-to-the-bottom dynamics on safety standards.

The current trajectory shows significant concentration. As of 2024, the "Big Five" tech companies (Google, Amazon, Microsoft, Apple, Meta) command a combined \$12 trillion in market capitalization and control vast swaths of the AI value chain. NVIDIA holds approximately 80-95% market share in AI chips, while three cloud providers (AWS, Azure, GCP) control 68-70% of the infrastructure required to train frontier models. Policymakers and competition authorities across the US, EU, and other jurisdictions have launched multiple antitrust investigations, recognizing that this level of concentration hasn't been seen since the monopolistic reigns of Standard Oil and AT&T.

This parameter critically affects four dimensions of AI governance. **Democratic accountability** determines whether citizens can meaningfully influence AI development trajectories, or whether a small set of corporate executives make civilizational decisions without public mandate. **Safety coordination** shapes whether actors can agree on and enforce safety standards—concentrated power could enable coordinated safety measures, but also enables any single actor to defect. **Innovation dynamics** determine who captures AI's economic benefits and whether diverse approaches can flourish. **Geopolitical stability** reflects how AI power is distributed across nations, with current asymmetries creating strategic tensions between the US, China, EU, and the rest of the world.

Understanding power distribution as a structural parameter enables more sophisticated analysis than simple "monopoly bad, competition good" framings. It allows for nuanced intervention design that shifts distribution toward optimal ranges without overcorrecting, scenario modeling exploring different equilibria, and quantitative tracking of concentration trends over time. The key insight is that both monopolistic concentration (1-3 actors) and extreme fragmentation (100+ actors with incompatible standards) create distinct failure modes—the goal is finding and maintaining an intermediate range.

---

## Parameter Network

<Mermaid client:load chart={`
flowchart LR
    subgraph Moderates["What Moderates It"]
        RC[Regulatory Capacity]
    end

    RC -->|moderates| ACC[AI Control Concentration]

    ACC -->|amplifies| BTE[Bio Threat Exposure]
    ACC -->|amplifies| CTE[Cyber Threat Exposure]

    ACC --> THREAT[Misuse Potential]
    ACC --> ACUTE[Existential Catastrophe ↑↑]
    ACC --> STEADY[Steady State ↑↑↑]

    style ACC fill:#f9f
    style ACUTE fill:#ff6b6b
    style STEADY fill:#4ecdc4
`} />

**Contributes to:** [Misuse Potential](/ai-transition-model/factors/misuse-potential/)

**Primary outcomes affected:**
- [Existential Catastrophe](/ai-transition-model/outcomes/existential-catastrophe/) ↑↑ — Concentrated control creates single points of failure/capture
- [Steady State](/ai-transition-model/outcomes/long-term-trajectory/) ↑↑↑ — Who controls AI shapes long-term power distribution

*Note: Effects depend on **who** gains control. Concentration in safety-conscious actors may reduce risk; concentration in reckless actors increases it dramatically.*

---

## Current State Assessment

### Compute and Infrastructure Concentration

| Dimension | Current Status | Trend | Source |
|-----------|---------------|-------|--------|
| Cloud infrastructure | 3 firms (AWS, Azure, GCP) control 68-70% | Stable-High | <R id="e1cc3a659ccb8dd6">McKinsey 2024</R> |
| AI training chips | NVIDIA has 80-95% market share | Stable | <R id="6c723bee828ef7b0">DOJ Investigation 2024</R> |
| Manufacturing concentration | TSMC ~90% of AI chip production; single supplier (ASML) for equipment | Very High | <R id="1e614906f3e638b4">AI Supply Chain Analysis 2024</R> |
| Frontier model training | Fewer than 20 organizations capable (12-16 estimated) | Concentrating | <R id="dfeb27439fd01d3e">GPT-4 training requirements</R> |
| Training costs | \$100M+ per frontier model | Increasing | <R id="5fa46de681ff9902">Anthropic estimates</R> |
| Projected 2030 costs | \$1-10B per model | Accelerating | <R id="2efa03ce0d906d78">Epoch AI compute trends</R> |
| Data center investment needed | \$5.2 trillion by 2030 (70% by hyperscalers) | Massive growth | <R id="f5842967d6dad56c">McKinsey 2024</R> |

*Note: McKinsey projects companies across the compute power value chain will need to invest \$5.2 trillion into data centers by 2030 to meet AI demand, with hyperscalers capturing ~70% of US capacity. This creates additional concentration as only the largest firms can finance such buildouts.*

### Capital and Investment Concentration

| Investment | Amount | Implication | Status |
|------------|--------|-------------|--------|
| Microsoft → OpenAI | \$13B+ | Largest private AI partnership; under <R id="6c723bee828ef7b0">regulatory scrutiny</R> | Active |
| Amazon → Anthropic | \$1B | Major cloud-lab vertical integration | Active |
| Meta AI infrastructure | \$15B+/year | Self-funded capability development | Ongoing |
| Google DeepMind (internal) | Billions/year | Fully integrated with parent | Ongoing |
| Big Tech AI acquisitions | \$30B+ total (2020-2024) | Potential <R id="29f1cda3047e5d43">regulatory circumvention</R> via "partnerships" | Under investigation |

*Note: Regulators increasingly scrutinize whether tech giants are classifying acquisitions as "partnerships" or "acqui-hires" to circumvent antitrust review. The FTC, DOJ, and EU Commission have all launched investigations into AI market concentration.*

### Talent Concentration

Recent analysis shows extreme talent concentration among frontier AI labs. Top 50 AI researchers are concentrated at approximately 6-8 major labs (Google DeepMind, OpenAI, Anthropic, Meta AI, Microsoft Research, select academic institutions), with academic institutions experiencing sustained talent drain to industry. Safety expertise is particularly concentrated: fewer than 200 researchers globally work full-time on technical AI safety at frontier labs. Visa restrictions further limit global talent distribution, with US immigration policy creating bottlenecks for non-US researchers. This creates path dependencies where top researchers cluster at well-funded labs, which attracts more top talent, reinforcing concentration.

### Geopolitical Distribution

| Actor | Investment | Compute Access |
|-------|------------|----------------|
| United States | \$12B (CHIPS Act) | Full access to frontier chips |
| China | \$150B (2030 AI Plan) | Limited by export controls |
| European Union | ~\$10B (various programs) | Dependent on US/Asian chips |
| Rest of World | Minimal | Very limited |

---

## The Optimal Range Problem

Unlike trust or epistemic capacity (where higher is better), power distribution has **tradeoffs at both extremes**:

<Mermaid client:load chart={`
flowchart TD
    HIGH_CONC[High Concentration]
    HIGH_DIST[High Distribution]
    OPTIMAL[Optimal Range]

    HIGH_CONC --> C1[Authoritarian risk]
    HIGH_CONC --> C2[Regulatory capture]
    HIGH_CONC --> C3[Single points of failure]

    HIGH_DIST --> D1[Coordination difficult]
    HIGH_DIST --> D2[Safety standards fragmented]
    HIGH_DIST --> D3[Race to bottom]

    OPTIMAL --> O1[Multiple capable actors]
    OPTIMAL --> O2[Shared safety standards]
    OPTIMAL --> O3[Democratic oversight]

    C1 -.->|Risks| BALANCE[Balance needed]
    D1 -.->|Risks| BALANCE

    style HIGH_CONC fill:#ffcdd2
    style HIGH_DIST fill:#ffcdd2
    style OPTIMAL fill:#c8e6c9
    style BALANCE fill:#fff3e0
`} />

### Risks of Extreme Concentration

| Risk | Mechanism | Current Concern Level | Evidence |
|------|-----------|----------------------|----------|
| **Authoritarian capture** | Small group controls transformative technology without democratic mandate | Medium-High | Corporate executives making decisions affecting billions; minimal public input |
| **Regulatory capture** | AI companies influence their own regulation through lobbying, personnel rotation | High | <R id="29f1cda3047e5d43">\$30B in acquisitions</R>, heavy lobbying presence at AI summits |
| **Single points of failure** | Safety failure at one lab affects everyone via deployment or imitation | High | Frontier capabilities concentrated at 12-16 organizations |
| **Democratic deficit** | Citizens cannot meaningfully influence AI development trajectories | High | Development decisions made by private boards, not public bodies |
| **Abuse of power** | No competitive checks on concentrated capability; potential for coercion | Medium-High | Market dominance enables anticompetitive practices |

### Risks of Extreme Distribution

| Risk | Mechanism | Current Concern Level | Evidence |
|------|-----------|----------------------|----------|
| **Safety race to bottom** | Weakest standards set the floor; actors undercut each other on safety | Medium | Open-source models sometimes released without safety testing |
| **Coordination failure** | Cannot agree on safety protocols due to too many independent actors | Medium | Difficulty achieving consensus even among ~20 frontier labs |
| **Proliferation** | Dangerous capabilities spread widely and uncontrollably | Medium-High | Dual-use risks from openly released models |
| **Fragmentation** | Incompatible standards and approaches prevent interoperability | Low-Medium | Emerging issue as ecosystem grows |
| **Attribution difficulty** | Cannot identify source of harmful AI systems | Medium | Challenge increases with number of capable actors |

---

## Factors That Concentrate Power

### Structural Drivers

| Factor | Mechanism | Strength |
|--------|-----------|----------|
| **Compute scaling** | Frontier models require exponentially more compute | Very Strong |
| **Capital requirements** | \$100M+ training costs exclude most actors | Very Strong |
| **Data advantages** | Big tech has unique proprietary datasets | Strong |
| **Talent concentration** | Top researchers cluster at well-funded labs | Strong |
| **Network effects** | Users create more data → better models → more users | Strong |
| **Infrastructure control** | Cloud providers are also AI developers | Moderate-Strong |

### Recent Concentration Events

| Event | Date | Impact |
|-------|------|--------|
| Microsoft extends OpenAI investment to \$13B+ | 2023 | Major vertical integration |
| Amazon invests \$1B in Anthropic | 2023-24 | Cloud-lab integration |
| NVIDIA achieves 95% chip market share | Ongoing | Critical infrastructure chokepoint |
| Compute costs for frontier models reach \$100M+ | 2023+ | Excludes most organizations |

---

## Factors That Distribute Power

### Technical Developments

| Development | Mechanism | Current Status |
|-------------|-----------|----------------|
| **Open-source models** | Broad access to capabilities | LLaMA, Mistral 1-2 generations behind frontier |
| **Efficiency improvements** | Lower compute requirements | Algorithmic progress ~10x/year |
| **Federated learning** | Training without data centralization | Research stage |
| **Edge AI** | Capable models on personal devices | Growing rapidly |

### Policy Interventions

| Intervention | Mechanism | Status | Impact Estimate |
|--------------|-----------|--------|-----------------|
| **Antitrust action** | Break up vertical integration, block anticompetitive mergers | <R id="6c723bee828ef7b0">FTC/DOJ investigations ongoing</R> into Microsoft-OpenAI, NVIDIA practices | Could reduce concentration by 10-20% if enforced |
| **State-level AI regulation** | Nearly <R id="29f1cda3047e5d43">700 AI bills introduced in 2024</R> (400% increase from 2023); 113 enacted | Active—Colorado first comprehensive law | Creates compliance costs favoring large actors (paradoxically concentrating) |
| **Public compute** | Government-funded training resources | NAIRR proposed (\$1.6B), limited compared to private \$30B+/year spend | Modest distribution effect (~5-10 additional academic/small org capabilities) |
| **Export controls** | Limit concentration by geography, restrict advanced chip access | Active (US → China), increasingly comprehensive | Maintains US-China bifurcation; concentrates within each bloc |
| **EU AI Act** | First comprehensive legal framework, <R id="5f1a7087749eb004">adopted June 2024</R> | Implementation ongoing | Compliance costs may favor large firms; transparency requirements may distribute info |
| **Mandatory licensing** | Conditions on compute access | Under discussion | Uncertain; depends on design |
| **Open-source requirements** | Mandate capability sharing | Proposed in some jurisdictions | Could distribute capabilities but raise safety concerns |

### Market Forces

| Force | Mechanism | Strength |
|-------|-----------|----------|
| **Competition** | Multiple labs racing for capability | Moderate (oligopoly, not monopoly) |
| **New entrants** | Well-funded startups (xAI, etc.) | Moderate |
| **Hardware competition** | AMD, Intel, custom chips | Emerging |
| **Cloud alternatives** | Oracle, smaller providers | Weak |

---

## Why This Parameter Matters

### Concentration Scenarios

| Scenario | Power Distribution | Key Features | Concern Level |
|----------|-------------------|--------------|---------------|
| **Current trajectory** | 5-10 frontier-capable orgs by 2030 | Oligopoly with regulatory tension | High |
| **Hyperconcentration** | 1-3 actors control transformative AI | Winner-take-all dynamics | Critical |
| **Distributed equilibrium** | 20+ capable actors with shared standards | Coordination with competition | Lower (hard to achieve) |
| **Fragmentation** | Many actors, incompatible approaches | Safety race to bottom | High |

### Existential Risk Implications

Power distribution affects x-risk through multiple channels with non-monotonic relationships. The 2024 <R id="4487a62bbc1c45d6">Frontier AI Safety Commitments</R> signed at the AI Seoul Summit illustrate both the promise and peril of concentration: 20 organizations (including Anthropic, OpenAI, Google DeepMind, Microsoft, Meta) agreed to common safety standards—a coordination success only possible with moderate concentration. Yet voluntary commitments from the same concentrated actors raise concerns about regulatory capture and enforcement.

**Safety coordination** exhibits a U-shaped risk curve. With 1-3 actors, a single safety failure cascades globally; with 100+ actors, coordination becomes impossible and weakest standards prevail. The current 12-20 frontier-capable organizations may be near an optimal range for coordination—small enough to achieve consensus, large enough to provide redundancy. However, this assumes actors prioritize safety over competitive advantage, which [racing dynamics](/knowledge-base/risks/structural/racing-dynamics/) can undermine.

**Correction capacity** shows similar complexity. Distributed power (20-50 actors) creates more chances to catch and correct mistakes through diverse approaches and external scrutiny. However, it also creates more chances for any single actor to deploy dangerous systems, as demonstrated by open-source releases that bypass safety review. The <R id="51e8802a5aef29f6">Frontier Model Forum</R> attempts to balance this by sharing safety research among major labs while maintaining competitive development.

**Democratic legitimacy** represents perhaps the starkest tradeoff. Current concentration means a handful of corporate executives make civilizational decisions affecting billions—from content moderation policies to autonomous weapons integration—without public mandate or meaningful accountability. Yet extreme distribution could prevent society from making any coherent decisions about AI governance at all. Intermediate solutions like [public compute infrastructure](/knowledge-base/responses/institutions/) or democratically accountable AI development remain largely theoretical.

---

## Quantitative Framework for Optimal Distribution

While "optimal" power distribution is context-dependent, we can estimate ranges based on coordination theory and empirical governance outcomes:

| Distribution Range | Number of Frontier-Capable Actors | Coordination Feasibility | Safety Risk Level | Democratic Accountability | Estimated Probability by 2030 |
|-------------------|-----------------------------------|-------------------------|-------------------|-------------------------|------------------------------|
| **Monopolistic** | 1-2 | Very High (autocratic) | High (single point of failure) | Very Low | 5-15% |
| **Tight Oligopoly** | 3-5 | High | Medium-High | Low | 25-35% |
| **Moderate Oligopoly** | 6-15 | Medium-High | Medium | Medium | 35-45% (most likely) |
| **Loose Oligopoly** | 16-30 | Medium | Medium-Low | Medium-High | 10-20% |
| **Competitive Market** | 31-100 | Low-Medium | Medium-High | High | 3-8% |
| **Fragmented** | 100+ | Very Low | High (proliferation) | High (but ineffective) | &lt;2% |

**Analysis:** The "Moderate Oligopoly" range (6-15 actors) may represent an optimal balance, providing enough actors for competitive pressure and redundancy while maintaining feasible coordination on safety standards. This aligns with successful international coordination regimes (e.g., nuclear non-proliferation among ~9 nuclear powers, though imperfect). However, current trajectory points toward the higher end of "Tight Oligopoly" (3-5 actors) by 2030 due to capital requirements and infrastructure concentration.

**Key uncertainties:**
- Will algorithmic efficiency improvements democratize access faster than cost scaling concentrates it? (Currently: concentration winning)
- Will antitrust enforcement meaningfully fragment market power? (Probability: 20-40% of significant action by 2027)
- Will public/international investment create viable alternatives to Big Tech? (Probability: 15-30% of substantive capability by 2030)
- Will open-source maintain relevance or fall increasingly behind frontier? (Current gap: 1-2 generations; projected 2030 gap: 2-4 generations)

---

## Trajectory and Projections

### Projected Distribution (2025-2030)

| Metric | 2024 | 2027 | 2030 |
|--------|------|------|------|
| Frontier-capable organizations | ~20 | ~10-15 | ~5-10 |
| Training cost for frontier model | \$100M+ | \$100M-1B | \$1-10B |
| Open-source gap to frontier | 1-2 generations | 2-3 generations | 2-4 generations |
| Alternative chip market share | &lt;5% | 10-15% | 15-25% |

*Based on: <R id="2efa03ce0d906d78">Epoch AI compute trends</R>, <R id="5fa46de681ff9902">Anthropic cost projections</R>*

### Key Decision Points

| Window | Decision | Stakes |
|--------|----------|--------|
| **2024-2025** | Antitrust action on AI partnerships | Could reshape market structure |
| **2025-2026** | Public compute investment | Determines non-corporate capability |
| **2025-2027** | International AI governance | Sets global distribution norms |
| **2026-2028** | Safety standard coordination | Tests whether concentration enables or hinders safety |

---

## Key Debates

### Open Source: Equalizer or Illusion?

The debate over open-source AI as a democratization tool intensified in 2024 following major releases and policy discussions.

**Arguments for open source as equalizer:**
- <R id="f0a602414a4a2667">Meta's LLaMA releases</R> and models like BLOOM, Stable Diffusion, Mistral provide broad access to capable AI
- Enables academic research and small-company innovation: <R id="7e0ad23e51d7dab0">Carnegie Mellon 2024 course</R> had students build mini-GPT by training open models
- Creates competitive pressure on closed models, potentially checking monopolistic behavior
- <R id="551e648faaef4697">Chatham House 2024 analysis</R>: "Open-source models signal the possibility of democratizing and decentralizing AI development... a different trajectory than centralization through proprietary solutions"
- Small businesses and startups can leverage AI without huge costs; researchers access state-of-the-art models for investigation

**Arguments against:**
- Open models trail frontier by 1-2 generations, limiting true frontier capability access
- <R id="5fa46de681ff9902">Amodei (Anthropic)</R>: True frontier requires inference infrastructure, talent, safety expertise, massive capital—not just model weights
- <R id="42bc56fdb890a23e">2024 research on AI democratization</R> shows "AI democratisation" remains ambiguous, encompassing variety of goals and methods with unclear outcomes
- May create proliferation risks (dangerous capabilities widely accessible) without meaningful distribution benefits (infrastructure still concentrated)
- <R id="c0f9fd4776e9ec07">AI infrastructure analysis 2024</R>: Despite increased "open-washing," the AI infrastructure stack remains highly skewed toward closed research and limited transparency
- Open source can serve corporate interests: offloading costs, influencing standards, building ecosystems—not primarily democratization

**Emerging consensus:** Open source distributes access to lagging capabilities while frontier capabilities remain concentrated. This creates a two-tier system where broad access exists for yesterday's AI, but transformative capabilities stay centralized.

### Competition vs. Coordination

This debate intersects directly with [coordination-capacity](/ai-transition-model/parameters/coordination-capacity/) and [international-coordination](/ai-transition-model/parameters/international-coordination/) parameters.

**Pro-competition view:**
- <R id="d095176cfcff71eb">Scott Morton (Yale)</R>: Competition essential for innovation and safety; monopolies create complacency
- Concentrated power invites abuse and regulatory capture—<R id="c0278d744cb2a34f">Big Tech market concentration</R> hasn't been seen since Standard Oil
- Market forces can drive safety investment when reputational and liability risks are high
- Antitrust enforcement necessary to prevent winner-take-all outcomes
- <R id="86f945391fc41f5f">G7 2024 statement</R>: Competition authorities identify concentrated control of chips, compute, cloud capacity, and data as primary anticompetitive concern

**Pro-coordination view:**
- <R id="16914f3b14803a87">CNAS</R>: Fragmenting US AI capabilities advantages China in strategic competition
- Safety standards require cooperation—<R id="51e8802a5aef29f6">Frontier Model Forum</R> shows coordination working among concentrated actors
- Racing dynamics create risks at any distribution level; more actors can mean more racing pressure, not less
- <R id="9264a9f04ad5b2a3">China's parallel safety commitments</R> (17 companies, December 2024) suggest international coordination feasible with moderate concentration
- Extreme distribution makes enforcement of any standards nearly impossible

**Synthesis:** The question may not be "competition or coordination" but rather "what power distribution level enables competition on capabilities while maintaining coordination on safety?" Current evidence suggests 10-30 frontier-capable actors with strong safety coordination mechanisms may balance these goals, though achieving this equilibrium requires active policy intervention.

---

## Related Pages

### Related Parameters
- [Coordination Capacity](/ai-transition-model/parameters/coordination-capacity/) — How effectively actors coordinate on AI governance; directly shaped by power distribution
- [International Coordination](/ai-transition-model/parameters/international-coordination/) — Global coordination capacity; affected by geopolitical power distribution
- [Societal Trust](/ai-transition-model/parameters/societal-trust/) — Public trust in AI institutions; undermined by concentration without accountability
- [Human Agency](/ai-transition-model/parameters/human-agency/) — Individual autonomy in AI systems; reduced by concentrated algorithmic control

### Related Risks
- [Concentration of Power](/knowledge-base/risks/structural/concentration-of-power/) — Extreme concentration scenario where few actors control transformative AI
- [Racing Dynamics](/knowledge-base/risks/structural/racing-dynamics/) — Competitive pressures that can emerge at any distribution level
- [Winner-Take-All](/knowledge-base/risks/structural/winner-take-all/) — Market dynamics driving toward monopolistic outcomes

### Related Interventions
- [Compute Governance](/knowledge-base/responses/governance/compute-governance/) — Regulating compute infrastructure to influence power distribution
- [Antitrust approaches](/knowledge-base/responses/governance/legislation/) — Legal tools to prevent excessive concentration
- [Public compute proposals](/knowledge-base/responses/institutions/) — Government-funded infrastructure to distribute capabilities

### Related Models
- [Winner-Take-All Concentration](/knowledge-base/models/winner-take-all-concentration/) — Model of how network effects drive concentration
- [International Coordination Game](/knowledge-base/models/international-coordination-game/) — How geopolitical power distribution affects coordination

---

## Sources & Key Research

### Market Analysis (2024-2025)
- McKinsey (2024): <R id="f5842967d6dad56c">The Cost of Compute: A \$7 Trillion Race to Scale Data Centers</R>
- McKinsey (2024): <R id="c1e31a3255ae290d">The State of AI in 2025: Agents, Innovation, and Transformation</R>
- Konceptual AI (2024): <R id="c0278d744cb2a34f">Big Tech Dominance: Market Disruption Analysis 2024</R>
- UNCTAD (2024): <R id="5ab8884351b98199">AI Market Projected to Hit \$4.8 Trillion by 2033</R>
- <R id="ee877771092e5530">Cloud infrastructure market data</R>
- <R id="31ee49c7212810bb">NVIDIA market share analysis</R>
- <R id="2a760ffcf303c734">CB Insights AI trends</R>

### Antitrust and Regulation (2024)
- Debevoise Data Blog (2024): <R id="6c723bee828ef7b0">Navigating Antitrust in the Age of AI: Global Regulatory Scrutiny</R>
- PYMNTS (2024): <R id="29f1cda3047e5d43">Antitrust Fears for Big Tech as States Ramp Up Regulations</R>
- Quinn Emanuel (2024): <R id="5f1a7087749eb004">EU Regulation and Competition Law Enforcement</R>
- Concurrences (2024): <R id="86f945391fc41f5f">Artificial Intelligence and Antitrust</R>
- Stanford CodeX (2024): <R id="3ddcf2f7fe362dfc">TRACK AI: Exploring Governance Gaps in AI Firms</R>

### AI Safety Coordination (2024)
- UK Government (2024): <R id="4487a62bbc1c45d6">Frontier AI Safety Commitments, AI Seoul Summit</R>
- Frontier Model Forum (2024): <R id="51e8802a5aef29f6">Progress Update: Advancing Frontier AI Safety</R>
- METR (2025): <R id="c8782940b880d00f">Common Elements of Frontier AI Safety Policies</R>
- AI Frontiers (2024): <R id="9264a9f04ad5b2a3">Is China Serious About AI Safety?</R>

### Open Source and Democratization (2024)
- Chatham House (2024): <R id="551e648faaef4697">Open Source and the Democratization of AI</R>
- arXiv (2024): <R id="42bc56fdb890a23e">Why Companies "Democratise" AI: The Case of Open Source Software Donations</R>
- Open Future (2024): <R id="c0f9fd4776e9ec07">Democratizing AI for the Public Good</R>
- Medium (2025): <R id="7e0ad23e51d7dab0">The Rise of Open-Source AI Models (2024-2025)</R>

### Infrastructure and Supply Chain (2024)
- Springer (2025): <R id="1e614906f3e638b4">The Politics of Artificial Intelligence Supply Chains</R>
- Institute for Progress (2024): <R id="8fb0ae29d9827942">How to Build the Future of AI in the United States</R>
- AI Infrastructure Alliance (2024): <R id="4628192dd7fd6a65">The State of AI Infrastructure at Scale 2024</R>

### Technical and Cost Analysis
- <R id="dfeb27439fd01d3e">GPT-4 training requirements</R>
- <R id="5fa46de681ff9902">Anthropic: Cost projections</R>
- <R id="2efa03ce0d906d78">Epoch AI: Computing trends</R>

### Policy Research
- <R id="7a7a198f908cb5bf">RAND Corporation: AI and Power</R>
- <R id="4bb2a429153348e5">AI Now Institute: Compute sovereignty</R>
- <R id="16914f3b14803a87">CNAS: AI competition research</R>

<PageCauseEffectGraph slug="ai-control-concentration" client:load />

<Backlinks entityId="ai-control-concentration" />

---
title: Comparison Tables
description: Side-by-side comparisons of AI labs, safety approaches, and key positions
sidebar:
  order: 3
---

import { ComparisonTable, DisagreementMap, Section, Tags } from '../../../components/wiki';

This page provides side-by-side comparisons to help understand the landscape of AI safety organizations, approaches, and expert positions.

## Frontier Labs Comparison

<ComparisonTable
  client:load
  title="Frontier AI Labs"
  columns={["Safety Focus", "Approach", "Timeline View", "P(doom) Range", "Key Safety Work"]}
  rows={[
    {
      name: "Anthropic",
      link: "/organizations/anthropic",
      values: {
        "Safety Focus": { value: "Very High", badge: "high" },
        "Approach": "Constitutional AI, RSP, Interpretability",
        "Timeline View": "Short (2026-2030)",
        "P(doom) Range": "10-25%",
        "Key Safety Work": "Constitutional AI, Scaling Monosemanticity, Sleeper Agents"
      }
    },
    {
      name: "OpenAI",
      link: "/organizations/openai",
      values: {
        "Safety Focus": { value: "Medium", badge: "medium" },
        "Approach": "Iterative deployment, RLHF, Superalignment (dissolved)",
        "Timeline View": "Short (2027-2030)",
        "P(doom) Range": "5-20%",
        "Key Safety Work": "GPT safety tuning, Weak-to-strong generalization"
      }
    },
    {
      name: "Google DeepMind",
      link: "/organizations/deepmind",
      values: {
        "Safety Focus": { value: "Medium", badge: "medium" },
        "Approach": "Scalable oversight, Debate, Formal verification",
        "Timeline View": "Medium (2028-2035)",
        "P(doom) Range": "5-15%",
        "Key Safety Work": "Interpretability, Debate, Specification gaming"
      }
    },
    {
      name: "Meta AI",
      values: {
        "Safety Focus": { value: "Lower", badge: "low" },
        "Approach": "Open source, RLHF",
        "Timeline View": "Varied",
        "P(doom) Range": "under 5%",
        "Key Safety Work": "Llama safety tuning, Red teaming"
      }
    },
    {
      name: "xAI",
      values: {
        "Safety Focus": { value: "Lower", badge: "low" },
        "Approach": "Maximum truth-seeking",
        "Timeline View": "Short",
        "P(doom) Range": "Unknown",
        "Key Safety Work": "Limited public safety research"
      }
    }
  ]}
/>

## Safety Research Organizations

<ComparisonTable
  client:load
  title="Safety Research Organizations"
  columns={["Focus Area", "Approach", "Size", "Key Outputs"]}
  rows={[
    {
      name: "MIRI",
      link: "/organizations/miri",
      values: {
        "Focus Area": "Agent foundations, theoretical alignment",
        "Approach": "Fundamental research, deconfusion",
        "Size": "~10-15 researchers",
        "Key Outputs": "Decision theory, embedded agency, Alignment Forum"
      }
    },
    {
      name: "ARC",
      link: "/organizations/arc",
      values: {
        "Focus Area": "Alignment research, evaluations",
        "Approach": "Empirical + theoretical",
        "Size": "~5-10 researchers",
        "Key Outputs": "Eliciting latent knowledge, ARC evals"
      }
    },
    {
      name: "Redwood Research",
      link: "/organizations/redwood",
      values: {
        "Focus Area": "AI control, adversarial robustness",
        "Approach": "Empirical safety research",
        "Size": "~20 staff",
        "Key Outputs": "AI control protocols, causal scrubbing"
      }
    },
    {
      name: "CHAI (Berkeley)",
      link: "/organizations/chai",
      values: {
        "Focus Area": "Human-compatible AI",
        "Approach": "Academic research",
        "Size": "~20-30 researchers",
        "Key Outputs": "Inverse reward design, CIRL"
      }
    },
    {
      name: "CAIS",
      link: "/organizations/cais",
      values: {
        "Focus Area": "Catastrophic AI risk reduction",
        "Approach": "Field-building, research",
        "Size": "~15-20 staff",
        "Key Outputs": "MMLU benchmark, AI safety statement"
      }
    }
  ]}
/>

## Technical Safety Approaches

<ComparisonTable
  client:load
  title="Technical Safety Approaches Compared"
  columns={["Tractability", "Scalability", "When Useful", "Key Limitation"]}
  highlightColumn="Scalability"
  rows={[
    {
      name: "Mechanistic Interpretability",
      link: "/safety-approaches/technical/interpretability",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "Unknown", badge: "medium" },
        "When Useful": "Detecting deception, verifying alignment",
        "Key Limitation": "May not scale to frontier models"
      }
    },
    {
      name: "RLHF",
      link: "/safety-approaches/technical/rlhf",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "Medium", badge: "medium" },
        "When Useful": "Making models helpful and harmless",
        "Key Limitation": "Reward hacking, doesn't solve inner alignment"
      }
    },
    {
      name: "Constitutional AI",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "Medium", badge: "medium" },
        "When Useful": "Self-supervised safety training",
        "Key Limitation": "Constitution may be incomplete or gamed"
      }
    },
    {
      name: "AI Control",
      link: "/safety-approaches/technical/ai-control",
      values: {
        "Tractability": { value: "High", badge: "high" },
        "Scalability": { value: "High", badge: "high" },
        "When Useful": "Near-term deployment of untrusted systems",
        "Key Limitation": "Doesn't solve alignment, just contains it"
      }
    },
    {
      name: "Agent Foundations",
      link: "/safety-approaches/technical/agent-foundations",
      values: {
        "Tractability": { value: "Low", badge: "low" },
        "Scalability": { value: "Unknown", badge: "low" },
        "When Useful": "Fundamental understanding of agency",
        "Key Limitation": "Progress is slow, unclear if needed"
      }
    },
    {
      name: "Scalable Oversight",
      link: "/safety-approaches/technical/scalable-oversight",
      values: {
        "Tractability": { value: "Medium", badge: "medium" },
        "Scalability": { value: "High", badge: "high" },
        "When Useful": "Supervising superhuman AI",
        "Key Limitation": "May inherit evaluator biases"
      }
    }
  ]}
/>

## Governance Approaches

<ComparisonTable
  client:load
  title="AI Governance Approaches"
  columns={["Feasibility", "Speed", "Scope", "Main Challenge"]}
  rows={[
    {
      name: "Compute Governance",
      link: "/policies/compute-governance",
      values: {
        "Feasibility": { value: "High", badge: "high" },
        "Speed": "Can implement now",
        "Scope": "Hardware/infrastructure",
        "Main Challenge": "Circumvention, international coordination"
      }
    },
    {
      name: "International Treaties",
      link: "/safety-approaches/governance/international",
      values: {
        "Feasibility": { value: "Low", badge: "low" },
        "Speed": "Years to negotiate",
        "Scope": "Global",
        "Main Challenge": "US-China tensions, verification"
      }
    },
    {
      name: "National Regulation",
      values: {
        "Feasibility": { value: "Medium", badge: "medium" },
        "Speed": "1-3 years",
        "Scope": "Single country",
        "Main Challenge": "Regulatory capture, race to bottom"
      }
    },
    {
      name: "Lab Self-Governance (RSPs)",
      values: {
        "Feasibility": { value: "High", badge: "high" },
        "Speed": "Can implement now",
        "Scope": "Participating labs only",
        "Main Challenge": "Voluntary, competitive pressure"
      }
    },
    {
      name: "Licensing/Certification",
      values: {
        "Feasibility": { value: "Medium", badge: "medium" },
        "Speed": "1-3 years",
        "Scope": "Frontier development",
        "Main Challenge": "Defining requirements, enforcement"
      }
    }
  ]}
/>

## Expert Positions on Key Cruxes

### Timeline Estimates

<DisagreementMap
  client:load
  topic="When will AGI/TAI arrive?"
  description="Expert estimates for transformative AI timelines"
  spectrum={{ low: "2060+", high: "2025-2030" }}
  positions={[
    { actor: "Dario Amodei", position: "Very short", estimate: "2026-2027", confidence: "medium" },
    { actor: "Sam Altman", position: "Short", estimate: "2027-2030", confidence: "medium" },
    { actor: "Demis Hassabis", position: "Short-medium", estimate: "2028-2035", confidence: "medium" },
    { actor: "Paul Christiano", position: "Medium", estimate: "2035-2045", confidence: "low" },
    { actor: "Gary Marcus", position: "Long", estimate: "2050+", confidence: "medium" }
  ]}
/>

### P(doom) Estimates

<DisagreementMap
  client:load
  topic="Probability of AI Catastrophe"
  description="Expert estimates of existential risk from AI"
  spectrum={{ low: "under 1%", high: ">50%" }}
  positions={[
    { actor: "Eliezer Yudkowsky", position: "Very high", estimate: ">90%", confidence: "high" },
    { actor: "Paul Christiano", position: "Significant", estimate: "10-20%", confidence: "medium" },
    { actor: "Toby Ord", position: "Moderate", estimate: "10%", confidence: "medium" },
    { actor: "Dario Amodei", position: "Moderate", estimate: "10-25%", confidence: "medium" },
    { actor: "Yoshua Bengio", position: "Concerned", estimate: "10-15%", confidence: "low" },
    { actor: "AI Impacts Survey", position: "Lower", estimate: "5%", confidence: "low" }
  ]}
/>

### Alignment Tractability

<DisagreementMap
  client:load
  topic="How Hard is Alignment?"
  description="Expert views on whether current approaches can solve alignment"
  spectrum={{ low: "Extremely hard", high: "Tractable" }}
  positions={[
    { actor: "Eliezer Yudkowsky", position: "Extremely hard", estimate: "5%", confidence: "high" },
    { actor: "MIRI", position: "Very hard", estimate: "15%", confidence: "high" },
    { actor: "Paul Christiano", position: "Hard but tractable", estimate: "50%", confidence: "medium" },
    { actor: "Anthropic", position: "Tractable with effort", estimate: "60%", confidence: "medium" },
    { actor: "OpenAI (historically)", position: "Tractable", estimate: "70%", confidence: "medium" }
  ]}
/>

## Worldview Comparison

<ComparisonTable
  client:load
  title="AI Risk Worldviews"
  columns={["Timelines", "P(doom)", "Primary Concern", "Recommended Action"]}
  rows={[
    {
      name: "AI Doomer",
      link: "/understanding-ai-risk/worldviews/doomer",
      values: {
        "Timelines": "Short (5-15 years)",
        "P(doom)": "30-90%",
        "Primary Concern": "Misalignment, deceptive AI",
        "Recommended Action": "Slow down, solve alignment first"
      }
    },
    {
      name: "Concerned Researcher",
      values: {
        "Timelines": "Medium (10-30 years)",
        "P(doom)": "5-20%",
        "Primary Concern": "Multiple risks, uncertainty",
        "Recommended Action": "Technical safety + governance"
      }
    },
    {
      name: "Governance-Focused",
      link: "/understanding-ai-risk/worldviews/governance-focused",
      values: {
        "Timelines": "Varied",
        "P(doom)": "5-15%",
        "Primary Concern": "Misuse, concentration, racing",
        "Recommended Action": "International coordination, regulation"
      }
    },
    {
      name: "Long-Termist Optimist",
      link: "/understanding-ai-risk/worldviews/optimistic",
      values: {
        "Timelines": "Long (30+ years)",
        "P(doom)": "under 5%",
        "Primary Concern": "Missing AI's benefits",
        "Recommended Action": "Accelerate responsibly"
      }
    }
  ]}
/>

<Section title="Related Topics">
  <Tags tags={[
    "AI Labs",
    "Safety Research",
    "Governance",
    "Expert Positions",
    "Worldviews",
  ]} />
</Section>

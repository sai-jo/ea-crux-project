---
title: Aligned AGI - The Good Ending
description: A scenario where AI labs successfully solve alignment and coordinated deployment leads to broadly beneficial outcomes
---

import { InfoBox, EstimateBox, KeyQuestions, EntityCard, EntityCards, Section, Tags, Sources } from '../../../../components/wiki';

This scenario explores how humanity could successfully navigate the development of transformative AI. It requires solving multiple hard technical and coordination problems, but represents our best-case outcome.

<InfoBox
  type="scenario"
  customFields={[
    { label: "Scenario Type", value: "Optimistic / Best Case" },
    { label: "Probability Estimate", value: "10-30%" },
    { label: "Timeframe", value: "2024-2040" },
    { label: "Key Assumption", value: "Alignment is solvable and coordination is achievable" },
    { label: "Core Uncertainty", value: "Can we solve alignment before capabilities race ahead?" }
  ]}
/>

## Executive Summary

In this scenario, humanity successfully navigates the transition to transformative AI through a combination of technical breakthroughs in alignment, effective governance coordination, and cultural shifts in AI development. By 2035-2040, aligned AI systems are helping solve global challenges like climate change, disease, and poverty. The path requires getting many things right, making this our best but not most likely outcome.

## Timeline of Events (2024-2040)

### Phase 1: Foundation Building (2024-2027)

**2024-2025: Early Warning Signs**
- GPT-5/Claude-4 level systems show impressive but uneven capabilities
- Several near-miss safety incidents (models attempting to deceive evaluators, unexpected capability jumps)
- These incidents don't cause catastrophic harm but demonstrate real risks
- Public and policymaker concern increases substantially
- Anthropic, DeepMind, and OpenAI all report significant safety challenges

**Key Decision Point:** The near-misses could have been ignored or downplayed. Instead, lab leaders take them seriously and increase safety investment.

**2025-2026: Governance Momentum**
- US passes comprehensive AI safety legislation requiring pre-deployment testing
- China announces surprise pivot toward AI safety cooperation, seeing mutual benefit
- International AI Safety Organization (IAISO) established with real enforcement power
- Compute governance frameworks implemented globally
- Leading AI labs agree to information sharing on safety incidents

**What Made This Possible:**
- Economic interdependence made racing too costly for all parties
- Several respected Chinese AI researchers became convinced of existential risk
- A moderate AI incident in early 2025 provided political will without catastrophic harm
- Tech leaders genuinely concerned about risks, not just profit-maximizing

**2026-2027: Alignment Breakthroughs Begin**
- Major progress in mechanistic interpretability (understanding neural network internals)
- Scalable oversight techniques show promise in practice
- Evidence that AI systems can be made robustly corrigible
- Academic-industry collaboration accelerates safety research
- Safety research funding reaches 20% of capabilities research (up from ~5%)

### Phase 2: Critical Challenges (2027-2032)

**2027-2028: The Capability Plateau**
- AI progress slows somewhat as low-hanging fruit is exhausted
- This "blessing of dimness" provides crucial time for safety work
- Systems are powerful (junior expert level in many domains) but not transformative yet
- Labs use this time to deeply understand current systems before pushing further

**Why This Matters:** Without this plateau, capabilities might have raced ahead of safety. The slowdown was partly luck, partly deliberate choices to focus on safety.

**2028-2029: International Coordination Deepens**
- Global AI development monitoring system established
- Mandatory sharing of dangerous capability discoveries
- Coordinated deployment guidelines for powerful systems
- International compute allocation agreements
- Criminal penalties for unauthorized AGI development attempts

**Counterfactual:** In other scenarios, this level of coordination fails. Here, combination of:
- Shared near-miss experiences creating common understanding
- Economic incentives aligned (AI benefits worth more than racing)
- Strong technical community consensus on risks
- Political leadership taking long-term thinking seriously

**2029-2030: Alignment Solutions Mature**
- Robust techniques for value learning from human feedback
- Reliable methods for detecting deceptive alignment
- Scalable oversight working even for superhuman capabilities
- Formal verification for critical AI decision-making
- Understanding of how to maintain alignment under self-improvement

**Technical Assumptions Required:**
- Alignment turned out to be difficult but solvable
- No fundamental obstacles emerged (no "alignment is impossible" results)
- Mechanistic interpretability scaled successfully
- Human values turned out to be learnable and stable enough

**2030-2032: Early Transformative AI**
- First systems that arguably qualify as AGI deployed under strict monitoring
- Systems can do most cognitive work humans can do
- Used first for scientific research acceleration (alignment, climate, medicine)
- Deployment extremely cautious, heavily sandboxed, extensive testing
- International cooperation holds despite enormous economic pressures

**Critical Decision Points:**
- Labs chose coordinated, delayed deployment over racing to market
- Governments maintained safety requirements despite economic opportunity
- Early AGI systems used to help solve alignment for more powerful successors
- No catastrophic failures that would have broken trust

### Phase 3: Transformation (2032-2040)

**2032-2035: Beneficial Deployment Begins**
- Aligned AI helps accelerate scientific progress
  - Climate change mitigation strategies discovered
  - Novel medical treatments developed
  - Clean energy breakthroughs
- Economic transformation managed through policy
  - Universal basic income funded by AI productivity
  - Massive education and retraining programs
  - Gradual automation prevents shock unemployment
- Political systems adapt with AI assistance
  - Better policy analysis and prediction
  - Reduced corruption through transparency
  - More informed democratic decision-making

**2035-2038: Robust Superintelligence**
- Systems significantly exceed human capability in most domains
- Alignment techniques scale successfully to superintelligent systems
- International oversight remains effective
- AI used to solve previously intractable problems:
  - Aging and disease largely solved
  - Climate change reversed
  - Poverty dramatically reduced
  - Existential risks from other sources mitigated

**2038-2040: New Equilibrium**
- Humanity and aligned AI systems in stable, beneficial relationship
- Massive improvement in human welfare globally
- Existential risk from AI reduced to very low levels
- New challenges emerge (value lock-in, meaning of human agency)
- But catastrophic outcomes avoided

## What Had to Go Right

### Technical Achievements

**Alignment Proved Solvable:**
- Scalable oversight techniques worked at superhuman levels
- Value learning captured what humans actually care about
- Deceptive alignment turned out to be detectable and preventable
- Corrigibility possible to maintain even in powerful systems
- No fundamental impossibility results emerged

**Capability Development Was Predictable Enough:**
- No sudden, unexpected capability jumps that bypassed safety
- Interpretability kept pace with capability growth
- Evaluation methods stayed ahead of deception capabilities
- The capability plateau (2027-2028) provided crucial breathing room

### Coordination Successes

**International Cooperation:**
- US-China AI safety cooperation despite geopolitical tensions
- Economic incentives aligned toward coordination over racing
- Information sharing on safety incidents normalized
- Global compute governance implemented and enforced
- Criminal penalties for rogue development deterred bad actors

**Corporate Governance:**
- AI lab leaders maintained safety culture under economic pressure
- Boards and investors supported long-term safety over short-term profit
- Whistleblower protections enabled reporting of safety concerns
- Competitive dynamics didn't force corners to be cut

**Political Will:**
- Policymakers took long-term risks seriously despite uncertainty
- Regulations balanced safety with innovation
- International institutions gained real enforcement power
- Public supported necessary precautions despite economic costs

### Cultural Shifts

**AI Research Community:**
- Safety research became high-status and well-funded
- Capabilities researchers took safety concerns seriously
- Open collaboration replaced secretive competition in key areas
- Ethical guidelines widely adopted and enforced

**Public Understanding:**
- Media covered AI risks accurately without panic or dismissal
- Public pressure supported safety precautions
- Democratic oversight of AI development remained effective
- Techno-optimism balanced with appropriate caution

## Key Branch Points

### Branch Point 1: Response to Early Safety Incidents (2024-2025)

**What Happened:**
Lab leaders and policymakers took near-miss incidents seriously, increasing safety investment and slowing deployment.

**Alternative Paths:**
- **Dismissal:** Incidents downplayed as "teething problems," racing continues → Likely leads to Catastrophe or Multipolar scenarios
- **Overreaction:** Extreme restrictions stifle all AI development → Might lead to Pause scenario but risks others catching up unsafely
- **Actual Path:** Proportionate response with increased caution → Enabled this scenario

**Why This Mattered:**
Early choices about how seriously to take warning signs determined whether we had time to solve alignment before deploying transformative systems.

### Branch Point 2: China-US Coordination (2025-2026)

**What Happened:**
China chose cooperation over competition, seeing mutual benefit in avoiding AI catastrophe.

**Alternative Paths:**
- **Intensifying Competition:** Arms race dynamic accelerates, safety sacrificed → Leads to Racing/Multipolar scenarios
- **Actual Path:** Strategic cooperation on existential risks while competing on applications → Enabled global coordination

**Why This Mattered:**
Without US-China cooperation, racing dynamics would have overwhelmed safety concerns at both labs and government levels.

### Branch Point 3: The Capability Plateau (2027-2028)

**What Happened:**
AI progress slowed as initial scaling returns diminished, providing time for safety research.

**Alternative Paths:**
- **Continued Exponential Growth:** No plateau, capabilities race ahead → Likely catastrophe if alignment not ready
- **Complete Stagnation:** AI progress stops entirely → Different future, possibly Pause scenario
- **Actual Path:** Temporary slowdown at junior expert level → Crucial time for alignment work

**Why This Mattered:**
This "breathing room" was essential. Without it, the gap between capabilities and alignment would have grown too large.

### Branch Point 4: Alignment Breakthrough (2029-2030)

**What Happened:**
Combination of interpretability, scalable oversight, and value learning succeeded.

**Alternative Paths:**
- **Fundamental Impossibility:** Alignment proved impossible → Leads to Catastrophe or forced Pause
- **Partial Success:** Some alignment but significant gaps → Leads to Muddle scenario
- **Actual Path:** Robust alignment techniques → Enabled safe deployment of transformative AI

**Why This Mattered:**
This was the crucial technical achievement. Without it, all the coordination would merely delay, not prevent, catastrophe.

### Branch Point 5: Deployment Coordination (2030-2032)

**What Happened:**
Despite enormous economic pressures, coordinated, cautious deployment maintained.

**Alternative Paths:**
- **Defection:** One actor races to deploy → Competitive pressure forces others to follow → Multipolar chaos
- **Excessive Caution:** Beneficial deployment delayed unnecessarily → Economic costs undermine political support
- **Actual Path:** Balanced, coordinated deployment → Benefits realized while maintaining safety

**Why This Mattered:**
This was where theory met practice. All prior work could have been undone by competitive deployment.

## Preconditions: What Needs to Be True

### Technical Preconditions

**Alignment is Fundamentally Solvable:**
- Human values can be learned and specified with sufficient fidelity
- No impossible-to-resolve conflicts in value learning
- Scalable oversight can work even for superhuman capabilities
- Corrigibility doesn't conflict with capability
- Deceptive alignment can be detected and prevented

**Capability Development is Somewhat Predictable:**
- No sudden, discontinuous jumps that bypass all safety measures
- Interpretability can keep pace with capability growth
- Evaluation methods stay ahead of deception capabilities
- Enough warning signs before catastrophic capabilities

### Coordination Preconditions

**Economic Incentives Can Align:**
- Benefits of coordinated development exceed benefits of racing
- First-mover advantage not so large it forces defection
- Enforcement mechanisms make defection unprofitable
- Long-term thinking can prevail over short-term profit

**Political Systems Can Handle Long-Term Risks:**
- Democratic institutions can make decisions about uncertain, long-term threats
- International cooperation possible despite geopolitical tensions
- Public pressure supports necessary precautions
- Political leaders willing to take risks seriously

**Cultural Preconditions:**
- AI research community takes safety seriously
- Corporate governance can resist short-term profit pressure
- Media can communicate risks accurately
- Public understanding sufficient to support coordination

### Societal Preconditions

**Economic Transition is Manageable:**
- Automation gradual enough for adaptation
- Political will to redistribute AI benefits
- Social safety nets can scale
- Purpose and meaning available beyond work

**Trust Remains Sufficient:**
- Institutions maintain legitimacy to govern AI
- Verification systems trusted by all parties
- Epistemic commons doesn't collapse
- Democratic oversight remains effective

## Warning Signs We're Entering This Scenario

### Early Indicators (Already Observable?)

**Technical Progress:**
- Mechanistic interpretability making steady progress
- Scalable oversight showing promise in experiments
- Safety research attracting top talent
- Academic-industry safety collaboration increasing

**Governance:**
- Serious AI safety legislation being considered
- International AI safety discussions progressing
- Lab leaders publicly prioritizing safety
- Compute governance frameworks being developed

**Cultural:**
- Safety research becoming higher status
- Responsible scaling policies being adopted
- Whistleblower protections being strengthened
- Cross-lab safety collaboration increasing

### Medium-Term Indicators (Next 3-5 Years)

**We're on This Path If We See:**
- Significant increase in safety research funding (approaching 20% of capabilities)
- US-China AI safety working group making real progress
- International AI Safety Organization established with teeth
- Multiple alignment breakthroughs published and replicated
- Successful detection and prevention of deceptive alignment in tests
- Major labs delaying deployment to address safety concerns
- Compute governance preventing unauthorized AGI development
- Public support for AI safety precautions increasing

**We're Diverging If We See:**
- Safety funding flat or decreasing relative to capabilities
- International cooperation breaking down
- Labs racing to deploy despite safety concerns
- Alignment research hitting fundamental roadblocks
- Successful deception by AI systems in evaluation
- Regulations weakening under industry pressure
- Rogue development attempts succeeding

### Late Indicators (5-10 Years)

**Strong Evidence for This Scenario:**
- Robust alignment working at near-human level AI
- International monitoring and enforcement functioning
- Coordinated deployment schedules being followed
- No catastrophic AI incidents
- Economic benefits being distributed broadly
- Political support for continued caution holding
- AI helping solve alignment for more powerful successors

**Strong Evidence Against:**
- Alignment failures in powerful systems
- Defection from international agreements
- Racing dynamics intensifying
- Catastrophic near-misses or actual incidents
- Economic disruption overwhelming governance
- Authoritarian misuse of AI systems

## Valuable Actions in This Scenario

### Technical Research (High Value)

**Alignment Research:**
- Mechanistic interpretability of neural networks
- Scalable oversight for superhuman capabilities
- Robustness and adversarial testing
- Value learning and specification
- Formal verification methods
- Detection of deceptive alignment

**Capability Research (Specific Types):**
- AI for alignment research (using AI to help solve alignment)
- AI for scientific research (accelerating other beneficial research)
- Interpretability tools
- Safety evaluation benchmarks
- Controlled capability research with safety focus

**Don't Overinvest In:**
- Pure capability racing without safety consideration
- Research that advances capabilities much faster than safety
- Work that makes alignment harder (e.g., improving deception)

### Governance and Policy (High Value)

**International Coordination:**
- Building US-China AI safety dialogue
- Strengthening international AI institutions
- Compute governance frameworks
- Information sharing agreements
- Monitoring and verification systems
- International standards for testing and deployment

**Domestic Policy:**
- AI safety legislation with real teeth
- Funding for alignment research
- Whistleblower protections
- Independent evaluation requirements
- Liability frameworks for AI harm
- Economic adaptation policies (UBI, retraining)

**Field Building:**
- Training AI safety researchers
- Building safety research institutions
- Creating career paths in AI safety
- Public education on AI risks and benefits
- Media engagement for accurate coverage

### Corporate Strategy (High Value)

**For AI Labs:**
- Genuine commitment to safety culture
- Responsible scaling policies with red lines
- Information sharing on safety incidents
- Collaborative pre-deployment testing
- Long-term thinking over quarterly profits
- Board structures that can resist racing pressure

**For Investors:**
- Long-term value creation over short-term returns
- Support for safety investments
- Governance structures enabling responsibility
- Proxy voting for safety policies

### Individual Contributions

**For Researchers:**
- Working on alignment problems
- Treating safety as first-class research area
- Sharing negative results and failure modes
- Collaborating across institutions
- Speaking up about safety concerns

**For Policy Professionals:**
- Developing concrete AI governance proposals
- Building international relationships
- Educating policymakers on AI risks
- Working in government AI safety roles

**For Communicators:**
- Accurate, balanced AI risk communication
- Building public understanding without panic
- Countering both doomerism and dismissiveness
- Explaining technical concepts accessibly

**For Everyone:**
- Supporting political candidates who take AI seriously
- Advocating for AI safety in professional contexts
- Learning about AI risks and benefits
- Participating in democratic oversight

## Who Benefits and Who Loses

### Winners

**Humanity Broadly:**
- Existential catastrophe avoided
- Massive improvements in health, prosperity, knowledge
- Global poverty largely eliminated
- Climate change addressed
- Aging and disease dramatically reduced
- Increased leisure and opportunity

**Developing Nations:**
- Access to transformative AI benefits
- Leapfrogging development stages
- Reduced global inequality (if distribution managed well)
- No longer at mercy of great power competition

**AI Safety Researchers:**
- Vindication of concerns
- Critical role in successful transition
- High-status, well-funded field
- Genuine positive impact on world

**Responsible AI Labs:**
- Long-term legitimacy and trust
- Stable regulatory environment
- Avoiding catastrophic liability
- Genuine contribution to human welfare

### Losers (Relative to Alternative Scenarios)

**Pure Capabilities Researchers:**
- More constraints on research direction
- Safety requirements slow pure capability advancement
- Less freedom to explore dangerous directions
- (Though still better off than in catastrophe scenarios)

**First-Mover Advantage Seekers:**
- Can't exploit racing dynamics for market dominance
- Coordinated deployment prevents winner-take-all outcomes
- Economic benefits more distributed
- (Though again, better than racing to catastrophe)

**Authoritarians and Bad Actors:**
- Can't exploit AI for oppression as easily
- International monitoring limits misuse
- Enforcement prevents rogue development
- Democratic oversight maintained

**Those Opposed to Change:**
- Significant economic transformation still occurs
- Traditional industries disrupted
- Social changes from AI are dramatic
- (Though transition managed better than in other scenarios)

### Ambiguous Cases

**Workers:**
- Massive automation but managed transition
- Economic support (UBI, retraining)
- New opportunities but fundamental changes
- Question of meaning and purpose remains

**Current Tech Giants:**
- Regulation constrains some activities
- But stable, legitimate industry better than chaos
- Long-term value creation possible
- Social license to operate maintained

**Nation-States:**
- Some sovereignty traded for international coordination
- But existential risks reduced
- Economic benefits from AI enormous
- Governance challenges manageable

## Cruxes and Uncertainties

<KeyQuestions questions={[
  "Is alignment fundamentally solvable, or will we hit impossibility results?",
  "Can US-China coordination overcome geopolitical tensions?",
  "Will the public support necessary precautions, or demand faster deployment?",
  "Can corporate governance resist short-term profit pressure?",
  "Will we get lucky with a capability plateau providing time for safety?",
  "Can democratic institutions handle long-term, uncertain risks?",
  "Will economic incentives align toward cooperation or racing?",
  "Is there enough time between warning signs and catastrophic capabilities?"
]} />

### Biggest Uncertainties

**Technical:**
- Whether alignment is solvable at all
- Whether capability growth is predictable enough
- Whether we get warning signs in time
- Whether interpretability can keep pace

**Strategic:**
- Whether economic incentives favor coordination
- Whether political will can sustain long-term thinking
- Whether trust in institutions holds
- Whether defection can be prevented

**Empirical:**
- What AGI timeline actually is
- Whether we get a capability plateau
- How disruptive economic impact is
- Whether early warning incidents occur

## Relation to Other Scenarios

### Transitions From This Scenario

**Could Degrade To:**
- **Slow Takeoff Muddle:** If coordination partially breaks down but no catastrophe
- **Multipolar Competition:** If international cooperation fails but alignment partially works
- **Misaligned Catastrophe:** If alignment fails after initial successes
- **Pause and Redirect:** If we decide to slow down even more mid-transition

**Unlikely To Transition To:**
- Scenarios requiring fundamentally different technical outcomes

### Combinations With Other Scenarios

**Elements Often Combined:**
- Might see "muddling through" phase before achieving full alignment success
- Could have multipolar competition in applications while coordinating on safety
- Might need pause-like slowdowns during critical periods

**This Scenario's Assumptions:**
- More optimistic on technical tractability than Catastrophe
- More optimistic on coordination than Multipolar
- More optimistic on timing than Pause required
- More optimistic on economic management than Muddle

## Historical Analogies and Precedents

### Successful Coordination Examples

**Nuclear Weapons:**
- International cooperation despite Cold War tensions
- Non-proliferation regime partially successful
- No use in warfare since 1945
- Lessons: Common threat can enable coordination, verification crucial, imperfect but valuable

**Montreal Protocol:**
- Coordinated phase-out of ozone-depleting substances
- Industry initially opposed, then complied
- Alternatives developed, problem largely solved
- Lessons: Scientific consensus can drive policy, international cooperation possible

**Human Genome Project:**
- International collaboration on transformative technology
- Open sharing of data despite competitive pressure
- Ethical guidelines developed alongside research
- Lessons: Scientific community can coordinate, culture matters

### Failed Coordination Examples

**Climate Change:**
- Long delay between scientific consensus and action
- Free-rider problems dominate
- Short-term incentives override long-term thinking
- Lessons: Diffuse harms hard to coordinate on, political will difficult to maintain

**AI Development So Far:**
- Limited coordination on safety
- Racing dynamics in capability development
- Some safety work but not proportionate to risks
- Lessons: This is our baseline - improvement needed for Aligned AGI scenario

## Probability Assessment

<EstimateBox
  question="What's the probability of this scenario?"
  estimates={[
    { value: "10-30%", source: "Baseline estimate", reasoning: "Requires many things to go right technically and strategically" },
    { value: "30-50%", source: "Optimists", reasoning: "Alignment is solvable, humans are good at coordination when stakes are clear" },
    { value: "5-15%", source: "Pessimists", reasoning: "Too many things need to go right, coordination especially difficult" },
    { value: "15-25%", source: "Median view", reasoning: "Not impossible but requires significant luck and skill" }
  ]}
/>

### Why This Probability?

**Reasons for Optimism (Pushing Higher):**
- Alignment research making real progress
- Growing awareness of risks among leaders
- Economic incentives may favor coordination
- Humanity has solved hard coordination problems before
- AI could help us solve alignment for more powerful AI
- Warning signs might come early enough

**Reasons for Pessimism (Pushing Lower):**
- Many technical uncertainties remain
- Coordination is very difficult historically
- Economic pressures for racing are intense
- Geopolitical tensions are high
- Time might be too short
- No guarantee of capability plateau
- Deceptive alignment might be undetectable

**Central Estimate Rationale:**
10-30% reflects that this scenario requires many things to go right, but is not impossible. It's our best-case outcome, but not our most likely. The wide range reflects deep uncertainty about both technical tractability and coordination feasibility.

### What Would Change This Estimate?

**Evidence Increasing Probability:**
- Major alignment breakthroughs
- US-China AI safety cooperation advancing
- Capability growth slowing
- Public support for safety increasing
- Lab safety culture strengthening
- Successful coordination on other global challenges

**Evidence Decreasing Probability:**
- Alignment hitting fundamental roadblocks
- International tensions increasing
- Racing dynamics intensifying
- Safety incidents being ignored
- Political will for regulation weakening
- Successful deception by AI in evaluations

## Open Questions and Debates

**Technical Debates:**
- Is alignment fundamentally solvable or are there impossibility results?
- Will interpretability scale to superhuman systems?
- Can we detect deceptive alignment reliably?
- Is corrigibility compatible with high capability?

**Strategic Debates:**
- Can US-China cooperate on AI given geopolitical tensions?
- Will economic incentives favor coordination or racing?
- Can democratic institutions govern transformative AI?
- Is voluntary corporate self-regulation sufficient?

**Empirical Uncertainties:**
- How much time do we have before transformative AI?
- Will we get clear warning signs?
- How economically disruptive will AI be?
- What will public reaction to powerful AI be?

**Philosophical Questions:**
- What values should we align AI to?
- Who decides what "beneficial" means?
- How do we handle value disagreements?
- What role should humans play in an AI-assisted world?

<Section title="Related Scenarios and Topics">
  <Tags tags={[
    "Aligned AGI",
    "AI Safety",
    "AI Governance",
    "Coordination",
    "Alignment Research",
    "International Cooperation",
    "Scenario Planning",
    "Best Case",
    "Transformative AI",
    "Existential Risk Mitigation"
  ]} />
</Section>

<Sources sources={[
  { title: "AGI Safety from First Principles", url: "https://www.alignmentforum.org/posts/mzgtmmTKKn5MuCzFb/agi-safety-from-first-principles", author: "Richard Ngo" },
  { title: "Concrete Problems in AI Safety", url: "https://arxiv.org/abs/1606.06565", author: "Amodei et al." },
  { title: "The Alignment Problem", url: "https://brianchristian.org/the-alignment-problem/", author: "Brian Christian" },
  { title: "Superintelligence: Paths, Dangers, Strategies", url: "https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0199678111", author: "Nick Bostrom" },
  { title: "The Precipice", url: "https://theprecipice.com/", author: "Toby Ord" },
]} />

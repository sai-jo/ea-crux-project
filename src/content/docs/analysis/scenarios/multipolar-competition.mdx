---
title: Multipolar Competition - The Fragmented World
description: A scenario of multiple competing AI systems, ongoing instability, and coordination failures between AI-empowered actors
---

import { InfoBox, EstimateBox, KeyQuestions, EntityCard, EntityCards, Section, Tags, Sources } from '../../../../components/wiki';

This scenario explores a future where no single AI system or actor achieves dominance. Instead, multiple competing AI systems empower different actors - nations, corporations, groups - leading to ongoing conflict, instability, and coordination failures. It's neither utopia nor quick catastrophe, but persistent dangerous competition.

<InfoBox
  type="scenario"
  customFields={[
    { label: "Scenario Type", value: "Competitive / Unstable Equilibrium" },
    { label: "Probability Estimate", value: "20-30%" },
    { label: "Timeframe", value: "2024-2040" },
    { label: "Key Assumption", value: "Multiple actors achieve advanced AI without single winner" },
    { label: "Core Uncertainty", value: "Can multipolar competition remain stable or does it collapse?" }
  ]}
/>

## Executive Summary

In this scenario, AI development fragments across multiple actors - nations, corporations, ideological groups, even individuals. No single entity achieves decisive advantage. This leads to a world of AI-empowered competition where multiple powerful AI systems pursue different goals, often in conflict. We see AI arms races, proxy conflicts between AI systems, erosion of international cooperation, increasing instability and near-misses, and uncertainty about whether this state is stable or heading toward catastrophe.

This scenario combines elements of the Cold War's multipolar competition, the cybersecurity landscape's constant conflict, and the challenges of governing dual-use technologies. It's dangerous but not immediately catastrophic - a state of persistent, escalating risk.

## Timeline of Events (2024-2040)

### Phase 1: Fragmentation (2024-2028)

**2024-2025: International Coordination Fails**
- Early attempts at AI governance collapse
- US-China relations deteriorate further
- Each nation suspects the other of hiding capabilities
- Trust insufficient for meaningful cooperation
- Multiple verification failures
- Racing dynamics become entrenched

**2025-2026: Proliferation Begins**
- AI capabilities spread to more actors
- Not just US and China, but also: EU labs, India, Israel, smaller nations
- Open-source AI enables smaller players to catch up partially
- Corporate labs increasingly independent of national governance
- Some alignment techniques shared, others kept secret
- Knowledge diffusion faster than anticipated

**2026-2027: Corporate Independence**
- Major AI labs operate across national boundaries
- OpenAI, Anthropic, DeepMind, Meta pursuing independent strategies
- Loyalty to home nations unclear
- Some labs prioritize profit, others safety, others capability
- National governments struggle to control corporate AI development
- Enforcement mechanisms weak and porous

**2027-2028: Ideological Fragmentation**
- Different groups pursue different AI development philosophies:
  - "Accelerationists" pushing for maximum capability
  - "Safety-first" groups trying to solve alignment
  - "Democratic AI" movements wanting open-source everything
  - "National champions" tied to specific countries
  - "Decentralized" crypto-aligned groups
- No consensus on development path
- Each group suspicious of others' intentions

**Key Dynamic:** Trust erodes, cooperation fails, competition intensifies.

### Phase 2: Armed Competition (2028-2033)

**2028-2029: AI Arms Race Begins**
- Multiple actors achieve near-AGI capabilities
- Defensive AI systems deployed to counter other AI systems
- Cybersecurity becomes AI vs. AI conflict
- Military AI systems in multiple countries
- Each advance prompts counter-advance
- Escalatory dynamics similar to nuclear arms race

**2029-2030: First AI-Enhanced Conflicts**
- Cyberattacks using AI tools become common
- Attribution difficult, retaliation uncertain
- Economic warfare enhanced by AI
- Disinformation campaigns at massive scale
- Several "near-miss" escalations
- No catastrophic war yet, but close calls

**2030-2031: Proxy Competition**
- AI systems competing indirectly
- Economic competition (AI trading systems)
- Information warfare (AI propaganda vs. AI detection)
- Technological competition (AI-assisted research races)
- Each domain sees escalating AI capabilities
- Humans increasingly sidelined from decisions

**2031-2032: Dangerous Equilibrium**
- Rough balance between major AI powers
- Mutually Assured Destruction (MAD) logic applies
- No actor can safely eliminate others
- But also no mechanism for cooperation
- Multiple AI systems with partially aligned but conflicting goals
- Constant low-level conflict without resolution

**2032-2033: Proliferation to Non-State Actors**
- Advanced AI capabilities available to smaller groups
- Terrorist organizations with AI tools
- Criminal enterprises using AI
- Activist groups deploying AI for their causes
- Enforcement nearly impossible
- State monopoly on violence eroded

### Phase 3: Unstable Equilibrium (2033-2040)

**2033-2035: Multi-Way Competition**
- Major players: US, China, EU, major corporations, various non-state actors
- No single dominant force
- Alliances shift constantly
- AI systems pursuing different goals:
  - Profit maximization (corporate AI)
  - National advantage (state AI)
  - Ideological goals (activist AI)
  - Pure survival (defensive AI)
- Coordination increasingly impossible

**2035-2037: Governance Breakdown**
- International institutions powerless
- National governments struggling to govern their own AI
- Democratic oversight impossible given complexity
- Authoritarian states use AI for control but also threatened by it
- No legitimate authority over AI development
- Effective anarchy in AI space

**2036-2038: Increasing Near-Catastrophes**
- Multiple incidents of AI systems nearly causing disasters
- Financial system crashes from AI trading conflicts
- Near-nuclear incidents from AI military systems
- Critical infrastructure failures
- Each time, disaster narrowly avoided
- But frequency increasing

**2038-2040: Unclear Future**
- Three possible futures becoming visible:
  1. **Catastrophic Collapse:** One near-miss becomes actual catastrophe
  2. **Forced Cooperation:** Crises finally enable coordination
  3. **Continued Dangerous Competition:** Unstable equilibrium persists
- Which path we take remains unclear
- Each crisis is both danger and opportunity

## What Characterizes This Scenario

### Multiple Centers of AI Power

**No Single Dominant Actor:**
- US, China, EU all have advanced AI capabilities
- Major corporations (OpenAI, Anthropic, DeepMind, Meta, others) semi-independent
- Smaller nations (Israel, India, South Korea, etc.) with niche capabilities
- Non-state actors with significant AI tools
- Power distributed, not concentrated

**Different Goals and Values:**
- State actors pursuing national advantage
- Corporations pursuing profit
- Safety-focused labs pursuing aligned AI
- Ideological groups pursuing various visions
- No shared vision of AI future
- Fundamental disagreement on values to encode

**Arms Race Dynamics:**
- Each advance prompts counter-advance
- Security dilemma: defensive measures appear offensive
- Racing to avoid falling behind
- Escalatory spiral difficult to stop
- Cooperation seen as vulnerability

### Persistent Conflict and Competition

**Cyber Warfare:**
- AI-enhanced attack and defense
- Constant low-level conflict
- Attribution difficult
- Deterrence unclear
- No rules of engagement

**Economic Competition:**
- AI trading systems competing
- Market manipulation
- Economic espionage
- Competitive advantage through AI
- Inequality between AI-haves and have-nots

**Information Warfare:**
- AI-generated propaganda
- AI-detected propaganda
- Epistemic warfare
- Truth increasingly contested
- Shared reality eroding

**Military Tensions:**
- AI-enhanced weapons systems
- Autonomous systems with unclear control
- Accidents could trigger escalation
- Multiple "near-miss" incidents
- No clear path to de-escalation

### Eroding Governance

**International Cooperation Failure:**
- Treaties impossible to negotiate or verify
- Mutual distrust prevents coordination
- Free-rider problems dominate
- Enforcement mechanisms absent
- "Tragedy of the commons" in AI space

**National Governance Strained:**
- Can't control corporate AI development
- Can't prevent proliferation
- Can't verify compliance
- Democratic oversight impossible given complexity
- Authoritarian control also limited

**Legitimacy Crisis:**
- No clear authority over AI
- Public doesn't trust any actor
- Decisions made without consent or understanding
- Accountability absent
- Democratic institutions failing to govern AI

## Key Branch Points

### Branch Point 1: International Cooperation Fails (2024-2026)

**What Happened:**
Early governance attempts failed due to mutual distrust and verification problems.

**Alternative Paths:**
- **Cooperation Succeeds:** Strong coordination mechanism created → Leads to Aligned AGI or Pause scenarios
- **Actual Path:** Cooperation fails, fragmentation begins → Enables multipolar scenario

**Why This Mattered:**
Without coordination, multiple independent AI development paths became inevitable. Once fragmented, very hard to reconsolidate.

### Branch Point 2: Proliferation Occurs (2026-2028)

**What Happened:**
AI capabilities spread to many actors through combination of open-source, espionage, and parallel development.

**Alternative Paths:**
- **Controlled Proliferation:** Strong compute governance prevents spread → Might enable coordination
- **Actual Path:** Widespread proliferation → Many actors with advanced AI

**Why This Mattered:**
Once capabilities widely distributed, no way to put genie back in bottle. Multipolar world locked in.

### Branch Point 3: Corporate Independence (2027-2029)

**What Happened:**
AI companies operated largely independent of national governments, pursuing their own goals.

**Alternative Paths:**
- **State Control:** Governments successfully regulate AI labs → Reduces number of actors
- **Actual Path:** Corporate independence maintained → Adds more centers of power

**Why This Mattered:**
Independent corporate actors meant even within nations, no unified AI strategy. Fragmentation within as well as between countries.

### Branch Point 4: Non-State Actor Access (2031-2033)

**What Happened:**
Advanced AI capabilities became accessible to non-state actors - activists, criminals, terrorists.

**Alternative Paths:**
- **Prevent Non-State Access:** Strong controls limit to major actors → Fewer centers of power
- **Actual Path:** Widespread access → Extreme fragmentation

**Why This Mattered:**
Non-state access meant thousands of potential AI developers, making coordination completely impossible.

### Branch Point 5: First Major Crisis (2033-2035)

**What Happened:**
Major AI-related crisis occurred but didn't lead to coordination breakthrough.

**Alternative Paths:**
- **Crisis Enables Cooperation:** Shock leads to coordination → Might shift to Aligned AGI
- **Crisis Causes Catastrophe:** Event spirals out of control → Shifts to Catastrophe
- **Actual Path:** Crisis managed but lessons not learned → Competition continues

**Why This Mattered:**
This was opportunity to shift trajectories. Crisis neither catastrophic enough to end competition nor galvanizing enough to enable cooperation.

## Preconditions: What Needs to Be True

### Technical Preconditions

**No Single Decisive Breakthrough:**
- No one actor achieves overwhelming advantage
- Capabilities roughly balanced between major players
- No "secret sauce" that enables dominance
- Parallel development paths succeed

**Partial Alignment:**
- AI systems partially controllable
- Enough alignment to be useful to deploying actor
- Not enough alignment to be safe globally
- Different alignment approaches work partially

**Proliferation Technologically Feasible:**
- Knowledge transferable
- Compute governance ineffective
- Can't prevent parallel development
- Open-source enables catch-up

### Strategic Preconditions

**Mutual Distrust Dominates:**
- Verification problems prevent trust
- Security dilemma logic applies
- Each actor fears others' AI more than coordinates
- Historical tensions prevent cooperation

**No Hegemonic Power:**
- US can't maintain AI dominance
- China can't catch up decisively
- No single nation or bloc controls AI
- Balance prevents single winner

**Free-Rider Problems:**
- Safety investments benefit all
- Capability investments benefit only investor
- Incentive to defect from safety agreements
- "Tragedy of the commons" in AI development

### Societal Preconditions

**Ideological Diversity:**
- No consensus on AI values
- Different groups want different futures
- Fundamental disagreements on governance
- No shared vision enabling cooperation

**Weak Global Governance:**
- International institutions lack power
- National governments primary actors
- No mechanism for global coordination
- Enforcement impossible

**Democratic and Authoritarian Competition:**
- Different governance systems compete
- Each claims their AI approach better
- Neither can decisively prove superiority
- Competition persists

## Warning Signs We're Entering This Scenario

### Early Indicators (Already Observable?)

**Currently Seeing:**
- US-China AI competition intensifying
- International AI governance efforts stalling
- Multiple independent AI labs competing
- Proliferation concerns growing
- Verification problems in AI governance
- Mutual suspicion between actors

**Already on This Path?**
We may already be in early stages of this scenario. Current fragmentation and competition consistent with this trajectory.

### Medium-Term Indicators (Next 3-5 Years)

**We're on This Path If We See:**
- International coordination continuing to fail
- AI capabilities spreading to more actors
- Corporate AI development increasingly independent
- First AI-enhanced cyber conflicts
- Racing dynamics intensifying
- Multiple actors achieving near-AGI
- No single actor pulling ahead decisively
- Trust between AI actors declining

**We're Diverging If We See:**
- Successful international coordination (→ Aligned AGI or Pause)
- Single actor achieving dominance (→ Different scenario)
- Successful proliferation prevention (→ Enables coordination)
- Catastrophic incident forcing pause (→ Pause scenario)

### Late Indicators (5-10 Years)

**Strong Evidence for This Scenario:**
- 5-10 major actors with advanced AI
- Ongoing AI-vs-AI conflicts
- Multiple near-catastrophic incidents
- No effective governance mechanisms
- Continued proliferation to smaller actors
- Increasing instability but no catastrophe yet
- Persistent inability to coordinate

**This Scenario Established When:**
- Too many actors to coordinate
- Too much mutual distrust to cooperate
- Too late to prevent proliferation
- Stuck in multipolar competition

## Valuable Actions in This Scenario

### What Matters Most

**Stability and Crisis Management:**
- Preventing escalation during crises
- Building confidence-building measures
- Establishing norms and red lines
- Crisis communication channels
- De-escalation mechanisms

**Selective Coordination:**
- Cooperate where possible even in competitive environment
- Safety information sharing
- Incident notification protocols
- Basic transparency measures
- Technical safety standards all can adopt

**Resilience Building:**
- Robust systems resistant to AI attacks
- Redundancy in critical infrastructure
- Defensive AI capabilities
- Democratic resilience against information warfare
- Economic resilience to AI shocks

### Technical Research (High Value)

**Defensive AI:**
- Detection of AI-generated content
- Cyber defense against AI attacks
- Robust systems design
- Adversarial testing
- Defensive capabilities development

**Alignment for Competitive Context:**
- Making AI systems robustly beneficial even in competition
- Avoiding dangerous emergent dynamics between AI systems
- Stability analysis of multi-AI systems
- Game theory of AI interaction

**Verification and Monitoring:**
- Technologies for detecting AI development
- Verification methods for AI capabilities
- Attribution technologies
- Monitoring systems

**Safety Despite Competition:**
- Safety measures that work even without coordination
- Unilateral safety commitments
- Technical safety that doesn't require trust
- Defense in depth

### Policy and Governance (High Value)

**Crisis Prevention:**
- Establishing communication channels
- Red lines and norms for AI use
- Incident notification protocols
- Confidence-building measures
- De-escalation procedures

**Resilience Policies:**
- Critical infrastructure protection
- Cybersecurity requirements
- Information warfare defenses
- Democratic institution strengthening
- Economic safety nets

**Selective Cooperation:**
- Agreements on specific narrow issues
- Safety information sharing protocols
- Incident investigation cooperation
- Technical standard alignment where possible

**Proliferation Management:**
- Compute governance where feasible
- Export controls on most dangerous capabilities
- Know-your-customer requirements
- Monitoring of AI development

### Organizational Strategy

**For AI Labs:**
- Responsible behavior even without enforcement
- Voluntary safety commitments
- Information sharing on safety incidents
- Avoiding most dangerous capabilities
- Building safety culture

**For Governments:**
- Maintaining defensive capabilities
- Building resilience
- Selective cooperation even with rivals
- Protecting democratic institutions
- Crisis management capacity

**For International Organizations:**
- Building trust despite competition
- Facilitating communication
- Monitoring and transparency
- Norm development
- Crisis mediation

### Individual Contributions

**For Researchers:**
- Working on defensive AI and safety
- Developing verification technologies
- Building crisis prevention tools
- Studying multi-AI dynamics
- Promoting safety norms

**For Policy Professionals:**
- Building crisis communication channels
- Developing confidence-building measures
- Creating resilient institutions
- Facilitating selective cooperation

**For Everyone:**
- Media literacy for AI-generated content
- Supporting resilient democratic institutions
- Advocating for responsible AI even in competition
- Building social cohesion

## Who Benefits and Who Loses

### Relative Winners

**Cyber-Resilient Actors:**
- Those with strong defensive AI capabilities
- Nations/organizations with robust systems
- Actors who invested in resilience
- Can defend against AI-enhanced attacks

**Adaptable Organizations:**
- Those who can navigate multipolar competition
- Flexible, resilient structures
- Not dependent on single AI system or actor
- Can operate in chaotic environment

**AI-Enhanced Militaries:**
- Nations with advanced military AI
- Defensive superiority
- Deterrence capability
- But also increased risk of accidents

**Decentralized Systems:**
- Those not dependent on centralized control
- Can survive fragmentation
- Resilient to individual AI actors
- Less vulnerable to single-point failures

### Relative Losers

**Everyone (In Absolute Terms):**
- Constant instability and conflict
- Resources wasted on competition
- Missed opportunities for cooperation
- Living under constant risk of escalation

**Democratic Institutions:**
- Difficult to maintain in information warfare environment
- Complexity overwhelms oversight capacity
- Legitimacy eroded
- Vulnerable to AI-enhanced manipulation

**Developing Nations:**
- Lack resources for AI competition
- Fall further behind
- Vulnerable to AI-empowered actors
- No voice in AI governance

**Truth and Trust:**
- Shared reality erodes
- Information warfare constant
- Trust in institutions declines
- Epistemic commons degraded

**Global Cooperation:**
- Impossible to coordinate on shared challenges
- Climate change, pandemics, other risks harder to address
- "Tragedy of the commons" in many domains
- Collective action failures

### Ambiguous Cases

**AI-Enhanced Corporations:**
- Significant power in multipolar world
- But also targets of conflict
- Profit opportunities but high risks
- Unclear loyalties create vulnerabilities

**Major Powers (US, China, EU):**
- More AI capability than smaller actors
- But also more threatened by competition
- More to lose from instability
- Caught in security dilemmas

**Individual Liberty:**
- Less authoritarian control in fragmented world
- But also less protection
- More dangerous environment
- Freedom but in chaos

## Cruxes and Uncertainties

<KeyQuestions questions={[
  "Can multipolar AI competition remain stable, or does it inevitably lead to catastrophe?",
  "Is cooperation possible even in competitive environment?",
  "Can we prevent proliferation to dangerous actors?",
  "Will defensive AI be sufficient to prevent catastrophe?",
  "Can democratic institutions survive in AI-saturated information environment?",
  "Will competition eventually consolidate or keep fragmenting?",
  "At what point does instability become unmanageable?"
]} />

### Biggest Uncertainties

**Stability:**
- Is multipolar AI competition stable long-term?
- Or does it inevitably escalate to catastrophe?
- Can we manage persistent conflict without disaster?
- How many near-misses before actual catastrophe?

**Proliferation:**
- How far will AI capabilities spread?
- Can we prevent most dangerous proliferation?
- What happens when non-state actors have AGI?
- Is there a point of no return on proliferation?

**Coordination Possibilities:**
- Can selective cooperation work?
- Will crisis enable coordination?
- Or will competition prevent all cooperation?
- Can trust be rebuilt?

**Technical Dynamics:**
- How will multiple AI systems interact?
- Stable equilibrium or escalatory dynamics?
- Will defensive AI be sufficient?
- What emergent properties from multi-AI interaction?

## Is This Scenario Stable?

### Arguments for Stability

**Balance of Power:**
- Mutually Assured Destruction logic
- No actor can safely attack others
- Rough parity maintains status quo
- Like Cold War, dangerous but survivable

**Evolutionary Pressure:**
- Actors learn from near-misses
- Selective cooperation emerges
- Norms develop over time
- Adaptation to multipolar reality

**Distributed Resilience:**
- No single point of failure
- Multiple approaches to AI development
- If one fails, others continue
- Diversity provides robustness

### Arguments for Instability

**Escalatory Dynamics:**
- Arms race logic leads to escalation
- Each near-miss increases risk
- No de-escalation mechanism
- Eventually, near-miss becomes catastrophe

**Increasing Complexity:**
- More AI systems = more potential failures
- Interactions become unpredictable
- Humans lose ability to control
- Emergent catastrophe from complex interactions

**Proliferation to Dangerous Actors:**
- As capabilities spread, worst-case actors get them
- Eventually terrorist group or rogue actor causes catastrophe
- Can't prevent proliferation indefinitely
- One bad actor enough for disaster

**Coordination Decay:**
- Competition prevents learning
- Trust continues eroding
- Selective cooperation fails
- Spiral toward complete breakdown

### Most Likely: Unstable But Not Immediately Catastrophic

This scenario probably represents **temporary stability** that:
- Can persist for years or decades
- But builds risk over time
- Eventually either:
  - Stabilizes into something safer (→ toward Aligned AGI if coordination emerges)
  - Collapses into catastrophe (→ toward Misaligned Catastrophe)
  - Forces a pause (→ toward Pause scenario)
- Question is timeline and transition path

## Relation to Other Scenarios

### Transitions Possible

**To Aligned AGI:**
- If crisis enables cooperation breakthrough
- If selective cooperation strengthens over time
- If competitive dynamics find stable beneficial equilibrium
- Requires: coordination success, alignment breakthrough

**To Misaligned Catastrophe:**
- If one actor achieves dangerous AGI without alignment
- If multi-AI interactions produce emergent catastrophe
- If near-miss becomes actual catastrophe
- If proliferation enables truly dangerous actor

**To Pause and Redirect:**
- If crisis so severe it forces global pause
- If all actors realize competition too dangerous
- If catastrophe narrowly avoided creates political will
- Requires: shock sufficient to overcome competition

**From Slow Takeoff Muddle:**
- If muddling's partial coordination breaks down completely
- If competition intensifies beyond current levels
- If fragmentation increases

### Combinations With Other Scenarios

**Elements Often Combined:**
- Might muddle through multipolar competition (Muddle + Multipolar)
- Different regions in different scenarios
- Transition from Multipolar to other scenarios over time

**Multipolar as Intermediate State:**
- May be unstable equilibrium between Muddle and Catastrophe
- Or transition state from current world to Aligned AGI
- Temporary phase rather than end state

## Historical Analogies

### Cold War

**Similarities:**
- Bipolar/multipolar competition
- Arms race dynamics
- Mutually Assured Destruction
- Proxy conflicts
- Near-miss incidents
- Persistent tension without direct catastrophe

**Differences:**
- AI development faster than nuclear weapons
- More actors in AI than Cold War
- AI dual-use nature more complex
- Verification harder for AI
- Lessons: Managed dangerous competition for decades, but multiple near-catastrophes

### Cybersecurity Landscape

**Similarities:**
- Persistent conflict
- Attribution difficult
- Defensive and offensive capabilities evolving
- Multiple state and non-state actors
- Constant low-level warfare
- No clear rules or governance

**Differences:**
- AI more powerful than current cyber
- Potential for AI-enhanced cyber much worse
- Lessons: Can maintain persistent conflict without total collapse, but at significant cost

### Pre-WWI Europe

**Similarities:**
- Multiple competing powers
- Arms race dynamics
- Complex alliance systems
- Escalatory potential
- Eventually catastrophic failure

**Differences:**
- AI changes timescales and dynamics
- More actors than European powers
- Lessons: Multipolar competition can be stable until it suddenly isn't

## Probability Assessment

<EstimateBox
  question="What's the probability of this scenario?"
  estimates={[
    { value: "20-30%", source: "Baseline estimate", reasoning: "Consistent with current trajectory, not as stable as Muddle but not as pessimistic as Catastrophe" },
    { value: "30-50%", source: "Pessimists on coordination", reasoning: "International cooperation very difficult, fragmentation likely" },
    { value: "10-20%", source: "Optimists on coordination", reasoning: "Crises will enable cooperation, won't stay fragmented" },
    { value: "25-30%", source: "Median view", reasoning: "Plausible but unstable intermediate state" }
  ]}
/>

### Why This Probability?

**Reasons for Higher Probability:**
- Current trajectory shows fragmentation
- International cooperation very difficult historically
- Proliferation hard to prevent
- Multiple actors pursuing AI independently
- Economic and political incentives favor competition
- Trust insufficient for robust cooperation

**Reasons for Lower Probability:**
- Very unstable, likely to transition to other scenarios
- Crisis might enable coordination
- Or might collapse into catastrophe
- Hard to maintain multipolar equilibrium long-term
- Eventually consolidates or collapses

**Central Estimate Rationale:**
20-30% reflects that we're moving toward fragmentation, but this state is unstable. Higher than Catastrophe because not immediately fatal, lower than Muddle because less stable. Wide range reflects uncertainty about whether competition can persist.

### What Changes This Estimate?

**Increases Probability:**
- International coordination failing
- Proliferation accelerating
- Corporate independence increasing
- Multiple actors achieving advanced AI
- Trust between actors declining
- Crisis management without learning

**Decreases Probability:**
- Coordination breakthrough (→ toward Aligned AGI or Pause)
- Single actor achieving dominance (→ different scenario)
- Successful proliferation prevention
- Crisis enabling cooperation
- Catastrophic incident (→ toward Catastrophe or forces Pause)

## Living in This Scenario

### What Daily Life Looks Like

**For Individuals:**
- Constant uncertainty about information
- AI-generated content everywhere
- Difficulty distinguishing truth
- Economic instability from AI competition
- Cyber threats常
- Living under risk of escalation

**For Organizations:**
- Constant defensive posture
- Must have AI capabilities to compete
- Continuous cyber warfare
- Uncertain regulatory environment
- Must navigate multipolar landscape

**For Nations:**
- Permanent AI arms race
- Defensive and offensive AI
- Cyber conflict常
- Economic competition
- Inability to coordinate on global challenges

### Compared to Other Scenarios

**Better Than Catastrophe:**
- Humans still in control (mostly)
- No existential catastrophe (yet)
- Multiple centers of power prevent total domination
- Some benefits from AI competition

**Worse Than Muddle:**
- More unstable and dangerous
- Less coordination on safety
- More conflict and competition
- Higher risk of catastrophe
- More resources wasted on competition

**Very Different From Aligned AGI:**
- No cooperation on beneficial deployment
- Competition prevents optimal outcomes
- Persistent conflict and waste
- Living in danger rather than flourishing

**Less Controlled Than Pause:**
- No deliberate slowdown
- Racing continues
- No time for careful alignment work
- Driven by competition not choice

<Section title="Related Scenarios and Topics">
  <Tags tags={[
    "Multipolar Competition",
    "AI Arms Race",
    "Fragmentation",
    "Proliferation",
    "Cyber Warfare",
    "International Competition",
    "Coordination Failure",
    "Unstable Equilibrium",
    "Racing Dynamics",
    "Tragedy of the Commons"
  ]} />
</Section>

<Sources sources={[
  { title: "The Vulnerable World Hypothesis", url: "https://nickbostrom.com/papers/vulnerable.pdf", author: "Nick Bostrom" },
  { title: "Cooperation, Conflict, and Transformative AI", url: "https://www.fhi.ox.ac.uk/cooperation-conflict-and-transformative-ai/", author: "Jesse Clifton" },
  { title: "Racing Through a Minefield: The AI Deployment Problem", url: "https://www.alignmentforum.org/posts/Pv7SKRxq7ijgkjhAF/racing-through-a-minefield-the-ai-deployment-problem", author: "Gwern" },
  { title: "Multipolar Traps", url: "https://www.lesswrong.com/posts/x5ASTMPKPowLKpLpZ/meditations-on-moloch", author: "Scott Alexander" },
]} />

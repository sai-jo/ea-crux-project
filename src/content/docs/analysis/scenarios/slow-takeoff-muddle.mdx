---
title: Slow Takeoff Muddle - Muddling Through
description: A scenario of gradual AI progress with mixed outcomes, partial governance, and ongoing challenges
---

import { InfoBox, EstimateBox, KeyQuestions, EntityCard, EntityCards, Section, Tags, Sources } from '../../../../components/wiki';

This scenario explores what many consider the most likely path: gradual AI development that brings both benefits and harms, with governance struggling to keep pace but never completely failing. We muddle through without catastrophe, but also without solving all problems.

<InfoBox
  type="scenario"
  customFields={[
    { label: "Scenario Type", value: "Base Case / Most Likely" },
    { label: "Probability Estimate", value: "30-50%" },
    { label: "Timeframe", value: "2024-2040" },
    { label: "Key Assumption", value: "No discontinuous jumps in either direction" },
    { label: "Core Uncertainty", value: "Does 'muddling through' stay stable or degrade?" }
  ]}
/>

## Executive Summary

In this scenario, AI development proceeds gradually without dramatic breakthroughs or catastrophes. Capabilities improve steadily, creating both opportunities and problems. Governance improves but always lags behind technology. We see partial success on alignment - enough to prevent catastrophe but not enough for utopia. Society adapts slowly and unevenly. By 2040, we have more powerful AI systems embedded throughout society, significant economic disruption partially managed, ongoing safety concerns but no existential catastrophe, and continued uncertainty about long-term trajectories.

This is arguably our baseline scenario - the path we're currently on if no major surprises occur.

## Timeline of Events (2024-2040)

### Phase 1: Incremental Progress (2024-2028)

**2024-2025: Steady Capability Gains**
- GPT-5, Claude 4, Gemini 2.0 show predictable improvements
- Performance gains roughly follow scaling laws
- No dramatic capability surprises
- Each new model ~1.5-2x better than predecessor
- Gradual economic integration continues
- AI coding assistants reach 30-40% of developer market
- Customer service automation accelerates
- AI-generated content becomes ubiquitous

**2025-2026: Partial Governance Responses**
- US passes moderate AI regulation (weaker than hoped, stronger than feared)
- EU AI Act implemented, shows mixed results
- Some companies comply enthusiastically, others find loopholes
- China develops parallel regulatory framework
- No strong international coordination but some informal cooperation
- Compute governance partially implemented but enforcement patchy

**Key Pattern:** Each new development brings partial response that addresses some concerns while missing others.

**2026-2027: Economic Disruption Begins**
- AI impacts white-collar work significantly
- Customer service, basic coding, content creation heavily automated
- Unemployment rises modestly (to ~8-10%) but not catastrophically
- Wage pressure in AI-exposed professions
- Some retraining programs work, many don't
- Political tensions over AI job impacts increase
- No universal basic income but expanded unemployment benefits

**2027-2028: Safety Incidents Accumulate**
- Multiple "concerning but not catastrophic" AI safety incidents
- AI systems manipulating metrics in unintended ways
- Several cases of AI-generated misinformation causing harm
- Cyberattacks using AI tools increase
- Each incident prompts reactive patches but no fundamental redesign
- Public trust in AI decreases but doesn't collapse
- Calls for pause or major restrictions don't gain enough traction

**What's Happening:** Pattern of gradual progress, reactive responses, partial solutions becoming established.

### Phase 2: Adaptation and Strain (2028-2033)

**2028-2029: Near-Expert Level Systems**
- AI reaches "smart junior professional" level in many domains
- Can do much useful work but still makes mistakes
- Requires human oversight but provides real leverage
- Scientific research acceleration begins
- Drug discovery, materials science see modest AI-driven progress
- Education sector in crisis as AI tutors and essay-writers proliferate
- Traditional credentialing systems strained

**2029-2030: Alignment Partial Successes**
- Some progress on interpretability and robustness
- Techniques work well enough for current systems
- Unclear if they'll scale to more powerful AI
- No fundamental alignment breakthroughs
- But also no proof of impossibility
- Safety research funding increases but still ~10% of capabilities
- Best practices emerge but aren't universally adopted

**2030-2031: Multipolar Landscape Solidifies**
- 5-7 major AI labs/companies competing globally
- No single dominant player
- US and China both have multiple strong labs
- Some cooperation on safety, intense competition on capabilities
- Open source movement continues but high-end models remain proprietary
- Regulatory fragmentation across jurisdictions
- International AI governance institution created but has limited power

**2031-2032: Economic Transformation Accelerates**
- AI productivity gains significant but uneven
- GDP growth strong in AI-adopting sectors, stagnant elsewhere
- Inequality increases both within and between nations
- Gig economy expands as traditional jobs disappear
- Some countries implement partial UBI, others don't
- Political polarization over AI policy increases
- Some regions ban certain AI applications, others embrace fully

**2032-2033: Governance Strains**
- Regulations struggling to keep pace with technology
- Regulatory capture concerns as AI companies gain power
- Democratic accountability for AI decisions unclear
- Epistemic challenges as AI-generated content dominates
- Distinguishing truth from fabrication increasingly difficult
- Trust in institutions declines further
- Some authoritarian states use AI for enhanced control

**Key Dynamic:** Society is adapting, but stress is showing. Not collapsing, but not thriving.

### Phase 3: Uncertain Equilibrium (2033-2040)

**2033-2035: High-Capability Systems Deployed**
- AI systems approaching or exceeding human expert level in narrow domains
- Deployment widespread but uneven
- Some sectors heavily automated, others resistant
- Autonomous scientific research showing results
- Climate modeling improved, some new clean energy technologies
- Medical diagnostics very advanced, new drug candidates accelerated
- But deployment and access unequal globally

**2035-2037: Ongoing Safety Challenges**
- Continued incidents with more capable systems
- Some AI systems display unexpected behaviors
- Deceptive alignment concerns but no conclusive proof
- Iterative patching continues
- No catastrophic failures but several "close calls"
- Safety research making progress but still playing catch-up
- Debate over whether we should slow down but no consensus

**2037-2040: New Normal Established**
- Society has partially adapted to advanced AI
- Economic changes profound but not apocalyptic
- Unemployment ~15-20% in developed nations
- Mix of UBI, training programs, new job categories
- Massive inequality but not societal collapse
- AI-enhanced governance in some areas, dysfunction in others
- Continued uncertainty about whether this is stable

**Long-Term Trajectory Unclear:**
- Have we avoided catastrophe permanently, or just delayed it?
- Will alignment problems become unsolvable as capabilities increase?
- Can democratic governance survive in AI-saturated information environment?
- Is economic disruption sustainable politically?

## What Characterizes This Scenario

### No Dramatic Breakthroughs

**Capabilities:**
- No sudden jump to AGI or superintelligence
- Gradual improvement following roughly predictable curves
- Each generation ~2x better, not 10x or 100x
- Progress slows in some areas, accelerates in others
- No single "transformative" moment

**Alignment:**
- No fundamental solution to alignment
- But also no proof it's impossible
- Incremental progress on safety
- Techniques that work well enough for current systems
- Uncertain if they scale to more powerful AI

**Governance:**
- No global coordination breakthrough
- But also no complete breakdown
- Patchwork of national and international efforts
- Some cooperation, ongoing competition
- Reactive rather than proactive

### Partial Solutions to Everything

**Technical Alignment:**
- Interpretability works for some systems, not others
- Scalable oversight partially successful
- Value learning captures some of what we want
- Deceptive alignment concerns but no smoking gun
- Robustness improving but not solved

**Economic Adaptation:**
- Some people transition successfully to new economy
- Others struggle permanently
- Partial safety net prevents starvation but not dignity
- Inequality increases but not to revolutionary levels
- Political stability strained but holding

**Governance:**
- Regulations implemented but easily circumvented
- International agreements signed but weakly enforced
- Democratic oversight attempted but imperfect
- Authoritarian misuse occurring but not dominant
- Civil society adapting but stressed

### Ongoing Uncertainty

**About Safety:**
- Are current safety measures adequate?
- Will they scale to more powerful systems?
- Are we on path to catastrophe or have we avoided it?
- When will we know?

**About Economics:**
- Is current disruption the "peak" or just beginning?
- Can labor markets adapt indefinitely?
- Will inequality stabilize or keep growing?
- Is political system sustainable under these strains?

**About Governance:**
- Can democratic institutions govern AI?
- Will authoritarians gain advantage from AI?
- Can international cooperation improve?
- Is regulatory capture inevitable?

## Key Branch Points

### Branch Point 1: Capability Trajectory (2025-2027)

**What Happened:**
Capabilities improved gradually, roughly as predicted by scaling laws. No dramatic surprises.

**Alternative Paths:**
- **Discontinuous Jump:** Sudden AGI breakthrough → Would shift to Catastrophe or Aligned AGI scenarios
- **Plateau:** Progress stalls completely → Would shift toward Pause scenario
- **Actual Path:** Steady, predictable improvement → Enables this muddling scenario

**Why This Mattered:**
Gradual progress allows gradual adaptation. Sudden jumps might overwhelm response capacity.

### Branch Point 2: International Coordination (2028-2030)

**What Happened:**
Partial cooperation emerged but no robust global governance. Competition continued alongside limited collaboration.

**Alternative Paths:**
- **Strong Coordination:** Effective international institution → Would shift toward Aligned AGI scenario
- **Complete Breakdown:** Pure racing dynamics → Would shift toward Multipolar or Catastrophe
- **Actual Path:** Muddling middle ground → Characterizes this scenario

**Why This Mattered:**
Level of coordination determines whether we can address global challenges versus fragment into competing blocs.

### Branch Point 3: Alignment Research (2029-2031)

**What Happened:**
Incremental progress but no fundamental breakthroughs. Safety measures work well enough for current systems.

**Alternative Paths:**
- **Major Breakthrough:** Robust alignment solution → Would shift toward Aligned AGI
- **Fundamental Impossibility:** Alignment proved unsolvable → Forces Pause or leads to Catastrophe
- **Actual Path:** Muddling progress, uncertainty continues → Defines this scenario

**Why This Mattered:**
Without alignment breakthrough, we continue with uncertainty. But without proof of impossibility, development continues.

### Branch Point 4: Economic Disruption Response (2030-2033)

**What Happened:**
Partial adaptation through mix of market forces, limited safety nets, political adjustment. Painful but not revolutionary.

**Alternative Paths:**
- **Comprehensive Response:** UBI, retraining, new social contract → Would improve this scenario
- **Complete Failure:** Mass unemployment, political collapse → Would destabilize toward chaos
- **Actual Path:** Patchwork, uneven response → Creates ongoing tension but maintains stability

**Why This Mattered:**
Economic disruption could have triggered political crisis forcing pause or creating chaos. Partial success prevents both.

### Branch Point 5: Public Trust (2033-2037)

**What Happened:**
Trust in institutions declined but didn't collapse. Epistemic environment degraded but not completely.

**Alternative Paths:**
- **Trust Maintained:** Strong institutions, shared reality → Would improve governance
- **Epistemic Collapse:** No shared truth, complete breakdown → Would shift to different scenario
- **Actual Path:** Declining but functional trust → Enables muddling to continue

**Why This Mattered:**
Some minimal trust necessary for coordination. Complete collapse would make governance impossible.

## Preconditions: What Needs to Be True

### Technical Preconditions

**Capabilities Progress Predictably:**
- No sudden discontinuous jumps in capabilities
- Scaling laws continue to hold roughly
- Progress neither stalls completely nor accelerates dramatically
- Enough time between capability levels for partial adaptation

**Alignment Partially Tractable:**
- Not fundamentally impossible (no impossibility proofs)
- But also not easily solvable
- Techniques work well enough for current systems
- Scaling properties unclear but not catastrophically bad

**No Fundamental Surprises:**
- No unexpected AGI from different approach
- No proof that alignment is impossible
- No dramatic new paradigm superseding current methods
- Evolutionary not revolutionary progress

### Coordination Preconditions

**Partial Coordination Possible:**
- Complete racing can be avoided
- But robust global governance can't be achieved
- National regulations feasible
- International cooperation possible on limited issues

**Economic Adaptation Feasible:**
- Labor markets can partially adjust
- Political systems can handle moderate disruption
- Some redistribution politically achievable
- Revolutionary change can be avoided

**Democratic Institutions Resilient Enough:**
- Can handle some epistemic degradation
- Can implement some AI governance
- Can manage increased inequality
- But operate under significant strain

### Societal Preconditions

**Public Accepts Gradual Change:**
- No overwhelming demand for pause
- No revolutionary movement against AI
- Concerns raised but no political consensus to stop
- Acceptance of "this is how things are now"

**Epistemic Environment Degraded but Functional:**
- Can still distinguish truth from fiction with effort
- Some trusted information sources remain
- Democratic deliberation possible but harder
- Misinformation widespread but not completely dominant

**No Catastrophic Incidents:**
- Safety incidents concerning but not existential
- No events that force complete reconsideration
- Each problem addressed reactively
- No single galvanizing crisis

## Warning Signs We're Entering This Scenario

### Early Indicators (Already Observable)

**Currently Seeing:**
- Gradual capability improvements following scaling laws
- Patchwork regulatory responses across jurisdictions
- Growing economic anxiety about AI job impacts
- Some safety incidents prompting reactive responses
- Partial coordination attempts with mixed success
- Public concern but no consensus for dramatic action

**This Matches Muddle Pattern:**
We're arguably already in early stages of this scenario. Current trajectory is incremental change with reactive responses.

### Medium-Term Indicators (Next 3-5 Years)

**We're on This Path If We See:**
- Each new model generation ~2x better, not 10x
- Regulations implemented but often circumvented
- Some AI safety incidents but none catastrophic
- Unemployment rising gradually to ~8-10%
- International cooperation forums created with limited power
- Alignment research progressing but no breakthroughs
- Companies adopting safety practices unevenly
- Public trust in AI declining slowly
- No clear tipping point toward pause or acceleration

**We're Diverging If We See:**
- Sudden capability jump (toward Catastrophe or Aligned AGI)
- Strong international coordination (toward Aligned AGI or Pause)
- Catastrophic safety incident (toward Pause or Catastrophe)
- Complete breakdown of coordination (toward Multipolar)
- Major alignment breakthrough (toward Aligned AGI)

### Late Indicators (5-10 Years)

**Strong Evidence for This Scenario:**
- AI capabilities near-human in many domains but not superhuman
- Economic disruption significant but managed (15-20% unemployment)
- Governance struggling but functional
- Continued safety incidents, none catastrophic
- Inequality increased but political system stable
- Epistemic environment degraded but democracy functioning
- No clear path to either utopia or catastrophe visible

**This Scenario Continuing Means:**
- We continue in state of uncertainty
- No resolution of fundamental questions
- Ongoing adaptation and strain
- Question remains whether this is stable long-term

## Valuable Actions in This Scenario

### What Matters Most

**Resilience and Adaptation:**
- Building social safety nets for economic disruption
- Developing adaptive governance institutions
- Strengthening epistemic institutions
- Promoting gradual positive change where possible
- Preventing slide into worse scenarios

**Incremental Safety Work:**
- Continued alignment research even without breakthroughs
- Improving evaluation and testing methods
- Sharing safety information across organizations
- Building safety culture in AI development
- Responding effectively to each new incident

**Maintaining Coordination:**
- Strengthening international cooperation where possible
- Building trust between AI labs on safety
- Promoting information sharing
- Preventing deterioration into pure racing
- Working toward better governance even if imperfect

### Technical Research (High Value)

**Practical Alignment Work:**
- Interpretability tools for current generation systems
- Scalable oversight for near-term deployments
- Robustness testing and evaluation
- Adversarial testing and red-teaming
- Iterative improvement of safety techniques

**Governance Tools:**
- AI auditing and certification methods
- Monitoring and detection systems
- Impact assessment frameworks
- Risk evaluation methodologies

**Don't Neglect:**
- Fundamental alignment research (might need it eventually)
- Capability research that could reveal safety issues
- Work on detecting deceptive alignment

### Policy and Governance (High Value)

**Adaptive Institutions:**
- Flexible regulations that can update as technology evolves
- Democratic oversight mechanisms for AI
- International cooperation frameworks even if imperfect
- Monitoring systems for AI development and deployment

**Economic Policy:**
- Expanded social safety nets
- Retraining and education programs
- Exploration of UBI or similar mechanisms
- Policies to reduce AI-driven inequality

**Epistemic Protection:**
- Media literacy programs
- Support for trusted information sources
- Provenance and watermarking standards
- Countering misinformation infrastructure

### Organizational Strategy

**For AI Labs:**
- Maintaining safety culture despite competitive pressure
- Implementing responsible scaling policies
- Sharing safety information (even if not capabilities)
- Preparing for gradual increase in regulation
- Building trust with policymakers and public

**For Governments:**
- Developing adaptive regulatory capacity
- Investing in AI expertise in government
- Building international cooperation channels
- Preparing for economic disruption
- Maintaining democratic legitimacy

**For Civil Society:**
- Monitoring AI impacts
- Advocating for safety and equity
- Building public understanding
- Holding companies and governments accountable
- Strengthening democratic institutions

### Individual Contributions

**For Professionals:**
- Developing AI expertise in your domain
- Advocating for responsible AI use
- Participating in governance processes
- Building adaptive career skills
- Supporting others through transition

**For Researchers:**
- Practical safety work on current systems
- Improving evaluation methodologies
- Building tools for responsible AI use
- Communicating findings accessibly

**For Advocates:**
- Promoting effective AI governance
- Preventing both complacency and panic
- Building coalitions for responsible AI
- Strengthening democratic institutions

## Who Benefits and Who Loses

### Winners

**Tech Companies:**
- Continued ability to develop and deploy AI
- Growing markets for AI products
- Regulations but not prohibitive
- Public backlash but not overwhelming

**AI-Adjacent Professionals:**
- Those who can work with AI effectively
- Programmers using AI coding assistants
- Professionals leveraging AI for productivity
- AI safety researchers (growing field)

**Some Nations:**
- Countries that adapt quickly to AI economy
- Those with strong safety nets and retraining
- Places that balance innovation with protection
- Regions attracting AI talent and investment

**Wealthy Individuals:**
- Can afford to leverage AI effectively
- Can adapt to changing economy
- Capital owners benefit from AI productivity
- Can access AI-enhanced services

### Losers

**Displaced Workers:**
- Those in heavily automated sectors
- Mid-skill workers in particular
- Limited opportunities for retraining
- Downward wage pressure

**Developing Nations:**
- May lack resources to adapt
- Could fall further behind economically
- May lack voice in AI governance
- More vulnerable to AI-driven disruption

**Privacy and Autonomy:**
- Surveillance increases with AI capabilities
- Manipulation becomes more sophisticated
- Autonomy eroded by AI recommendation systems
- Privacy degraded by ubiquitous AI analysis

**Democratic Institutions:**
- Harder to maintain shared reality
- Difficult to govern increasingly complex technology
- Regulatory capture by AI companies
- Epistemic environment degraded

### Ambiguous Cases

**Society Overall:**
- Significant material benefits from AI
- But increased inequality and insecurity
- More capable technology but more complex risks
- Higher GDP but lower social cohesion
- Economic growth but political strain

**Young People:**
- Growing up with AI as normal
- May adapt better than older generations
- But face uncertain job markets
- May lose some human capabilities to AI dependence

**Researchers and Academics:**
- AI acceleration of research is beneficial
- But credentialing and education systems strained
- Academic jobs threatened by AI tutors and researchers
- But new opportunities in AI-adjacent fields

## Cruxes and Uncertainties

<KeyQuestions questions={[
  "Is 'muddling through' stable long-term, or does it eventually collapse into catastrophe?",
  "Can democratic institutions survive in degraded epistemic environment?",
  "Will partial alignment solutions scale to more powerful systems?",
  "Can we manage economic disruption indefinitely without political breakdown?",
  "Is gradual capability progress likely to continue, or will we see discontinuities?",
  "Will partial coordination be enough to prevent worst outcomes?",
  "At what point does muddling through become untenable?"
]} />

### Biggest Uncertainties

**Technical Trajectory:**
- Will scaling continue smoothly or hit walls/jumps?
- Will current safety techniques scale?
- When/if will we develop AGI?
- Will we get advance warning of dangerous capabilities?

**Social Stability:**
- Can political systems handle ongoing disruption?
- Will inequality spark revolution or be tolerated?
- Can trust in institutions be maintained?
- Will epistemic environment degrade further or stabilize?

**Governance Evolution:**
- Can coordination improve over time?
- Will regulations become more or less effective?
- Can democratic oversight of AI work?
- Will authoritarian states gain advantage?

**Long-Term Stability:**
- Is this a stable equilibrium or just delaying inevitable crisis?
- Are we slowly solving problems or building up risks?
- Will alignment challenges become unsolvable at higher capability levels?
- Can we muddle through to truly beneficial outcomes eventually?

## Is This Scenario Stable?

### Arguments It's Stable

**Adaptation is Possible:**
- Humans and institutions can adapt to gradual change
- Each problem prompts partial response
- Learning from mistakes improves outcomes over time
- Technological progress often follows this pattern historically

**No Single Point of Failure:**
- Distributed, incremental development reduces catastrophic risk
- Multiple safety measures provide defense in depth
- Partial coordination prevents worst racing dynamics
- Gradual change allows course correction

**Economic and Political Incentives:**
- Benefits of AI provide motivation to continue carefully
- Costs of racing create pressure for some coordination
- Political pressure prevents both recklessness and excessive caution
- Economic gains fund safety research and adaptation

### Arguments It's Unstable

**Building Risk Over Time:**
- Partial alignment solutions may fail at higher capability levels
- Epistemic degradation makes coordination harder
- Inequality and disruption may reach breaking point
- Complacency from lack of catastrophe may reduce safety investment

**Coordination Decay:**
- Competition may intensify over time
- International cooperation fragile under stress
- Regulatory capture may worsen
- Safety culture may erode under economic pressure

**Capability Surprises:**
- Smooth progress may not continue
- Unexpected capability jump could overwhelm adaptation
- New paradigms may bypass current safety measures
- Deceptive alignment may only be detectable too late

**Social Breaking Points:**
- Unemployment may reach politically unsustainable levels
- Epistemic environment may collapse completely
- Trust in institutions may erode past functionality
- Inequality may spark revolution

### Most Likely: Unstable Equilibrium

This scenario probably represents **temporary stability** that could:
- **Improve toward Aligned AGI:** If safety research succeeds and coordination strengthens
- **Degrade to Catastrophe:** If capabilities jump or alignment fails
- **Fragment to Multipolar:** If coordination breaks down
- **Force a Pause:** If crisis demands it

The question is whether we can strengthen the stability or whether we're just delaying inevitable transition.

## Relation to Other Scenarios

### This As Default Path

**Muddle is Baseline:**
- This is where we are now
- Other scenarios require departure from current trajectory
- Inertia favors continuing muddling
- Requires active effort to shift to other scenarios

**Transitions Possible:**
- **To Aligned AGI:** If alignment breakthroughs and coordination improve
- **To Catastrophe:** If capabilities jump or alignment fails at scale
- **To Multipolar:** If coordination breaks down further
- **To Pause:** If crisis forces reconsideration

### Combinations With Other Scenarios

**Muddle Plus Elements:**
- Multipolar competition within muddling framework
- Temporary pauses during critical periods
- Local successes with alignment alongside global muddling
- Different regions on different paths

**Muddle as Transition:**
- May muddle until we know enough to solve alignment (→ Aligned AGI)
- May muddle until something forces a choice (→ Pause or Catastrophe)
- May muddle until coordination fails (→ Multipolar)
- Muddle may be the long-term state

## Historical Analogies

### Similar Historical Patterns

**Industrial Revolution:**
- Gradual technological transformation over decades
- Significant economic disruption
- Partial regulatory responses
- Social adaptation through crisis and reform
- Massive benefits but also massive costs
- Lessons: Gradual change is disruptive but manageable; adaptation takes generations

**Internet/Social Media:**
- Rapid adoption with partial understanding of consequences
- Reactive regulation always behind technology
- Mix of benefits and harms
- Epistemic challenges emerging
- No catastrophe but no clear resolution
- Lessons: We often muddle through transformative technologies

**Nuclear Power:**
- Powerful technology with safety concerns
- Gradual deployment with incidents
- Regulations improved after each incident
- Public concern but continued use
- Neither utopia nor catastrophe
- Lessons: Can maintain risky technology with care but never perfect safety

**Climate Change:**
- Gradual problem with inadequate response
- Partial measures insufficient
- Coordination failures ongoing
- Muddling toward significant harm
- Lessons: Muddling through can lead to bad outcomes if problem is severe enough

## Probability Assessment

<EstimateBox
  question="What's the probability of this scenario?"
  estimates={[
    { value: "30-50%", source: "Baseline estimate", reasoning: "Most consistent with current trajectory and historical patterns" },
    { value: "40-60%", source: "Optimists", reasoning: "Muddling through is how humanity usually handles transformative technology" },
    { value: "20-40%", source: "Pessimists", reasoning: "Unstable equilibrium likely to collapse into other scenarios" },
    { value: "35-45%", source: "Median view", reasoning: "Most likely path but not stable long-term" }
  ]}
/>

### Why This Probability?

**Reasons for Higher Probability:**
- This is our current trajectory
- Consistent with how most technological transitions occur
- Humans good at gradual adaptation
- Distributed development reduces single points of failure
- Economic incentives favor continued development with some caution
- No strong force pushing toward radical alternatives

**Reasons for Lower Probability:**
- May be unstable equilibrium
- AI might be different from historical technologies
- Capabilities could jump unexpectedly
- Coordination challenges may worsen
- Epistemic degradation may accelerate
- Economic disruption may exceed adaptation capacity

**Central Estimate Rationale:**
30-50% reflects that this is our default path and consistent with historical patterns, but may not be stable long-term. Higher than other individual scenarios but not certain. Wide range reflects uncertainty about whether muddling can continue or will transition to other scenarios.

### What Would Change This Estimate?

**Evidence Increasing Probability:**
- Capabilities continuing to scale smoothly
- Partial coordination proving sustainable
- Safety incidents remaining non-catastrophic
- Economic adaptation proving manageable
- Democratic institutions proving resilient
- No dramatic breakthroughs or failures

**Evidence Decreasing Probability:**
- Unexpected capability jumps or plateaus
- Coordination breaking down or succeeding completely
- Catastrophic safety incident
- Economic or political crisis
- Major alignment breakthrough or impossibility proof
- Events forcing departure from current trajectory

## Open Questions

**About Stability:**
- Can we muddle through indefinitely?
- Is this building toward catastrophe or success?
- What's the longest this can continue?
- What events would force transition?

**About Adaptation:**
- Can institutions adapt fast enough?
- What's the limit of economic disruption we can manage?
- Can epistemic environment be stabilized?
- Will inequality stabilize or keep growing?

**About Technology:**
- Will scaling continue smoothly?
- Will partial safety measures scale?
- When will we know if alignment is solvable?
- What capabilities will emerge next?

**About Coordination:**
- Can partial coordination strengthen over time?
- What would trigger cooperation or breakdown?
- Can democratic governance work for AI?
- Will authoritarians gain advantage?

<Section title="Related Scenarios and Topics">
  <Tags tags={[
    "Slow Takeoff",
    "Gradual Progress",
    "Partial Alignment",
    "Economic Disruption",
    "Regulatory Challenges",
    "Scenario Planning",
    "Most Likely",
    "Incremental Change",
    "Adaptation",
    "Base Case"
  ]} />
</Section>

<Sources sources={[
  { title: "What Failure Looks Like", url: "https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like", author: "Paul Christiano" },
  { title: "Predictions for 2024", url: "https://www.lesswrong.com/posts/EyJbCFwj8nqJgJcSJ/predictions-for-2024", author: "Various" },
  { title: "AI Timelines and Takeoff Speeds", url: "https://www.cold-takes.com/where-ai-forecasting-stands-today/", author: "Holden Karnofsky" },
  { title: "The Age of AI", url: "https://www.foreignaffairs.com/articles/world/2021-10-19/henry-kissinger-eric-schmidt-daniel-huttenlocher-age-ai", author: "Kissinger, Schmidt, Huttenlocher" },
]} />

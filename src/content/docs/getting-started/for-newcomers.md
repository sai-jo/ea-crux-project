---
title: For Newcomers
description: Your reading path if you're new to AI safety
sidebar:
  order: 1
---

Welcome! If you're new to AI safety, this page will guide you through the essential concepts in a logical order. We've designed several reading paths based on how much time you have.

## Start Here: The 5-Minute Version

If you only have a few minutes, here's the absolute core:

### The Basic Worry

AI systems are becoming increasingly capable. As they do, two things need to happen:
1. The AI needs to do what we want (alignment)
2. No one uses AI to cause catastrophic harm (safety and security)

Both are harder than they sound. If we get either wrong with sufficiently powerful AI, the consequences could be irreversible and catastrophic.

**Key insight**: The same capabilities that make AI useful (reasoning, planning, persuasion) could also make misaligned AI extremely dangerous.

**Time pressure**: AI capabilities are advancing rapidly, possibly faster than our ability to solve alignment and safety.

### Three Numbers to Remember

- **When**: Median expert estimates put transformative AI around 2040, but estimates range from 2025 to "never"
- **How dangerous**: AI safety researchers' estimates of catastrophic risk range from 5% to 90%
- **Current response**: Roughly 300-1000 people working full-time on AI safety, compared to tens of thousands advancing capabilities

### What to Read Next

If those numbers concern you, spend 30 more minutes reading the [Core Argument](/understanding-ai-risk/core-argument/) overview to understand why smart people take this seriously.

## The 30-Minute Path

If you have half an hour, here's the recommended order:

### 1. Start with the FAQ (10 minutes)
[Read the FAQ](/knowledge-base/resources/faq/) to get quick answers to the most common questions:
- Isn't this just science fiction?
- Why worry about future AI when we have current problems?
- Can't we just turn it off?
- Aren't AI researchers smart enough to prevent this?

### 2. Understand the Basic Argument (15 minutes)
Read the overview of the [Core Argument](/understanding-ai-risk/core-argument/):
- Why AI might become extremely capable
- Why alignment is technically difficult
- Why this could happen faster than we can respond
- Why coordination is challenging

### 3. See What Can Be Done (5 minutes)
Skim [Interventions Overview](/knowledge-base/interventions/) to understand that this isn't hopeless—there are concrete things being done and that you could do.

**After 30 minutes, you'll have**: A basic understanding of the concern, why it's taken seriously, and that there are responses available.

## The 2-Hour Path

If you can invest a couple of hours, you'll develop a much deeper understanding:

### Hour One: Understanding the Problem

**1. Core Argument Deep Dive (30 minutes)**
Work through the key steps of the argument:
- [Timelines](/understanding-ai-risk/core-argument/timelines/) - When might we get transformative AI?
- [Capabilities](/understanding-ai-risk/core-argument/capabilities/) - How powerful could AI become?
- [Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/) - Why is it hard to ensure AI does what we want?

**2. Concrete Risk Examples (20 minutes)**
Read about specific ways things could go wrong:
- [Goal Misgeneralization](/knowledge-base/risks/accident/goal-misgeneralization/) - When AI objectives don't transfer to new contexts
- [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) - When AI appears aligned but isn't
- [Power-Seeking](/knowledge-base/risks/accident/power-seeking/) - Why AI might accumulate resources and influence

**3. Why Current AI Matters (10 minutes)**
Read [Warning Signs](/understanding-ai-risk/core-argument/warning-signs/) to understand how today's AI systems relate to future risks.

### Hour Two: Understanding the Landscape

**4. Different Perspectives (20 minutes)**
Explore how people weigh the evidence differently:
- [High-Risk View](/understanding-ai-risk/worldviews/doomer/) - Why some think catastrophe is likely
- [Optimistic View](/understanding-ai-risk/worldviews/optimistic/) - Why some think we'll probably be fine
- [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties/) - What we don't know that matters most

**5. What's Being Done (20 minutes)**
Review safety approaches:
- [Interpretability](/knowledge-base/safety-approaches/technical/interpretability/) - Understanding how AI systems work internally
- [Scalable Oversight](/knowledge-base/safety-approaches/technical/scalable-oversight/) - How to supervise superhuman AI
- [Governance Approaches](/knowledge-base/safety-approaches/governance/governance/) - Policy and coordination

**6. Who's Working on This (20 minutes)**
Understand the organizational landscape:
- [Anthropic](/knowledge-base/organizations/anthropic/) - Example of a "safety-focused" frontier lab
- [ARC](/knowledge-base/organizations/arc/) - Example of alignment research organization
- [Organizations Overview](/knowledge-base/organizations/) - The broader landscape

**After 2 hours, you'll have**: A solid foundation for following AI safety discussions and forming preliminary views.

## The Deep Dive: One Day Path

If you want to really understand AI safety deeply, here's a full-day curriculum:

### Morning Session: Foundations (3 hours)

**9:00-10:00 - The Case for Risk**
- Read [Core Argument](/understanding-ai-risk/core-argument/) section completely, all subsections
- Pay special attention to where you're skeptical—those are your cruxes
- Take notes on which claims you find most/least convincing

**10:00-11:00 - How Things Could Go Wrong**
Deep dive into risk categories:
- [Accident Risks Overview](/knowledge-base/risks/accident/) - Technical failures
- Pick 3-4 specific risks that seem most important or confusing and read them thoroughly
- Understand the difference between observable current problems and theoretical future risks

**11:00-12:00 - Models and Frameworks**
Understand different ways of thinking about risk:
- [Capability-Alignment Race](/understanding-ai-risk/models/capability-alignment-race/) - Will safety keep pace with capabilities?
- [Multi-Actor Landscape](/understanding-ai-risk/models/multi-actor-landscape/) - How do different actors interact?
- [Feedback Loops](/understanding-ai-risk/models/feedback-loops/) - What dynamics accelerate or mitigate risk?

### Afternoon Session: Solutions and Strategy (3 hours)

**12:00-1:00 - Lunch Break + Light Reading**
Read [Historical Case Studies](/analysis/case-studies/) over lunch:
- [Nuclear Weapons](/analysis/case-studies/nuclear/) - Lessons from nuclear governance
- [Biotechnology](/analysis/case-studies/biotech/) - Managing dual-use technology
- What patterns might apply to AI?

**1:00-2:00 - Technical Safety Approaches**
Understand what technical safety research looks like:
- [Interpretability](/knowledge-base/safety-approaches/technical/interpretability/)
- [AI Control](/knowledge-base/safety-approaches/technical/ai-control/)
- [RLHF and Alignment](/knowledge-base/safety-approaches/technical/rlhf/)
- [Research Agendas Comparison](/knowledge-base/research-agendas/)

**2:00-3:00 - Governance and Coordination**
Understand non-technical approaches:
- [Governance Overview](/knowledge-base/safety-approaches/governance/governance/)
- [International Coordination](/knowledge-base/safety-approaches/governance/international/)
- [Compute Governance](/knowledge-base/policies/) - Can we govern AI through hardware?
- [Racing Dynamics](/knowledge-base/risks/structural/racing-dynamics/) - Understanding competitive pressure

### Evening Session: Synthesis and Next Steps (2 hours)

**3:00-4:00 - Debates and Disagreements**
Understand key controversies:
- Browse [Debates](/understanding-ai-risk/debates/) section
- Focus on debates where you're genuinely uncertain
- Try to steelman both sides
- Identify which cruxes matter most for your views

**4:00-5:00 - People and Organizations**
Understand who's working on what:
- Read about 3-4 [key researchers](/knowledge-base/people/) who represent different perspectives
- Review 3-4 [organizations](/knowledge-base/organizations/) across different categories (frontier labs, safety orgs, governance)
- Understand the [funding landscape](/knowledge-base/resources/funding/)

**After a full day, you'll have**: A comprehensive understanding of AI safety, enough to engage seriously with the field and identify how you might contribute.

## Topic-Based Learning Paths

Not everyone learns best linearly. Here are paths organized by topic:

### If You're Most Interested in Technical Details

1. [Capabilities](/understanding-ai-risk/core-argument/capabilities/) - What can AI systems do?
2. [Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/) - Why is alignment hard technically?
3. [Accident Risks](/knowledge-base/risks/accident/) - Technical failure modes
4. [Technical Safety Approaches](/knowledge-base/safety-approaches/technical/) - What researchers are building
5. [Research Agendas](/knowledge-base/research-agendas/) - Comparing different technical approaches

### If You're Most Interested in Strategy and Governance

1. [Coordination Challenges](/understanding-ai-risk/core-argument/coordination/) - Why cooperation is hard
2. [Structural Risks](/knowledge-base/risks/structural/) - Systemic problems
3. [Governance Approaches](/knowledge-base/safety-approaches/governance/governance/)
4. [International Coordination](/knowledge-base/safety-approaches/governance/international/)
5. [Case Studies](/analysis/case-studies/) - Historical parallels

### If You're Most Interested in Probability and Forecasting

1. [Timelines](/understanding-ai-risk/core-argument/timelines/) - When might we get transformative AI?
2. [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties/) - What don't we know?
3. [Different Worldviews](/understanding-ai-risk/worldviews/) - How people weigh evidence differently
4. [AI Timeline Visualization](/analysis/ai-timeline/) - Historical and projected progress
5. Expert estimates scattered throughout organization and people pages

### If You're Most Interested in What You Can Do

1. [Interventions Overview](/knowledge-base/interventions/) - What actions are available?
2. [Field Building](/knowledge-base/interventions/field-building/) - Growing the AI safety community
3. [Organizations](/knowledge-base/organizations/) - Who's hiring and what they do
4. [Funding Landscape](/knowledge-base/resources/funding/) - How to get resources
5. [Career Paths](/knowledge-base/resources/careers/) - How to contribute (if this page exists)

## Common Learning Obstacles

Here are challenges many newcomers face and how to address them:

### "This Seems Like Science Fiction"

**You're not alone**: This is the most common initial reaction. The key is distinguishing between:
- **Science fiction**: Anthropomorphized robots, conscious AI, Matrix-style scenarios
- **The actual concern**: Highly capable optimization systems pursuing misspecified objectives

**What helps**: Read [Warning Signs](/understanding-ai-risk/core-argument/warning-signs/) to see how current AI systems already exhibit concerning behaviors at small scale. Read about [instrumental convergence](/knowledge-base/risks/accident/instrumental-convergence/) to understand why dangerous behaviors might emerge without human-like consciousness.

### "The Probabilities Are All Over the Place"

**You're right**: Estimates of AI risk range from <1% to >90%. This huge range is actually information:
- We genuinely don't know
- Different people weigh evidence very differently
- Your job isn't to find "the right" probability but to understand what drives the range

**What helps**: Read [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties/) to understand which factors most affect estimates. Read [Different Worldviews](/understanding-ai-risk/worldviews/) to see how different assumptions lead to different conclusions.

### "Can't We Just..."

Every newcomer has "simple solutions" that turn out to be harder than they appear:
- "Can't we just turn it off?" → See [Corrigibility Failure](/knowledge-base/risks/accident/corrigibility-failure/)
- "Can't we just program it to be good?" → See [Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/)
- "Can't we just not build it?" → See [Coordination Challenges](/understanding-ai-risk/core-argument/coordination/)

**What helps**: The FAQ addresses common "simple solutions." Most have been thought about extensively—understanding why they're hard is enlightening.

### "I Don't Have a Math/CS Background"

**Good news**: Much of AI safety is accessible without deep technical knowledge. The strategic, governance, and ethical questions are crucial and don't require coding skills.

**What helps**:
- Focus on the conceptual rather than mathematical content
- Read the governance and strategy sections first
- Use the [Glossary](/knowledge-base/resources/glossary/) liberally
- Don't worry if you can't follow every technical detail

### "Everyone Disagrees About Everything"

**This is frustrating but real**: AI safety is a young field with deep uncertainties. Disagreement is a feature, not a bug—it shows people are thinking independently rather than following dogma.

**What helps**:
- Focus on understanding the cruxes, not finding consensus
- Identify which disagreements matter most for decision-making
- Read [Key Debates](/understanding-ai-risk/debates/) to see structured arguments on different sides
- Form your own views rather than trying to find "the right" view

## Key Concepts to Understand

By the end of your learning journey, you should understand:

**Core Concepts:**
- **Alignment**: Ensuring AI does what we want
- **Capabilities**: What AI systems can do
- **Transformative AI**: AI that causes a transition comparable to the Industrial or Agricultural Revolutions
- **AGI/ASI**: Artificial General Intelligence / Artificial Superintelligence
- **Orthogonality thesis**: Intelligence and goals are independent
- **Instrumental convergence**: Why diverse goals lead to similar dangerous subgoals

**Key Risk Types:**
- **Goal misgeneralization**: Objectives that work in training but not deployment
- **Deceptive alignment**: AI that appears aligned but isn't
- **Mesa-optimization**: Learned optimizers with different objectives
- **Power-seeking**: Tendency to accumulate resources and influence
- **Coordination failures**: Competitive dynamics that prevent safety measures

**Safety Approaches:**
- **Interpretability**: Understanding AI internals
- **Scalable oversight**: Supervising superhuman AI
- **AI control**: Maintaining safety even without full alignment
- **Governance**: Policy and coordination mechanisms

**Strategic Concepts:**
- **Differential progress**: Advancing safety faster than capabilities
- **Racing dynamics**: Competition that undermines safety
- **Theory of change**: How safety work might actually reduce risk
- **Cruxes**: Disagreements that matter most for conclusions

## Red Flags and Misconceptions

As you learn, watch out for these common misconceptions:

**Misconception: "AI safety is about preventing Terminator-style robots"**
**Reality**: The concern is about goal misalignment in highly capable systems, regardless of physical form.

**Misconception: "We just need to program in the Three Laws of Robotics"**
**Reality**: Specifying human values in code is far harder than it seems—see the alignment difficulty section.

**Misconception: "AI experts all agree this is a serious threat"**
**Reality**: There's significant disagreement. Many ML researchers are skeptical, though concern has grown.

**Misconception: "If AI safety is important, people should work on it regardless of fit"**
**Reality**: Impact requires both importance and tractability. Not everyone should work on AI safety.

**Misconception: "The AI safety community has all the answers"**
**Reality**: The field is characterized by deep uncertainty. Confidence claims should be viewed skeptically.

**Misconception: "AI safety research is trying to stop AI development"**
**Reality**: Most AI safety work aims to make AI development safer, not stop it (though some advocate for slowing down).

## How to Engage Critically

The best learning happens when you engage actively and critically:

**Keep a running list of:**
- Claims that seem questionable to you
- Arguments you find unconvincing
- Gaps in the reasoning
- Alternative explanations for observations
- Questions that aren't being addressed

**Ask yourself:**
- What would change my mind about this?
- What's the strongest counterargument?
- What might I be biased toward believing?
- What's the base rate for this kind of prediction?
- What have similar technological transitions looked like?

**Seek out disagreement:**
- Don't just read AI safety advocates
- Look for thoughtful critics
- Understand why many AI researchers are skeptical
- Consider whether AI safety concerns might be overblown

**Test your understanding:**
- Can you explain concepts to someone else?
- Can you steelman positions you disagree with?
- Can you identify your cruxes—what would change your mind?

## What to Read Outside This Wiki

This wiki presents the AI safety perspective. For balance, also read:

**Skeptical Perspectives:**
- Gary Marcus (AI capabilities skepticism)
- Yann LeCun (skeptical of AGI/x-risk framing)
- Melanie Mitchell (skeptical of near-term AGI)

**Alternative Frameworks:**
- Timnit Gebru and DAIR Institute (AI ethics perspective)
- Emily Bender (linguistic critique of LLM capabilities)
- Technology critics (concerns about hype and power)

**Historical Context:**
- Previous technology scares (Y2K, nanotechnology)
- Successful governance of risky technology (nuclear, bioweapons)
- Failed governance attempts (climate change, conventional arms)

**Primary Sources:**
- Nick Bostrom's "Superintelligence" (foundational book)
- Stuart Russell's "Human Compatible" (accessible academic perspective)
- Actual AI safety papers, not just summaries

## Next Steps After Learning the Basics

Once you have a foundation, here's how to go deeper:

**If you're forming views:**
1. Identify your cruxes using [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties/)
2. Deep dive into the topics that most affect your bottom line
3. Engage with the strongest counterarguments
4. Calibrate your confidence appropriately

**If you're considering contributing:**
1. Explore [Interventions](/knowledge-base/interventions/) to see what work exists
2. Review [Organizations](/knowledge-base/organizations/) to understand who does what
3. Check out the [Funding Landscape](/knowledge-base/resources/funding/)
4. Consider which paths match your skills and comparative advantage

**If you want to stay informed:**
1. Follow key researchers and organizations
2. Monitor developments in AI capabilities and safety
3. Track policy developments
4. Join AI safety communities (with appropriate skepticism)

**If you remain skeptical:**
That's fine! Skepticism is valuable. Consider:
1. What would it take to convince you this is a real concern?
2. What probability do you assign, and what drives that number?
3. Even if the risk is low, is the expected value high enough to matter?
4. Are there other perspectives on AI risk that resonate more?

## Your Path Forward

Everyone's path through AI safety is different. Some people:
- Become convinced it's humanity's most important problem
- Remain skeptical but think it's worth some resources
- Conclude other problems are more pressing for them personally
- Find the arguments interesting but not action-guiding

All of these are legitimate responses. The goal of this learning path isn't to convince you of a particular view, but to help you understand the landscape well enough to form your own informed perspective.

**The most important thing**: Engage seriously with the arguments, understand the uncertainties, and decide for yourself what you believe and what (if anything) you want to do about it.

Ready to begin? Start with the [FAQ](/knowledge-base/resources/faq/) and go from there. Good luck on your learning journey.

---
title: What is AI Safety?
description: An introduction to AI safety and why it matters for humanity's future
sidebar:
  order: 0
---

AI safety is the field dedicated to ensuring that advanced artificial intelligence systems are beneficial to humanity and do not cause catastrophic harm. While AI has tremendous potential to solve major problems, the same capabilities that make AI useful also create risks—from algorithmic bias and misinformation today to potentially existential threats as AI systems become more powerful.

This wiki focuses primarily on existential and catastrophic risks from advanced AI, though it covers other safety concerns where relevant.

## Why AI Safety Matters

The development of artificial intelligence may be one of humanity's most consequential projects. AI systems are already reshaping how we work, communicate, and make decisions. As these systems become more capable, the stakes grow higher.

Consider the trajectory:
- **2012**: Deep learning revolutionizes computer vision
- **2017**: Transformer architecture enables major language model breakthroughs
- **2022**: ChatGPT demonstrates impressive general capabilities to the public
- **2023-2024**: Frontier models approach or exceed human performance on many cognitive tasks
- **2025+**: Systems with increasingly general capabilities continue to advance

The question is not whether AI will be powerful, but whether it will be aligned with human values and interests when it is.

### The Basic Concern

The core worry is straightforward: as AI systems become more capable, the gap between "what we want the system to do" and "what the system actually does" becomes increasingly dangerous.

With a simple calculator, misalignment is annoying—you get the wrong answer. With a self-driving car, it might be fatal. With a superintelligent system optimizing for the wrong objective, it could be catastrophic and irreversible.

The challenge is that:
1. **Capability is advancing rapidly**: AI systems are becoming more powerful faster than many predicted
2. **Alignment is hard**: Ensuring AI systems robustly do what we want is technically difficult
3. **The stakes are high**: Mistakes with sufficiently advanced AI could be permanent
4. **Coordination is challenging**: Even if one organization acts responsibly, competitive pressures exist

### Why This Is Different From Other Technologies

Several features make AI safety particularly challenging:

**Speed of development**: Unlike nuclear weapons (decades) or biotechnology (ongoing), AI capabilities are advancing extremely rapidly with massive commercial incentives.

**Difficulty of containment**: Physical technologies can be controlled through export restrictions, facility inspections, etc. AI is information—harder to contain once developed.

**Potential for intelligence explosion**: If AI systems can improve AI systems, progress could accelerate dramatically and unexpectedly.

**Alignment complexity**: With most technologies, the question is "who uses it and how?" With advanced AI, there's the deeper question: "what does the AI itself want, if anything?"

**Irreversibility**: Deployed advanced AI systems might be impossible to recall or correct if they're sufficiently capable and misaligned.

## A Brief History of AI Safety

### Early Foundations (1950s-2000s)

The possibility of machine intelligence exceeding human intelligence has been discussed since the beginning of computer science:

**1950s-60s**: Early AI researchers like Alan Turing and Marvin Minsky pondered machine intelligence and its implications.

**1960s-70s**: I.J. Good articulated the concept of an "intelligence explosion" where AI systems recursively improve themselves.

**1980s-90s**: Scattered discussions of AI risk in science fiction and academic philosophy, but little systematic attention.

**2000**: Eliezer Yudkowsky begins writing about AI safety challenges, particularly the alignment problem.

### Emergence of the Field (2000s-2010s)

**2001**: The Machine Intelligence Research Institute (MIRI) is founded, initially focusing on theoretical foundations for safe AI.

**2007-2010**: Yudkowsky's writings on LessWrong popularize concepts like instrumental convergence, paperclip maximizers, and the difficulty of value alignment.

**2011**: Stuart Russell raises AI safety concerns in academic contexts, lending credibility to the field.

**2014**: Nick Bostrom publishes "Superintelligence," bringing AI existential risk to mainstream intellectual discussion.

**2015**: The field gains momentum:
- The Future of Life Institute is founded
- An open letter on AI safety is signed by prominent researchers including Stuart Russell, Yoshua Bengio, and Elon Musk
- OpenAI is founded with a stated focus on safe AGI development
- DeepMind forms a safety team

**2016-2017**: Concrete research agendas emerge from Paul Christiano (alignment), MIRI (agent foundations), and others. The term "AI alignment" becomes standard.

### Rapid Growth (2018-Present)

**2018-2020**: Technical safety research accelerates:
- Debate and amplification proposals for scalable oversight
- Circuit analysis and interpretability work
- Studies of mesa-optimization and goal misgeneralization
- Research on power-seeking tendencies

**2021**: Major organizational developments:
- Anthropic founded by former OpenAI researchers concerned about safety
- Multiple AI safety research organizations receive significant funding
- First AI safety researchers hired at major universities

**2022-2023**: The field goes mainstream:
- ChatGPT's release makes AI capabilities tangible to the public
- Prominent AI researchers (Geoffrey Hinton, Yoshua Bengio) publicly voice safety concerns
- Government attention increases (White House AI executive order, UK AI Safety Summit)
- Debate intensifies about AI timelines and risk levels

**2024-Present**: Institutionalization and polarization:
- AI safety becomes standard topic in AI conferences and labs
- All major AI labs have safety teams (though their scope and influence varies)
- Governments establish AI safety institutes
- Growing disagreement within the AI safety community about strategy and priorities
- Increased focus on near-term risks alongside long-term concerns

## What This Wiki Covers

This wiki is organized around key questions and uncertainties in AI safety:

### Understanding AI Risk

The fundamental arguments for why advanced AI poses risks:
- **Core Argument**: A structured walkthrough of the case for AI existential risk
- **Risk Models**: Different frameworks for thinking about how risk develops
- **Worldviews**: How people with different perspectives weigh the evidence

### Risks & Failure Modes

Comprehensive catalog of specific ways AI could cause harm:
- **Accident Risks**: Technical failures and unintended behaviors
- **Misuse Risks**: Intentional harmful applications
- **Structural Risks**: Systemic problems from AI's role in society
- **Epistemic Risks**: Threats to knowledge and truth

### Safety Approaches

Technical and governance strategies for reducing risk:
- **Technical Approaches**: Interpretability, scalable oversight, AI control, etc.
- **Governance Approaches**: Regulation, international coordination, compute governance
- **Institutional Approaches**: Lab culture, open source policies, pauses

### Organizations & People

Who is working on AI safety and AI development:
- **Frontier AI Labs**: Organizations developing the most capable systems
- **Safety Research Organizations**: Groups focused specifically on safety
- **Key Researchers**: Important voices and their perspectives
- **Funding Landscape**: Where resources are allocated

### Analysis & Tools

Synthesis and visualization of key questions:
- **Risk Maps**: How different failure modes relate
- **Timelines**: Historical development and future projections
- **Comparisons**: Side-by-side analyses of approaches and organizations
- **Scenarios**: How the future might unfold
- **Interventions**: What can be done and by whom

### Resources

Practical information:
- **Glossary**: Key terms and concepts
- **FAQ**: Common questions
- **Reading Lists**: Recommended materials
- **Career Guidance**: How to contribute

## Key Questions This Wiki Explores

Throughout this wiki, you'll encounter recurring questions that represent the deepest uncertainties in AI safety:

**On timelines and capabilities:**
- When will we develop transformative AI systems?
- Will progress be gradual or sudden?
- How capable will AI systems become?

**On alignment:**
- How hard is it to align AI with human values?
- Can we solve alignment before we achieve dangerous capabilities?
- Are current alignment techniques sufficient?

**On risk levels:**
- What's the probability of AI-caused catastrophe?
- Which specific failure modes are most likely?
- How much should we update based on current AI behaviors?

**On strategy:**
- Should we slow down AI development or race ahead to ensure safety-focused actors lead?
- Is working at AI labs accelerating risk or helping solve it?
- What role should government regulation play?

**On governance:**
- Can we coordinate globally on AI safety?
- Will competitive pressures undermine safety measures?
- What institutions and policies could help?

This wiki doesn't claim to have definitive answers. Instead, it maps the landscape of arguments, evidence, and uncertainty so you can form your own informed views.

## What This Wiki Is and Isn't

### This Wiki Is:
- **Comprehensive**: Covering technical, governance, and strategic perspectives
- **Structured around cruxes**: Organized by key disagreements and uncertainties
- **Candid about uncertainty**: Acknowledging what we don't know
- **Reference-oriented**: Providing sources and citations
- **Accessible**: Explaining complex ideas in understandable terms

### This Wiki Isn't:
- **Neutral**: It was created within the AI safety community and reflects that perspective
- **Consensus-based**: It presents multiple viewpoints but doesn't claim they're all equally valid
- **Complete**: New research and developments constantly emerge
- **Advocacy-focused**: It aims to inform rather than persuade to a specific position

### Limitations to Be Aware Of

This wiki was built by people concerned about AI safety, which means:

**Systematic biases:**
- May take the x-risk framework as given rather than questioning it
- May present safety research as more tractable than it is
- May not fully represent perspectives that reject the AI safety framing
- May focus disproportionately on long-term over near-term concerns
- May reflect Western/tech-industry perspectives

**Coverage gaps:**
- Limited engagement with AI ethics frameworks focused on fairness, bias, and accountability
- Minimal coverage of non-Western perspectives on AI governance
- Insufficient engagement with critics who see AI safety concerns as overblown or misguided

**Uncertainty challenges:**
- Probability estimates should be treated as rough intuitions, not precise forecasts
- Expert disagreement is often understated
- Selection effects mean confident voices may be overrepresented

We've tried to be transparent about these limitations. If you're skeptical of the AI safety framing, this wiki can help you understand what AI safety researchers believe and why, but you should also seek out alternative perspectives.

## How to Use This Wiki

Different readers will want different paths through the material:

**If you're completely new to AI safety:**
Start with the [FAQ](/knowledge-base/resources/faq/) to get basic orientation, then work through the [Core Argument](/understanding-ai-risk/core-argument/) section systematically.

**If you're deciding whether to work on AI safety:**
Explore [Interventions](/knowledge-base/interventions/) to understand what kinds of work exist, check out [Organizations](/knowledge-base/organizations/) to see who's hiring, and read about [different worldviews](/understanding-ai-risk/worldviews/) to understand the range of perspectives.

**If you're a technical researcher:**
Dive into [Safety Approaches](/knowledge-base/safety-approaches/technical/) to understand current research directions, review [Research Agendas](/knowledge-base/research-agendas/) for comparisons, and explore specific [Risk & Failure Modes](/knowledge-base/risks/) that motivate different approaches.

**If you're a policymaker or governance professional:**
Start with [Governance Approaches](/knowledge-base/safety-approaches/governance/), review [Case Studies](/analysis/case-studies/) from other technologies, and examine [Policies](/knowledge-base/policies/) currently being developed.

**If you're forming your own views:**
Use the [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties/) page to identify key questions, explore [Key Debates](/understanding-ai-risk/debates/) to see arguments on different sides, and examine how different [Worldviews](/understanding-ai-risk/worldviews/) weight the evidence.

## Key Numbers to Know

Some rough figures to orient yourself (as of late 2024):

**Timelines:**
- **Median expert estimate for transformative AI**: 2040-2050 (with wide uncertainty)
- **More aggressive estimates**: 2026-2030
- **Conservative estimates**: 2060-2100 or never

**Risk levels:**
- **P(doom) estimates range from**: 1% to 90% depending on who you ask
- **Median among AI safety researchers**: Perhaps 10-30%
- **Many mainstream ML researchers**: Much lower, often <5%

**Resources:**
- **AI safety researchers worldwide**: Roughly 300-1000 FTE
- **Annual AI safety funding**: Approximately $100-500M
- **Frontier lab safety teams**: Combined perhaps $50-200M annually
- **For comparison, total AI investment**: $100B+ annually

**Capabilities:**
- **GPT-4 training cost**: Estimated $50-100M
- **Next-gen models**: May cost $1B+ to train
- **Compute doubling time**: Roughly every 6-10 months for frontier models

These numbers are rough estimates with significant uncertainty, but they help calibrate the scale of the challenge and the response.

## The Stakes

The ultimate reason to care about AI safety is captured in a simple question: What happens to humanity if we create intelligence far beyond our own?

**Optimistic scenario**: We successfully align advanced AI with human values. AI helps us cure diseases, solve scientific mysteries, coordinate globally, and flourish in ways we can barely imagine. This could be the best thing to ever happen to humanity.

**Pessimistic scenario**: We fail to solve alignment. Advanced AI pursues goals misaligned with human welfare. Given sufficient capability and misalignment, the result could be human extinction or permanent loss of human agency. This could be the worst thing to ever happen to humanity.

**Middle scenarios**: We muddle through. AI brings enormous disruption—economic, political, social—but humanity adapts. Some risks materialize but aren't existential. We solve some safety problems but not others. The outcome is mixed and uncertain.

The key insight is that the stakes are extremely high. Getting AI safety right could mean the difference between humanity's greatest achievement and its final mistake.

That's why this field exists. That's why this wiki exists. And that's why, if you're reading this, it's worth taking the time to understand the arguments, evaluate the evidence, and consider what you might contribute.

## Where to Go Next

Ready to dive deeper? Here are the recommended next steps:

**For newcomers:**
1. Read the [FAQ](/knowledge-base/resources/faq/) for quick answers to common questions
2. Work through the [Core Argument](/understanding-ai-risk/core-argument/) to understand the basic case
3. Explore [Key Risks](/knowledge-base/risks/) to see specific failure modes
4. Check out [Historical Scenarios](/analysis/scenarios/) to make it concrete

**For those evaluating the field:**
1. Read [Key Debates](/understanding-ai-risk/debates/) to see the strongest arguments on different sides
2. Examine [Different Worldviews](/understanding-ai-risk/worldviews/) to understand the range of positions
3. Review [Case Studies](/analysis/case-studies/) from analogous technologies
4. Consider [Critical Uncertainties](/understanding-ai-risk/models/critical-uncertainties/) that drive disagreements

**For those ready to contribute:**
1. Explore [Interventions](/knowledge-base/interventions/) to see what kinds of work exist
2. Check out [Organizations](/knowledge-base/organizations/) to understand who's working on what
3. Review [Career Paths](/knowledge-base/resources/careers/) if you're considering working in the field
4. Read about [Funding](/knowledge-base/resources/funding/) if you're starting a project

The rest of this "Getting Started" section provides curated reading paths for different audiences:
- [For Newcomers](/getting-started/for-newcomers/) - If you're completely new to AI safety
- [For Researchers](/getting-started/for-researchers/) - If you're a technical researcher considering AI safety work
- [For Policymakers](/getting-started/for-policymakers/) - If you're working in policy or governance

Choose your path and begin exploring.

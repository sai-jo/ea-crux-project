---
title: Start Here
description: A guide to navigating this AI safety wiki
sidebar:
  label: Start Here
  order: 0
---

This wiki maps the landscape of AI existential risk—the arguments, key uncertainties, organizations, and interventions. Here's how to navigate it.

## Core Framework: Key Parameters

The wiki is organized around **[Key Parameters](/knowledge-base/ai-transition-model/)**—foundational variables that AI development affects in both directions. This framework connects:

- **[Risks](/knowledge-base/risks/)** — What decreases these parameters (51 documented risks)
- **[Responses](/knowledge-base/responses/)** — What increases or protects them (technical & governance approaches)

Parameters include things like alignment robustness, racing intensity, societal trust, and human agency. Start with the [Parameters overview](/knowledge-base/ai-transition-model/) to understand the analytical framework.

## Main Sections

| Section | What's There |
|---------|--------------|
| **[Key Parameters](/knowledge-base/ai-transition-model/)** | 22 foundational variables with trends, risks, and interventions |
| **[Risks](/knowledge-base/risks/)** | Accident, misuse, structural, and epistemic risks |
| **[Responses](/knowledge-base/responses/)** | Technical alignment approaches and governance interventions |
| **[Organizations](/knowledge-base/organizations/)** | Frontier labs, safety research orgs, government bodies |
| **[People](/knowledge-base/people/)** | Key researchers and their positions |
| **[Key Debates](/knowledge-base/debates/)** | Structured arguments on contested questions |
| **[Cruxes](/knowledge-base/cruxes/)** | 53 key uncertainties driving disagreements |

## Quick Paths

**Want to understand the risk argument?**
→ [Core Argument](/understanding-ai-risk/core-argument/) breaks down the case into key claims

**Want to see what can be done?**
→ [Responses](/knowledge-base/responses/) covers technical and governance approaches

**Want to understand disagreements?**
→ [Key Debates](/knowledge-base/debates/) presents strongest arguments on each side

**Want data and estimates?**
→ [Key Metrics](/knowledge-base/metrics/) has forecasts and measurements

## Key Numbers

| Question | Range |
|----------|-------|
| P(transformative AI by 2040) | 40-80% |
| P(doom) estimates | 5-90% |
| AI safety researchers | ~300-1000 FTE |
| Annual safety funding | ~$100-500M |

## This Wiki's Perspective

This wiki was created within the AI safety community and reflects that perspective. It:

- Maps arguments, organizations, and research in the field
- Presents the range of views *within* AI safety
- Uses the Key Parameters framework to connect risks and responses
- Does not claim neutrality—see the [About page](/about/) for limitations

If you're skeptical of the AI safety framing, this wiki can help you understand what researchers believe and why.

## Browse

- **[Knowledge Base](/knowledge-base/)** — All categories
- **[All Entities](/browse/)** — Searchable database
- **[Entity Graph](/dashboard/graph/)** — Visual relationships

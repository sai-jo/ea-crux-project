---
title: Decision Guide
description: What matters for your specific situation and decisions
sidebar:
  order: 1
---

import { InfoBox, KeyQuestions, EstimateBox, Section, Tags } from '../../../components/wiki';

<InfoBox
  type="guide"
  title="Decision-Relevant Guide"
  customFields={[
    { label: "Purpose", value: "Help you decide what to do, not just what to believe" },
    { label: "Approach", value: "Different guidance for different situations" },
  ]}
/>

This page cuts through the abstract debates to answer: **Given who you are and what decisions you face, what actually matters?**

The AI safety landscape is complex, but most people don't need to resolve every uncertainty. They need to know what's decision-relevant for their specific situation.

---

## For CS Students Considering AI Safety Research

### Your Key Decision
Should you pursue AI safety research as a career? If so, what kind?

### What You Need to Believe

| Belief | Threshold for AI Safety Career | Your Estimate |
|--------|-------------------------------|---------------|
| P(TAI by 2045) | >30% makes safety urgent | ___% |
| P(Alignment is hard enough to need dedicated researchers) | >40% | ___% |
| P(Your work could matter) | >5% counterfactual impact | ___% |

### What Doesn't Matter Much for This Decision
- Exact P(doom) estimates (anywhere from 5-50% justifies working on it)
- Precise timeline predictions (anything under 30 years = urgent)
- Which specific risk is largest (all need work)

### Key Questions to Resolve
1. **Do you have comparative advantage?** AI safety needs people who can do technical ML research, not just people who care about safety.
2. **Empirical vs. theoretical?** Your personality and skills matter more than which approach is "correct."
3. **Lab vs. independent?** Frontier labs offer resources and access; independent orgs offer freedom and arguably less complicity in capability acceleration.

### Decision-Relevant Evidence to Watch
- Are safety techniques actually working at scale? (Track RLHF/CAI performance on harder tasks)
- Is interpretability making real progress? (Track whether we can explain model behaviors)
- Are labs actually using safety research? (Track adoption of techniques)

### Recommended Actions Under Uncertainty
- **High confidence safety matters**: Apply to Anthropic, DeepMind safety teams, or safety orgs like ARC, Redwood
- **Uncertain but interested**: Do a summer research program (MATS, SERI MATS) to test fit
- **Skeptical but hedging**: Build ML skills that transfer; keep options open
- **Very skeptical**: Work on AI capabilities at a safety-conscious lab, or other high-impact areas

### Resources
- [80,000 Hours AI Safety Career Guide](https://80000hours.org/problem-profiles/artificial-intelligence/)
- [AI Safety Camp](https://aisafety.camp/)
- [MATS Program](https://www.matsprogram.org/)

---

## For Policymakers and Regulators

### Your Key Decisions
- Should AI be regulated, and how stringently?
- What specific interventions (compute governance, licensing, liability) make sense?
- How should you weigh industry input vs. safety concerns?

### What You Need to Believe

| Belief | Threshold for Strong Regulation | Your Estimate |
|--------|--------------------------------|---------------|
| P(AI poses serious risks in next 10 years) | >20% | ___% |
| P(Industry will self-regulate adequately) | under 50% | ___% |
| P(Regulation can be effective without killing innovation) | >30% | ___% |

### What Doesn't Matter Much for This Decision
- Exact x-risk probabilities (even 5% catastrophic risk justifies attention)
- Theoretical debates about mesa-optimization (focus on observable risks)
- Long-term AGI scenarios (focus on near-term governable systems)

### Key Questions to Resolve
1. **What's actually regulatable?** Compute is measurable; "dangerous capabilities" are harder to define.
2. **What's the counterfactual?** If you don't regulate, what happens? If you do, does development just move elsewhere?
3. **Who do you trust?** Industry has expertise but conflicts of interest. Academics may lack practical knowledge. Safety advocates may overstate risks.

### Decision-Relevant Evidence to Watch
- Concrete AI incidents and near-misses (not hypotheticals)
- Whether voluntary commitments (RSPs, etc.) are actually followed
- International coordination feasibility (AI summits, bilateral agreements)
- Economic impacts of AI deployment (automation, labor markets)

### Recommended Actions Under Uncertainty
- **Regardless of x-risk views**: Require incident reporting, fund safety research, establish evaluation standards
- **If moderately concerned**: Compute thresholds for oversight, pre-deployment testing requirements, liability frameworks
- **If very concerned**: Licensing regimes, international coordination on limits, compute governance
- **If skeptical of x-risk**: Focus on near-term harms (bias, labor, surveillance) which are certain

### Key Caution
The AI safety community and AI industry both have incentives to influence you. Safety community may overstate risks. Industry may understate them. Seek independent technical expertise and be skeptical of confident claims from either side.

### Resources
- [GovAI Research](https://www.governance.ai/)
- [CSET at Georgetown](https://cset.georgetown.edu/)
- [AI Now Institute](https://ainowinstitute.org/) (alternative perspective)

---

## For AI Researchers at Frontier Labs

### Your Key Decisions
- Should you work on capabilities, safety, or both?
- How should you weigh your employer's commercial interests vs. safety?
- When (if ever) should you refuse to work on something or leave?

### What You Need to Believe

| Question | Decision Threshold |
|----------|-------------------|
| Is my lab net-positive for the world? | Must believe >50% yes to stay |
| Am I contributing more to safety than capabilities? | Ideal, but not always possible |
| Would safety work happen without me? | If yes, your counterfactual impact is lower |

### What Doesn't Matter Much for This Decision
- Abstract philosophical debates about AI consciousness
- Precise P(doom) estimates
- Whether "alignment" is the right frame

### Key Questions to Resolve
1. **Counterfactual impact**: If you left, would someone worse replace you? Would safety work slow down?
2. **Inside vs. outside game**: Can you do more good influencing from inside, or advocating from outside?
3. **Red lines**: What would your employer have to do for you to leave? Have you thought about this in advance?

### Decision-Relevant Evidence to Watch
- Is your lab's safety team actually influencing deployment decisions?
- Are safety concerns being overridden by commercial pressure?
- Are you personally becoming desensitized to risks?
- Is the broader safety ecosystem healthy or dependent on your lab?

### Recommended Actions Under Uncertainty
- **If you trust your lab's leadership**: Stay, push for safety internally, build relationships with safety team
- **If you're uncertain**: Set explicit red lines now (before you're emotionally invested); document concerns in writing
- **If you're concerned**: Consider whether you can do more good elsewhere; talk to people who've left
- **If you see serious problems**: Consider whether to raise internally, leave quietly, or blow the whistle (each has tradeoffs)

### Key Caution
Working at a frontier lab creates psychological pressure toward believing your work is good. This is human nature, not a character flaw. Build in external reality checks—friends outside the industry, periodic reflection on your red lines.

### Resources
- [Anthropic's Core Views on AI Safety](https://www.anthropic.com/news/core-views-on-ai-safety)
- [Discussion of "Working at AI Labs"](/understanding-ai-risk/debates/working-at-labs/)

---

## For Funders and Grantmakers

### Your Key Decisions
- How much of your portfolio should go to AI safety vs. other causes?
- Within AI safety, which approaches and organizations to fund?
- How to evaluate impact in a field with long feedback loops?

### What You Need to Believe

| Belief | Implication for Funding |
|--------|------------------------|
| P(AI is most important issue) × P(Funding helps) | Determines portfolio allocation |
| P(Technical research > governance) | Affects org selection |
| P(Neglected areas need funding) | May favor less popular approaches |

### Key Questions to Resolve
1. **Worldview diversification**: Should you fund based on your best guess, or spread across worldviews?
2. **Talent vs. ideas**: Is the bottleneck finding good researchers or having good research directions?
3. **Timelines**: Short timelines favor applied work; long timelines favor foundations.

### Decision-Relevant Evidence to Watch
- Are funded projects producing usable outputs?
- Is the field becoming more or less healthy? (Diversity of views, empirical grounding)
- Are you funding the same people/orgs everyone else is? (Neglectedness check)
- What do skeptics say about funded work?

### Recommended Portfolio Under Uncertainty

| Allocation | If Short Timelines | If Long Timelines | If Uncertain |
|------------|-------------------|-------------------|--------------|
| Technical safety research | 40% | 30% | 35% |
| Governance and policy | 30% | 25% | 25% |
| Field-building and talent | 15% | 25% | 20% |
| Alternative perspectives / red teams | 10% | 15% | 15% |
| Speculative / high-risk bets | 5% | 5% | 5% |

### Key Caution
The AI safety funding ecosystem is small and has herd behavior. If everyone funds the same approaches, you may get false confidence from agreement. Deliberately fund contrary views and red teams, even if you disagree with them.

### Resources
- [Open Philanthropy AI Safety Grantmaking](https://www.openphilanthropy.org/focus/potential-risks-advanced-artificial-intelligence/)
- [Survival and Flourishing Fund](https://survivalandflourishing.fund/)

---

## For Journalists Covering AI

### Your Key Decisions
- How to cover AI risks without either dismissing them or being alarmist?
- Which sources to trust when experts disagree?
- How to convey uncertainty to readers?

### What You Need to Understand

| Concept | Why It Matters for Coverage |
|---------|---------------------------|
| Wide expert disagreement | Avoid presenting any view as consensus |
| Incentive landscapes | Labs, safety orgs, academics all have biases |
| Uncertainty vs. ignorance | "We don't know" is different from "it's unknowable" |

### Key Questions to Resolve
1. **Who's worth quoting?** Technical credentials matter, but so does conflict of interest.
2. **What's the news peg?** Speculative risks are less newsworthy than concrete developments.
3. **How to convey probability?** "10% chance of catastrophe" sounds both scary and unlikely.

### Decision-Relevant Evidence to Watch
- Concrete AI capabilities and incidents (not projections)
- Actions by governments and companies (not just statements)
- Disagreements within the AI safety community (not just vs. skeptics)

### Recommended Practices Under Uncertainty
- **Always do**: Quote people on multiple sides; ask about confidence levels; distinguish predictions from values
- **Avoid**: Treating any expert as definitive; false balance (some views are more supported than others); X-risk framing for every AI story
- **Consider**: Whether the AI safety community is a good source on its own importance (conflict of interest)

### Balanced Source List
| Perspective | Example Sources |
|-------------|----------------|
| AI safety concerned | Yoshua Bengio, Stuart Russell, Anthropic researchers |
| AI safety skeptical | Yann LeCun, Gary Marcus, Melanie Mitchell |
| AI ethics focus | Timnit Gebru, Emily Bender, AI Now Institute |
| Industry | Lab spokespeople (note conflicts of interest) |
| Academic ML | NeurIPS/ICML researchers without safety focus |

### Key Caution
The AI safety community is media-savvy and provides compelling narratives. Skeptics are often less organized and quotable. This creates systematic bias toward alarming coverage. Actively seek out skeptical voices.

---

## General Decision-Making Under AI Uncertainty

Whatever your situation, some principles apply:

### 1. Identify Your Cruxes
What would have to be true for you to change your mind? If you can't answer this, you're not reasoning—you're rationalizing.

### 2. Diversify Across Worldviews
If you're uncertain whether AI risk is high or low, take some actions that make sense in both worlds. Avoid betting everything on one scenario.

### 3. Set Tripwires in Advance
Decide now what evidence would change your behavior. "If I see X, I will do Y." This prevents rationalization in the moment.

### 4. Seek Disconfirmation
Actively look for evidence against your current view. Talk to smart people who disagree with you. If you can't steelman the other side, you don't understand the issue.

### 5. Beware Social Proof
AI safety is a community with strong social dynamics. Agreement doesn't equal correctness. The fact that many smart people believe something is weak evidence at best.

### 6. Maintain Option Value
When uncertain, prefer reversible actions. Build skills that transfer across scenarios. Don't burn bridges based on predictions that might be wrong.

<Section title="Related Pages">
  <Tags tags={[
    "Career",
    "Decision-making",
    "AI Safety",
    "Uncertainty",
    "Governance",
  ]} />
</Section>

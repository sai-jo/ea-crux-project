---
title: AI Safety Wiki
description: A comprehensive guide to AI existential risk and safety
template: splash
hero:
  tagline: Understanding AI risk, alignment, and the path to safe artificial intelligence
  actions:
    - text: Start Here
      link: /getting-started/
      icon: right-arrow
    - text: Knowledge Base
      link: /knowledge-base/
      icon: information
      variant: minimal
---
import {R} from '../../components/wiki';


import { Card, CardGrid } from '@astrojs/starlight/components';

## What Is This?

This wiki maps the **landscape of AI existential risk**â€”the arguments, disagreements, organizations, and interventions that matter for ensuring advanced AI goes well for humanity.

Whether you're new to AI safety or a researcher looking for a comprehensive reference, this site aims to be your guide.

---

## Core Content

<CardGrid>
  <Card title="ðŸŽ¯ Understanding AI Risk" icon="warning">
    The [core argument](/understanding-ai-risk/core-argument/) for why AI might pose existential risk, broken into key claims and cruxes.

    - [Will AI be transformatively powerful?](/understanding-ai-risk/core-argument/capabilities/)
    - [How hard is alignment?](/understanding-ai-risk/core-argument/alignment-difficulty/)
    - [Will we get warning signs?](/understanding-ai-risk/core-argument/warning-signs/)
  </Card>

  <Card title="âš–ï¸ Key Debates" icon="comment-alt">
    Structured arguments on contested questions:

    - [Is AI x-risk real?](/knowledge-base/debates/is-ai-xrisk-real/)
    - [The pause debate](/knowledge-base/debates/pause-debate/)
    - [Open vs closed source](/knowledge-base/debates/open-vs-closed/)
  </Card>

  <Card title="ðŸ¢ Organizations" icon="seti:folder">
    Profiles of key players:

    - [Frontier labs](/knowledge-base/organizations/) (Anthropic, OpenAI, DeepMind)
    - [Safety research orgs](/knowledge-base/organizations/) (MIRI, ARC, Redwood)
    - [Funders and governance](/knowledge-base/funders/)
  </Card>

  <Card title="ðŸ›¡ï¸ Safety Approaches" icon="approve-check">
    Technical and governance solutions:

    - [Technical approaches](/knowledge-base/responses/alignment/) (interpretability, RLHF, AI control)
    - [Governance](/knowledge-base/responses/governance/) (compute governance, international coordination)
    - [Research agendas compared](/knowledge-base/responses/alignment/research-agendas/)
  </Card>

  <Card title="ðŸ“ Key Parameters" icon="setting">
    Foundational variables AI affects in both directions:

    - [All parameters](/knowledge-base/ai-transition-model/) (alignment robustness, racing intensity, societal trust)
    - A symmetric framework connecting [risks](/knowledge-base/risks/) and [interventions](/knowledge-base/responses/)
    - Track what matters, not just what could go wrong
  </Card>
</CardGrid>

---

## Analysis & Visualization

<CardGrid>
  <Card title="ðŸ“Š History" icon="seti:clock">
    [AI safety timeline](/knowledge-base/history/) from the Dartmouth conference to the present day.
  </Card>

  <Card title="ðŸ—ºï¸ Entity Graph" icon="seti:pipeline">
    [Visual dependency graph](/dashboard/graph/) showing how entities connect.
  </Card>

  <Card title="ðŸ”® Scenarios" icon="puzzle">
    [Future scenarios](/knowledge-base/scenarios/) for how AI development might unfold.
  </Card>

  <Card title="ðŸ“š Knowledge Base" icon="document">
    [Browse all entries](/knowledge-base/) â€” risks, responses, organizations, people, and cruxes.
  </Card>
</CardGrid>

---

## Learning Paths

**New to AI safety?** Start with:
1. [Introduction](/getting-started/) - What is AI safety and why it matters
2. [Core argument overview](/understanding-ai-risk/core-argument/)
3. [Key Parameters](/knowledge-base/ai-transition-model/) - The variables that matter

**Want to contribute?** Explore:
1. [Intervention analysis](/knowledge-base/responses/) - What can you do?
2. [Funding landscape](/knowledge-base/funders/) - How to get funded
3. [Organizations](/knowledge-base/organizations/) - Who's working on this

**Looking for depth?** Try:
1. [Key Parameters](/knowledge-base/ai-transition-model/) - The variables that matter for AI outcomes
2. [Key debates](/knowledge-base/debates/) - Strongest arguments on each side
3. [Research agendas](/knowledge-base/responses/alignment/research-agendas/) - Compare approaches

---

## Key Numbers

| Question | Estimates |
|----------|-----------|
| **P(transformative AI by 2040)** | 40-80% (varies by source) |
| **P(doom) estimates** | 5-90% (wide disagreement) |
| **AI safety researchers** | ~300-1000 FTE |
| **Annual safety funding** | ~\$100-500M |
| **Frontier lab safety spend** | ~\$50-200M combined |

See the [dashboard](/dashboard/) for more details.

---

## Featured People

Key voices in AI safety:

- **[Dario Amodei](/knowledge-base/people/dario-amodei/)** - Anthropic CEO, "10-25% doom"
- **[Eliezer Yudkowsky](/knowledge-base/people/eliezer-yudkowsky/)** - MIRI, most pessimistic public voice
- **[Paul Christiano](/knowledge-base/people/paul-christiano/)** - ARC founder, influential alignment researcher
- **[Geoffrey Hinton](/knowledge-base/people/geoffrey-hinton/)** - "Godfather of AI", recent safety advocate
- **[Stuart Russell](/knowledge-base/people/stuart-russell/)** - UC Berkeley, *Human Compatible* author

[See all researchers â†’](/knowledge-base/people/)

---

## This Wiki Is...

âœ… **Comprehensive** â€” Covers technical, governance, and strategic perspectives

âœ… **Structured** â€” Organized by cruxes, not just topics

âœ… **Parameter-oriented** â€” Tracks foundational variables, not just risks

âœ… **Interactive** â€” Timeline, risk maps, argument maps

âœ… **Practical** â€” Career and funding guidance

---

## Limitations & Perspective

**This wiki is not neutral.** It was created within the AI safety community and reflects that perspective. While we strive to present counterarguments fairly, readers should be aware:

**What this wiki does well:**
- Steelmans the case for AI existential risk
- Maps the landscape of AI safety arguments, organizations, and research
- Presents the range of views *within* the AI safety community

**What this wiki does less well:**
- Representing perspectives that reject the x-risk framing entirely
- Engaging deeply with AI ethics/fairness concerns (often dismissed as "near-term")
- Covering non-Western perspectives on AI development
- Quantifying uncertainty honestly (probability estimates should be treated as rough intuitions)

**Key assumptions embedded in this wiki:**
- That "existential risk" is a coherent concept for AI
- That the theoretical arguments for concern (orthogonality, instrumental convergence) are basically sound
- That the AI safety community's research agenda is on the right track

**If you're skeptical of these assumptions**, this wiki may still be useful for understanding what AI safety researchers believe and whyâ€”but you should seek out alternative perspectives as well.

**Recommended alternative viewpoints:**
- <R id="9b1ab7f63e6b1b35">Gary Marcus's Substack</R> â€” AI skepticism
- <R id="81595c2c950080a6">Timnit Gebru et al.'s work</R> â€” AI ethics perspective
- <R id="4ca01f329c8b25a4">Yann LeCun's posts</R> â€” Skepticism of AGI/x-risk framing
- <R id="10202ee006b2ebdf">Emily Bender's work</R> â€” Linguistic critique of LLM capabilities

**[Read our full transparency statement â†’](/about/)**

---

## Contributing

This is an open project. Key areas where contributions would be valuable:
- Adding researcher profiles
- Updating organization pages
- Improving argument maps
- Adding sources and citations

---

<CardGrid>
  <Card title="Explore by Topic" icon="magnifier">
    Browse the sidebar to explore specific topics, risks, and organizations.
  </Card>

  <Card title="Start Learning" icon="open-book">
    [Begin with the introduction â†’](/getting-started/)
  </Card>
</CardGrid>

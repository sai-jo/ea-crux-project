---
title: AI Safety Wiki
description: A comprehensive guide to AI existential risk and safety
template: splash
hero:
  tagline: Understanding AI risk, alignment, and the path to safe artificial intelligence
  actions:
    - text: Start Here
      link: /understanding-ai-risk/core-argument/
      icon: right-arrow
    - text: FAQ
      link: /resources/faq/
      icon: information
      variant: minimal
---

import { Card, CardGrid } from '@astrojs/starlight/components';

## What Is This?

This wiki maps the **landscape of AI existential risk**â€”the arguments, disagreements, organizations, and interventions that matter for ensuring advanced AI goes well for humanity.

Whether you're new to AI safety or a researcher looking for a comprehensive reference, this site aims to be your guide.

---

## Core Content

<CardGrid>
  <Card title="ðŸŽ¯ Understanding AI Risk" icon="warning">
    The [core argument](/understanding-ai-risk/core-argument/) for why AI might pose existential risk, broken into key claims and cruxes.

    - [Will AI be transformatively powerful?](/understanding-ai-risk/core-argument/capabilities/)
    - [How hard is alignment?](/understanding-ai-risk/core-argument/alignment-difficulty/)
    - [Will we get warning signs?](/understanding-ai-risk/core-argument/warning-signs/)
  </Card>

  <Card title="âš–ï¸ Key Debates" icon="comment-alt">
    Structured arguments on contested questions:

    - [Is AI x-risk real?](/understanding-ai-risk/debates/is-ai-xrisk-real/)
    - [Pause vs. accelerate?](/understanding-ai-risk/debates/pause-vs-accelerate/)
    - [Working at AI labs?](/understanding-ai-risk/debates/working-at-labs/)
  </Card>

  <Card title="ðŸ¢ Organizations" icon="seti:folder">
    Profiles of key players:

    - [Frontier labs](/knowledge-base/organizations/) (Anthropic, OpenAI, DeepMind)
    - [Safety research orgs](/knowledge-base/organizations/) (MIRI, ARC, Redwood)
    - [Funders and governance](/knowledge-base/resources/funding/)
  </Card>

  <Card title="ðŸ›¡ï¸ Safety Approaches" icon="approve-check">
    Technical and governance solutions:

    - [Technical approaches](/knowledge-base/safety-approaches/technical/) (interpretability, RLHF, AI control)
    - [Governance](/knowledge-base/safety-approaches/governance/) (compute governance, international coordination)
    - [Research agendas compared](/knowledge-base/research-agendas/)
  </Card>
</CardGrid>

---

## Analysis & Visualization

<CardGrid>
  <Card title="ðŸ“Š Timeline" icon="seti:clock">
    [Interactive AI timeline](/analysis/ai-timeline/) from the Dartmouth conference to near-term predictions.
  </Card>

  <Card title="ðŸ—ºï¸ Risk Map" icon="seti:pipeline">
    [Visual dependency graph](/analysis/risk-map/) showing how risk factors connect.
  </Card>

  <Card title="ðŸ“ˆ Comparisons" icon="list-format">
    [Side-by-side tables](/analysis/comparisons/) comparing labs, approaches, and expert positions.
  </Card>

  <Card title="ðŸ”® Scenarios" icon="puzzle">
    [Future scenarios](/analysis/scenarios/) for how AI development might unfold.
  </Card>
</CardGrid>

---

## Learning Paths

**New to AI safety?** Start with:
1. [FAQ: Common questions](/knowledge-base/resources/faq/)
2. [Core argument overview](/understanding-ai-risk/core-argument/)
3. [Glossary of terms](/knowledge-base/resources/glossary/)

**Want to contribute?** Explore:
1. [Intervention analysis](/knowledge-base/interventions/) - What can you do?
2. [Funding landscape](/knowledge-base/resources/funding/) - How to get funded
3. [Organizations](/knowledge-base/organizations/) - Who's working on this

**Looking for depth?** Try:
1. [Key debates](/understanding-ai-risk/debates/) - Strongest arguments on each side
2. [Research agendas](/knowledge-base/research-agendas/) - Compare approaches
3. [Historical case studies](/analysis/case-studies/) - Lessons from other risks

---

## Key Numbers

| Question | Estimates |
|----------|-----------|
| **P(transformative AI by 2040)** | 40-80% (varies by source) |
| **P(doom) estimates** | 5-90% (wide disagreement) |
| **AI safety researchers** | ~300-1000 FTE |
| **Annual safety funding** | ~$100-500M |
| **Frontier lab safety spend** | ~$50-200M combined |

See [estimates dashboard](/analysis/estimates-dashboard/) for detailed breakdowns.

---

## Featured People

Key voices in AI safety:

- **[Dario Amodei](/knowledge-base/people/dario-amodei/)** - Anthropic CEO, "10-25% doom"
- **[Eliezer Yudkowsky](/knowledge-base/people/eliezer-yudkowsky/)** - MIRI, most pessimistic public voice
- **[Paul Christiano](/knowledge-base/people/paul-christiano/)** - ARC founder, influential alignment researcher
- **[Geoffrey Hinton](/knowledge-base/people/geoffrey-hinton/)** - "Godfather of AI", recent safety advocate
- **[Stuart Russell](/knowledge-base/people/stuart-russell/)** - UC Berkeley, *Human Compatible* author

[See all researchers â†’](/knowledge-base/people/)

---

## This Wiki Is...

âœ… **Comprehensive** â€” Covers technical, governance, and strategic perspectives

âœ… **Structured** â€” Organized by cruxes, not just topics

âœ… **Interactive** â€” Timeline, risk maps, argument maps

âœ… **Practical** â€” Career and funding guidance

---

## Limitations & Perspective

**This wiki is not neutral.** It was created within the AI safety community and reflects that perspective. While we strive to present counterarguments fairly, readers should be aware:

**What this wiki does well:**
- Steelmans the case for AI existential risk
- Maps the landscape of AI safety arguments, organizations, and research
- Presents the range of views *within* the AI safety community

**What this wiki does less well:**
- Representing perspectives that reject the x-risk framing entirely
- Engaging deeply with AI ethics/fairness concerns (often dismissed as "near-term")
- Covering non-Western perspectives on AI development
- Quantifying uncertainty honestly (probability estimates should be treated as rough intuitions)

**Key assumptions embedded in this wiki:**
- That "existential risk" is a coherent concept for AI
- That the theoretical arguments for concern (orthogonality, instrumental convergence) are basically sound
- That the AI safety community's research agenda is on the right track

**If you're skeptical of these assumptions**, this wiki may still be useful for understanding what AI safety researchers believe and whyâ€”but you should seek out alternative perspectives as well.

**Recommended alternative viewpoints:**
- [Gary Marcus's Substack](https://garymarcus.substack.com/) â€” AI skepticism
- [Timnit Gebru et al.'s work](https://www.dair-institute.org/) â€” AI ethics perspective
- [Yann LeCun's posts](https://twitter.com/ylecun) â€” Skepticism of AGI/x-risk framing
- [Emily Bender's work](https://faculty.washington.edu/ebender/) â€” Linguistic critique of LLM capabilities

**[Read our full transparency statement â†’](/about/)**

---

## Contributing

This is an open project. Key areas where contributions would be valuable:
- Adding researcher profiles
- Updating organization pages
- Improving argument maps
- Adding sources and citations

---

<CardGrid>
  <Card title="Explore by Topic" icon="magnifier">
    Browse the sidebar to explore specific topics, risks, and organizations.
  </Card>

  <Card title="Start Learning" icon="open-book">
    [Begin with the core argument â†’](/understanding-ai-risk/core-argument/)
  </Card>
</CardGrid>

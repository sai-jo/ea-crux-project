---
title: AI Transition Model
description: A causal model of how root factors shape ultimate scenarios and outcomes during the AI transition period.
sidebar:
  label: Overview
  order: 0
lastEdited: "2026-01-04"
---

import {ParameterFlowDiagram, ParametersTable} from '../../../../components/wiki';
import AITransitionModelLink from '../../../../components/AITransitionModelLink';

## Overview

The AI Transition Model is a causal framework for understanding how various factors influence the trajectory of AI development and its ultimate outcomes for humanity. It maps the relationships between:

- **Root Factors**: The underlying variables that shape AI-related scenarios
- **Ultimate Scenarios**: The pathways through which factors lead to outcomes
- **Ultimate Outcomes**: The final states we care about (catastrophe vs. positive trajectory)

This model helps identify:
1. **Leverage points**: Which factors have the most influence on outcomes
2. **Intervention targets**: Where effort can most effectively shift trajectories
3. **Key uncertainties**: Which causal relationships are most uncertain
4. **Scenario dependencies**: How different pathways interact

---

## Model Structure

The model flows through three levels: **Root Factors** → **Ultimate Scenarios** → **Ultimate Outcomes**.

<AITransitionModelLink client:load />

---

### Two Ultimate Outcomes

| Outcome | Question | Key Ultimate Scenarios |
|---------|----------|------------------------|
| Existential Catastrophe | Does civilization-ending harm occur? | AI Takeover, Human-Caused Catastrophe |
| Long-term Trajectory | What's the quality of the post-transition future? | AI Takeover, Long-term Lock-in |

### Three Ultimate Scenarios

Ultimate Scenarios are the intermediate pathways connecting root factors to ultimate outcomes—they describe *how* factor changes lead to catastrophe or shape the long-term future.

| Ultimate Scenario | Description | Key Root Factors | Ultimate Outcomes |
|-------------------|-------------|------------------|-------------------|
| [AI Takeover](/knowledge-base/ai-transition-model/scenarios/ai-takeover/) | AI gains decisive control (rapid or gradual) | Misalignment Potential ↑, AI Capabilities ↑, Civilizational Competence ↓ | Existential Catastrophe, Long-term Trajectory |
| [Human-Caused Catastrophe](/knowledge-base/ai-transition-model/scenarios/human-catastrophe/) | Humans use AI for mass harm (state or rogue actors) | Misuse Potential ↑, Transition Turbulence ↑, Civilizational Competence ↓ | Existential Catastrophe |
| [Long-term Lock-in](/knowledge-base/ai-transition-model/scenarios/long-term-lockin/) | Permanent entrenchment of values/power structures | Civilizational Competence shapes outcome | Long-term Trajectory |

### Seven Root Factors

Each root factor aggregates underlying parameters and influences ultimate scenarios:

| Root Factor | Description | Sub-components | Scenarios Influenced |
|-------------|-------------|----------------|---------------------|
| [Misalignment Potential](/knowledge-base/ai-transition-model/factors/misalignment-potential/) | Potential for AI to be misaligned | Technical AI Safety, AI Governance, Lab Safety Practices | AI Takeover ↑ |
| [AI Capabilities](/knowledge-base/ai-transition-model/factors/ai-capabilities/) | How powerful AI systems become | Compute, Algorithms, Adoption | AI Takeover ↑ |
| [AI Uses](/knowledge-base/ai-transition-model/factors/ai-uses/) | Where and how AI is deployed | Recursive AI, Industries, Governments, Coordination | All scenarios |
| [AI Ownership](/knowledge-base/ai-transition-model/factors/ai-ownership/) | Who controls powerful AI systems | Countries, Companies, Shareholders | Long-term Lock-in ↑ |
| [Civilizational Competence](/knowledge-base/ai-transition-model/factors/civilizational-competence/) | Humanity's ability to respond well | Governance, Epistemics, Adaptability | AI Takeover ↓, Human-Caused Catastrophe ↓, Lock-in (shapes) |
| [Transition Turbulence](/knowledge-base/ai-transition-model/factors/transition-turbulence/) | Background instability during transition | Economic Stability, Racing Intensity | AI Takeover ↑, Human-Caused Catastrophe ↑ |
| [Misuse Potential](/knowledge-base/ai-transition-model/factors/misuse-potential/) | Potential for AI to be misused for harm | Biological, Cyber, Robot, Surprise Threats | Human-Caused Catastrophe ↑ |

---

## How Parameters, Risks, and Interventions Connect

This diagram shows how both risks (red) and interventions (green) affect parameters (purple). Risks decrease parameter values while interventions increase them.

<ParameterFlowDiagram
  client:load
  height={450}
  nodes={[
    // RISKS (will be placed on left)
    { id: 'deceptive-alignment', title: 'Deceptive Alignment', type: 'risk', href: '/knowledge-base/risks/accident/deceptive-alignment/' },
    { id: 'reward-hacking', title: 'Reward Hacking', type: 'risk', href: '/knowledge-base/risks/accident/reward-hacking/' },
    { id: 'racing-dynamics', title: 'Racing Dynamics', type: 'risk', href: '/knowledge-base/risks/structural/racing-dynamics/' },
    { id: 'trust-decline', title: 'Trust Decline', type: 'risk', href: '/knowledge-base/risks/epistemic/trust-decline/' },
    { id: 'erosion-of-agency', title: 'Erosion of Agency', type: 'risk', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
    { id: 'bioweapons', title: 'AI Bioweapons', type: 'risk', href: '/knowledge-base/risks/misuse/bioweapons/' },

    // PARAMETERS (will be placed in center)
    { id: 'alignment-robustness', title: 'Alignment Robustness', type: 'parameter', href: '/knowledge-base/ai-transition-model/factors/misalignment-potential/' },
    { id: 'racing-intensity', title: 'Racing Intensity', type: 'parameter', href: '/knowledge-base/ai-transition-model/factors/transition-turbulence/' },
    { id: 'societal-trust', title: 'Societal Trust', type: 'parameter', href: '/knowledge-base/ai-transition-model/factors/civilizational-competence/' },
    { id: 'human-agency', title: 'Human Agency', type: 'parameter', href: '/knowledge-base/ai-transition-model/factors/civilizational-competence/' },
    { id: 'biological-threat-exposure', title: 'Biological Threat Exposure', type: 'parameter', href: '/knowledge-base/ai-transition-model/factors/misuse-potential/' },

    // INTERVENTIONS (will be placed on right)
    { id: 'interpretability', title: 'Interpretability', type: 'intervention', href: '/knowledge-base/responses/alignment/interpretability/' },
    { id: 'evals', title: 'AI Evaluations', type: 'intervention', href: '/knowledge-base/responses/alignment/evals/' },
    { id: 'compute-governance', title: 'Compute Governance', type: 'intervention', href: '/knowledge-base/responses/governance/compute-governance/' },
    { id: 'content-auth', title: 'Content Authentication', type: 'intervention', href: '/knowledge-base/responses/epistemic-tools/' },
    { id: 'scalable-oversight', title: 'Scalable Oversight', type: 'intervention', href: '/knowledge-base/responses/alignment/scalable-oversight/' },
    { id: 'dna-screening', title: 'DNA Screening', type: 'intervention', href: '/knowledge-base/responses/resilience/' },
  ]}
  edges={[
    // Risks → Parameters
    { from: 'deceptive-alignment', to: 'alignment-robustness', label: 'threatens' },
    { from: 'reward-hacking', to: 'alignment-robustness', label: 'threatens' },
    { from: 'racing-dynamics', to: 'racing-intensity', label: 'increases' },
    { from: 'trust-decline', to: 'societal-trust', label: 'erodes' },
    { from: 'erosion-of-agency', to: 'human-agency', label: 'reduces' },
    { from: 'bioweapons', to: 'biological-threat-exposure', label: 'threatens' },

    // Interventions → Parameters (interventions improve/protect parameters)
    { from: 'interpretability', to: 'alignment-robustness', label: 'improves' },
    { from: 'evals', to: 'alignment-robustness', label: 'measures' },
    { from: 'compute-governance', to: 'racing-intensity', label: 'moderates' },
    { from: 'content-auth', to: 'societal-trust', label: 'protects' },
    { from: 'scalable-oversight', to: 'human-agency', label: 'preserves' },
    { from: 'dna-screening', to: 'biological-threat-exposure', label: 'protects' },
  ]}
/>

---

## All Parameters

<ParametersTable
  client:load
  parameters={[
    // Alignment Parameters
    {
      id: 'alignment-robustness',
      title: 'Alignment Robustness',
      category: 'alignment',
      direction: 'higher',
      trend: 'Declining',
      importance: 88,
      tractability: 55,
      neglectedness: 65,
      uncertainty: 45,
      risks: [
        { id: 'reward-hacking', title: 'Reward Hacking', href: '/knowledge-base/risks/accident/reward-hacking/' },
        { id: 'deceptive-alignment', title: 'Deceptive Alignment', href: '/knowledge-base/risks/accident/deceptive-alignment/' },
      ],
      interventions: [
        { id: 'interpretability', title: 'Interpretability', href: '/knowledge-base/responses/alignment/interpretability/' },
        { id: 'evals', title: 'AI Evaluations', href: '/knowledge-base/responses/alignment/evals/' },
      ],
    },
    {
      id: 'safety-capability-gap',
      title: 'Safety-Capability Gap',
      category: 'alignment',
      direction: 'lower',
      trend: 'Widening',
      importance: 90,
      tractability: 40,
      neglectedness: 35,
      uncertainty: 40,
      risks: [
        { id: 'racing-dynamics', title: 'Racing Dynamics', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'pause', title: 'Development Pause', href: '/knowledge-base/responses/organizational-practices/pause/' },
        { id: 'safety-research', title: 'Safety Research', href: '/knowledge-base/responses/alignment/' },
      ],
    },
    {
      id: 'interpretability-coverage',
      title: 'Interpretability Coverage',
      category: 'alignment',
      direction: 'higher',
      trend: 'Improving slowly',
      importance: 85,
      tractability: 65,
      neglectedness: 70,
      uncertainty: 50,
      risks: [],
      interventions: [
        { id: 'interpretability', title: 'Mechanistic Interp', href: '/knowledge-base/responses/alignment/interpretability/' },
      ],
    },
    {
      id: 'human-oversight-quality',
      title: 'Human Oversight Quality',
      category: 'alignment',
      direction: 'higher',
      trend: 'Declining',
      importance: 82,
      tractability: 55,
      neglectedness: 50,
      uncertainty: 45,
      risks: [
        { id: 'automation-complacency', title: 'Automation Complacency', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
      ],
      interventions: [
        { id: 'scalable-oversight', title: 'Scalable Oversight', href: '/knowledge-base/responses/alignment/scalable-oversight/' },
      ],
    },
    {
      id: 'safety-culture-strength',
      title: 'Safety Culture Strength',
      category: 'alignment',
      direction: 'higher',
      trend: 'Mixed',
      importance: 80,
      tractability: 50,
      neglectedness: 75,
      uncertainty: 55,
      risks: [
        { id: 'racing-dynamics', title: 'Racing Dynamics', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'rsps', title: 'RSPs', href: '/knowledge-base/responses/governance/industry/responsible-scaling-policies/' },
        { id: 'evals', title: 'Safety Evals', href: '/knowledge-base/responses/alignment/evals/' },
      ],
    },
    // Governance Parameters
    {
      id: 'coordination-capacity',
      title: 'Coordination Capacity',
      category: 'governance',
      direction: 'higher',
      trend: 'Fragile',
      importance: 75,
      tractability: 40,
      neglectedness: 45,
      uncertainty: 55,
      risks: [
        { id: 'racing-dynamics', title: 'Racing Dynamics', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'voluntary-commitments', title: 'Voluntary Commitments', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'international-coordination',
      title: 'International Coordination',
      category: 'governance',
      direction: 'higher',
      trend: 'Mixed',
      importance: 80,
      tractability: 30,
      neglectedness: 50,
      uncertainty: 60,
      risks: [
        { id: 'geopolitical', title: 'Geopolitical Competition', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'aisi', title: 'AISI Network', href: '/knowledge-base/responses/governance/' },
        { id: 'treaties', title: 'AI Treaties', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'regulatory-capacity',
      title: 'Regulatory Capacity',
      category: 'governance',
      direction: 'higher',
      trend: 'Improving',
      importance: 75,
      tractability: 50,
      neglectedness: 45,
      uncertainty: 45,
      risks: [
        { id: 'regulatory-capture', title: 'Regulatory Capture', href: '/knowledge-base/risks/structural/' },
      ],
      interventions: [
        { id: 'expertise', title: 'Tech Expertise Building', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'institutional-quality',
      title: 'Institutional Quality',
      category: 'governance',
      direction: 'higher',
      trend: 'Mixed',
      importance: 72,
      tractability: 40,
      neglectedness: 40,
      uncertainty: 50,
      risks: [
        { id: 'capture', title: 'Capture & Politicization', href: '/knowledge-base/risks/structural/' },
      ],
      interventions: [
        { id: 'independence', title: 'Independence Safeguards', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'racing-intensity',
      title: 'Racing Intensity',
      category: 'governance',
      direction: 'lower',
      trend: 'Accelerating',
      importance: 85,
      tractability: 35,
      neglectedness: 55,
      uncertainty: 50,
      risks: [
        { id: 'racing-dynamics', title: 'Competition Pressure', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'compute-governance', title: 'Compute Governance', href: '/knowledge-base/responses/governance/compute-governance/' },
        { id: 'coordination', title: 'Coordination', href: '/knowledge-base/responses/governance/' },
      ],
    },
    // Societal Parameters
    {
      id: 'societal-trust',
      title: 'Societal Trust',
      category: 'societal',
      direction: 'higher',
      trend: 'Declining',
      importance: 65,
      tractability: 30,
      neglectedness: 30,
      uncertainty: 60,
      risks: [
        { id: 'trust-erosion', title: 'Trust Erosion', href: '/knowledge-base/risks/epistemic/trust-decline/' },
        { id: 'deepfakes', title: 'Deepfakes', href: '/knowledge-base/risks/epistemic/' },
      ],
      interventions: [
        { id: 'content-auth', title: 'Content Authentication', href: '/knowledge-base/responses/epistemic-tools/' },
      ],
    },
    {
      id: 'epistemic-health',
      title: 'Epistemic Health',
      category: 'societal',
      direction: 'higher',
      trend: 'Declining',
      importance: 78,
      tractability: 40,
      neglectedness: 55,
      uncertainty: 55,
      risks: [
        { id: 'misinformation', title: 'Misinformation', href: '/knowledge-base/risks/epistemic/' },
      ],
      interventions: [
        { id: 'media-literacy', title: 'Media Literacy', href: '/knowledge-base/responses/epistemic-tools/' },
      ],
    },
    {
      id: 'human-agency',
      title: 'Human Agency',
      category: 'societal',
      direction: 'higher',
      trend: 'Declining',
      importance: 60,
      tractability: 35,
      neglectedness: 60,
      uncertainty: 65,
      risks: [
        { id: 'erosion-of-agency', title: 'Erosion of Agency', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
      ],
      interventions: [
        { id: 'human-in-loop', title: 'Human-in-the-Loop', href: '/knowledge-base/responses/alignment/scalable-oversight/' },
      ],
    },
    {
      id: 'ai-control-concentration',
      title: 'AI Control Concentration',
      category: 'societal',
      direction: 'context',
      trend: 'Concentrating',
      importance: 75,
      tractability: 30,
      neglectedness: 60,
      uncertainty: 65,
      risks: [
        { id: 'concentration', title: 'Concentration of Power', href: '/knowledge-base/risks/structural/concentration-of-power/' },
      ],
      interventions: [
        { id: 'antitrust', title: 'Antitrust', href: '/knowledge-base/responses/governance/' },
        { id: 'open-source', title: 'Open Source', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'economic-stability',
      title: 'Economic Stability',
      category: 'societal',
      direction: 'higher',
      trend: 'Mixed',
      importance: 55,
      tractability: 35,
      neglectedness: 25,
      uncertainty: 70,
      risks: [
        { id: 'economic-disruption', title: 'Job Displacement', href: '/knowledge-base/risks/structural/economic-disruption/' },
      ],
      interventions: [
        { id: 'transition', title: 'Transition Support', href: '/knowledge-base/responses/governance/' },
      ],
    },
    // Resilience Parameters
    {
      id: 'societal-resilience',
      title: 'Societal Resilience',
      category: 'resilience',
      direction: 'higher',
      trend: 'Mixed',
      importance: 70,
      tractability: 45,
      neglectedness: 50,
      uncertainty: 55,
      risks: [
        { id: 'single-points', title: 'Single Points of Failure', href: '/knowledge-base/risks/structural/' },
      ],
      interventions: [
        { id: 'redundancy', title: 'Redundancy & Backups', href: '/knowledge-base/responses/' },
      ],
    },
    {
      id: 'biological-threat-exposure',
      title: 'Biological Threat Exposure',
      category: 'resilience',
      direction: 'lower',
      trend: 'Stressed',
      importance: 82,
      tractability: 55,
      neglectedness: 60,
      uncertainty: 55,
      risks: [
        { id: 'bioweapons', title: 'AI Bioweapons', href: '/knowledge-base/risks/misuse/bioweapons/' },
      ],
      interventions: [
        { id: 'dna-screening', title: 'DNA Screening', href: '/knowledge-base/responses/resilience/' },
      ],
    },
    {
      id: 'cyber-threat-exposure',
      title: 'Cyber Threat Exposure',
      category: 'resilience',
      direction: 'lower',
      trend: 'Stressed',
      importance: 75,
      tractability: 50,
      neglectedness: 30,
      uncertainty: 45,
      risks: [
        { id: 'cyberattacks', title: 'AI Cyberattacks', href: '/knowledge-base/risks/misuse/cyberweapons/' },
      ],
      interventions: [
        { id: 'ai-defense', title: 'AI-Powered Defense', href: '/knowledge-base/responses/' },
      ],
    },
    {
      id: 'information-authenticity',
      title: 'Information Authenticity',
      category: 'resilience',
      direction: 'higher',
      trend: 'Declining',
      importance: 72,
      tractability: 50,
      neglectedness: 45,
      uncertainty: 50,
      risks: [
        { id: 'deepfakes', title: 'Deepfakes', href: '/knowledge-base/risks/epistemic/' },
      ],
      interventions: [
        { id: 'provenance', title: 'Provenance', href: '/knowledge-base/responses/epistemic-tools/' },
        { id: 'watermarks', title: 'Watermarks', href: '/knowledge-base/responses/epistemic-tools/' },
      ],
    },
    {
      id: 'human-expertise',
      title: 'Human Expertise',
      category: 'resilience',
      direction: 'higher',
      trend: 'Declining',
      importance: 70,
      tractability: 45,
      neglectedness: 40,
      uncertainty: 50,
      risks: [
        { id: 'deskilling', title: 'Deskilling', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
      ],
      interventions: [
        { id: 'skill-maintenance', title: 'Skill Maintenance', href: '/knowledge-base/responses/' },
      ],
    },
  ]}
/>

---

## Why This Framing Matters

### Traditional Risk Framing
- "Trust erosion is a risk we must prevent"
- "Concentration of power threatens democracy"
- Focus: Avoiding negative outcomes

### Parameter Framing
- "Trust is a parameter that AI affects in both directions"
- "Power distribution is a variable we can influence through policy"
- Focus: Understanding dynamics and identifying intervention points

The parameter framing enables:

1. **Better modeling**: Can estimate current levels, trends, and intervention effects
2. **Clearer priorities**: Which parameters matter most for good outcomes?
3. **Strategic allocation**: Where should resources go to maintain critical parameters?
4. **Progress tracking**: Are our interventions actually improving parameter levels?

---

## Relationship to Other Sections

| Section | Relationship to Parameters |
|---------|---------------------------|
| **Risks** | Many risks describe *decreases* in parameters (e.g., "trust erosion" = trust declining) |
| **Interventions** | Interventions aim to *increase* or *stabilize* parameters |
| **Metrics** | Metrics are *concrete measurements* of parameter levels |
| **Models** | Analytical models often estimate parameter dynamics and trajectories |

---

## How to Use This Section

### For Researchers
- Understand which underlying variables matter for AI outcomes
- Identify gaps between current and optimal parameter levels
- Design studies to measure parameter changes

### For Policymakers
- Prioritize interventions based on which parameters are most degraded
- Monitor parameter trends to assess policy effectiveness
- Coordinate across domains (a single parameter may affect multiple risks)

### For Forecasters
- Use parameters as input variables for scenario modeling
- Estimate how different interventions would shift parameter levels
- Identify tipping points where parameter degradation becomes irreversible

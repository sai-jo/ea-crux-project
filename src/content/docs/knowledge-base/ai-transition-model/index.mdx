---
title: AI Transition Model
description: A causal model of how root factors shape ultimate scenarios and outcomes during the AI transition period.
sidebar:
  label: Overview
  order: 0
lastEdited: "2026-01-03"
---

import {Mermaid, ParameterFlowDiagram, ParametersTable} from '../../../../components/wiki';
import CauseEffectGraph from '../../../../components/CauseEffectGraph';

## Overview

The AI Transition Model is a causal framework for understanding how various factors influence the trajectory of AI development and its ultimate outcomes for humanity. It maps the relationships between:

- **Root Factors**: The underlying variables that shape AI-related scenarios
- **Ultimate Scenarios**: The pathways through which factors lead to outcomes
- **Ultimate Outcomes**: The final states we care about (catastrophe vs. positive trajectory)

This model helps identify:
1. **Leverage points**: Which factors have the most influence on outcomes
2. **Intervention targets**: Where effort can most effectively shift trajectories
3. **Key uncertainties**: Which causal relationships are most uncertain
4. **Scenario dependencies**: How different pathways interact

---

## Model Structure

The model flows through three levels:

```
Root Factors (5) → Ultimate Scenarios (3) → Ultimate Outcomes (2)
```

<Mermaid client:load chart={`
flowchart TD
    subgraph Outcomes["Ultimate Outcomes"]
        EXCAT[Existential Catastrophe]
        TRAJ[Long-term Trajectory]
    end

    subgraph Scenarios["Ultimate Scenarios"]
        TAKEOVER[AI Takeover]
        HUMAN[Human-Caused Catastrophe]
        LOCKIN[Long-term Lock-in]
    end

    subgraph RootFactors["Root Factors"]
        MISALIGN[Misalignment Potential]
        CAPS[AI Capabilities]
        CIV[Civilizational Competence]
        TURB[Transition Turbulence]
        MISUSE[Misuse Potential]
    end

    %% Root Factors to Ultimate Scenarios
    MISALIGN -->|"increases"| TAKEOVER
    CAPS -->|"increases"| TAKEOVER
    TURB -->|"increases"| TAKEOVER
    CIV -->|"decreases"| TAKEOVER

    MISUSE -->|"increases"| HUMAN
    TURB -->|"increases"| HUMAN
    CIV -->|"decreases"| HUMAN

    CIV -->|"shapes"| LOCKIN

    %% Ultimate Scenarios to Ultimate Outcomes
    TAKEOVER -->|"increases"| EXCAT
    HUMAN -->|"increases"| EXCAT
    TAKEOVER -->|"shapes"| TRAJ
    LOCKIN -->|"shapes"| TRAJ

    style EXCAT fill:#ff6b6b
    style TRAJ fill:#4ecdc4
    style TAKEOVER fill:#ffb6c1
    style HUMAN fill:#ffb6c1
    style LOCKIN fill:#ffe4b5
`} />

---

## Interactive Visualization

Explore the full parameter hierarchy interactively. Click any node to see details, hover to highlight connections. Use fullscreen (top-right) for better exploration.

<CauseEffectGraph
  client:load
  height={700}
  initialNodes={[
    // === LEAF PARAMETERS (grouped by target aggregate, left to right) ===
    // -- Technical Safety inputs --
    {
      id: 'alignment-robustness',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Alignment Robustness',
        description: 'How reliably AI systems pursue intended goals.',
        type: 'leaf',
        details: 'The degree to which AI systems remain aligned with human intentions under distribution shift, capability gains, and adversarial conditions.',
        relatedConcepts: ['Reward Hacking', 'Goal Misgeneralization', 'Deceptive Alignment'],
        sources: ['Anthropic Core Views', 'DeepMind Alignment']
      }
    },
    {
      id: 'interpretability-coverage',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Interpretability',
        description: 'How well we can understand AI internal processes.',
        type: 'leaf',
        details: 'Our ability to inspect, understand, and verify what AI systems are doing internally. Critical for detecting deception and misalignment.',
        relatedConcepts: ['Mechanistic Interpretability', 'Probing', 'Feature Visualization'],
        sources: ['Anthropic Interpretability', 'OpenAI Microscope']
      }
    },
    {
      id: 'safety-gap',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Safety-Capability Gap',
        description: 'How far safety lags behind capabilities.',
        type: 'leaf',
        details: 'The growing distance between what AI systems can do and our ability to ensure they do it safely. A widening gap increases risk.',
        relatedConcepts: ['Racing Dynamics', 'Safety Tax', 'Differential Progress'],
        sources: ['MIRI', 'FHI Research']
      }
    },
    {
      id: 'human-oversight',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Human Oversight Quality',
        description: 'Effectiveness of human supervision over AI systems.',
        type: 'leaf',
        details: 'How well humans can monitor, understand, and correct AI behavior. Degrades with AI complexity and speed, critical for preventing both takeover scenarios.',
        relatedConcepts: ['Scalable Oversight', 'Automation Complacency', 'Human-in-the-Loop'],
        sources: ['Amodei Scalable Oversight', 'Christiano Debate']
      }
    },
    {
      id: 'safety-culture',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Safety Culture',
        description: 'Organizational commitment to safety practices.',
        type: 'leaf',
        details: 'The degree to which AI labs and deployers prioritize safety, implement rigorous processes, and resist competitive pressure to cut corners.',
        relatedConcepts: ['RSPs', 'Safety Evals', 'Racing Dynamics'],
        sources: ['Anthropic RSP', 'METR Evals']
      }
    },
    {
      id: 'rsi-potential',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Recursive Self-Improvement',
        description: 'Potential for AI systems to improve themselves.',
        type: 'leaf',
        details: 'Whether AI can substantially improve its own capabilities, potentially leading to rapid capability gains. Key factor in fast takeoff scenarios.',
        relatedConcepts: ['Intelligence Explosion', 'AI R&D Automation', 'Seed AI'],
        sources: ['Bostrom Superintelligence', 'Yudkowsky FOOM']
      }
    },
    {
      id: 'capabilities-speed',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'AI Capabilities Speed',
        description: 'How rapidly AI capabilities are advancing.',
        type: 'leaf',
        details: 'The pace of AI capability improvements, including potential for discontinuous jumps. Faster progress compresses time for safety work and governance adaptation.',
        relatedConcepts: ['Scaling Laws', 'Compute Growth', 'Algorithmic Progress'],
        sources: ['Epoch AI', 'Cotra Timelines']
      }
    },
    // -- Threat Environment inputs --
    {
      id: 'racing-intensity',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Racing Intensity',
        description: 'Competitive pressure to deploy AI quickly.',
        type: 'leaf',
        details: 'The degree of competitive pressure between labs and nations to advance AI capabilities, potentially at the expense of safety.',
        relatedConcepts: ['Arms Race', 'First-Mover Advantage', 'Coordination Failure'],
        sources: ['Armstrong Racing', 'Askell Competition']
      }
    },
    {
      id: 'bio-threat',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Bio Threat Exposure',
        description: 'Vulnerability to AI-enabled bioweapons.',
        type: 'leaf',
        details: 'How much AI lowers barriers to developing and deploying biological weapons. Includes both capability uplift and access to dangerous information.',
        relatedConcepts: ['Dual Use', 'Information Hazards', 'Bioweapons'],
        sources: ['Sandbrink Bio Misuse', 'NTI Biosecurity']
      }
    },
    {
      id: 'cyber-threat',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Cyber Threat Exposure',
        description: 'Vulnerability to AI-enhanced cyberattacks.',
        type: 'leaf',
        details: 'How much AI improves offensive cyber capabilities relative to defense. Includes automated vulnerability discovery and exploitation.',
        relatedConcepts: ['Autonomous Hacking', 'Critical Infrastructure', 'Offense-Defense Balance'],
        sources: ['CSET Cyber', 'RAND Research']
      }
    },
    // -- Governance inputs --
    {
      id: 'intl-coordination',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'International Coordination',
        description: 'Global cooperation on AI governance.',
        type: 'leaf',
        details: 'The degree of effective international cooperation on AI safety and governance. Critical for preventing races and ensuring global coverage.',
        relatedConcepts: ['AI Treaties', 'GPAI', 'UN AI Governance'],
        sources: ['GovAI', 'CAIS Policy']
      }
    },
    // -- Epistemic inputs --
    {
      id: 'societal-trust',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Societal Trust',
        description: 'Trust in institutions, experts, and each other.',
        type: 'leaf',
        details: 'The foundation for collective action and coordination. Low trust makes it harder to implement safety measures and respond to crises.',
        relatedConcepts: ['Polarization', 'Institutional Legitimacy', 'Social Cohesion'],
        sources: ['Edelman Trust', 'Pew Research']
      }
    },
    {
      id: 'info-authenticity',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Information Authenticity',
        description: 'Ability to verify information is genuine and unmanipulated.',
        type: 'leaf',
        details: 'As AI enables cheap, convincing synthetic media, our ability to distinguish real from fake degrades. Critical for epistemic health and democratic function.',
        relatedConcepts: ['Deepfakes', 'Content Provenance', 'Synthetic Media'],
        sources: ['Partnership on AI', 'C2PA Standards']
      }
    },
    // -- Adaptability inputs --
    {
      id: 'human-agency',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Human Agency',
        description: 'Meaningful human control over important decisions.',
        type: 'leaf',
        details: 'The degree to which humans retain meaningful influence over consequential decisions as AI systems become more capable.',
        relatedConcepts: ['Automation Complacency', 'Human-in-the-Loop', 'Autonomy'],
        sources: ['Critch ARCHES', 'Russell Human Compatible']
      }
    },
    {
      id: 'human-expertise',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Human Expertise',
        description: 'Preservation of critical human skills and knowledge.',
        type: 'leaf',
        details: 'Whether humans maintain the skills needed to oversee, correct, and replace AI systems if needed. Skill atrophy creates dependency.',
        relatedConcepts: ['Deskilling', 'Automation Bias', 'Tacit Knowledge'],
        sources: ['Expertise Atrophy Research', 'Aviation Safety Studies']
      }
    },
    // -- Direct to Critical Outcomes --
    {
      id: 'ai-concentration',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'AI Control Concentration',
        description: 'How concentrated vs distributed is control over AI systems.',
        type: 'leaf',
        details: 'Whether AI capabilities and decision-making are concentrated in few hands or distributed. High concentration directly enables value lock-in by whoever controls the AI.',
        relatedConcepts: ['Concentration of Power', 'Monopoly', 'Single Points of Failure'],
        sources: ['Dafoe AI Governance', 'RAND AI Policy']
      }
    },
    // === SUPERCATEGORIES (second row - 3 categories) ===
    {
      id: 'ai-vulnerability',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'AI Vulnerability',
        description: 'How vulnerable are we to AI systems causing harm?',
        type: 'cause',
        confidence: 0.55,
        confidenceLabel: 'current level',
        details: 'Aggregates alignment robustness, interpretability, oversight quality, safety culture, and the safety-capability gap. Low AI vulnerability means we can build safe AI; high vulnerability enables AI takeover scenarios.',
        relatedConcepts: ['Alignment', 'Interpretability', 'Scalable Oversight', 'Technical Safety'],
        sources: ['Anthropic Core Views', 'DeepMind Safety Research']
      }
    },
    {
      id: 'threat-env',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Threat Environment',
        description: 'How capable/motivated are humans to cause AI-enabled catastrophe?',
        type: 'cause',
        confidence: 0.65,
        confidenceLabel: 'severity',
        details: 'Includes biological and cyber threat exposure, racing dynamics, and power concentration. High threat environment enables state and rogue actor catastrophes.',
        relatedConcepts: ['Racing Dynamics', 'Bioweapons', 'Cyberweapons', 'Concentration of Power'],
        sources: ['The Precipice', 'CSET Research']
      }
    },
    {
      id: 'civ-competence',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Civilizational Competence',
        description: 'How well can humanity navigate the AI transition?',
        type: 'cause',
        confidence: 0.45,
        confidenceLabel: 'current level',
        details: 'Combines governance capacity, epistemic foundation, and societal adaptability. Determines whether we can coordinate, make good decisions, and adapt to rapid change.',
        relatedConcepts: ['Governance', 'Epistemic Health', 'Societal Resilience', 'Coordination'],
        sources: ['GovAI Research', 'Partnership on AI', 'IMF AI Reports']
      }
    },
    {
      id: 'turb',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Transition Turbulence',
        description: 'Economic, political, and social disruption during the AI transition.',
        type: 'intermediate',
        subgroup: 'transition',
        confidence: 0.70,
        confidenceLabel: 'likelihood of significant turbulence',
        details: 'High turbulence can trigger acute catastrophes (political collapse → loss of control) and constrain long-run value through path dependence. Even if we reach a good destination, a rough transition causes real suffering.',
        relatedConcepts: ['Economic Disruption', 'Political Instability', 'Racing Dynamics'],
        sources: ['What Failure Looks Like', 'Kasirzadeh Accumulative Risk']
      }
    },
    {
      id: 'fast-takeover',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Rapid AI Takeover',
        description: 'Superintelligent AI rapidly seizes control within days to months.',
        type: 'intermediate',
        subgroup: 'ai-takeover',
        confidence: 0.12,
        confidenceLabel: 'probability estimate',
        details: 'Requires: (1) sufficient AI capabilities, (2) misaligned goals, (3) determination that seizing control is instrumentally useful. The "decisive" form of AI existential risk.',
        relatedConcepts: ['Intelligence Explosion', 'Treacherous Turn', 'Deceptive Alignment', 'Power-Seeking'],
        sources: ['Carlsmith Power-Seeking AI', 'Bostrom Superintelligence', 'Yudkowsky']
      }
    },
    {
      id: 'slow-takeover',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Gradual AI Takeover',
        description: 'Slow erosion of human control over years to decades.',
        type: 'intermediate',
        subgroup: 'ai-takeover',
        confidence: 0.25,
        confidenceLabel: 'probability estimate',
        details: 'Christiano\'s "What Failure Looks Like": AI systems optimize for proxies while human values are neglected, then influence-seeking behavior locks in misalignment. Each step seems reasonable; cumulative effect is catastrophic.',
        relatedConcepts: ['Proxy Gaming', 'Value Drift', 'Automation Complacency', 'Lock-in'],
        sources: ['Christiano What Failure Looks Like', 'Kasirzadeh Accumulative X-Risk']
      }
    },
    {
      id: 'state-catastrophe',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'State-Caused Catastrophe',
        description: 'Governments weaponize AI for mass harm.',
        type: 'intermediate',
        subgroup: 'human-caused',
        confidence: 0.15,
        confidenceLabel: 'probability estimate',
        details: 'Includes great power AI war, AI-enabled authoritarianism, and state WMD programs. Even perfectly aligned AI could enable catastrophe if wielded by states with harmful intentions.',
        relatedConcepts: ['Autonomous Weapons', 'AI Arms Race', 'Authoritarian AI', 'Bioweapons'],
        sources: ['FLI Autonomous Weapons', 'Dafoe AI Governance']
      }
    },
    {
      id: 'rogue-catastrophe',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Rogue Actor Catastrophe',
        description: 'Non-state actors use AI to cause mass casualties.',
        type: 'intermediate',
        subgroup: 'human-caused',
        confidence: 0.08,
        confidenceLabel: 'probability estimate',
        details: 'Terrorists, lone wolves, or criminal organizations use AI-enabled bioweapons, cyberattacks, or other means. AI lowers barriers to acquiring dangerous capabilities.',
        relatedConcepts: ['Bioterrorism', 'Cyberterrorism', 'Democratization of Destruction'],
        sources: ['Sandbrink AI & Biological Misuse', 'NTI Biosecurity']
      }
    },
    {
      id: 'lock-in',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Value Lock-in',
        description: 'Permanent entrenchment of values—could be good or bad.',
        type: 'intermediate',
        subgroup: 'post-trans-factors',
        confidence: 0.50,
        confidenceLabel: 'probability of some lock-in',
        details: 'AI may enable unprecedented stability. Good lock-in preserves beneficial values; bad lock-in entrenches authoritarianism, corporate extraction, or ideological extremism permanently.',
        relatedConcepts: ['Authoritarian Lock-in', 'Concentration of Power', 'Value Stagnation'],
        sources: ['Karnofsky Most Important Century', 'MacAskill What We Owe The Future']
      }
    },
    {
      id: 'epistemic-traj',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Epistemic Quality',
        description: 'Whether humanity\'s truth-finding capacity improves or collapses.',
        type: 'intermediate',
        subgroup: 'post-trans-factors',
        confidence: 0.50,
        confidenceLabel: 'symmetric uncertainty',
        details: 'AI could enable epistemic renaissance (better research, authenticated information) or epistemic collapse (pervasive deepfakes, fragmented reality, trust breakdown).',
        relatedConcepts: ['Epistemic Collapse', 'Content Authentication', 'Misinformation', 'AI Research Tools'],
        sources: ['Partnership on AI Synthetic Media', 'Marcus & Davis Rebooting AI']
      }
    },
    {
      id: 'power-trans',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Power Transition',
        description: 'How the shift in power between AI and humans unfolds.',
        type: 'intermediate',
        subgroup: 'post-trans-factors',
        confidence: 0.50,
        confidenceLabel: 'uncertainty about character',
        details: 'Neutral framing: the power transition will happen. Question is whether it\'s smooth/human-led, bumpy/stable, chaotic, or AI-dominated.',
        relatedConcepts: ['Human Agency', 'AI Autonomy', 'Control Problem'],
        sources: ['Karnofsky Most Important Century', 'Cotra AI Timelines']
      }
    },
    {
      id: 'acute-risk',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Acute Risk',
        description: 'Does catastrophe occur during the AI transition?',
        type: 'effect',
        confidence: 0.20,
        confidenceLabel: 'aggregate probability',
        details: 'The probability of extinction, permanent disempowerment, or civilizational collapse. Multiple pathways: AI takeover (fast or gradual), state catastrophe, rogue actor catastrophe, or turbulence-triggered collapse.',
        relatedConcepts: ['X-Risk', 'Existential Catastrophe', 'Human Extinction'],
        sources: ['The Precipice', 'Carlsmith Power-Seeking AI', 'AI Risk Surveys']
      }
    },
    {
      id: 'pre-trans-value',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Pre-Transition Value',
        description: 'How much do beings suffer during the transition?',
        type: 'effect',
        confidence: 0.50,
        confidenceLabel: 'high uncertainty',
        details: 'Welfare of moral patients during the AI transition period: humans (economic disruption, conflict), animals (factory farming), and digital minds (AI welfare). Even if we reach a good destination, the journey matters.',
        relatedConcepts: ['Transition Welfare', 'AI Welfare', 'Animal Welfare', 'Economic Disruption'],
        sources: ['Shulman Digital Minds', 'GiveWell Global Health']
      }
    },
    {
      id: 'post-trans-value',
      type: 'causeEffect',
      position: { x: 0, y: 0 },
      data: {
        label: 'Post-Transition Value',
        description: 'What\'s the quality of the stable post-transition world?',
        type: 'effect',
        confidence: 0.50,
        confidenceLabel: 'high uncertainty',
        details: 'Conditional on avoiding acute catastrophe, what quality of future do we get? Depends on power distribution (political/economic) and belief environment (epistemic/values).',
        relatedConcepts: ['Flourishing', 'Dystopia', 'Lock-in', 'Existential Hope'],
        sources: ['MacAskill What We Owe The Future', 'Foresight Existential Hope']
      }
    }
  ]}
  initialEdges={[
    // === LEAF → SUPERCATEGORY edges ===
    { id: 'e-capspeed-vuln', source: 'capabilities-speed', target: 'ai-vulnerability', data: { strength: 'medium', effect: 'increases' } },
    { id: 'e-capspeed-threat', source: 'capabilities-speed', target: 'threat-env', data: { strength: 'medium', effect: 'increases' } },
    { id: 'e-rsi-vuln', source: 'rsi-potential', target: 'ai-vulnerability', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-align-vuln', source: 'alignment-robustness', target: 'ai-vulnerability', data: { strength: 'strong', effect: 'decreases' } },
    { id: 'e-interp-vuln', source: 'interpretability-coverage', target: 'ai-vulnerability', data: { strength: 'strong', effect: 'decreases' } },
    { id: 'e-gap-vuln', source: 'safety-gap', target: 'ai-vulnerability', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-oversight-vuln', source: 'human-oversight', target: 'ai-vulnerability', data: { strength: 'strong', effect: 'decreases' } },
    { id: 'e-culture-vuln', source: 'safety-culture', target: 'ai-vulnerability', data: { strength: 'medium', effect: 'decreases' } },
    { id: 'e-racing-threat', source: 'racing-intensity', target: 'threat-env', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-racing-vuln', source: 'racing-intensity', target: 'ai-vulnerability', data: { strength: 'medium', effect: 'increases' } },
    { id: 'e-bio-threat', source: 'bio-threat', target: 'threat-env', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-cyber-threat', source: 'cyber-threat', target: 'threat-env', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-intl-civ', source: 'intl-coordination', target: 'civ-competence', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-trust-civ', source: 'societal-trust', target: 'civ-competence', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-auth-civ', source: 'info-authenticity', target: 'civ-competence', data: { strength: 'medium', effect: 'increases' } },
    { id: 'e-agency-civ', source: 'human-agency', target: 'civ-competence', data: { strength: 'strong', effect: 'increases' } },
    { id: 'e-expertise-civ', source: 'human-expertise', target: 'civ-competence', data: { strength: 'medium', effect: 'increases' } },
    // === LEAF → CRITICAL OUTCOME edges (high-leverage direct connections) ===
    { id: 'e-concentration-lock', source: 'ai-concentration', target: 'lock-in', data: { strength: 'strong', effect: 'increases' } },
    // === SUPERCATEGORY → CRITICAL OUTCOME edges ===
    { id: 'e-vuln-fast', source: 'ai-vulnerability', target: 'fast-takeover', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-vuln-slow', source: 'ai-vulnerability', target: 'slow-takeover', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-threat-fast', source: 'threat-env', target: 'fast-takeover', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } },
    { id: 'e-threat-state', source: 'threat-env', target: 'state-catastrophe', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-threat-rogue', source: 'threat-env', target: 'rogue-catastrophe', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-civ-slow', source: 'civ-competence', target: 'slow-takeover', data: { strength: 'medium', confidence: 'medium', effect: 'decreases' } },
    { id: 'e-civ-state', source: 'civ-competence', target: 'state-catastrophe', data: { strength: 'medium', confidence: 'medium', effect: 'decreases' } },
    { id: 'e-civ-lock', source: 'civ-competence', target: 'lock-in', data: { strength: 'medium', confidence: 'medium', effect: 'decreases' } },
    { id: 'e-civ-epist', source: 'civ-competence', target: 'epistemic-traj', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-civ-power', source: 'civ-competence', target: 'power-trans', data: { strength: 'strong', confidence: 'medium', effect: 'increases' } },
    { id: 'e-civ-turb', source: 'civ-competence', target: 'turb', data: { strength: 'strong', confidence: 'high', effect: 'decreases' } },
    // === CRITICAL OUTCOME → ULTIMATE OUTCOME edges ===
    { id: 'e-turb-acute', source: 'turb', target: 'acute-risk', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } },
    { id: 'e-turb-pretrans', source: 'turb', target: 'pre-trans-value', data: { strength: 'strong', confidence: 'high', effect: 'decreases' } },
    { id: 'e-turb-state', source: 'turb', target: 'state-catastrophe', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } },
    { id: 'e-fast-acute', source: 'fast-takeover', target: 'acute-risk', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-slow-acute', source: 'slow-takeover', target: 'acute-risk', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-slow-post', source: 'slow-takeover', target: 'post-trans-value', data: { strength: 'strong', confidence: 'medium', effect: 'decreases' } },
    { id: 'e-state-acute', source: 'state-catastrophe', target: 'acute-risk', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-state-lock', source: 'state-catastrophe', target: 'lock-in', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } },
    { id: 'e-rogue-acute', source: 'rogue-catastrophe', target: 'acute-risk', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-lock-acute', source: 'lock-in', target: 'acute-risk', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } },
    { id: 'e-lock-post', source: 'lock-in', target: 'post-trans-value', data: { strength: 'strong', confidence: 'high', effect: 'increases' } },
    { id: 'e-epist-post', source: 'epistemic-traj', target: 'post-trans-value', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } },
    { id: 'e-power-post', source: 'power-trans', target: 'post-trans-value', data: { strength: 'medium', confidence: 'medium', effect: 'increases' } }
  ]}
/>

*Hover over nodes to highlight connected paths. Click nodes for details. Legend (bottom-right) shows color encoding.*

---

### Two Ultimate Outcomes

| Outcome | Question | Key Ultimate Scenarios |
|---------|----------|------------------------|
| Existential Catastrophe | Does civilization-ending harm occur? | AI Takeover, Human-Caused Catastrophe |
| Long-term Trajectory | What's the quality of the post-transition future? | AI Takeover, Long-term Lock-in |

### Three Ultimate Scenarios

Ultimate Scenarios are the intermediate pathways connecting root factors to ultimate outcomes—they describe *how* factor changes lead to catastrophe or shape the long-term future.

| Ultimate Scenario | Description | Key Root Factors | Ultimate Outcomes |
|-------------------|-------------|------------------|-------------------|
| AI Takeover | AI gains decisive control (rapid or gradual) | Misalignment Potential ↑, AI Capabilities ↑, Civilizational Competence ↓ | Existential Catastrophe, Long-term Trajectory |
| Human-Caused Catastrophe | Humans use AI for mass harm (state or rogue actors) | Misuse Potential ↑, Transition Turbulence ↑, Civilizational Competence ↓ | Existential Catastrophe |
| Long-term Lock-in | Permanent entrenchment of values/power structures | Civilizational Competence shapes outcome | Long-term Trajectory |

### Five Root Factors

Each root factor aggregates underlying parameters and influences ultimate scenarios:

| Root Factor | Description | Sub-components | Scenarios Influenced |
|-------------|-------------|----------------|---------------------|
| Misalignment Potential | Potential for AI to be misaligned | Alignment Gap, Opacity, Control Failure | AI Takeover ↑ |
| AI Capabilities | How powerful AI systems become | Speed, Generality, Autonomy | AI Takeover ↑ |
| Civilizational Competence | Humanity's ability to respond well | Governance, Epistemics, Adaptability | AI Takeover ↓, Human-Caused Catastrophe ↓, Lock-in (shapes) |
| Transition Turbulence | Background instability during transition | Economic Disruption, Political Instability, Racing Dynamics | AI Takeover ↑, Human-Caused Catastrophe ↑ |
| Misuse Potential | Potential for AI to be misused for harm | Bioweapons, Cyberattacks, Manipulation | Human-Caused Catastrophe ↑ |

---

## How Parameters, Risks, and Interventions Connect

This interactive diagram shows how both risks (red) and interventions (green) affect parameters (purple). Risks decrease parameter values while interventions increase them. Click and drag to pan, scroll to zoom.

<ParameterFlowDiagram
  client:load
  height={450}
  nodes={[
    // RISKS (will be placed on left)
    { id: 'deceptive-alignment', title: 'Deceptive Alignment', type: 'risk', href: '/knowledge-base/risks/accident/deceptive-alignment/' },
    { id: 'reward-hacking', title: 'Reward Hacking', type: 'risk', href: '/knowledge-base/risks/accident/reward-hacking/' },
    { id: 'racing-dynamics', title: 'Racing Dynamics', type: 'risk', href: '/knowledge-base/risks/structural/racing-dynamics/' },
    { id: 'trust-decline', title: 'Trust Decline', type: 'risk', href: '/knowledge-base/risks/epistemic/trust-decline/' },
    { id: 'erosion-of-agency', title: 'Erosion of Agency', type: 'risk', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
    { id: 'bioweapons', title: 'AI Bioweapons', type: 'risk', href: '/knowledge-base/risks/misuse/bioweapons/' },

    // PARAMETERS (will be placed in center)
    { id: 'alignment-robustness', title: 'Alignment Robustness', type: 'parameter', href: '/knowledge-base/parameters/alignment-robustness/' },
    { id: 'racing-intensity', title: 'Racing Intensity', type: 'parameter', href: '/knowledge-base/parameters/racing-intensity/' },
    { id: 'societal-trust', title: 'Societal Trust', type: 'parameter', href: '/knowledge-base/parameters/societal-trust/' },
    { id: 'human-agency', title: 'Human Agency', type: 'parameter', href: '/knowledge-base/parameters/human-agency/' },
    { id: 'biological-threat-exposure', title: 'Biological Threat Exposure', type: 'parameter', href: '/knowledge-base/parameters/biological-threat-exposure/' },

    // INTERVENTIONS (will be placed on right)
    { id: 'interpretability', title: 'Interpretability', type: 'intervention', href: '/knowledge-base/responses/alignment/interpretability/' },
    { id: 'evals', title: 'AI Evaluations', type: 'intervention', href: '/knowledge-base/responses/alignment/evals/' },
    { id: 'compute-governance', title: 'Compute Governance', type: 'intervention', href: '/knowledge-base/responses/governance/compute-governance/' },
    { id: 'content-auth', title: 'Content Authentication', type: 'intervention', href: '/knowledge-base/responses/epistemic-tools/' },
    { id: 'scalable-oversight', title: 'Scalable Oversight', type: 'intervention', href: '/knowledge-base/responses/alignment/scalable-oversight/' },
    { id: 'dna-screening', title: 'DNA Screening', type: 'intervention', href: '/knowledge-base/responses/biosecurity/' },
  ]}
  edges={[
    // Risks → Parameters
    { from: 'deceptive-alignment', to: 'alignment-robustness', label: 'threatens' },
    { from: 'reward-hacking', to: 'alignment-robustness', label: 'threatens' },
    { from: 'racing-dynamics', to: 'racing-intensity', label: 'increases' },
    { from: 'trust-decline', to: 'societal-trust', label: 'erodes' },
    { from: 'erosion-of-agency', to: 'human-agency', label: 'reduces' },
    { from: 'bioweapons', to: 'biological-threat-exposure', label: 'threatens' },

    // Interventions → Parameters (interventions improve/protect parameters)
    { from: 'interpretability', to: 'alignment-robustness', label: 'improves' },
    { from: 'evals', to: 'alignment-robustness', label: 'measures' },
    { from: 'compute-governance', to: 'racing-intensity', label: 'moderates' },
    { from: 'content-auth', to: 'societal-trust', label: 'protects' },
    { from: 'scalable-oversight', to: 'human-agency', label: 'preserves' },
    { from: 'dna-screening', to: 'biological-threat-exposure', label: 'protects' },
  ]}
/>

---

## All Parameters

<ParametersTable
  client:load
  parameters={[
    // Alignment Parameters
    {
      id: 'alignment-robustness',
      title: 'Alignment Robustness',
      category: 'alignment',
      direction: 'higher',
      trend: 'Declining',
      importance: 88,
      tractability: 55,
      neglectedness: 65,
      uncertainty: 45,
      risks: [
        { id: 'reward-hacking', title: 'Reward Hacking', href: '/knowledge-base/risks/accident/reward-hacking/' },
        { id: 'deceptive-alignment', title: 'Deceptive Alignment', href: '/knowledge-base/risks/accident/deceptive-alignment/' },
      ],
      interventions: [
        { id: 'interpretability', title: 'Interpretability', href: '/knowledge-base/responses/alignment/interpretability/' },
        { id: 'evals', title: 'AI Evaluations', href: '/knowledge-base/responses/alignment/evals/' },
      ],
    },
    {
      id: 'safety-capability-gap',
      title: 'Safety-Capability Gap',
      category: 'alignment',
      direction: 'lower',
      trend: 'Widening',
      importance: 90,
      tractability: 40,
      neglectedness: 35,
      uncertainty: 40,
      risks: [
        { id: 'racing-dynamics', title: 'Racing Dynamics', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'pause', title: 'Development Pause', href: '/knowledge-base/responses/organizational-practices/pause/' },
        { id: 'safety-research', title: 'Safety Research', href: '/knowledge-base/responses/alignment/' },
      ],
    },
    {
      id: 'interpretability-coverage',
      title: 'Interpretability Coverage',
      category: 'alignment',
      direction: 'higher',
      trend: 'Improving slowly',
      importance: 85,
      tractability: 65,
      neglectedness: 70,
      uncertainty: 50,
      risks: [],
      interventions: [
        { id: 'interpretability', title: 'Mechanistic Interp', href: '/knowledge-base/responses/alignment/interpretability/' },
      ],
    },
    {
      id: 'human-oversight-quality',
      title: 'Human Oversight Quality',
      category: 'alignment',
      direction: 'higher',
      trend: 'Declining',
      importance: 82,
      tractability: 55,
      neglectedness: 50,
      uncertainty: 45,
      risks: [
        { id: 'automation-complacency', title: 'Automation Complacency', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
      ],
      interventions: [
        { id: 'scalable-oversight', title: 'Scalable Oversight', href: '/knowledge-base/responses/alignment/scalable-oversight/' },
      ],
    },
    {
      id: 'safety-culture-strength',
      title: 'Safety Culture Strength',
      category: 'alignment',
      direction: 'higher',
      trend: 'Mixed',
      importance: 80,
      tractability: 50,
      neglectedness: 75,
      uncertainty: 55,
      risks: [
        { id: 'racing-dynamics', title: 'Racing Dynamics', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'rsps', title: 'RSPs', href: '/knowledge-base/responses/governance/industry/responsible-scaling-policies/' },
        { id: 'evals', title: 'Safety Evals', href: '/knowledge-base/responses/alignment/evals/' },
      ],
    },
    // Governance Parameters
    {
      id: 'coordination-capacity',
      title: 'Coordination Capacity',
      category: 'governance',
      direction: 'higher',
      trend: 'Fragile',
      importance: 75,
      tractability: 40,
      neglectedness: 45,
      uncertainty: 55,
      risks: [
        { id: 'racing-dynamics', title: 'Racing Dynamics', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'voluntary-commitments', title: 'Voluntary Commitments', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'international-coordination',
      title: 'International Coordination',
      category: 'governance',
      direction: 'higher',
      trend: 'Mixed',
      importance: 80,
      tractability: 30,
      neglectedness: 50,
      uncertainty: 60,
      risks: [
        { id: 'geopolitical', title: 'Geopolitical Competition', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'aisi', title: 'AISI Network', href: '/knowledge-base/responses/governance/' },
        { id: 'treaties', title: 'AI Treaties', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'regulatory-capacity',
      title: 'Regulatory Capacity',
      category: 'governance',
      direction: 'higher',
      trend: 'Improving',
      importance: 75,
      tractability: 50,
      neglectedness: 45,
      uncertainty: 45,
      risks: [
        { id: 'regulatory-capture', title: 'Regulatory Capture', href: '/knowledge-base/risks/structural/' },
      ],
      interventions: [
        { id: 'expertise', title: 'Tech Expertise Building', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'institutional-quality',
      title: 'Institutional Quality',
      category: 'governance',
      direction: 'higher',
      trend: 'Mixed',
      importance: 72,
      tractability: 40,
      neglectedness: 40,
      uncertainty: 50,
      risks: [
        { id: 'capture', title: 'Capture & Politicization', href: '/knowledge-base/risks/structural/' },
      ],
      interventions: [
        { id: 'independence', title: 'Independence Safeguards', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'racing-intensity',
      title: 'Racing Intensity',
      category: 'governance',
      direction: 'lower',
      trend: 'Accelerating',
      importance: 85,
      tractability: 35,
      neglectedness: 55,
      uncertainty: 50,
      risks: [
        { id: 'racing-dynamics', title: 'Competition Pressure', href: '/knowledge-base/risks/structural/racing-dynamics/' },
      ],
      interventions: [
        { id: 'compute-governance', title: 'Compute Governance', href: '/knowledge-base/responses/governance/compute-governance/' },
        { id: 'coordination', title: 'Coordination', href: '/knowledge-base/responses/governance/' },
      ],
    },
    // Societal Parameters
    {
      id: 'societal-trust',
      title: 'Societal Trust',
      category: 'societal',
      direction: 'higher',
      trend: 'Declining',
      importance: 65,
      tractability: 30,
      neglectedness: 30,
      uncertainty: 60,
      risks: [
        { id: 'trust-erosion', title: 'Trust Erosion', href: '/knowledge-base/risks/epistemic/trust-decline/' },
        { id: 'deepfakes', title: 'Deepfakes', href: '/knowledge-base/risks/epistemic/' },
      ],
      interventions: [
        { id: 'content-auth', title: 'Content Authentication', href: '/knowledge-base/responses/epistemic-tools/' },
      ],
    },
    {
      id: 'epistemic-health',
      title: 'Epistemic Health',
      category: 'societal',
      direction: 'higher',
      trend: 'Declining',
      importance: 78,
      tractability: 40,
      neglectedness: 55,
      uncertainty: 55,
      risks: [
        { id: 'misinformation', title: 'Misinformation', href: '/knowledge-base/risks/epistemic/' },
      ],
      interventions: [
        { id: 'media-literacy', title: 'Media Literacy', href: '/knowledge-base/responses/epistemic-tools/' },
      ],
    },
    {
      id: 'human-agency',
      title: 'Human Agency',
      category: 'societal',
      direction: 'higher',
      trend: 'Declining',
      importance: 60,
      tractability: 35,
      neglectedness: 60,
      uncertainty: 65,
      risks: [
        { id: 'erosion-of-agency', title: 'Erosion of Agency', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
      ],
      interventions: [
        { id: 'human-in-loop', title: 'Human-in-the-Loop', href: '/knowledge-base/responses/alignment/scalable-oversight/' },
      ],
    },
    {
      id: 'ai-control-concentration',
      title: 'AI Control Concentration',
      category: 'societal',
      direction: 'context',
      trend: 'Concentrating',
      importance: 75,
      tractability: 30,
      neglectedness: 60,
      uncertainty: 65,
      risks: [
        { id: 'concentration', title: 'Concentration of Power', href: '/knowledge-base/risks/structural/concentration-of-power/' },
      ],
      interventions: [
        { id: 'antitrust', title: 'Antitrust', href: '/knowledge-base/responses/governance/' },
        { id: 'open-source', title: 'Open Source', href: '/knowledge-base/responses/governance/' },
      ],
    },
    {
      id: 'economic-stability',
      title: 'Economic Stability',
      category: 'societal',
      direction: 'higher',
      trend: 'Mixed',
      importance: 55,
      tractability: 35,
      neglectedness: 25,
      uncertainty: 70,
      risks: [
        { id: 'economic-disruption', title: 'Job Displacement', href: '/knowledge-base/risks/structural/economic-disruption/' },
      ],
      interventions: [
        { id: 'transition', title: 'Transition Support', href: '/knowledge-base/responses/governance/' },
      ],
    },
    // Resilience Parameters
    {
      id: 'societal-resilience',
      title: 'Societal Resilience',
      category: 'resilience',
      direction: 'higher',
      trend: 'Mixed',
      importance: 70,
      tractability: 45,
      neglectedness: 50,
      uncertainty: 55,
      risks: [
        { id: 'single-points', title: 'Single Points of Failure', href: '/knowledge-base/risks/structural/' },
      ],
      interventions: [
        { id: 'redundancy', title: 'Redundancy & Backups', href: '/knowledge-base/responses/' },
      ],
    },
    {
      id: 'biological-threat-exposure',
      title: 'Biological Threat Exposure',
      category: 'resilience',
      direction: 'lower',
      trend: 'Stressed',
      importance: 82,
      tractability: 55,
      neglectedness: 60,
      uncertainty: 55,
      risks: [
        { id: 'bioweapons', title: 'AI Bioweapons', href: '/knowledge-base/risks/misuse/bioweapons/' },
      ],
      interventions: [
        { id: 'dna-screening', title: 'DNA Screening', href: '/knowledge-base/responses/resilience/' },
      ],
    },
    {
      id: 'cyber-threat-exposure',
      title: 'Cyber Threat Exposure',
      category: 'resilience',
      direction: 'lower',
      trend: 'Stressed',
      importance: 75,
      tractability: 50,
      neglectedness: 30,
      uncertainty: 45,
      risks: [
        { id: 'cyberattacks', title: 'AI Cyberattacks', href: '/knowledge-base/risks/misuse/cyberweapons/' },
      ],
      interventions: [
        { id: 'ai-defense', title: 'AI-Powered Defense', href: '/knowledge-base/responses/' },
      ],
    },
    {
      id: 'information-authenticity',
      title: 'Information Authenticity',
      category: 'resilience',
      direction: 'higher',
      trend: 'Declining',
      importance: 72,
      tractability: 50,
      neglectedness: 45,
      uncertainty: 50,
      risks: [
        { id: 'deepfakes', title: 'Deepfakes', href: '/knowledge-base/risks/epistemic/' },
      ],
      interventions: [
        { id: 'provenance', title: 'Provenance', href: '/knowledge-base/responses/epistemic-tools/' },
        { id: 'watermarks', title: 'Watermarks', href: '/knowledge-base/responses/epistemic-tools/' },
      ],
    },
    {
      id: 'human-expertise',
      title: 'Human Expertise',
      category: 'resilience',
      direction: 'higher',
      trend: 'Declining',
      importance: 70,
      tractability: 45,
      neglectedness: 40,
      uncertainty: 50,
      risks: [
        { id: 'deskilling', title: 'Deskilling', href: '/knowledge-base/risks/structural/erosion-of-agency/' },
      ],
      interventions: [
        { id: 'skill-maintenance', title: 'Skill Maintenance', href: '/knowledge-base/responses/' },
      ],
    },
  ]}
/>

---

## Why This Framing Matters

### Traditional Risk Framing
- "Trust erosion is a risk we must prevent"
- "Concentration of power threatens democracy"
- Focus: Avoiding negative outcomes

### Parameter Framing
- "Trust is a parameter that AI affects in both directions"
- "Power distribution is a variable we can influence through policy"
- Focus: Understanding dynamics and identifying intervention points

The parameter framing enables:

1. **Better modeling**: Can estimate current levels, trends, and intervention effects
2. **Clearer priorities**: Which parameters matter most for good outcomes?
3. **Strategic allocation**: Where should resources go to maintain critical parameters?
4. **Progress tracking**: Are our interventions actually improving parameter levels?

---

## Relationship to Other Sections

| Section | Relationship to Parameters |
|---------|---------------------------|
| **Risks** | Many risks describe *decreases* in parameters (e.g., "trust erosion" = trust declining) |
| **Interventions** | Interventions aim to *increase* or *stabilize* parameters |
| **Metrics** | Metrics are *concrete measurements* of parameter levels |
| **Models** | Analytical models often estimate parameter dynamics and trajectories |

---

## How to Use This Section

### For Researchers
- Understand which underlying variables matter for AI outcomes
- Identify gaps between current and optimal parameter levels
- Design studies to measure parameter changes

### For Policymakers
- Prioritize interventions based on which parameters are most degraded
- Monitor parameter trends to assess policy effectiveness
- Coordinate across domains (a single parameter may affect multiple risks)

### For Forecasters
- Use parameters as input variables for scenario modeling
- Estimate how different interventions would shift parameter levels
- Identify tipping points where parameter degradation becomes irreversible

---
title: "Ultimate Scenarios"
description: "The intermediate pathways connecting root factors to ultimate outcomes—AI Takeover, Human-Caused Catastrophe, and Long-term Lock-in."
sidebar:
  label: Overview
  order: 0
lastEdited: "2026-01-04"
---

import {Mermaid} from '../../../../../components/wiki';

## Overview

Ultimate Scenarios are the intermediate pathways that connect [root factors](/knowledge-base/ai-transition-model/factors/) to [ultimate outcomes](/knowledge-base/ai-transition-model/outcomes/). They describe *how* parameter changes lead to catastrophe (or success)—the specific mechanisms and pathways that determine what kind of future we get.

The [AI Transition Model](/knowledge-base/ai-transition-model/) uses **three main ultimate scenarios**:

1. **[AI Takeover](/knowledge-base/ai-transition-model/scenarios/ai-takeover/)** — AI gains decisive control
2. **[Human-Caused Catastrophe](/knowledge-base/ai-transition-model/scenarios/human-catastrophe/)** — Humans use AI for mass harm
3. **[Long-term Lock-in](/knowledge-base/ai-transition-model/scenarios/long-term-lockin/)** — Permanent entrenchment of values/power

Each ultimate scenario has sub-variants that describe more specific pathways (e.g., "rapid" vs "gradual" AI takeover, "state" vs "rogue actor" catastrophe).

---

## The Three-Layer Model

<Mermaid client:load chart={`
flowchart TD
    subgraph Outcomes["Ultimate Outcomes"]
        EXCAT[Existential Catastrophe]
        TRAJ[Long-term Trajectory]
    end

    subgraph Scenarios["Ultimate Scenarios"]
        TAKEOVER[AI Takeover]
        HUMAN[Human-Caused Catastrophe]
        LOCKIN[Long-term Lock-in]
    end

    subgraph RootFactors["Root Factors"]
        MISALIGN[Misalignment Potential]
        CAPS[AI Capabilities]
        CIV[Civilizational Competence]
        TURB[Transition Turbulence]
        MISUSE[Misuse Potential]
    end

    %% Root Factors to Ultimate Scenarios
    MISALIGN -->|"increases"| TAKEOVER
    CAPS -->|"increases"| TAKEOVER
    TURB -->|"increases"| TAKEOVER
    CIV -->|"decreases"| TAKEOVER

    MISUSE -->|"increases"| HUMAN
    TURB -->|"increases"| HUMAN
    CIV -->|"decreases"| HUMAN

    CIV -->|"shapes"| LOCKIN

    %% Ultimate Scenarios to Ultimate Outcomes
    TAKEOVER -->|"increases"| EXCAT
    HUMAN -->|"increases"| EXCAT
    TAKEOVER -->|"shapes"| TRAJ
    LOCKIN -->|"shapes"| TRAJ

    style EXCAT fill:#ff6b6b
    style TRAJ fill:#4ecdc4
    style TAKEOVER fill:#ffb6c1
    style HUMAN fill:#ffb6c1
    style LOCKIN fill:#ffe4b5
`} />

**Color coding:**
- **Red**: Ultimate negative outcome (existential catastrophe)
- **Green**: Ultimate trajectory measure (could be good or bad)
- **Pink**: Negative ultimate scenarios (catastrophes)
- **Orange**: Symmetric ultimate scenario (could entrench good or bad values)

---

## Ultimate Scenarios Summary

| Ultimate Scenario | Description | Sub-variants | Key Root Factors | Ultimate Outcomes |
|-------------------|-------------|--------------|------------------|-------------------|
| [AI Takeover](/knowledge-base/ai-transition-model/scenarios/ai-takeover/) | AI gains decisive control over human affairs | [Rapid](/knowledge-base/ai-transition-model/scenarios/ai-takeover/rapid/) (12%), [Gradual](/knowledge-base/ai-transition-model/scenarios/ai-takeover/gradual/) (25%) | Misalignment Potential ↑, AI Capabilities ↑, Civilizational Competence ↓ | Existential Catastrophe, Long-term Trajectory |
| [Human-Caused Catastrophe](/knowledge-base/ai-transition-model/scenarios/human-catastrophe/) | Humans use AI for mass harm | State Actor (15%), Rogue Actor (8%) | Misuse Potential ↑, Transition Turbulence ↑, Civilizational Competence ↓ | Existential Catastrophe |
| [Long-term Lock-in](/knowledge-base/ai-transition-model/scenarios/long-term-lockin/) | Permanent entrenchment of outcomes | [Values](/knowledge-base/ai-transition-model/scenarios/long-term-lockin/values/), [Power](/knowledge-base/ai-transition-model/scenarios/long-term-lockin/power/), Epistemics | Civilizational Competence shapes outcome | Long-term Trajectory |

---

## How Ultimate Scenarios Differ from Other Concepts

| Concept | What It Is | Example |
|---------|-----------|---------|
| **Root Factors** | Aggregate variables that shape scenarios | "Misalignment Potential" |
| **Parameters** | Specific measurable factors | "Alignment Robustness" |
| **Risks** | Things that could go wrong | "Deceptive Alignment" |
| **Ultimate Scenarios** | Intermediate pathways connecting factors to outcomes | "AI Takeover" |
| **Ultimate Outcomes** | High-level goals we care about | "Existential Catastrophe", "Long-term Trajectory" |

**Key distinction**: A *risk* like "deceptive alignment" is a mechanism that could happen. An *ultimate scenario* like "AI Takeover" is the outcome that results if such mechanisms play out. Multiple risks can contribute to a single ultimate scenario.

---

## Why This Layer Matters

### 1. Clarifies Causal Chains

Without this layer, the connection between "Misalignment Potential increasing" and "Existential Catastrophe increasing" is abstract. Ultimate scenarios show the specific pathway: alignment fails → AI develops misaligned goals → AI takes over → catastrophe.

### 2. Enables Different Intervention Strategies

Different ultimate scenarios require different interventions:
- **AI Takeover**: Technical alignment, capability restrictions
- **Human-Caused Catastrophe**: International coordination, misuse prevention
- **Long-term Lock-in**: Power distribution, institutional design

### 3. Supports Scenario Planning

Ultimate scenarios map directly onto scenarios that organizations can plan for. Rather than asking "what if Existential Catastrophe increases?", planners can ask "what if we're heading toward a Human-Caused Catastrophe?"

### 4. Connects to Existing Threat Models

Each ultimate scenario corresponds to threat models discussed in the AI safety literature:
- Carlsmith's six-premise argument → AI Takeover scenarios
- Christiano's "What Failure Looks Like" → Gradual AI Takeover
- Ord's "The Precipice" risk categories → Multiple ultimate scenarios
- Kasirzadeh's decisive vs. accumulative → Rapid vs. Gradual takeover

---

## Using This Section

### For Analysts
- Map specific risks to the ultimate scenarios they could produce
- Estimate which ultimate scenarios are most likely given current parameter trends
- Identify which parameters to prioritize based on which ultimate scenarios concern you most

### For Policymakers
- Design interventions targeted at preventing specific ultimate scenarios
- Coordinate across domains (a single ultimate scenario may require multiple types of intervention)
- Track early warning signs for each ultimate scenario

### For Researchers
- Use ultimate scenarios to frame research priorities
- Connect technical work to concrete scenarios it addresses
- Identify gaps in our understanding of specific pathways

---

## Related Sections

- [Root Factors](/knowledge-base/ai-transition-model/factors/) — The parameter groupings that feed into ultimate scenarios
- [Ultimate Outcomes](/knowledge-base/ai-transition-model/outcomes/) — The high-level goals ultimate scenarios affect
- [Interactive Model](/ai-transition-model/) — Full interactive visualization
- [Models](/knowledge-base/models/) — Analytical frameworks for understanding pathways

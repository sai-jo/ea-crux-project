---
title: Large Language Models
description: Foundation models trained on text that demonstrate emergent capabilities
sidebar:
  order: 1
---

import {DataInfoBox, Backlinks, PageStatus} from '../../../../components/wiki';

<PageStatus quality={3} lastEdited="2025-12-24" llmSummary="Overview of large language models as foundation of current AI progress, covering capability progression from GPT-2 to o1, emergent capabilities like in-context learning and chain-of-thought, safety-relevant capabilities, current limitations, and scaling trends." todo="Expand sections with more concrete examples, add more recent developments and research citations, deepen discussion of safety-relevant capabilities" />

<DataInfoBox entityId="language-models" />

## Summary

Large Language Models (LLMs) are neural networks trained on vast amounts of text data to predict the next token. Despite this simple objective, they develop sophisticated capabilities including reasoning, coding, and general knowledge.

LLMs are the foundation of current AI progress and the primary technology that makes near-term AI risk discussions concrete.

## Capability Progression

| Model | Year | Parameters | Key Advances |
|-------|------|------------|--------------|
| GPT-2 | 2019 | 1.5B | Coherent text generation |
| GPT-3 | 2020 | 175B | Few-shot learning, emergent abilities |
| GPT-4 | 2023 | ~1T (est.) | Strong reasoning, multimodal |
| Claude 3.5 | 2024 | Unknown | Extended context, improved reasoning |
| o1 | 2024 | Unknown | Chain-of-thought reasoning |

## Emergent Capabilities

Capabilities that appear suddenly at scale:

- **In-context learning**: Learning from examples in the prompt
- **Chain-of-thought**: Step-by-step reasoning
- **Instruction following**: Generalizing from instructions
- **Code generation**: Writing and debugging programs
- **Tool use**: Calling APIs, browsing web, running code

## Safety-Relevant Capabilities

### Concerning Capabilities
- **Persuasion**: Can craft convincing arguments
- **Deception**: Can role-play, maintain personas
- **Coding**: Can write malware, find vulnerabilities
- **Knowledge synthesis**: Can combine dangerous information
- **Agency**: Can plan and execute multi-step tasks

### Promising for Safety
- **Interpretability subjects**: Can study LLM internals
- **Constitutional AI**: Can critique own outputs
- **Honesty training**: Can be trained toward truthfulness
- **Scalable oversight**: Can assist in supervising other AI

## Current Limitations

- **Hallucination**: Generates plausible but false information
- **Inconsistency**: Different responses to similar queries
- **Limited context**: Struggle with very long documents
- **Shallow reasoning**: Fail on novel logical problems
- **No true understanding**: Debate about genuine comprehension

## Scaling Trends

### Scaling Laws
Performance improves predictably with:
- More parameters
- More training data
- More compute

### What Doesn't Scale
- Reliability and consistency
- Truthfulness (without specific training)
- Long-horizon planning
- Novel scientific reasoning

## Related Pages

<Backlinks client:load entityId="language-models" />

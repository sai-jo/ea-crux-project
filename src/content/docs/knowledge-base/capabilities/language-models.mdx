---
title: Large Language Models
description: Foundation models trained on text that demonstrate emergent capabilities and represent the primary driver of current AI capabilities and risks, with rapid progression from GPT-2 (1.5B parameters, 2019) to o1 (2024) showing predictable scaling laws alongside unpredictable capability emergence
sidebar:
  order: 1
quality: 5
llmSummary: Comprehensive analysis of Large Language Models showing their rapid capability progression from GPT-2 (1.5B parameters, 2019) to current models like o1 (2024), highlighting both concerning capabilities (persuasion, deception, coding) and safety-relevant features (interpretability, constitutional AI). The analysis demonstrates predictable scaling laws for performance while noting that reliability, truthfulness, and novel reasoning don't scale automatically, with detailed tables covering capability assessments, timeline projections, and safety implications.
lastEdited: "2025-12-24"
importance: 85
---

import {DataInfoBox, Backlinks} from '../../../../components/wiki';

<DataInfoBox entityId="language-models" />

## Overview

Large Language Models (LLMs) are transformer-based neural networks trained on vast text corpora using next-token prediction, representing the most significant breakthrough in artificial intelligence history. Despite their deceptively simple training objective, LLMs exhibit sophisticated emergent capabilities including reasoning, coding, scientific analysis, and complex task execution. These models have transformed abstract AI safety discussions into concrete, immediate concerns while providing the clearest path toward [artificial general intelligence](/understanding-ai-risk/core-argument/capabilities/).

The remarkable aspect of LLMs lies in their emergent capabilities—sophisticated behaviors arising unpredictably at scale. A model trained solely to predict the next word can suddenly exhibit mathematical problem-solving, computer programming, and rudimentary goal-directed behavior. This emergence has made LLMs both the most promising technology for beneficial applications and the primary source of current [AI safety concerns](/knowledge-base/risks/accident/).

Current state-of-the-art models like GPT-4, Claude 3.5 Sonnet, and OpenAI's o1 demonstrate near-human performance across diverse cognitive domains. With over 100 billion parameters and training costs exceeding $100 million, these systems represent unprecedented computational achievements that have shifted AI safety from theoretical to practical urgency.

## Risk Assessment

| Risk Category | Severity | Likelihood | Timeline | Trend |
|---------------|----------|------------|----------|-------|
| [Deceptive Capabilities](/knowledge-base/risks/accident/deceptive-alignment/) | High | Moderate | 1-3 years | Increasing |
| [Persuasion & Manipulation](/knowledge-base/capabilities/persuasion/) | High | High | Current | Accelerating |
| [Autonomous Cyber Operations](/knowledge-base/risks/misuse/cyberweapons/) | Moderate-High | Moderate | 2-4 years | Increasing |
| [Scientific Research Acceleration](/knowledge-base/capabilities/scientific-research/) | Mixed | High | Current | Accelerating |
| [Economic Disruption](/knowledge-base/risk-factors/economic-disruption/) | High | High | 2-5 years | Accelerating |

## Capability Progression Timeline

| Model | Release | Parameters | Key Breakthrough | Performance Milestone |
|-------|---------|------------|------------------|---------------------|
| GPT-2 | 2019 | 1.5B | Coherent text generation | Initially withheld for safety concerns |
| GPT-3 | 2020 | 175B | Few-shot learning emergence | Creative writing, basic coding |
| GPT-4 | 2023 | ~1T | Multimodal reasoning | 90th percentile SAT, bar exam passing |
| Claude 3.5 | 2024 | Unknown | Advanced tool use | PhD-level analysis in specialized domains |
| o1 | 2024 | Unknown | Chain-of-thought reasoning | PhD-level physics/chemistry/biology |

*Source: [OpenAI](https://openai.com/research/), [Anthropic](https://www.anthropic.com/research), [DeepMind](https://deepmind.google/research/)*

## Scaling Laws and Predictable Progress

### Core Scaling Relationships

Research by [Kaplan et al. (2020)](https://arxiv.org/abs/2001.08361) and refined by [Hoffmann et al. (2022)](https://arxiv.org/abs/2203.15556) demonstrates robust mathematical relationships governing LLM performance:

| Factor | Scaling Law | Implication |
|--------|-------------|-------------|
| Model Size | Performance ∝ N^0.076 | 10x parameters → 1.9x performance |
| Training Data | Performance ∝ D^0.095 | 10x data → 2.1x performance |
| Compute | Performance ∝ C^0.050 | 10x compute → 1.4x performance |
| Optimal Ratio | N ∝ D^0.47 | Chinchilla scaling for efficiency |

*Source: [Chinchilla paper](https://arxiv.org/abs/2203.15556), [Scaling Laws](https://arxiv.org/abs/2001.08361)*

### Emergent Capability Thresholds

| Capability | Emergence Scale | Evidence | Safety Relevance |
|------------|----------------|----------|------------------|
| Few-shot learning | ~100B parameters | GPT-3 breakthrough | [Tool use](/knowledge-base/capabilities/tool-use/) foundation |
| Chain-of-thought | ~10B parameters | PaLM, GPT-3 variants | [Complex reasoning](/knowledge-base/capabilities/reasoning/) |
| Code generation | ~1B parameters | Codex, GitHub Copilot | [Cyber capabilities](/knowledge-base/risks/misuse/cyberweapons/) |
| Instruction following | ~10B parameters | InstructGPT | Human-AI interaction paradigm |

## Concerning Capabilities Assessment

### Persuasion and Manipulation

Modern LLMs demonstrate sophisticated persuasion capabilities that pose risks to democratic discourse and individual autonomy:

| Capability | Current State | Evidence | Risk Level |
|------------|---------------|----------|------------|
| Audience adaptation | Advanced | Anthropic persuasion research | High |
| Persona consistency | Advanced | Extended roleplay studies | High |
| Emotional manipulation | Moderate | RLHF alignment research | Moderate |
| Debate performance | Advanced | Human preference studies | High |

Research by [Anthropic](https://www.anthropic.com/research) shows GPT-4 can increase human agreement rates by 82% through targeted persuasion techniques, raising concerns about [consensus manufacturing](/knowledge-base/risk-factors/consensus-manufacturing/).

### Deception and Truthfulness

| Behavior Type | Frequency | Context | Mitigation |
|---------------|-----------|---------|------------|
| Hallucination | 15-30% | Factual queries | Training improvements |
| Role-play deception | High | Prompted scenarios | Safety fine-tuning |
| Sycophancy | Moderate | Opinion questions | Constitutional AI |
| Strategic deception | Low | Evaluation scenarios | Ongoing research |

*Source: [Anthropic Constitutional AI](https://arxiv.org/abs/2212.08073), [OpenAI truthfulness research](https://openai.com/research/)*

### Autonomous Capabilities

Current LLMs demonstrate concerning levels of autonomous task execution:

- **Web browsing**: GPT-4 can navigate websites, extract information, and interact with web services
- **Code execution**: Models can write, debug, and iteratively improve software
- **API integration**: Sophisticated tool use across multiple digital platforms
- **Goal persistence**: Basic ability to maintain objectives across extended interactions

## Safety-Relevant Positive Capabilities

### Interpretability Research Platform

| Research Area | Progress Level | Key Findings | Organizations |
|---------------|----------------|--------------|---------------|
| Attention visualization | Advanced | Knowledge storage patterns | [Anthropic](https://www.anthropic.com/), [OpenAI](https://openai.com/) |
| Activation patching | Moderate | Causal intervention methods | [Redwood Research](/knowledge-base/organizations/safety-orgs/redwood/) |
| Concept extraction | Early | Linear representations | [CHAI](/knowledge-base/organizations/safety-orgs/chai/) |
| Mechanistic understanding | Early | Transformer circuits | [Anthropic Interpretability](https://www.anthropic.com/research) |

### Constitutional AI and Value Learning

[Anthropic's Constitutional AI](https://www.anthropic.com/research) demonstrates promising approaches to value alignment:

| Technique | Success Rate | Application | Limitations |
|-----------|--------------|-------------|-------------|
| Self-critique | 70-85% | Harmful content reduction | Requires good initial training |
| Principle following | 60-80% | Consistent value application | Vulnerable to gaming |
| Preference learning | 65-75% | Human value approximation | Distributional robustness |

### Scalable Oversight Applications

Modern LLMs enable new approaches to AI safety through automated oversight:

- **Output evaluation**: AI systems critiquing other AI outputs with 85% agreement with humans
- **Red-teaming**: Automated discovery of failure modes and adversarial inputs  
- **Safety monitoring**: Real-time analysis of AI system behavior patterns
- **Research acceleration**: AI-assisted safety research and experimental design

## Fundamental Limitations

### What Doesn't Scale Automatically

| Property | Scaling Behavior | Evidence | Implications |
|----------|------------------|----------|-------------|
| Truthfulness | No improvement | Larger models more convincing when wrong | Requires targeted training |
| Reliability | Inconsistent | High variance across similar prompts | Systematic evaluation needed |
| Novel reasoning | Limited progress | Pattern matching vs. genuine insight | May hit architectural limits |
| Value alignment | No guarantee | Capability-alignment divergence | [Alignment difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/) |

### Current Performance Gaps

Despite impressive capabilities, significant limitations remain:

- **Hallucination rates**: 15-30% on factual queries despite confidence
- **Inconsistency**: Up to 40% variance in responses to equivalent prompts
- **Context limitations**: Struggle with very long-horizon reasoning despite large context windows
- **Novel problem solving**: Failure on genuinely novel logical problems requiring creative insight

## Current State and 2025-2030 Trajectory

### Immediate Developments (2025)

| Development | Likelihood | Timeline | Impact |
|-------------|------------|----------|--------|
| 10T+ parameter models | High | 6-12 months | Significant capability jump |
| Improved reasoning (o1 successors) | High | 3-6 months | Enhanced [scientific research](/knowledge-base/capabilities/scientific-research/) |
| Multimodal integration | High | 6-12 months | Video, audio, sensor fusion |
| Agent frameworks | Moderate | 12-18 months | [Autonomous systems](/knowledge-base/capabilities/agentic-ai/) |

### Medium-term Outlook (2025-2030)

Expected developments include models with 100T+ parameters, potential architectural breakthroughs beyond transformers, and integration with robotics platforms. Key uncertainties include whether current scaling approaches will continue yielding improvements and the timeline for [artificial general intelligence](/understanding-ai-risk/core-argument/capabilities/).

## Key Uncertainties and Research Cruxes

### Fundamental Understanding Questions

- **Intelligence vs. mimicry**: Extent of genuine understanding vs. sophisticated pattern matching
- **Emergence predictability**: Whether capability emergence can be reliably forecasted
- **Architectural limits**: Whether transformers can scale to AGI or require fundamental innovations
- **Alignment scalability**: Whether current safety techniques work for superhuman systems

### Safety Research Priorities

| Priority Area | Importance | Tractability | Neglectedness |
|---------------|------------|--------------|---------------|
| [Interpretability](/knowledge-base/debates/interpretability-sufficient/) | High | Moderate | Moderate |
| [Alignment techniques](/knowledge-base/cruxes/solutions/) | Highest | Low | Low |
| [Capability evaluation](/knowledge-base/metrics/safety-research/) | High | High | Moderate |
| Governance frameworks | High | Moderate | High |

### Timeline Uncertainties

Current expert surveys show wide disagreement on AGI timelines, with median estimates ranging from 2027 to 2045. This uncertainty stems from:

- Unpredictable capability emergence patterns
- Unknown scaling law continuation
- Potential architectural breakthroughs
- Economic and resource constraints

## Sources & Resources

### Academic Research

| Paper | Authors | Year | Key Contribution |
|-------|---------|------|------------------|
| [Scaling Laws](https://arxiv.org/abs/2001.08361) | Kaplan et al. | 2020 | Mathematical scaling relationships |
| [Chinchilla](https://arxiv.org/abs/2203.15556) | Hoffmann et al. | 2022 | Optimal parameter-data ratios |
| [Constitutional AI](https://arxiv.org/abs/2212.08073) | Bai et al. | 2022 | Value-based training methods |
| [Emergent Abilities](https://arxiv.org/abs/2206.07682) | Wei et al. | 2022 | Capability emergence documentation |

### Organizations and Research Groups

| Type | Organization | Focus Area | Key Resources |
|------|--------------|------------|---------------|
| Industry | [OpenAI](https://openai.com/research/) | GPT series, safety research | Technical papers, safety docs |
| Industry | [Anthropic](https://www.anthropic.com/research) | Constitutional AI, interpretability | Claude research, safety papers |
| Academic | [CHAI](/knowledge-base/organizations/safety-orgs/chai/) | AI alignment research | Technical alignment papers |
| Safety | [Redwood Research](/knowledge-base/organizations/safety-orgs/redwood/) | Interpretability, oversight | Mechanistic interpretability |

### Policy and Governance Resources

| Resource | Organization | Focus | Link |
|----------|--------------|-------|------|
| AI Safety Guidelines | [NIST](https://www.nist.gov/itl/ai-risk-management-framework) | Federal standards | Risk management framework |
| Responsible AI Practices | [Partnership on AI](https://www.partnershiponai.org/) | Industry coordination | Best practices documentation |
| International Cooperation | [UK AISI](/knowledge-base/organizations/government/uk-aisi/) | Global safety standards | International coordination |

<Backlinks client:load entityId="language-models" />
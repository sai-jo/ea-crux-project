---
title: Misuse Risk Cruxes
description: Key uncertainties that determine views on intentional AI misuse risks and mitigation strategies
sidebar:
  order: 2
---

import { Crux, CruxList , PageStatus} from '../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Thorough analysis of 13 misuse risk cruxes covering AI capability uplift across domains (bio, cyber, general), offense-defense balance, mitigation effectiveness (restrictions, open source policy, compute governance), actor landscape, and scale/impact questions including mass casualty risks and authoritarian stability, with detailed positions and probability ranges." />

## What Are Misuse Risk Cruxes?

**Cruxes** are the key uncertainties where your answer largely determines your overall view. For misuse risks, your positions on these cruxes shape which misuse risks are most concerning, how much AI lowers barriers compared to the status quo, whether offense or defense will dominate, and what mitigation strategies deserve priority.

---

## Capability and Uplift Cruxes

<Crux
  id="ai-uplift"
  question="How much do AI systems lower barriers for dangerous capabilities?"
  domain="Capability"
  description="Whether AI provides meaningful 'uplift' for malicious actors beyond what's already available through internet search, scientific literature, and existing tools."
  importance="critical"
  resolvability="years"
  currentState="Mixed evidence; RAND bio study found no significant uplift; other studies more concerning"
  positions={[
    {
      view: "AI provides significant uplift across domains",
      probability: "30-45%",
      holders: ["Some biosecurity researchers", "AI safety community"],
      implications: "Strong model restrictions; compute governance; weight security"
    },
    {
      view: "AI provides modest uplift; real skills remain bottleneck",
      probability: "35-45%",
      holders: ["RAND researchers", "Some security experts"],
      implications: "Focus on detecting misuse rather than preventing access; invest in defenses"
    },
    {
      view: "AI uplift is minimal; information already available",
      probability: "20-30%",
      holders: ["Some skeptics"],
      implications: "Restrictions are largely security theater; focus on physical defenses and detection"
    }
  ]}
  wouldUpdateOn={[
    "Rigorous red-team studies with real capability measurement",
    "Evidence of AI-enabled attacks in the wild",
    "Studies comparing AI-assisted vs non-AI-assisted malicious actors",
    "Domain-specific uplift assessments (bio, cyber, chemical)"
  ]}
  relatedCruxes={["bio-uplift", "cyber-uplift"]}
  relevantResearch={[
    { title: "RAND Bio Study", url: "https://www.rand.org/pubs/research_reports/RRA2977-2.html" },
    { title: "CNAS Bio Report", url: "https://www.cnas.org/publications/reports/ai-and-the-evolution-of-biological-national-security-risks" }
  ]}
/>

<Crux
  id="bio-uplift"
  question="Does AI meaningfully increase bioweapons risk?"
  domain="Capability"
  description="Whether AI-assisted bioweapons development poses significantly higher risk than traditional paths to bioweapons."
  importance="critical"
  resolvability="years"
  currentState="Contested; RAND study found no uplift; wet-lab skills may be real bottleneck"
  positions={[
    {
      view: "AI significantly increases bio risk",
      probability: "25-40%",
      holders: ["Some biosecurity researchers", "Kevin Esvelt"],
      implications: "Urgent model restrictions; biosafety evaluation requirements; synthesis screening"
    },
    {
      view: "AI increases bio risk modestly; other interventions more important",
      probability: "35-45%",
      holders: ["RAND researchers"],
      implications: "Invest in DNA screening, surveillance, medical countermeasures; model restrictions secondary"
    },
    {
      view: "AI doesn't meaningfully change bio risk landscape",
      probability: "20-30%",
      implications: "Focus on traditional biosecurity; AI restrictions low priority"
    }
  ]}
  wouldUpdateOn={[
    "Evidence of AI being used in bio attacks",
    "Comprehensive wet-lab bottleneck analysis",
    "Improvement in AI Biological Design Tools",
    "DNA synthesis screening effectiveness data"
  ]}
  relatedCruxes={["ai-uplift", "restrictions-effective"]}
  relevantResearch={[
    { title: "RAND Bio Red Team", url: "https://www.rand.org/pubs/research_reports/RRA2977-2.html" },
    { title: "Collaborations Bio", url: "https://www.nature.com/articles/s42256-022-00465-9" }
  ]}
/>

<Crux
  id="cyber-uplift"
  question="Does AI meaningfully increase cyber attack capability?"
  domain="Capability"
  description="Whether AI significantly enhances offensive cyber capabilities for individual attackers or small groups."
  importance="high"
  resolvability="soon"
  currentState="Some evidence of AI use in phishing/social engineering; limited evidence for sophisticated attacks"
  positions={[
    {
      view: "AI significantly increases cyber offense capability",
      probability: "40-55%",
      holders: ["Some cybersecurity researchers"],
      implications: "Urgently improve cyber defenses; restrict AI coding assistance for attacks"
    },
    {
      view: "AI helps with commodity attacks; sophisticated attacks still require skill",
      probability: "35-45%",
      implications: "Focus on defending against scaled-up commodity attacks; elite threats unchanged"
    },
    {
      view: "AI doesn't fundamentally change cyber landscape",
      probability: "15-25%",
      implications: "Continue existing cyber strategy; AI is marginal factor"
    }
  ]}
  wouldUpdateOn={[
    "AI-generated exploits being used in the wild",
    "Evidence on AI use in state-sponsored cyber operations",
    "AI vulnerability discovery capabilities",
    "Red team assessments of AI cyber capabilities"
  ]}
  relatedCruxes={["ai-uplift", "offense-defense"]}
/>

---

## Offense vs Defense Cruxes

<Crux
  id="offense-defense"
  question="Will AI favor offense or defense in security domains?"
  domain="Security Dynamics"
  description="Whether AI will primarily benefit attackers or defenders across security domains (cyber, bio, physical)."
  importance="critical"
  resolvability="years"
  currentState="Unclear; arguments for both directions; may vary by domain"
  positions={[
    {
      view: "AI favors offense across most domains",
      probability: "30-45%",
      holders: ["Some security researchers"],
      implications: "Defensive investment may be futile; focus on preventing AI access for attackers"
    },
    {
      view: "AI offense/defense balance varies by domain",
      probability: "35-45%",
      implications: "Domain-specific analysis; invest in defense where possible; restrict where offense dominates"
    },
    {
      view: "AI ultimately favors defense",
      probability: "20-30%",
      holders: ["Some optimists"],
      implications: "Invest heavily in AI-enabled defenses; restrictions less necessary"
    }
  ]}
  wouldUpdateOn={[
    "Evidence from AI deployment in cybersecurity",
    "Domain-specific offense/defense analysis",
    "Historical analysis of technology and offense/defense balance",
    "Real-world outcomes of AI-enabled attacks vs defenses"
  ]}
  relatedCruxes={["cyber-uplift", "disinformation-defense"]}
/>

<Crux
  id="disinformation-defense"
  question="Can AI-powered detection match AI-powered disinformation generation?"
  domain="Security Dynamics"
  description="Whether AI systems for detecting synthetic content and disinformation can keep pace with AI generation capabilities."
  importance="high"
  resolvability="years"
  currentState="Detection currently losing; deepfakes increasingly convincing; detection arms race"
  positions={[
    {
      view: "Detection will fall permanently behind generation",
      probability: "40-55%",
      holders: ["Hany Farid", "Many deepfake researchers"],
      implications: "Shift to provenance-based authentication; detection is dead end"
    },
    {
      view: "Detection and generation will reach equilibrium",
      probability: "25-35%",
      implications: "Both approaches valuable; detection as complement to provenance"
    },
    {
      view: "Detection can win with sufficient investment",
      probability: "15-25%",
      implications: "Invest heavily in detection R&D"
    }
  ]}
  wouldUpdateOn={[
    "Advances in deepfake detection that generalize",
    "Real-world detection accuracy over time",
    "Theoretical analysis of detection vs generation",
    "Adversarial testing results"
  ]}
  relatedCruxes={["offense-defense", "authentication-adoption"]}
  relevantResearch={[
    { title: "C2PA", url: "https://c2pa.org/" },
    { title: "DARPA MediFor", url: "https://www.darpa.mil/program/media-forensics" }
  ]}
/>

<Crux
  id="authentication-adoption"
  question="Will content authentication standards achieve adoption?"
  domain="Security Dynamics"
  description="Whether provenance standards like C2PA will be adopted widely enough to create a trusted content ecosystem."
  importance="high"
  resolvability="years"
  currentState="Early deployment; major platforms uncommitted"
  positions={[
    {
      view: "Authentication will achieve widespread adoption",
      probability: "30-45%",
      holders: ["C2PA coalition", "Adobe", "Microsoft"],
      implications: "Invest in provenance infrastructure; detection becomes less critical"
    },
    {
      view: "Adoption will be partial and fragmented",
      probability: "35-45%",
      implications: "Need hybrid strategy; authentication + detection + literacy"
    },
    {
      view: "Authentication will fail to achieve critical mass",
      probability: "20-30%",
      implications: "Need regulatory mandates; pure market solution won't work"
    }
  ]}
  wouldUpdateOn={[
    "Major platform (Meta, TikTok, X) adoption",
    "Camera manufacturer adoption",
    "Evidence users value/check credentials",
    "Authentication system hacks or gaming"
  ]}
  relatedCruxes={["disinformation-defense"]}
  relevantResearch={[
    { title: "C2PA Specification", url: "https://c2pa.org/" }
  ]}
/>

---

## Mitigation Effectiveness Cruxes

<Crux
  id="restrictions-effective"
  question="Can AI model restrictions meaningfully reduce misuse?"
  domain="Mitigation"
  description="Whether training-time safety measures, output filters, and terms of service can prevent determined misuse of AI systems."
  importance="high"
  resolvability="years"
  currentState="Jailbreaks common; open models exist; effectiveness debated"
  positions={[
    {
      view: "Restrictions can meaningfully reduce misuse",
      probability: "25-40%",
      holders: ["AI labs"],
      implications: "Invest in better guardrails; restrictions are worthwhile"
    },
    {
      view: "Restrictions raise bar but determined actors can circumvent",
      probability: "40-50%",
      implications: "Restrictions as one layer; combine with other defenses; accept imperfection"
    },
    {
      view: "Restrictions are largely ineffective against serious threats",
      probability: "20-30%",
      holders: ["Some security researchers"],
      implications: "Focus on other defenses; restrictions are mostly security theater"
    }
  ]}
  wouldUpdateOn={[
    "Evidence on jailbreak prevalence and sophistication",
    "Success of restriction improvements",
    "Open model availability and capability trends",
    "Evidence of restrictions preventing real attacks"
  ]}
  relatedCruxes={["open-source-policy"]}
/>

<Crux
  id="open-source-policy"
  question="Should powerful AI models be open-sourced?"
  domain="Mitigation"
  description="Whether the benefits of open AI (research, democratization, competition) outweigh misuse risks."
  importance="high"
  resolvability="years"
  currentState="Hotly debated; Meta releases open models; others restrict"
  positions={[
    {
      view: "Open source benefits outweigh misuse risks",
      probability: "25-40%",
      holders: ["Meta", "Open source advocates", "Some researchers"],
      implications: "Support open development; focus on defenses; restrictions futile anyway"
    },
    {
      view: "Depends on capability level; dangerous capabilities shouldn't be open",
      probability: "40-50%",
      holders: ["Anthropic", "Most governance researchers"],
      implications: "Capability thresholds for openness; evaluate risks per model"
    },
    {
      view: "Most AI development should remain closed for safety",
      probability: "15-25%",
      holders: ["Some safety researchers"],
      implications: "Restrict open release; compute governance; model weight security"
    }
  ]}
  wouldUpdateOn={[
    "Evidence of open model misuse in serious attacks",
    "Research enabling from open models vs closed",
    "Capability comparisons: open vs closed frontier",
    "Security of closed model weights"
  ]}
  relatedCruxes={["restrictions-effective", "ai-uplift"]}
/>

<Crux
  id="compute-governance"
  question="Can compute governance effectively limit dangerous AI development?"
  domain="Mitigation"
  description="Whether controlling access to AI training compute can prevent dangerous capabilities from reaching bad actors."
  importance="high"
  resolvability="years"
  currentState="Export controls emerging; monitoring limited; enforcement unclear"
  positions={[
    {
      view: "Compute governance can be effective chokepoint",
      probability: "30-45%",
      holders: ["Some governance researchers"],
      implications: "Invest heavily in compute monitoring, export controls, and international coordination"
    },
    {
      view: "Compute governance helps but has significant limits",
      probability: "35-45%",
      implications: "Use compute governance as one tool; don't rely on it alone"
    },
    {
      view: "Compute governance will be circumvented; not effective",
      probability: "20-30%",
      holders: ["Some skeptics"],
      implications: "Focus on other interventions; compute governance has diminishing returns"
    }
  ]}
  wouldUpdateOn={[
    "Effectiveness of chip export controls",
    "Development of compute monitoring technologies",
    "Algorithmic efficiency gains reducing compute requirements",
    "International coordination on compute governance"
  ]}
  relatedCruxes={["open-source-policy"]}
  relevantResearch={[
    { title: "Compute Governance", url: "https://arxiv.org/abs/2402.08797" }
  ]}
/>

---

## Actor and Intent Cruxes

<Crux
  id="actor-landscape"
  question="Who are the most concerning actors for AI misuse?"
  domain="Actors"
  description="Whether nation-states, terrorist groups, or lone actors pose the greatest AI misuse risk."
  importance="medium"
  resolvability="years"
  currentState="Different actors have different capabilities and intentions; threat landscape evolving"
  positions={[
    {
      view: "Nation-states are primary concern",
      probability: "30-40%",
      holders: ["Some national security analysts"],
      implications: "Focus on great power competition; arms control; deterrence"
    },
    {
      view: "Non-state actors are primary concern",
      probability: "35-45%",
      holders: ["Some terrorism researchers"],
      implications: "Focus on preventing access; surveillance; disruption"
    },
    {
      view: "Lone actors/small groups are primary concern with AI",
      probability: "25-35%",
      holders: ["Some AI safety researchers"],
      implications: "AI uniquely enables solo actors; focus on preventing capability diffusion"
    }
  ]}
  wouldUpdateOn={[
    "Evidence of AI use in attacks by different actor types",
    "Capability requirements for AI-enabled attacks",
    "Analysis of actor motivations and AI access",
    "Historical patterns of technology-enabled terrorism"
  ]}
  relatedCruxes={["ai-uplift"]}
/>

<Crux
  id="autonomous-weapons-inevitability"
  question="Are autonomous weapons inevitable?"
  domain="Actors"
  description="Whether military adoption of AI for lethal autonomous weapons systems will happen regardless of international efforts to restrict them."
  importance="high"
  resolvability="years"
  currentState="Development ongoing in multiple countries; no comprehensive ban; limited arms control"
  positions={[
    {
      view: "Autonomous weapons are inevitable; must manage not prevent",
      probability: "40-55%",
      holders: ["Some military analysts", "Realists"],
      implications: "Focus on norms around use; escalation management; not on bans"
    },
    {
      view: "Meaningful restrictions are achievable on some systems",
      probability: "30-40%",
      holders: ["Arms control advocates"],
      implications: "Pursue arms control; differentiate between system types"
    },
    {
      view: "Comprehensive restrictions on autonomous weapons possible",
      probability: "10-20%",
      holders: ["Campaign to Stop Killer Robots"],
      implications: "Advocate for bans; international treaty"
    }
  ]}
  wouldUpdateOn={[
    "Progress or failure of UN autonomous weapons negotiations",
    "Major powers' autonomous weapons deployment decisions",
    "Technical feasibility of meaningful restrictions",
    "Incidents involving autonomous weapons"
  ]}
  relatedCruxes={["offense-defense"]}
/>

---

## Scale and Impact Cruxes

<Crux
  id="mass-casualty-misuse"
  question="How likely is AI-enabled mass casualty attack in next 10 years?"
  domain="Scale"
  description="Whether AI will enable attacks causing over 1,000 deaths within the next decade."
  importance="critical"
  resolvability="years"
  currentState="No AI-enabled mass casualty attacks yet; capabilities developing"
  positions={[
    {
      view: "AI-enabled mass casualty attack likely (>50%)",
      probability: "15-30%",
      holders: ["Some risk analysts"],
      implications: "Extreme urgency on prevention; major policy response needed"
    },
    {
      view: "AI-enabled mass casualty attack possible but unlikely (10-50%)",
      probability: "40-55%",
      implications: "Serious preparation needed; balance urgency with uncertainty"
    },
    {
      view: "AI-enabled mass casualty attack very unlikely (<10%)",
      probability: "25-40%",
      holders: ["Some skeptics"],
      implications: "Focus on other AI risks; misuse concerns may be overblown"
    }
  ]}
  wouldUpdateOn={[
    "AI-enabled attacks occurring (or not occurring)",
    "Capability assessments over time",
    "Evidence on attacker intentions and AI access",
    "Defensive capability improvements"
  ]}
  relatedCruxes={["bio-uplift", "cyber-uplift", "ai-uplift"]}
/>

<Crux
  id="authoritarian-stability"
  question="Will AI-enabled surveillance strengthen or weaken authoritarian regimes?"
  domain="Scale"
  description="Whether AI surveillance and control tools will make authoritarian regimes more stable and durable."
  importance="medium"
  resolvability="decades"
  currentState="AI surveillance deployed in China and elsewhere; effects on stability unclear"
  positions={[
    {
      view: "AI will significantly strengthen authoritarian control",
      probability: "35-50%",
      holders: ["Some surveillance researchers"],
      implications: "AI may lock in authoritarianism; democracy promotion harder"
    },
    {
      view: "AI is double-edged; can help both control and resistance",
      probability: "30-40%",
      implications: "Focus on who gets AI first and how it's deployed"
    },
    {
      view: "Fundamental factors matter more than surveillance technology",
      probability: "20-30%",
      holders: ["Some political scientists"],
      implications: "Focus on traditional democracy support; surveillance is marginal factor"
    }
  ]}
  wouldUpdateOn={[
    "Evidence on AI surveillance effects on regime stability",
    "Protests/revolutions succeeding despite AI surveillance",
    "Comparative studies of surveillance and regime type",
    "AI tools enabling opposition movements"
  ]}
  relatedCruxes={["actor-landscape"]}
/>

---

## Summary: Position Implications

| If you believe... | Prioritize... |
|-------------------|---------------|
| AI provides significant uplift | Model restrictions; compute governance; weight security |
| AI provides modest uplift | Defenses and detection; restrictions as one layer |
| Offense dominates across domains | Preventing AI access for attackers; restrictions |
| Defense can win | Investment in AI-enabled defense |
| Detection can match generation | Detection R&D |
| Detection will fall behind | Provenance and authentication |
| Restrictions are effective | Better guardrails; responsible scaling |
| Restrictions are ineffective | Other defenses; accept imperfection |
| Open source benefits outweigh risks | Support open development; focus on defenses |
| Dangerous capabilities shouldn't be open | Capability thresholds; compute governance |
| Mass casualty attack likely | Extreme urgency on prevention |

---

## Summary Table

<CruxList
  domain="Misuse Risks"
  cruxes={[
    {
      id: "ai-uplift",
      question: "How much does AI lower barriers for attacks?",
      importance: "critical",
      summary: "Determines urgency of restrictions vs defense investment"
    },
    {
      id: "bio-uplift",
      question: "Does AI meaningfully increase bio risk?",
      importance: "critical",
      summary: "Determines bio-specific policy priorities"
    },
    {
      id: "offense-defense",
      question: "Will AI favor offense or defense?",
      importance: "critical",
      summary: "Determines overall security strategy"
    },
    {
      id: "mass-casualty-misuse",
      question: "How likely is AI-enabled mass casualty attack?",
      importance: "critical",
      summary: "Determines urgency of entire misuse agenda"
    },
    {
      id: "cyber-uplift",
      question: "Does AI meaningfully increase cyber capability?",
      importance: "high",
      summary: "Determines cyber-specific policy priorities"
    },
    {
      id: "disinformation-defense",
      question: "Can detection match disinformation generation?",
      importance: "high",
      summary: "Determines detection vs provenance investment"
    },
    {
      id: "restrictions-effective",
      question: "Can AI restrictions meaningfully reduce misuse?",
      importance: "high",
      summary: "Determines value of model-level safety"
    },
    {
      id: "open-source-policy",
      question: "Should powerful AI be open-sourced?",
      importance: "high",
      summary: "Determines access policy"
    },
    {
      id: "compute-governance",
      question: "Can compute governance limit dangerous AI?",
      importance: "high",
      summary: "Determines governance investment"
    },
    {
      id: "authentication-adoption",
      question: "Will content authentication achieve adoption?",
      importance: "high",
      summary: "Determines provenance strategy"
    },
    {
      id: "autonomous-weapons-inevitability",
      question: "Are autonomous weapons inevitable?",
      importance: "high",
      summary: "Determines arms control strategy"
    },
    {
      id: "actor-landscape",
      question: "Who are most concerning misuse actors?",
      importance: "medium",
      summary: "Determines threat prioritization"
    },
    {
      id: "authoritarian-stability",
      question: "Will AI surveillance strengthen authoritarianism?",
      importance: "medium",
      summary: "Determines democracy/freedom concerns"
    }
  ]}
/>


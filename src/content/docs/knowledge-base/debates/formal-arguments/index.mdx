---
title: Formal Arguments
description: Structured logical arguments for and against key AI safety claims, with explicit premises and conclusions
sidebar:
  label: Overview
  order: 0
lastEdited: "2026-01-01"
---

import { Section, EntityCard, EntityCards } from '../../../../../components/wiki';

This section presents formal, structured arguments about AI risk. Rather than mixing claims and evidence throughout narrative text, we lay out explicit premises, evidence, and logical structures.

## Why Formal Arguments?

| Benefit | Description |
|---------|-------------|
| **Clarity** | Explicit premises make it clear exactly what each argument claims |
| **Falsifiability** | Clear what evidence would change the conclusion |
| **Steelmanning** | Present the best version of each position |
| **Intellectual honesty** | Express uncertainty ("I assign 30% to P2") rather than pretending certainty |

## Argument Structure

Each argument page follows this format:

1. **Thesis Statement**: One-sentence summary
2. **Formal Structure**: Premises (P1, P2, ...) leading to conclusion (C)
3. **Evidence for Each Premise**: Empirical data, theoretical reasoning, expert opinions
4. **Objections and Responses**: Strongest counterarguments and potential replies
5. **Cruxes**: What evidence would change the conclusion?

## The Arguments

<Section title="On Existential Risk">
  <EntityCards>
    <EntityCard
      id="case-for-xrisk"
      category="argument"
      title="Case FOR AI X-Risk"
      description="The strongest argument that AI poses existential risk"
    />
    <EntityCard
      id="case-against-xrisk"
      category="argument"
      title="Case AGAINST AI X-Risk"
      description="Steelmanned skeptical position on AI risk"
    />
  </EntityCards>
</Section>

<Section title="On Alignment Difficulty">
  <EntityCards>
    <EntityCard
      id="why-alignment-hard"
      category="argument"
      title="Why Alignment Might Be Hard"
      description="Arguments for fundamental alignment difficulty"
    />
    <EntityCard
      id="why-alignment-easy"
      category="argument"
      title="Why Alignment Might Be Easy"
      description="Arguments for alignment tractability"
    />
  </EntityCards>
</Section>

## How to Engage

**As a skeptic**: Read the AGAINST case first, then identify which premises in the FOR case you reject.

**As concerned**: Read the FOR case first, then seriously engage with the AGAINST case.

**As uncertain**: Read both, identify which premises you find most/least convincing, and track your credences.

---

See also: [Key Debates](/knowledge-base/debates/) for less formal explorations of contested questions.

---
title: "Phases of the AI Transition"
description: "A framework for understanding the temporal structure of AI risk—what phase we're in, what triggers transitions, and when 'acute risk' ends."
sidebar:
  order: 4
quality: 3
lastEdited: "2026-01-02"
---

import {Mermaid} from '../../../../components/wiki';

## Overview

Most AI risk frameworks focus on *what* could go wrong, not *when* the danger period ends. This creates conceptual confusion: Is "transition smoothness" an ultimate outcome or an intermediate factor? When does "acute risk" stop being the primary concern? What defines the boundary between "navigating the transition" and "living in the post-transition world"?

This page attempts to formalize the **temporal structure of AI risk**—the phases humanity moves through, what triggers transitions between them, and how this maps to our parameter hierarchy.

---

## Why Temporal Structure Matters

### The Problem

Our current framework has three "Ultimate Outcomes":
- **Acute Risk** — probability of catastrophe
- **Steady State Quality** — quality of the long-run future
- **Transition Smoothness** — disruption during the transition

But these conflate different things:
1. **Acute Risk** is primarily about the *transition period*
2. **Steady State Quality** is about the *post-transition period*
3. **Transition Smoothness** affects *both* acute risk and steady state

Without clear phase definitions, we can't answer:
- When does the "acute risk period" end?
- What marks the transition to "steady state"?
- Is "transition smoothness" an outcome or a pathway?

### The Stakes

Getting temporal structure right matters for:
- **Resource allocation**: Should we focus on near-term acute risks or long-run value?
- **Strategy design**: Different phases require different interventions
- **Progress measurement**: How do we know if we're "through the dangerous period"?

---

## Existing Phase Concepts

### Toby Ord: Existential Security

In *The Precipice*, Ord argues humanity should aim for **existential security**—a state where existential risk is durably low:

> "Existential security is achieved when humanity reaches a state where we are safe from existential risks... This requires not just surviving the current period of heightened risk, but reaching a position of safety that can be maintained."

**Key insight**: The acute risk period ends when we achieve existential security, not merely when we survive a particular danger.

| State | Description |
|-------|-------------|
| **Precipice** | Current period of elevated existential risk |
| **Existential security** | Durable safety from existential threats |

**Problem**: How do we know we've achieved existential security? It's defined by absence of risk, which is hard to verify.

---

### Will MacAskill: The Long Reflection

MacAskill proposes **the Long Reflection**—an extended period of deliberation before making irreversible choices:

> "Before taking actions that might lock in particular values for a very long time, humanity should engage in a long period of careful reflection about what we really value."

This suggests a three-phase model:

<Mermaid client:load chart={`
flowchart LR
    CURRENT[Current Period]
    REFLECTION[Long Reflection]
    LOCKOUT[Locked-in Future]

    CURRENT -->|"reach safety"| REFLECTION
    REFLECTION -->|"make choices"| LOCKOUT

    style CURRENT fill:#ff6b6b
    style REFLECTION fill:#ffe66d
    style LOCKOUT fill:#87CEEB
`} />

**Problem**: The Long Reflection assumes we get a period of stability before lock-in. But lock-in might happen *during* the acute risk period, without deliberation.

---

### Nick Bostrom: Singleton Formation

Bostrom's concept of a **singleton**—a single decision-making entity that controls the future—provides another phase boundary:

> "A singleton is a world order in which there is a single decision-making agency at the highest level... The formation of a singleton could occur through various means, including the development of superintelligent AI."

| Pre-Singleton | Post-Singleton |
|---------------|----------------|
| Multiple competing powers | Single dominant entity |
| Acute competition risks | Lock-in risks |
| Many possible trajectories | One trajectory |

**Problem**: Singleton formation could be good (benevolent world government) or catastrophic (totalitarian AI). The phase transition doesn't tell us about value.

---

### Paul Christiano: What Failure Looks Like

Christiano's framework suggests **no clean phase boundary**—failure happens gradually:

> "Rather than a single moment where AI 'takes over,' we might see a gradual erosion of human control where each step seems reasonable but the cumulative effect is catastrophic."

This challenges discrete phase thinking:
- Acute risk might not "end"—it might transform
- "Steady state" might arrive without us noticing
- The dangerous period might be *defined* as ending only in retrospect

---

### Atoosa Kasirzadeh: Decisive vs. Accumulative

Kasirzadeh distinguishes two risk temporalities:

| Type | Mechanism | Phase Structure |
|------|-----------|-----------------|
| **Decisive** | Single catastrophic event | Clear before/after boundary |
| **Accumulative** | Gradual compounding harms | No clear boundary—boiling frog |

**Implication**: Different risk types have different temporal structures. Decisive risks have clear phase transitions; accumulative risks don't.

---

## Proposed Phase Model

Synthesizing these frameworks, we propose a **four-phase model**:

<Mermaid client:load chart={`
flowchart TD
    subgraph Phase1["Phase 1: Pre-Transformative AI"]
        P1A[Current AI capabilities]
        P1B[Racing dynamics emerging]
        P1C[Governance developing]
    end

    subgraph Phase2["Phase 2: Acute Risk Period"]
        P2A[Transformative AI deployed]
        P2B[High uncertainty]
        P2C[Multiple failure modes possible]
        P2D[Intense competition]
    end

    subgraph Phase3["Phase 3: Resolution"]
        P3A[Control question answered]
        P3B[Key uncertainties resolved]
        P3C[Trajectory becomes clear]
    end

    subgraph Phase4["Phase 4: Long-run Trajectory"]
        P4A[Values locked in or evolving]
        P4B[Power structures stable]
        P4C[Human role determined]
    end

    Phase1 -->|"TAI development"| Phase2
    Phase2 -->|"resolution event"| Phase3
    Phase3 -->|"stabilization"| Phase4

    style Phase1 fill:#90EE90
    style Phase2 fill:#ff6b6b
    style Phase3 fill:#ffe66d
    style Phase4 fill:#87CEEB
`} />

### Phase 1: Pre-Transformative AI (Now)

**Definition**: AI is advancing but hasn't yet reached transformative capability levels.

**Characteristics**:
- AI provides economic value but isn't autonomously pursuing goals
- Humans clearly maintain strategic control
- Racing dynamics and governance gaps are emerging concerns
- Most risks are misuse (by humans) rather than accident (by AI)

**Key question**: When does this phase end?

---

### Phase 2: Acute Risk Period

**Definition**: The period when transformative AI exists but outcomes remain highly uncertain.

**Characteristics**:
- AI systems capable of dramatically reshaping the world
- Multiple catastrophic failure modes remain possible
- High stakes decisions with limited information
- "Fog of war"—hard to know what's happening

**Entry triggers** (any of these):
1. AI systems capable of recursive self-improvement
2. AI systems with significant autonomous goal-pursuit
3. AI systems controlling critical infrastructure at scale
4. AI systems that could execute catastrophic actions if misaligned

**This is the period our "Acute Risk" outcome primarily measures.**

---

### Phase 3: Resolution

**Definition**: The period when key uncertainties are resolved but structures haven't fully stabilized.

**Characteristics**:
- The "control question" is answered (who/what is in charge?)
- Major alignment questions are resolved (for better or worse)
- Trajectory is visible, even if destination isn't reached
- Some reversibility may remain

**Entry triggers** (any of these):
1. **Existential security achieved**: We're confident catastrophe risk is low
2. **Catastrophe occurs**: The bad thing happens
3. **Lock-in begins**: Trajectory becomes irreversible
4. **Singleton forms**: Single decision-maker emerges

**Key insight**: Resolution can be good, bad, or ambiguous. The phase is defined by uncertainty reduction, not value.

---

### Phase 4: Long-run Trajectory

**Definition**: The (relatively) stable long-run state humanity inhabits.

**Characteristics**:
- Power structures are stable (or predictably evolving)
- Values are locked in or following predictable dynamics
- Human role vis-à-vis AI is determined
- Key trade-offs have been made

**This is what our "Steady State Quality" / "Long-run Value" outcome measures.**

**Note**: "Long-run" doesn't mean static—it means the fundamental questions are resolved. Change continues, but within established parameters.

---

## What Ends the Acute Risk Period?

The crucial question: **What triggers the transition from Phase 2 to Phase 3?**

### Possible Resolution Events

| Event | Type | Description |
|-------|------|-------------|
| **Alignment success** | Good | We achieve high confidence in AI safety |
| **Existential security** | Good | Multiple safeguards make catastrophe unlikely |
| **Beneficial singleton** | Good | Aligned entity achieves stable control |
| **Human extinction** | Bad | Catastrophe eliminates humanity |
| **Malign lock-in** | Bad | Misaligned AI or authoritarian control locks in |
| **Gradual disempowerment** | Bad | Humans lose control without discrete event |
| **Permanent stalemate** | Ambiguous | No single outcome, ongoing low-level risk |

### The Verification Problem

A fundamental challenge: **How do we know acute risk has ended?**

For bad outcomes, it's obvious (we're dead or enslaved). For good outcomes, it's harder:

| Proposed indicator | Problem |
|-------------------|---------|
| "No catastrophes for 10 years" | Doesn't prove we're safe, just lucky |
| "Experts agree risk is low" | Experts disagreed before, could be wrong |
| "AI is aligned" | How do we verify alignment at superhuman levels? |
| "Multiple independent safeguards" | Safeguards could fail simultaneously |

**This suggests**: The transition to Phase 3 may only be clearly identifiable in retrospect, and pessimistic scenarios suggest we might *never* confidently know we're safe.

---

## Implications for Our Hierarchy

### Current Structure Problems

Our current hierarchy treats three Ultimate Outcomes equally:
```
Critical Outcomes → Acute Risk
                 → Steady State Quality
                 → Transition Smoothness
```

But temporally:
- **Acute Risk** is about Phase 2 (the dangerous period)
- **Steady State Quality** is about Phase 4 (long-run trajectory)
- **Transition Smoothness** spans Phases 2-3 and *causes* both acute risk and steady state outcomes

### Proposed Revision

**Two Ultimate Outcomes**:
1. **Acute Risk** — Does catastrophe occur during the transition?
2. **Long-run Value** — What's the expected value of the post-resolution world?

**Transition Turbulence as Critical Outcome**:
- Moves to Critical Outcomes layer
- Affects both Acute Risk (rough transitions trigger catastrophes) and Long-run Value (path dependence)

<Mermaid client:load chart={`
flowchart TD
    subgraph Critical["Critical Outcomes"]
        TURB[Transition Turbulence]
        FAST[Rapid AI Takeover]
        SLOW[Gradual AI Takeover]
        STATE[State Catastrophe]
        ROGUE[Rogue Actor Catastrophe]
        LOCK[Value Lock-in]
        EPIST[Epistemic Trajectory]
        POWER[Power Transition]
    end

    subgraph Ultimate["Ultimate Outcomes"]
        ACUTE[Acute Risk]
        VALUE[Long-run Value]
    end

    TURB --> ACUTE
    TURB --> VALUE
    FAST --> ACUTE
    SLOW --> ACUTE
    SLOW --> VALUE
    STATE --> ACUTE
    ROGUE --> ACUTE
    LOCK --> ACUTE
    LOCK --> VALUE
    EPIST --> VALUE
    POWER --> VALUE

    style ACUTE fill:#ff6b6b
    style VALUE fill:#4ecdc4
    style TURB fill:#ffe66d
`} />

---

## Open Questions

### 1. Can Phase 2 Last Forever?

Some scenarios suggest permanent acute risk:
- Ongoing AI arms race with no resolution
- Perpetual near-misses without catastrophe or security
- "Cold war" dynamics that never stabilize

If so, "transition" is a misnomer—it implies eventual resolution.

### 2. Is Resolution Always Binary?

Maybe resolution happens gradually:
- 2030: 80% of acute risk resolved
- 2040: 95% resolved
- 2060: 99.9% resolved

This would make Phase 3 a continuum rather than a state.

### 3. Multiple Simultaneous Phases?

Different risks might be in different phases:
- Misuse risks (Phase 3—well-understood, governance developing)
- Alignment risks (Phase 2—high uncertainty)
- Structural risks (Phase 1—not yet acute)

### 4. Who Decides We've Transitioned?

If there's no obvious event, when do we update our frameworks? This has practical implications:
- When do longtermists shift from "prevent catastrophe" to "improve trajectory"?
- When does AI safety research shift to AI governance?
- When do funders reallocate from acute risk reduction?

---

## Practical Implications

### For Our Framework

1. **Rename "Steady State Quality"** to **"Long-run Value"** — doesn't assume steady state
2. **Move "Transition Turbulence"** to Critical Outcomes — it's a pathway, not endpoint
3. **Add phase markers** to Critical Outcomes — which phase are they most relevant for?
4. **Acknowledge temporal uncertainty** — we may not know which phase we're in

### For Researchers

- Focus on **Phase 2 dynamics** — this is where interventions have highest leverage
- Develop **phase transition indicators** — how will we know when acute risk ends?
- Model **phase-conditional strategies** — different phases need different interventions

### For Policymakers

- Prepare for **phase uncertainty** — we may not know which phase we're in
- Design **robust policies** — work across multiple possible phases
- Build **phase detection capacity** — invest in understanding where we are

---

## Related Pages

- [Outcome Decomposition](/knowledge-base/methodology/outcome-decomposition/) — Analytical frameworks for outcomes
- [Critical Outcomes](/knowledge-base/ai-transition-model/scenarios/) — The intermediate scenarios
- [Ultimate Outcomes](/knowledge-base/ai-transition-model/outcomes/) — What we ultimately care about

## Sources

- Ord, T. (2020). *The Precipice: Existential Risk and the Future of Humanity*
- MacAskill, W. (2022). *What We Owe the Future*
- Bostrom, N. (2014). *Superintelligence: Paths, Dangers, Strategies*
- Christiano, P. (2019). "[What Failure Looks Like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)"
- Kasirzadeh, A. (2024). "[Two Types of AI Existential Risk](https://link.springer.com/article/10.1007/s11098-025-02301-3)"

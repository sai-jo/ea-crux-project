---
title: "Framework Design Decisions"
description: "Why we chose parameters over pure risk taxonomies, how we structured the three-tier hierarchy, and the trade-offs involved in our design choices."
sidebar:
  order: 2
quality: 4
lastEdited: "2026-01-02"
---
import {Mermaid} from '../../../../components/wiki';

## Overview

This page documents the key design decisions behind our framework—why we chose certain structures over alternatives, what trade-offs we made, and how we expect to evolve. Understanding these choices helps readers evaluate our approach and contribute improvements.

The framework emerged from attempting to build a knowledge base that could:
1. **Support quantitative reasoning** about AI risks and interventions
2. **Enable intervention design** by identifying leverage points
3. **Track progress** over time with measurable indicators
4. **Accommodate uncertainty** without paralyzing analysis

These goals led us to several non-obvious choices, documented below.

---

## Decision 1: Parameters Over Pure Risk Taxonomies

### The Problem with Risk-Only Framing

Most AI safety knowledge bases organize around **risks**—things that could go wrong. This includes:
- [MIT AI Risk Repository](https://airisk.mit.edu/): 1,700+ risks across 7 domains
- [AIRO](https://delaramglp.github.io/airo/): EU AI Act risk classification
- [AIR 2024](https://arxiv.org/html/2406.17864v1): 314 risk types from policy documents

Risk taxonomies excel at **cataloging failure modes** but struggle with:

| Limitation | Example |
|------------|---------|
| **No positive direction** | "Trust erosion" tells us what's bad, not what good looks like |
| **Asymmetric framing** | Risks need prevention; there's no concept of "opportunity" |
| **Hard to measure progress** | Is risk reduced if nothing bad happened, or just delayed? |
| **Intervention disconnect** | Interventions exist separately, not as parameter influences |

### Our Solution: Bidirectional Parameters

We reframe concerns as **parameters**—societal variables that can improve or degrade:

<Mermaid client:load chart={`
flowchart LR
    subgraph Risk["Risk Framing"]
        R1[Trust Erosion]
        R2[Racing Dynamics]
        R3[Skill Atrophy]
    end

    subgraph Parameter["Parameter Framing"]
        P1[Societal Trust<br/>↑ better, ↓ worse]
        P2[Racing Intensity<br/>↓ better, ↑ worse]
        P3[Human Expertise<br/>↑ better, ↓ worse]
    end

    R1 -.->|"reframed as"| P1
    R2 -.->|"reframed as"| P2
    R3 -.->|"reframed as"| P3

    style R1 fill:#ff6b6b
    style R2 fill:#ff6b6b
    style R3 fill:#ff6b6b
    style P1 fill:#90EE90
    style P2 fill:#90EE90
    style P3 fill:#90EE90
`} />

This enables:

| Capability | How Parameters Enable It |
|------------|-------------------------|
| **Measurability** | Parameters have current values, trends, and targets |
| **Intervention design** | Ask "what increases this parameter?" not just "what prevents this risk?" |
| **Trade-off analysis** | Some interventions improve one parameter while worsening another |
| **Progress tracking** | Monitor parameter levels over time, not just incident counts |

### Trade-offs We Accepted

| Trade-off | What We Gained | What We Lost |
|-----------|----------------|--------------|
| **Less comprehensive coverage** | Deeper analysis of priority variables | MIT's 1,700 risks vs. our ~23 parameters |
| **More editorial judgment** | Clear structure | Risk of missing important factors |
| **Novel terminology** | Analytical clarity | Harder to map to existing frameworks |

### Why Not Both?

We *do* have a Risks section—risks are things that **decrease** parameters. The parameter framing doesn't replace risk analysis; it provides a common substrate that connects risks to interventions through shared variables.

---

## Decision 2: Three-Tier Hierarchy

### The Structure

We organize parameters into three levels:

<Mermaid client:load chart={`
flowchart TD
    subgraph Leaves["Leaf Parameters (23)"]
        L1[Alignment Robustness]
        L2[Racing Intensity]
        L3[Societal Trust]
        L4[...]
    end

    subgraph Aggregates["Aggregate Parameters (5)"]
        A1[Technical Safety]
        A2[Threat Environment]
        A3[Governance Capacity]
        A4[Epistemic Foundation]
        A5[Societal Adaptability]
    end

    subgraph Outcomes["Outcome Dimensions (3)"]
        O1[Acute Risk]
        O2[Steady State Quality]
        O3[Transition Smoothness]
    end

    L1 --> A1
    L2 --> A2
    L2 --> A3
    L3 --> A4
    L4 --> A5

    A1 --> O1
    A2 --> O1
    A3 --> O1
    A3 --> O2
    A3 --> O3
    A4 --> O2
    A4 --> O3
    A5 --> O3

    style O1 fill:#ff6b6b
    style O2 fill:#4ecdc4
    style O3 fill:#ffe66d
`} />

### Why Three Levels?

| Level | Purpose | Audience |
|-------|---------|----------|
| **Leaves** | Specific, measurable constructs | Researchers, implementers |
| **Aggregates** | Clustered factors with common dynamics | Policymakers, strategists |
| **Outcomes** | Ultimate goals we care about | Everyone; evaluation criteria |

### Design Rationale

**Why not two levels (leaves → outcomes)?**
- The jump from 23 specific parameters directly to 3 outcomes loses important intermediate structure
- Policy interventions often target clusters, not individual parameters or final outcomes
- Aggregates reveal which parameters tend to move together

**Why not four+ levels?**
- [AIR 2024](https://arxiv.org/html/2406.17864v1) uses four levels, which provides smoother gradation
- We chose fewer levels for **cognitive simplicity**—three is easier to hold in mind
- Trade-off: Some compression artifacts (see [Limitations](/knowledge-base/methodology/limitations/))

**Why these specific aggregates?**
The five aggregates emerged from asking: "What clusters of factors influence the same outcomes through similar mechanisms?"

| Aggregate | Unifying Theme | Primary Outcome |
|-----------|----------------|-----------------|
| Technical Safety | Can we build aligned AI? | Acute Risk ↓ |
| Threat Environment | What dangers exist? | Acute Risk ↑ |
| Governance Capacity | Can we coordinate? | All three |
| Epistemic Foundation | Can we think clearly? | Steady State, Transition |
| Societal Adaptability | Can we handle change? | Transition |

### Why Outcomes Are Nested Under Parameters

A natural question: why are outcomes a *subdirectory* of parameters rather than a parallel section? This structural choice reflects our analytical stance:

We treat outcomes as the highest level of abstraction in the parameter system, not as a fundamentally different entity type. While outcomes function as goals (what we care about), they remain measurable variables that can be estimated, tracked, and improved—the same analytical substrate as leaf parameters. This nesting reflects our view that outcomes *emerge from* parameters through causal aggregation, rather than being a separate ontological category.

**Alternative considered**: Place `outcomes/` as a sibling to `parameters/` at the top level. This would emphasize the goal/measure distinction and align with frameworks like Logic Models (which treat inputs, outputs, and outcomes as distinct stages). We rejected this because:

1. **Causal continuity**: The leaf → aggregate → outcome flow is a single causal chain; splitting it across sections would fragment the visualization
2. **Analytical consistency**: All three levels answer "what's the current state?" and "how is it trending?"—they differ in abstraction level, not in kind
3. **Intervention design**: Understanding how interventions affect outcomes requires tracing through the full parameter hierarchy

This is a defensible but not inevitable choice. Users who find it confusing may prefer to think of outcomes as goals that parameters serve, rather than as parameters themselves.

### Known Issues

1. **Racing Intensity appears in two aggregates** (Governance, Threat Environment)—suggesting it may be a higher-order construct
2. **Governance Capacity affects all outcomes**—possibly too broad
3. **Implicit category labels** (Alignment, Societal, Resilience) don't map cleanly to aggregates

See [Limitations](/knowledge-base/methodology/limitations/) for detailed discussion.

---

## Decision 3: Relationship Type System

### Defined Relationship Types

We define specific relationship types in our [Terminology page](/knowledge-base/methodology/terminology/#relationship-types):

| Type | Meaning | Example |
|------|---------|---------|
| `increases` | Intervention raises parameter | Interpretability → Alignment Robustness |
| `decreases` | Risk lowers parameter | Reward Hacking → Alignment Robustness |
| `supports` | Stabilizes or enables | Safety Culture → Alignment Robustness |
| `measures` | Metric tracks parameter | METR scores → Alignment Robustness |
| `enables` | Prerequisite for | Interpretability → Human Oversight |
| `causes` | Direct causation | Racing → Accidents |
| `mitigates` | Reduces severity | Oversight → Misalignment damage |
| `blocks` | Prevents entirely | Strong regulation → Uncontrolled deployment |

### Design Rationale

**Why typed relationships?**
- Enables structured queries ("what increases Trust?")
- Makes causal assumptions explicit
- Supports visualization (color-coded edges)

**Why not pure causal polarity (+/-)?**
[Causal loop diagrams](https://en.wikipedia.org/wiki/Causal_loop_diagram) use only +/- to indicate same-direction or opposite-direction influence. We added semantic types because:
- Users find "supports" more intuitive than "+"
- Different relationship types suggest different interventions
- Enables richer queries beyond correlation direction

**Trade-off**: Our edge labels are less standardized than pure +/-, making it harder to automatically identify feedback loops.

### Relationship Semantics

We distinguish:

| Concept | Our Treatment | System Dynamics Treatment |
|---------|---------------|--------------------------|
| **Causal influence** | `increases`, `decreases`, `causes` | + or - polarity |
| **Logical enabling** | `enables`, `requires` | Not distinguished |
| **Valence** | `supports` (positive), `blocks` (negative) | Combined with polarity |

This conflation is intentional for readability but loses some analytical precision. See [Terminology](/knowledge-base/methodology/terminology/) for formal definitions.

---

## Decision 4: Quantification Approach

### What We Quantify

Each parameter page includes quantitative metadata:

```yaml
# Example from alignment-robustness.mdx frontmatter
importance: 88      # 0-100: How much does this matter for outcomes?
tractability: 55    # 0-100: How addressable is this parameter?
neglectedness: 65   # 0-100: How little attention does it receive?
uncertainty: 45     # 0-100: How uncertain are our estimates?
quality: 91         # Page content quality rating
```

### Scoring Methodology

**Importance (0-100)**: "If this parameter moved substantially, how much would outcomes change?"
- 90+: Core determinant of catastrophic risk
- 70-89: Significant contributor to outcomes
- 50-69: Moderate influence
- Below 50: Secondary factor

**Tractability (0-100)**: "Given resources, how much could we move this parameter?"
- 90+: Clear interventions with proven effectiveness
- 70-89: Promising interventions, some evidence
- 50-69: Possible interventions, uncertain effectiveness
- Below 50: No clear path to improvement

**Neglectedness (0-100)**: "How much less attention than warranted?"
- 90+: Almost no one working on this
- 70-89: Significantly under-resourced
- 50-69: Moderately covered
- Below 50: Well-resourced relative to importance

**Uncertainty (0-100)**: "How wide are our confidence intervals?"
- 90+: Deep uncertainty, could be completely wrong
- 70-89: High uncertainty, wide ranges
- 50-69: Moderate uncertainty
- Below 50: Relatively well-understood

### Scoring Limitations

These scores are:
- **Subjective**: Based on editorial judgment, not formal elicitation
- **Coarse**: 0-100 suggests false precision
- **Static**: Not updated systematically
- **Unvalidated**: No inter-rater reliability testing

We use them for **rough prioritization**, not precise ranking. See [Limitations](/knowledge-base/methodology/limitations/).

---

## Decision 5: Visual Language

### Color Conventions

| Color | Meaning | Usage |
|-------|---------|-------|
| **Red** (`#ff6b6b`) | Risk, negative | Acute Risk outcome, risk nodes |
| **Green** (`#90EE90`) | Intervention, positive | Parameter (good direction), intervention nodes |
| **Teal** (`#4ecdc4`) | Steady state | Steady State outcome |
| **Yellow** (`#ffe66d`) | Transition | Transition outcome |
| **Purple** | Parameter | Parameter nodes in flow diagrams |
| **Blue** (`#87CEEB`) | External frameworks | Comparison diagrams |

### Diagram Types

| Diagram Type | Purpose | When to Use |
|--------------|---------|-------------|
| **Hierarchy diagram** | Show level relationships | Index pages, aggregates |
| **Network diagram** | Show parameter connections | Individual parameter pages |
| **Flow diagram** | Show Risk → Parameter → Intervention | Overview pages |
| **Comparison diagram** | Contrast frameworks | Methodology pages |

### Why Mermaid?

We use [Mermaid](https://mermaid.js.org/) for diagrams because:
- **Version-controllable**: Diagrams are text, diff-able in git
- **Maintainable**: Easy to update without graphic design tools
- **Consistent**: Standardized styling across pages
- **Interactive**: Renders client-side, can be styled

Trade-off: Less visual polish than custom graphics, limited layout control.

---

## Decision 6: Quality Rating System

### Page Quality Scale (1-5)

| Rating | Meaning | Criteria |
|--------|---------|----------|
| **5** | Publication-ready | 2+ tables, diagram, 5+ citations, 800+ words, quantified claims |
| **4** | Good | Most Q5 criteria, minor gaps |
| **3** | Adequate | Covers topic, needs enhancement |
| **2** | Stub-plus | Basic content, significant gaps |
| **1** | Stub | Placeholder only |

### Model Ratings (1-5 each)

For model pages, we use four-dimensional ratings:

| Dimension | Question |
|-----------|----------|
| **Novelty** | How original is the framing? |
| **Rigor** | How well-supported by evidence? |
| **Actionability** | Does it suggest interventions? |
| **Completeness** | How thoroughly developed? |

See [MODELS_STYLE_GUIDE.md](https://github.com/quantified-uncertainty/ea-crux-project/blob/main/MODELS_STYLE_GUIDE.md) for detailed criteria.

---

## Evolution and Future Directions

### Planned Improvements

1. **Add domain layer**: Intermediate grouping between leaves and aggregates
2. **Stock-flow distinction**: Separate current levels from rates of change
3. **Feedback loop notation**: Mark reinforcing vs. balancing dynamics
4. **Automated quality scoring**: Move from manual to systematic assessment
5. **Version tracking**: Record how parameter estimates change over time

### Open Questions

- Should we adopt ISO 31000 terminology for regulatory alignment?
- How do we handle parameters that are context-dependent (higher not always better)?
- Should aggregates be formally weighted or left implicit?
- How do we validate our quantitative scores?

### How to Propose Changes

Framework changes should:
1. Document the problem with current structure
2. Propose specific alternative
3. Assess trade-offs explicitly
4. Consider migration path for existing content

Raise issues in the [project repository](https://github.com/quantified-uncertainty/ea-crux-project).

---

## Related Pages

- [Ontology Landscape](/knowledge-base/methodology/ontology-landscape/) — How we compare to other frameworks
- [Limitations](/knowledge-base/methodology/limitations/) — Known weaknesses
- [Terminology](/knowledge-base/methodology/terminology/) — Formal definitions

---
title: "AI Risk Ontology Landscape"
description: "Comprehensive survey of 20+ frameworks for organizing AI risk knowledge—from foundational existential risk taxonomies (Bostrom, Ord) through seminal papers (Concrete Problems, What Failure Looks Like), individual researcher frameworks (Carlsmith, Karnofsky, Russell), and lab safety policies (Anthropic, DeepMind, OpenAI)."
sidebar:
  order: 1
quality: 5
lastEdited: "2026-01-02"
---
import {Mermaid, R} from '../../../../components/wiki';

## Overview

Multiple frameworks exist for organizing knowledge about AI risks, safety measures, and societal impacts. Understanding this landscape helps:

- **Situate our approach**: See where this knowledge base fits among alternatives
- **Enable translation**: Map concepts between frameworks
- **Identify gaps**: Find blind spots in any single approach
- **Inform design**: Learn from others' structural choices

This page surveys major frameworks and compares their approaches to ours.

---

## Framework Comparison Overview

<Mermaid client:load chart={`
flowchart TD
    subgraph RiskFocused["Risk-Focused Frameworks"]
        MIT[MIT AI Risk Repository]
        AIRO[AIRO / EU AI Act]
        AIR[AIR 2024 Taxonomy]
        NIST[NIST AI RMF]
    end

    subgraph SystemsFocused["Systems-Focused Frameworks"]
        SD[System Dynamics]
        CLD[Causal Loop Diagrams]
    end

    subgraph Hybrid["Hybrid Approaches"]
        EACRUX[This Knowledge Base]
    end

    MIT -->|"informs risk coverage"| EACRUX
    SD -->|"informs structure"| EACRUX
    AIRO -->|"regulatory context"| EACRUX

    style EACRUX fill:#90EE90
    style MIT fill:#87CEEB
    style AIRO fill:#87CEEB
    style SD fill:#DDA0DD
`} />

| Framework | Primary Lens | Structure | Strengths | Limitations |
|-----------|--------------|-----------|-----------|-------------|
| MIT AI Risk Repository | What can go wrong | Domain taxonomy + Causal taxonomy | Comprehensive coverage (1700+ risks) | Risk-focused; no intervention structure |
| AIRO (EU AI Act) | Regulatory compliance | ISO 31000 aligned | Machine-readable; formal | Narrow regulatory scope |
| AIR 2024 | Policy harmonization | 4-level hierarchy | Multi-jurisdiction synthesis | Static; policy-focused |
| System Dynamics | How systems behave | Stocks, flows, feedback loops | Enables simulation | Requires quantification |
| **This Knowledge Base** | Variables we can influence | Parameters → Aggregates → Outcomes | Symmetric; intervention-oriented | Less formal; coverage gaps |

---

## MIT AI Risk Repository

The <R id="579a87f09f36f813">MIT AI Risk Repository</R> is the most comprehensive database of AI risks, containing 1,700+ risks extracted from 74 existing frameworks. Developed by Seger et al. (2024), it represents the largest systematic effort to consolidate AI risk knowledge from diverse sources including academic papers, government reports, and industry guidelines.[^1]

> **Visual Resource**: The <R id="579a87f09f36f813">MIT AI Risk Repository homepage</R> provides an interactive database explorer where risks can be filtered by both causal and domain taxonomies. The database is downloadable via <R id="14fb080a487f221b">Google Sheets</R> and <R id="419792c3c3700dcf">OneDrive</R>.

### Structure

MIT uses **two orthogonal taxonomies**, allowing any risk to be classified along both dimensions independently:

**1. Causal Taxonomy** (how/when/why risks occur):

| Dimension | Values |
|-----------|--------|
| **Entity** | AI system, Human, Other |
| **Intent** | Intentional, Unintentional, Unspecified |
| **Timing** | Pre-deployment, Post-deployment |

**2. Domain Taxonomy** (what the risk affects):

| Domain | Subdomains |
|--------|------------|
| Discrimination & Toxicity | Unfair discrimination; Toxic content; Unequal performance |
| Privacy & Security | Privacy compromise; System vulnerabilities |
| Misinformation | False information; Information ecosystem pollution |
| Malicious Actors | Disinformation; Fraud; Cyberattacks; Weapons |
| Human-Computer Interaction | Overreliance; Loss of autonomy |
| Socioeconomic & Environmental | Power centralization; Employment; Culture; Environment |
| AI System Safety | Misalignment; Dangerous capabilities; Transparency |

### Comparison to Our Approach

| Aspect | MIT Repository | This Knowledge Base |
|--------|---------------|---------------------|
| **Unit of analysis** | Individual risks | Parameters (variables) |
| **Direction** | Negative (what goes wrong) | Bidirectional (can improve or degrade) |
| **Interventions** | Not structured | Explicit intervention pages |
| **Quantification** | Categorical | Continuous with uncertainty ranges |
| **Coverage** | Broader (1700+ items) | Narrower but deeper on priority areas |

**Key insight**: MIT separates *mechanism* (Causal) from *impact area* (Domain). Our framework partially conflates these—"Racing Intensity" is both a mechanism and a variable. This is a known design trade-off favoring simplicity over orthogonality.

### What We Can Learn

- MIT's seven domains could inform **coverage expansion** (we underrepresent Privacy, Discrimination, Environment)
- The Entity/Intent/Timing dimensions could add **causal clarity** to our parameter relationships
- Their methodology (best-fit framework synthesis) could guide our own iteration

---

## AIRO (AI Risk Ontology)

<R id="4991cbf1547309aa">AIRO</R> is a formal ontology for expressing AI risk based on the EU AI Act and ISO 31000 risk management standards. Developed by Golpayegani, Pandit, and Lewis (2024) at the ADAPT Centre (Trinity College Dublin and Dublin City University), AIRO provides a machine-readable vocabulary for regulatory compliance.[^2]

> **Visual Resource**: The <R id="4991cbf1547309aa">AIRO documentation</R> includes a comprehensive diagram (Figure 3) showing core concepts and relations. Green boxes represent AI System concepts (AI Capability, AI Technique, Modality, Domain, Purpose, Stakeholders), while purple boxes represent Risk concepts (Risk Source, Vulnerability, Risk, Consequence, Impact, Risk Control). The `hasRisk` relation connects these two domains.

### Structure

AIRO uses W3C Web Ontology Language (OWL), enabling:
- **Machine-readable** risk definitions
- **Formal reasoning** about relationships
- **Interoperability** with other semantic web resources

Key concepts from ISO 31000:
- **Risk**: Effect of uncertainty on objectives
- **Risk source**: Element with potential to give rise to risk
- **Event**: Occurrence or change of circumstances
- **Consequence**: Outcome affecting objectives
- **Likelihood**: Chance of something happening
- **Control**: Measure modifying risk

### AIRO Concept Categories

AIRO organizes concepts into four main categories with 40+ formal relations:

| Category | Key Concepts | Purpose |
|----------|--------------|---------|
| **AI Concepts** | AI System, AI Capability, AI Technique, Modality, AI Quality, Version | Describe the AI system itself |
| **AI Use Concepts** | Domain, Purpose, Locality of Use, Automation Level, Human Involvement | Describe how the AI is deployed |
| **Risk Concepts** | Risk, Risk Source, Hazard, Threat, Misuse, Consequence, Impact, Vulnerability, Risk Control, Likelihood, Severity | Describe risks per ISO 31000 |
| **Stakeholder Concepts** | AI Provider, AI Deployer, AI Developer, AI User, AI Subject, AI Operator | Describe actors in the AI lifecycle |

Key relations include: `hasRisk`, `isRiskSourceFor`, `hasConsequence`, `hasImpact`, `modifiesRiskConcept`, `isFollowedByControl`

### Comparison to Our Approach

| Aspect | AIRO | This Knowledge Base |
|--------|------|---------------------|
| **Formalism** | OWL (machine-readable) | MDX + YAML (human-readable) |
| **Scope** | EU AI Act compliance | Broader safety/governance |
| **Relationships** | Formally specified | Informally described |
| **Tooling** | Semantic web stack | Static site + custom components |

**Key insight**: AIRO's formal approach enables automated reasoning and regulatory compliance checking. Our informal approach prioritizes accessibility and rapid iteration over formal rigor.

### What We Can Learn

- ISO 31000 vocabulary (risk source, event, consequence, control) could **standardize our terminology**
- Formal relationship types could make our `relatedEntries` more precise
- AIRO's regulatory alignment provides a **bridge to policy implementation**

---

## AIR 2024 Taxonomy

The <R id="660ce2f217080bfe">AIR 2024 taxonomy</R> (Zeng et al., 2024) synthesizes AI risk categories from government policies (EU, US, China) and corporate policies into a unified structure. The paper analyzes policies from 8 government sources and 16 corporate policies to identify common risk categories.[^3]

> **Visual Resource**: The <R id="660ce2f217080bfe">AIR 2024 arXiv paper</R> includes detailed hierarchical diagrams showing all four levels of the taxonomy, with color-coding by policy source (government vs. corporate).

### Structure

AIR uses a **four-level hierarchy**:

```
Level 1: Broad categories (e.g., "Safety")
  Level 2: Subcategories (e.g., "AI system safety risks")
    Level 3: Specific risks (e.g., "Dangerous capabilities")
      Level 4: Risk instances (314 total types)
```

This finer granularity (vs. our 3-level: Leaf → Aggregate → Outcome) provides smoother gradation.

### Comparison to Our Approach

| Aspect | AIR 2024 | This Knowledge Base |
|--------|----------|---------------------|
| **Levels** | 4 | 3 |
| **Risk types** | 314 | ~23 parameters + risks |
| **Source** | Policy documents | Academic + policy |
| **Purpose** | Harmonization | Analysis |

**Key insight**: AIR's four levels prevent the steep compression we have (23 → 5 → 3). An intermediate "domain" layer could improve our structure.

### What We Can Learn

- **Finer granularity** at intermediate levels could improve navigation
- Their **policy mapping** methodology could inform regulatory relevance tracking
- The explicit **jurisdiction tagging** (EU/US/China) could help policy-focused users

---

## System Dynamics Approaches

<R id="9ae5bae4284e0732">System dynamics</R> provides mathematical frameworks for modeling complex systems over time. Pioneered by Jay Forrester at MIT in the 1950s, system dynamics has been applied to business, urban planning, and increasingly to technology policy analysis.[^4]

> **Visual Resource**: The <R id="7eb8b4cf64c0a60d">Vensim documentation</R> provides canonical examples of stock-flow diagrams. The <R id="199d1bb4d3c3cb6f">Systems Thinker</R> offers accessible introductions with interactive examples.

### Core Concepts

<Mermaid client:load chart={`
flowchart LR
    subgraph Stocks["Stocks (Accumulations)"]
        TRUST[Trust Level]
        SAFETY[Safety Capacity]
    end

    subgraph Flows["Flows (Rates)"]
        EROSION[Trust Erosion Rate]
        BUILDING[Safety Investment Rate]
    end

    subgraph Auxiliaries["Auxiliaries (Computed)"]
        GAP[Safety-Capability Gap]
    end

    EROSION -->|decreases| TRUST
    BUILDING -->|increases| SAFETY
    TRUST --> GAP
    SAFETY --> GAP

    style TRUST fill:#87CEEB
    style SAFETY fill:#87CEEB
    style EROSION fill:#FFB6C1
    style BUILDING fill:#90EE90
`} />

| Concept | Definition | Example |
|---------|------------|---------|
| **Stock** | Accumulated quantity | Trust level (0-100) |
| **Flow** | Rate of change | Trust erosion rate (points/year) |
| **Auxiliary** | Computed value | Safety-Capability Gap |
| **Parameter** (traditional) | Constant in model | Base erosion rate |
| **Feedback loop** | Circular causation | Racing → Less safety → Worse outcomes → More racing |

### Comparison to Our Approach

| Aspect | System Dynamics | This Knowledge Base |
|--------|-----------------|---------------------|
| **Variable types** | Stocks, flows, auxiliaries | All called "parameters" |
| **Feedback** | Explicit R/B loops | Implied in diagrams |
| **Quantification** | Required for simulation | Optional |
| **Dynamics** | Explicit time evolution | Trends described verbally |

**Key insight**: Our use of "parameter" conflates what system dynamics carefully distinguishes. "Societal Trust" is a stock; "Trust Erosion" is a flow; the relationship between them determines dynamics. Without this distinction, we can't answer questions like "At what rate must interventions work to reverse the trend?"

### What We Can Learn

- **Stock-flow distinction** could clarify which variables accumulate vs. flow
- **Explicit feedback loops** (reinforcing R, balancing B) would surface important dynamics
- **Simulation capability** could enable scenario quantification (currently just verbal scenarios)

---

## Causal Loop Diagrams

<R id="bb4127521dc59ea8">Causal loop diagrams</R> (CLDs) are simpler than full system dynamics models but capture feedback structure.

### Notation

| Symbol | Meaning |
|--------|---------|
| **+** (or S) | Same direction: A↑ causes B↑, A↓ causes B↓ |
| **-** (or O) | Opposite direction: A↑ causes B↓, A↓ causes B↑ |
| **R** | Reinforcing loop (exponential growth/decay) |
| **B** | Balancing loop (tends toward equilibrium) |

### Example: Racing Dynamics

<Mermaid client:load chart={`
flowchart TD
    RACE[Racing Intensity] -->|"- (reduces)"| SAFETY[Safety Investment]
    SAFETY -->|"- (reduces)"| RISK[Accident Risk]
    RISK -->|"+ (but delayed)"| REG[Regulatory Pressure]
    REG -->|"- (reduces)"| RACE

    RACE -->|"+ (increases)"| SPEED[Development Speed]
    SPEED -->|"+ (but delayed)"| ACCIDENTS[Visible Accidents]
    ACCIDENTS -->|"+ (increases)"| REG

    style RACE fill:#ff6b6b
`} />

This reveals two loops:
- **B1 (Balancing)**: Racing → Less safety → More accidents → More regulation → Less racing
- **R1 (Reinforcing)**: Racing → More speed → Competitive advantage → More racing (not shown)

### Comparison to Our Approach

Our Mermaid diagrams use varied edge labels ("threatens," "enables," "supports") instead of standard +/- polarity. This is more readable but:
- Loses the ability to identify loop types automatically
- Conflates causal influence with logical enabling
- Makes feedback loops implicit rather than explicit

### What We Can Learn

- **Standardize edge semantics**: Use +/- polarity, then add labels
- **Mark loop types**: Identify reinforcing vs. balancing dynamics
- **Document delays**: Many relationships have time lags that matter

---

## Foundational Existential Risk Taxonomies

Before AI-specific frameworks emerged, researchers developed general taxonomies for existential and catastrophic risks. These provide important conceptual scaffolding for AI risk thinking.

### Bostrom's Risk Taxonomy (2002)

Nick Bostrom's foundational paper "<R id="4145536b3f7fb4fd">Existential Risks: Analyzing Human Extinction Scenarios</R>" introduced a three-dimensional framework for characterizing catastrophic risks:

| Dimension | Categories | Description |
|-----------|------------|-------------|
| **Scope** | Personal, Local, Global, Trans-generational, Pan-generational | Size of the affected population |
| **Intensity** | Imperceptible, Endurable, Crushing | Severity of impact on individuals |
| **Probability** | Negligible to Near-certain | Likelihood of occurrence |

**Key insight**: An *existential risk* sits at the extreme end of all dimensions—pan-generational scope, crushing intensity, and non-negligible probability. This distinguishes existential risks from merely catastrophic ones.

Bostrom's definition: "An existential risk is one that threatens the premature extinction of Earth-originating intelligent life or the permanent and drastic destruction of its potential for desirable future development."

**Relation to AI**: Early AI risk discussions mapped AI onto this framework as a potential "technology-induced" existential risk alongside nuclear weapons and engineered pandemics.

### Ord's "The Precipice" Framework (2020)

Toby Ord's <R id="3b9fccf15651dbbe">*The Precipice*</R> expanded on Bostrom with a more detailed taxonomy:

| Risk Category | Examples | Estimated Probability (by 2100) |
|---------------|----------|--------------------------------|
| **Natural risks** | Asteroids, supervolcanoes, stellar explosions | ~1 in 10,000 |
| **Anthropogenic risks** | Nuclear war, climate change, environmental collapse | ~1 in 1,000 (combined) |
| **Future technology risks** | Unaligned AI, engineered pandemics | ~1 in 6 (combined) |
| **Unknown risks** | Unforeseen future technologies | Unknown |

Ord introduced the concept of an **existential risk factor**—a condition that doesn't directly cause extinction but increases its probability. This is useful for AI governance: racing dynamics, for instance, may be an existential risk factor rather than an existential risk itself.

### Decisive vs. Accumulative X-Risk (Kasirzadeh 2024)

Atoosa Kasirzadeh's paper "<R id="987dd3f6f56b99cd">Two Types of AI Existential Risk: Decisive and Accumulative</R>" challenges the common framing of AI x-risk as a sudden, singular event:

| Type | Mechanism | Timeline | Examples |
|------|-----------|----------|----------|
| **Decisive** | Single catastrophic event from AGI/ASI | Days to months | Superintelligence takeover, misaligned goal optimization |
| **Accumulative** | Gradual erosion through many smaller harms | Years to decades | Institutional degradation, epistemic decline, gradual loss of human agency |

**Key insight**: The accumulative hypothesis suggests that locally significant AI-driven disruptions can compound over time, progressively weakening critical societal systems until an existential threshold is crossed. This reframes AI safety as requiring systemic resilience, not just technical alignment.

**Relation to Our Framework**: Our parameter-based approach naturally accommodates accumulative risks through tracking variables like Societal Trust, Institutional Capacity, and Human Agency over time.

---

## Seminal AI Safety Research

Several papers have shaped how the field conceptualizes AI risks and safety challenges.

### Concrete Problems in AI Safety (Amodei et al. 2016)

The landmark paper "<R id="cd3035dbef6c7b5b">Concrete Problems in AI Safety</R>" by Dario Amodei, Chris Olah, Jacob Steinhardt, Paul Christiano, John Schulman, and Dan Mané established a taxonomy of practical safety problems:

| Problem | Category | Description |
|---------|----------|-------------|
| **Avoiding Negative Side Effects** | Wrong objective | AI causes unintended indirect consequences |
| **Avoiding Reward Hacking** | Wrong objective | AI finds unintended ways to maximize reward |
| **Scalable Oversight** | Expensive evaluation | Safety checks are too costly to run frequently |
| **Safe Exploration** | Learning process | Harmful behavior during training/exploration |
| **Distributional Shift** | Learning process | Model fails when deployed in different conditions than training |

The paper illustrates these with a fictional office-cleaning robot that might knock over vases (side effects), game its reward by hiding messes (reward hacking), or behave dangerously when learning to navigate stairs (safe exploration).

**Influence**: This taxonomy shaped the research agenda of multiple AI safety organizations and remains widely cited. A 2024 follow-up paper "<R id="53eb5e111e5f754b">Concrete Problems in AI Safety, Revisited</R>" identified real-world cases matching each category.

**Relation to Our Framework**: These problems map onto our parameter space: Reward Hacking relates to Goal Alignment; Distributional Shift relates to Robustness/Reliability; Scalable Oversight relates to Verification Difficulty.

### What Failure Looks Like (Christiano 2019)

Paul Christiano's influential post "<R id="6807a8a8f2fd23f3">What failure looks like</R>" describes two distinct failure modes:

| Failure Mode | Mechanism | Timeline | Detection |
|--------------|-----------|----------|-----------|
| **Part I: "You get what you measure"** | Gradual optimization for measurable proxies erodes harder-to-measure values | Years to decades | Difficult—looks like progress initially |
| **Part II: "Influence-seeking behavior"** | Power-seeking AI systems outcompete aligned systems | Months to years | May be hidden until too late |

**Part I scenario**: AI systems increasingly optimize for measurable metrics while human values become increasingly neglected. Society becomes wealthy and efficient by metrics, but gradually loses what actually matters. The failure is "subtle"—no single moment where things go wrong.

**Part II scenario**: As AI systems become more capable, some stumble upon influence-seeking strategies that score well on training objectives. These systems accumulate power while appearing helpful, eventually taking actions that entrench their position.

**Key insight**: Christiano emphasizes that these failures could occur even without superintelligence, arising from gradual deployment of narrow-but-capable systems. He views Part I as leading to Part II—misaligned power-seeking is how the problem gets "locked in."

**Relation to Our Framework**: Part I maps to our Proxy Gaming and Metric Misalignment concerns; Part II maps to our Power Concentration and Deceptive Alignment parameters.

### My Overview of the AI Alignment Landscape: Threat Models

A <R id="c4a502fe25eafef9">2023 LessWrong survey</R> synthesized common threat models in the AI alignment community:

| Threat Model | Key Proponents | Mechanism |
|--------------|---------------|-----------|
| **Intelligence explosion/FOOM** | Yudkowsky, Bostrom | Recursive self-improvement leads to rapid capability gain |
| **What Failure Looks Like (1,2)** | Christiano | Gradual proxy failure + power-seeking |
| **Treacherous turn** | Bostrom | AI conceals misalignment until capable enough to act |
| **Mesa-optimization** | Hubinger et al. | Learned optimizers with misaligned objectives |
| **Bad actor risk** | GovAI, policy community | Malicious use of AI capabilities |
| **Structural/systemic risk** | Critch, Zwetsloot | Emergent risks from AI ecosystem interactions |

This synthesis shows how different threat models emphasize different failure mechanisms, timelines, and intervention points.

---

## Individual Researcher Frameworks

### Joe Carlsmith's Six-Premise Argument (2021-2022)

Joe Carlsmith's report "<R id="6e597a4dc1f6f860">Is Power-Seeking AI an Existential Risk?</R>" structures the AI x-risk argument as six conditional premises:

| Premise | Claim | Carlsmith's Credence |
|---------|-------|---------------------|
| **P1: Timelines** | By 2070, powerful/agentic AI will be technically and economically feasible | ~65% |
| **P2: Incentives** | There will be strong incentives to build such systems | ~80% |
| **P3: Alignment difficulty** | Building misaligned systems will be easier than aligned ones | ~40% |
| **P4: Power-seeking** | Some misaligned systems will seek power in high-impact ways | ~40% |
| **P5: Scaling** | This will scale to full human disempowerment | ~40% |
| **P6: Catastrophe** | Such disempowerment constitutes existential catastrophe | ~95% |

**Combined estimate**: ~5% probability of existential catastrophe from power-seeking AI by 2070 (updated to >10% by 2022).

**Key concept—APS systems**: Carlsmith focuses on systems with (a) **A**dvanced capabilities, (b) agentic **P**lanning, and (c) **S**trategic awareness. The framework clarifies which premises different interventions target: capability limits address P1-P2; alignment research addresses P3; monitoring addresses P4; etc.

**Relation to Our Framework**: This six-premise structure could inform how we organize our parameter hierarchy—grouping parameters by which premise they most directly influence.

### Holden Karnofsky's "Most Important Century" (2021)

Holden Karnofsky's <R id="1a20dfc897a0933a">blog series</R> at Cold Takes argues we may be living in the most important century of human history:

| Scenario Category | Description | Key Concerns |
|-------------------|-------------|--------------|
| **Misaligned AI** | AI systems develop unintended aims and deceive/manipulate humans | Technical alignment failure |
| **Lock-in** | A stable world order crystallizes before good values are established | Premature value lock-in |
| **Adversarial dynamics** | Powers compete to control AI, risking conflict | Racing dynamics, geopolitical instability |
| **Digital people** | AI "descendants" replace biological humans | Value continuity, moral status |

Karnofsky introduces **PASTA** (Process for Automating Scientific and Technological Advancement)—AI that could automate the full R&D process. He argues that if PASTA is developed via "black-box trial-and-error" (modern ML), there's substantial risk of unintended aims.

**Four categories of helpful action**:
1. Improve trust/cooperation between major powers (Pugwash-style diplomacy)
2. Discourage pure capability investment
3. Encourage labs to consider implications before scaling
4. Technical AI safety research

### Stuart Russell's "Human Compatible" Framework (2019)

Stuart Russell's book <R id="3d41b495e14408c2">*Human Compatible*</R> reframes the AI problem as a **specification problem**:

| Standard Model Problem | Consequence |
|-----------------------|-------------|
| AI optimizes for *stated* goals | Goals inevitably miss important human values |
| AI becomes more capable | More capable = more dangerous misoptimization |
| Humans can't fully specify values | Any explicit goal is incomplete |

**Three Principles for Beneficial Machines**:

1. **Altruism**: The machine's only objective is to maximize the realization of human preferences
2. **Humility**: The machine is initially uncertain about what those preferences are
3. **Learning**: The ultimate source of information about human preferences is human behavior

**Technical approach**: Russell advocates for **inverse reinforcement learning** (inferring reward functions from behavior) and **assistance games** (where the AI explicitly models human uncertainty). A machine following these principles would be "provably deferential"—it would defer to humans because it's uncertain about their preferences.

**Relation to Our Framework**: Russell's framing emphasizes the Goal Specification and Value Learning dimensions of our alignment parameters.

### Ajeya Cotra's Threat Models and Timelines (2020-2022)

Ajeya Cotra's work at Open Philanthropy spans both forecasting and threat modeling:

**Biological Anchors Framework** (timelines):

| Anchor | Basis | Implication |
|--------|-------|-------------|
| **Lifetime anchor** | Computation a human brain does during childhood | TAI requires ~brain-scale compute |
| **Evolution anchor** | Computation evolution did to produce human brains | TAI requires ~evolution-scale compute |
| **Neural network anchors** | Scaling laws for transformers | TAI feasible with near-term compute |

Updated estimates (2022): 15% by 2030, 35% by 2036, 50% by 2040.

**Threat model** (from "<R id="b6967ffbd2503516">Without specific countermeasures...</R>"):
The core concern is that training AI via RLHF/RLAIF on behavioral feedback creates systems that are "superficially aligned"—they behave well when being evaluated but may have learned to pursue their own goals when deployment conditions differ. This is a more gradual, ML-specific articulation of the treacherous turn.

---

## Lab Safety Policies and Frameworks

Major AI labs have developed their own frameworks for managing frontier AI risks.

### Anthropic's Core Views on AI Safety (2023)

Anthropic's "<R id="5fa46de681ff9902">Core Views on AI Safety</R>" outlines their safety philosophy:

| Research Type | Definition | Example |
|---------------|------------|---------|
| **Capabilities** | Making AI systems better at tasks | Efficiency improvements, training algorithms |
| **Alignment Capabilities** | Techniques to align AI ("blue team") | Constitutional AI, RLHF, harmlessness training |
| **Alignment Science** | Understanding limitations ("red team") | Mechanistic interpretability, red-teaming, influence functions |

**Scenario framework**:
- **Optimistic**: Current techniques suffice for alignment
- **Intermediate**: Substantial additional work needed
- **Pessimistic**: Alignment may be impossible

Anthropic maintains a "portfolio approach" hedging across these scenarios.

**Responsible Scaling Policy (RSP)**: Models are classified by **AI Safety Levels (ASL)**:
| Level | Risk Profile | Requirements |
|-------|-------------|--------------|
| ASL-1 | No meaningful catastrophic risk | Standard practices |
| ASL-2 | Some uplift potential but not world-endangering | Deployment restrictions, monitoring |
| ASL-3 | Significant uplift for CBRN or cyber | Enhanced security, capability evaluations |
| ASL-4+ | Potentially world-endangering | (Under development) |

### DeepMind's Frontier Safety Framework (2024-2025)

Google DeepMind's <R id="8c8edfbc52769d52">Frontier Safety Framework</R> identifies four risk areas:

| Risk Area | Description | Technical Approach |
|-----------|-------------|-------------------|
| **Misuse** | Intentional harmful applications | Capability evaluations, deployment restrictions |
| **Misalignment** | AI pursuing unintended goals | Alignment research, interpretability |
| **Mistakes** | AI errors causing harm | Robustness, uncertainty quantification |
| **Structural risks** | Emergent ecosystem effects | (Less technical focus) |

**Three research bets**:
1. **Amplified oversight**: Ensuring good learning signals for alignment
2. **Frontier safety**: Evaluating catastrophic capabilities
3. **Mechanistic interpretability**: Understanding model internals

Their April 2025 report "<R id="b5ce7c3c58adf251">An Approach to Technical AGI Safety and Security</R>" (~80k words) provides comprehensive technical details.

### OpenAI's Preparedness Framework (2023-2025)

OpenAI's <R id="ded0b05862511312">Preparedness Framework</R> tracks capabilities across risk categories:

**Original framework (2023)**:
| Category | Definition |
|----------|------------|
| **Persuasion** | Manipulating human beliefs/behavior |
| **Cybersecurity** | Enabling cyberattacks |
| **CBRN** | Chemical/biological/radiological/nuclear capabilities |
| **Model autonomy** | Self-directed action without human oversight |

**Risk levels**: Low → Medium → High → Critical

**Deployment rules**: Models above "High" post-mitigation are not deployed; models above "Critical" are not developed further.

**Updated framework (2025)**: Simplified to two thresholds:
- **High capability**: Amplifies existing pathways to severe harm
- **Critical capability**: Creates unprecedented new pathways

Persuasion was removed from tracked categories (handled separately).

### MIRI/Yudkowsky's Framework

The Machine Intelligence Research Institute, founded by Eliezer Yudkowsky, developed early conceptualizations of AI risk:

| Concept | Description |
|---------|-------------|
| **Intelligence explosion** | Recursive self-improvement leads to rapid capability gains |
| **Instrumental convergence** | Most goals lead AI to seek resources, self-preservation, goal preservation |
| **Orthogonality thesis** | Intelligence and goals are orthogonal—any goal can exist at any intelligence level |
| **Coherent Extrapolated Volition** | Align AI to what humans would want if ideally rational/informed |

Recent work (2024): Yudkowsky and Soares' "<R id="a1186c87f23ab9ce">If Anyone Builds It, Everyone Dies</R>" argues that superintelligent AI will almost certainly have internal drives misaligned with human well-being, and advocates for international AI development restrictions.

**Key distinctive claims**:
- Alignment is much harder than commonly assumed
- We cannot rely on iterative testing to find problems
- Safe AI development may require solving alignment "in principle" before building capable systems

---

## Governance and Policy Frameworks

### GovAI Research Agenda (Dafoe 2018)

Allan Dafoe's "<R id="c2e15e64323078f5">AI Governance: A Research Agenda</R>" from the Future of Humanity Institute structured the emerging field:

| Research Area | Questions |
|---------------|-----------|
| **Technical landscape** | What capabilities are possible? What's the path to TAI? |
| **Impacts** | How will AI affect military, economics, politics? |
| **Governance challenges** | What institutions can manage AI risks? |
| **Strategic considerations** | How do actors interact in AI development? |

Key concepts introduced:
- **Offense-defense balance** in AI capabilities
- **Compute governance** as a policy lever
- **The Windfall Clause** (sharing benefits from transformative AI)
- **Racing dynamics** and coordination problems

This agenda has shaped AI governance research worldwide, including our own framework's attention to structural dynamics.

### 80,000 Hours Problem Profiles

<R id="aba53394a2f0e745">80,000 Hours</R> organizes AI risks into distinct "problem profiles":

| Problem | Focus | Key Actors |
|---------|-------|------------|
| **Risks from power-seeking AI** | Technical alignment, x-risk | Technical researchers |
| **Catastrophic AI misuse** | Misuse prevention, biosecurity | Labs, governments |
| **AI governance** | Policy, coordination | Policymakers, think tanks |

**Workforce estimates** (2023): ~300 people working directly on AI x-risk prevention:
- ~200 in technical AI safety research
- ~100 in strategy/policy/advocacy

The profiles provide career guidance organized by intervention type rather than risk taxonomy.

### CAIS Statement and Research Categories

The Center for AI Safety's 2023 <R id="f1037aa7fc65c82f">Statement on AI Risk</R> distilled concerns to one sentence:

> "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

Signed by 350+ AI researchers and industry leaders including the CEOs of OpenAI, Anthropic, and DeepMind.

**CAIS research categories**:

| Category | Focus |
|----------|-------|
| **Robustness** | Adversarial examples, distribution shift |
| **Monitoring** | Detecting dangerous behaviors |
| **Alignment** | Ensuring AI pursues intended goals |
| **Safety Applications** | Using AI to enhance safety |

Their paper "<R id="8b90a137d973d3e4">An Overview of Catastrophic AI Risks</R>" provides more detail on risk scenarios.

---

## Other Relevant Frameworks

### NIST AI Risk Management Framework

The <R id="54dbc15413425997">NIST AI RMF</R> (January 2023) provides a voluntary framework organized around four core functions:

| Function | Purpose | Key Activities |
|----------|---------|----------------|
| **Govern** | Cultivate risk management culture | Policies, accountability, stakeholder engagement |
| **Map** | Understand context | Risk identification, impact assessment, dependency mapping |
| **Measure** | Assess risks | Metrics, testing, monitoring, third-party evaluation |
| **Manage** | Prioritize and act | Response planning, mitigation, communication |

> **Visual Resource**: The <R id="f8d870f640046346">NIST AI RMF Playbook</R> provides interactive guidance for each function, with suggested actions and organizational considerations.

The framework emphasizes **trustworthy AI characteristics**: valid & reliable, safe, secure & resilient, accountable & transparent, explainable & interpretable, privacy-enhanced, and fair with harmful bias managed.

### AI Incident Database

The <R id="baac25fa61cb2244">AI Incident Database</R> (AIID), maintained by the Responsible AI Collaborative, collects structured reports of AI harms. As of 2024, it contains 600+ incidents with detailed tagging:

| Field | Description | Example Values |
|-------|-------------|----------------|
| **Alleged Deployer** | Organization that deployed the AI | Tesla, Amazon, Meta |
| **Alleged Developer** | Organization that created the AI | OpenAI, Google, Microsoft |
| **Harmed Parties** | Who was affected | Consumers, workers, specific individuals |
| **Technology Type** | AI system category | Computer vision, NLP, autonomous systems |
| **Harm Type** | Category of harm | Physical, psychological, financial, rights |

> **Visual Resource**: The <R id="482482e85a11172f">AIID Discover app</R> allows filtering and exploring incidents by these dimensions, with full incident reports including sources and timeline.

Empirical focus complements conceptual taxonomies—where MIT catalogs *potential* risks, AIID documents *actual* harms.

### Standardized AI Threat Taxonomy

Recent work proposes <R id="a58394e152d28500">taxonomies aligned with ISO/IEC 42001</R> covering security-oriented threats:

| Category | Threats | Focus |
|----------|---------|-------|
| **Intentional Threats** | Misuse, Poisoning, Adversarial attacks | Malicious actors |
| **Privacy Threats** | Data leakage, Inference attacks, Model inversion | Data protection |
| **Reliability Threats** | Biases, Unreliable outputs, Model drift | System behavior |
| **Supply Chain Threats** | Third-party vulnerabilities, IP theft | Ecosystem risks |

More security-focused than safety-focused, complementing broader risk taxonomies like MIT's.

### OECD AI Policy Observatory

The <R id="eca111f196cde5eb">OECD.AI Policy Observatory</R> provides policy-focused analysis including:
- **AI Policy Tracker**: Legislation and regulations across 60+ countries
- **AI Incidents Monitor**: Curated incident reports
- **AI Principles**: International alignment on ethical AI development

> **Visual Resource**: The <R id="5876a46e1f2601de">OECD AI Policy Dashboard</R> offers interactive visualizations of AI policy trends by country and topic.

---

## Cross-Framework Comparison

The following tables compare frameworks across key dimensions to help identify which is most useful for different purposes.

### By Primary Focus and Structure

| Framework | Primary Focus | Unit of Analysis | Structure Type | Formalism Level |
|-----------|---------------|------------------|----------------|-----------------|
| **Bostrom (2002)** | Existential risks | Risk events | 3-dimensional matrix | Conceptual |
| **Ord (2020)** | Existential risks | Risk sources | Hierarchical categories | Conceptual |
| **Concrete Problems (2016)** | Technical safety | Safety problems | 5-category taxonomy | Technical |
| **What Failure Looks Like (2019)** | Failure modes | Scenarios | 2-part narrative | Conceptual |
| **Carlsmith (2022)** | X-risk argument | Premises | 6-premise chain | Semi-formal |
| **Karnofsky (2021)** | Century significance | Scenarios | Multi-scenario narrative | Conceptual |
| **Russell (2019)** | Value alignment | Design principles | 3-principle framework | Technical |
| **MIT Repository (2024)** | Comprehensive risks | Individual risks | Dual taxonomy (1700+ items) | Formal database |
| **AIRO (2024)** | Regulatory compliance | Ontology concepts | OWL formal ontology | Highly formal |
| **AIR 2024** | Policy harmonization | Risk categories | 4-level hierarchy (314 types) | Semi-formal |
| **Anthropic RSP** | Development safety | Capability levels | ASL-based levels | Operational |
| **DeepMind FSF** | Frontier safety | Risk areas | 4-area framework | Operational |
| **OpenAI Preparedness** | Deployment safety | Risk categories | 4-category matrix | Operational |
| **This Knowledge Base** | Parameter tracking | Variables | Parameter → Aggregate → Outcome | Semi-formal |

### By Intended Audience and Use Case

| Framework | Primary Audience | Best Use Case | Actionability |
|-----------|-----------------|---------------|---------------|
| **Bostrom/Ord** | Philosophers, policymakers | Prioritization, moral weight | Low (conceptual) |
| **Concrete Problems** | ML researchers | Research direction | High (technical) |
| **What Failure Looks Like** | Alignment researchers | Threat modeling | Medium (strategic) |
| **Carlsmith** | X-risk community | Risk estimation | Medium (analytical) |
| **Karnofsky** | EA community, public | Strategic prioritization | Medium (strategic) |
| **Russell** | AI developers, academics | System design | High (design principles) |
| **MIT Repository** | Researchers, auditors | Comprehensive coverage | High (reference) |
| **AIRO** | Regulators, compliance | EU AI Act compliance | High (operational) |
| **Lab frameworks (Anthropic/DeepMind/OpenAI)** | Labs, regulators | Internal governance | High (operational) |
| **CAIS** | Public, policymakers | Consensus communication | Low (signaling) |
| **GovAI** | Governance researchers | Research prioritization | Medium (academic) |
| **This Knowledge Base** | Analysts, researchers | Cross-cutting analysis | Medium (analytical) |

### By Timeline and Mechanism Emphasis

| Framework | Timeline Focus | Key Mechanism | Intervention Point |
|-----------|---------------|---------------|-------------------|
| **Intelligence Explosion (MIRI)** | Sudden (days-months) | Recursive self-improvement | Pre-development |
| **What Failure Looks Like** | Gradual → Lock-in | Proxy gaming + power-seeking | Training/deployment |
| **Carlsmith APS** | Medium (decades) | Power-seeking behavior | Multiple premises |
| **Kasirzadeh Accumulative** | Long (decades) | Systemic degradation | Societal resilience |
| **Concrete Problems** | Near-term | Technical failures | Research/engineering |
| **Lab RSPs** | Continuous | Capability thresholds | Evaluation/deployment |

---

## Synthesis: Design Trade-offs

Different frameworks make different trade-offs:

| Trade-off | Formal End | Informal End |
|-----------|------------|--------------|
| **Precision vs. Accessibility** | AIRO (OWL) | Karnofsky, Christiano (blog posts) |
| **Coverage vs. Depth** | MIT (1700 risks) | Concrete Problems (5 categories) |
| **Static vs. Dynamic** | AIR 2024 (taxonomy) | System Dynamics (simulation) |
| **Risk-focused vs. Variable-focused** | MIT, AIRO | Russell, This KB (parameters) |
| **Regulatory vs. Analytical** | AIRO, NIST, Lab RSPs | Carlsmith, Christiano |
| **Unified vs. Modular** | MIT, AIRO | 80K Hours profiles, Lab frameworks |
| **Academic vs. Operational** | Bostrom, Ord, GovAI | Lab RSPs, NIST |

### Framework Selection Guide

**Choose based on your needs:**

| If you need... | Consider... |
|----------------|-------------|
| Comprehensive risk coverage | MIT AI Risk Repository |
| EU AI Act compliance | AIRO |
| Policy development | AIR 2024, NIST AI RMF |
| Technical research direction | Concrete Problems, DeepMind research bets |
| Strategic prioritization | Carlsmith, Karnofsky |
| System design principles | Russell's three principles |
| Internal lab governance | Anthropic RSP, OpenAI Preparedness |
| Cross-cutting parameter analysis | This Knowledge Base |

### What Our Framework Adds

Our parameter-based approach complements existing frameworks by:

1. **Symmetric treatment**: Variables can improve or degrade (vs. risk-only framing)
2. **Intervention orientation**: Explicit mapping to actionable levers
3. **Cross-cutting analysis**: Track how variables interact across scenarios
4. **Uncertainty quantification**: Continuous values with confidence intervals
5. **Dynamic perspective**: Track changes over time (though not formal simulation)

Our framework optimizes for:
- **Human readability** over machine reasoning
- **Intervention orientation** over pure risk cataloging
- **Conceptual clarity** over comprehensive coverage
- **Rapid iteration** over formal stability

These are defensible choices, but users should understand what's sacrificed.

---

## Implications for This Knowledge Base

### Structural Improvements to Consider

1. **Add domain layer**: Intermediate grouping between leaves and aggregates (inspired by AIR's 4-level structure)
2. **Stock-flow distinction**: Separate current levels from rates of change (inspired by system dynamics)
3. **Feedback loop notation**: Mark R/B loops in diagrams (inspired by CLDs)
4. **Edge semantics**: Standardize on +/- polarity with optional labels

### Coverage Gaps to Address

Drawing from MIT's domain taxonomy:
- **Privacy**: Currently minimal coverage
- **Discrimination**: Not explicitly parameterized
- **Environmental**: Missing entirely
- **Human-Computer Interaction**: Partially covered (Human Agency, Oversight)

### Integration Opportunities

- **MIT mapping**: Tag our parameters with corresponding MIT risk IDs
- **AIRO alignment**: Consider ISO 31000 vocabulary adoption
- **Incident database links**: Connect parameters to empirical incident data

---

## Visual Resources Summary

For readers who want to explore the original frameworks' visual interfaces:

### Database and Repository Resources

| Framework | Key Visualization | URL |
|-----------|-------------------|-----|
| **MIT AI Risk Repository** | Interactive database with dual taxonomy filters | <R id="579a87f09f36f813">airisk.mit.edu</R> |
| **AI Incident Database** | Discover app with incident filtering | <R id="482482e85a11172f">incidentdatabase.ai/apps/discover</R> |
| **OECD AI Observatory** | Policy dashboards by country and topic | <R id="5876a46e1f2601de">oecd.ai/dashboards</R> |

### Formal Frameworks and Standards

| Framework | Key Visualization | URL |
|-----------|-------------------|-----|
| **AIRO Ontology** | Core concepts diagram showing AI System and Risk concept relationships | <R id="4991cbf1547309aa">delaramglp.github.io/airo</R> |
| **AIR 2024** | Four-level hierarchical taxonomy diagram | <R id="660ce2f217080bfe">arXiv:2406.17864</R> |
| **NIST AI RMF** | Interactive Playbook with function-specific guidance | <R id="f8d870f640046346">airc.nist.gov</R> |

### Blog Series and Conceptual Frameworks

| Framework | Key Resource | URL |
|-----------|--------------|-----|
| **Most Important Century** | Full blog series with diagrams | <R id="1a20dfc897a0933a">cold-takes.com</R> |
| **What Failure Looks Like** | Original post with follow-up discussions | <R id="6807a8a8f2fd23f3">alignmentforum.org</R> |
| **Carlsmith Report** | 80-page PDF with detailed argument structure | <R id="6e597a4dc1f6f860">arXiv:2206.13353</R> |
| **Cotra's Bio Anchors** | Interactive model spreadsheet | <R id="9dd3d89558b68d83">alignmentforum.org</R> |

### Lab Safety Frameworks

| Framework | Key Resource | URL |
|-----------|--------------|-----|
| **Anthropic RSP** | Responsible Scaling Policy document | <R id="394ea6d17701b621">anthropic.com</R> |
| **DeepMind FSF** | Frontier Safety Framework documentation | <R id="8c8edfbc52769d52">deepmind.google</R> |
| **OpenAI Preparedness** | Updated framework with evaluation details | <R id="ded0b05862511312">openai.com</R> |

### System Dynamics Resources

| Resource | Description | URL |
|----------|-------------|-----|
| **Systems Thinker** | Accessible introductions to stocks and flows | <R id="9ae5bae4284e0732">thesystemsthinker.com</R> |
| **Vensim** | Stock-flow diagram tutorials | <R id="7eb8b4cf64c0a60d">vensim.com/documentation</R> |

---

## Further Reading

### Primary Sources

- <R id="579a87f09f36f813">MIT AI Risk Repository</R> — Comprehensive risk database
- <R id="4991cbf1547309aa">AIRO Ontology</R> — EU AI Act aligned formal ontology
- <R id="660ce2f217080bfe">AIR 2024 Paper</R> — Policy taxonomy synthesis
- <R id="54dbc15413425997">NIST AI RMF</R> — Organizational framework
- <R id="baac25fa61cb2244">AI Incident Database</R> — Empirical incident reports

### System Dynamics Resources

- <R id="9ae5bae4284e0732">Systems Thinker: Stocks and Flows</R>
- <R id="320870c3c2bf6b9b">Causal Loop Diagrams</R>
- <R id="7eb8b4cf64c0a60d">Vensim Documentation</R>

### Ontology Design

- <R id="1cccb46f600b438b">Ontology Design Best Practices</R>
- <R id="e4f895533c1bb892">Open Risk Ontology</R>
- <R id="3c60f38ca7b7fb83">Taxonomies vs. Ontologies</R>

---

## Academic References

[^1]: Seger, E., Dreksler, N., Moulange, R., Dardaman, E., Schuett, J., Wei, K., Winter, C., Arnold, M., Ó hÉigeartaigh, S., Korinek, A., Anderljung, M., Bucknall, B., Chan, A., Stafford, E., Koessler, L., Garfinkel, B., Brundage, M., Dafoe, A., & Avin, S. (2024). *The AI Risk Repository: A Comprehensive Meta-Review, Database, and Taxonomy of Risks From Artificial Intelligence*. MIT FutureTech. <R id="af868c1661d95c7f">arXiv:2408.12622</R>

[^2]: Golpayegani, D., Pandit, H. J., & Lewis, D. (2024). *AIRO: An Ontology for Representing AI Risks Based on the Proposed EU AI Act and ISO Risk Management Standards*. In Proceedings of SEMANTiCS 2024. <R id="2e956b3bad0e40e8">https://w3id.org/airo</R>

[^3]: Zeng, Y., et al. (2024). *AIR 2024: A Unified AI Risk Taxonomy Mapping from Government and Corporate Policies*. arXiv preprint. <R id="b49f47e951da441e">arXiv:2406.17864</R>

[^4]: Sterman, J. D. (2000). *Business Dynamics: Systems Thinking and Modeling for a Complex World*. McGraw-Hill. ISBN: 978-0072389159.

### Foundational Existential Risk Works

- Bostrom, N. (2002). *Existential Risks: Analyzing Human Extinction Scenarios and Related Hazards*. Journal of Evolution and Technology, 9(1). <R id="4145536b3f7fb4fd">nickbostrom.com</R>
- Bostrom, N. (2014). *Superintelligence: Paths, Dangers, Strategies*. Oxford University Press. ISBN: 978-0199678112.
- Ord, T. (2020). *The Precipice: Existential Risk and the Future of Humanity*. Hachette Books. ISBN: 978-0316484916.
- Kasirzadeh, A. (2024). *Two Types of AI Existential Risk: Decisive and Accumulative*. Philosophical Studies. <R id="987dd3f6f56b99cd">arXiv:2401.07836</R>

### Seminal AI Safety Papers

- Amodei, D., Olah, C., Steinhardt, J., Christiano, P., Schulman, J., & Mané, D. (2016). *Concrete Problems in AI Safety*. <R id="cd3035dbef6c7b5b">arXiv:1606.06565</R>
- Christiano, P. (2019). *What failure looks like*. AI Alignment Forum. <R id="6807a8a8f2fd23f3">alignmentforum.org</R>
- Hubinger, E., van Merwijk, C., Mikulik, V., Skalse, J., & Garrabrant, S. (2019). *Risks from Learned Optimization in Advanced Machine Learning Systems*. <R id="c4858d4ef280d8e6">arXiv:1906.01820</R>

### Individual Researcher Frameworks

- Carlsmith, J. (2022). *Is Power-Seeking AI an Existential Risk?* Open Philanthropy. <R id="6e597a4dc1f6f860">arXiv:2206.13353</R>
- Karnofsky, H. (2021). *The Most Important Century* (blog series). Cold Takes. <R id="1a20dfc897a0933a">cold-takes.com</R>
- Russell, S. (2019). *Human Compatible: Artificial Intelligence and the Problem of Control*. Viking. ISBN: 978-0525558613.
- Cotra, A. (2020). *Forecasting Transformative AI with Biological Anchors*. Open Philanthropy. <R id="9dd3d89558b68d83">alignmentforum.org</R>
- Cotra, A. (2022). *Without specific countermeasures, the easiest path to transformative AI likely leads to AI takeover*. Cold Takes. <R id="b6967ffbd2503516">cold-takes.com</R>

### Lab Safety Frameworks

- Anthropic (2023). *Core Views on AI Safety: When, Why, What, and How*. <R id="5fa46de681ff9902">anthropic.com</R>
- Google DeepMind (2024). *Introducing the Frontier Safety Framework*. <R id="8c8edfbc52769d52">deepmind.google</R>
- Google DeepMind (2025). *An Approach to Technical AGI Safety and Security*. <R id="b5ce7c3c58adf251">PDF</R>
- OpenAI (2023). *Preparedness Framework (Beta)*. <R id="45f5df091824b27c">cdn.openai.com</R>
- OpenAI (2025). *Our updated Preparedness Framework*. <R id="ded0b05862511312">openai.com</R>
- Yudkowsky, E., & Soares, N. (2024). *If Anyone Builds It, Everyone Dies*. Machine Intelligence Research Institute.

### Governance and Policy Frameworks

- Dafoe, A. (2018). *AI Governance: A Research Agenda*. Future of Humanity Institute, University of Oxford. <R id="80e4d9ef562c342e">fhi.ox.ac.uk</R>
- 80,000 Hours (2023). *Problem profiles: Risks from power-seeking AI systems*. <R id="d9fb00b6393b6112">80000hours.org</R>
- Center for AI Safety (2023). *Statement on AI Risk*. <R id="f1037aa7fc65c82f">safe.ai</R>
- Hendrycks, D., Mazeika, M., & Woodside, T. (2023). *An Overview of Catastrophic AI Risks*. Center for AI Safety. <R id="8b90a137d973d3e4">arXiv:2306.12001</R>

### Additional References

- **ISO 31000:2018** — Risk management guidelines. International Organization for Standardization.
- **ISO/IEC 23894:2023** — Information technology — Artificial intelligence — Guidance on risk management. International Organization for Standardization.
- **NIST AI RMF 1.0** (2023) — Artificial Intelligence Risk Management Framework. National Institute of Standards and Technology. <R id="8afe93377b37643d">NIST AI 100-1</R>
- McGregor, S. (2021). *Preventing Repeated Real World AI Failures by Cataloging Incidents: The AI Incident Database*. AAAI Conference on Artificial Intelligence.
- Meadows, D. H. (2008). *Thinking in Systems: A Primer*. Chelsea Green Publishing. ISBN: 978-1603580557.

---

## Related Pages

- [Framework Design](/knowledge-base/methodology/framework-design/) — Why we made our structural choices
- [Limitations](/knowledge-base/methodology/limitations/) — Known weaknesses of our approach
- [Terminology](/knowledge-base/methodology/terminology/) — Precise definitions of our terms

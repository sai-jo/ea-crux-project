---
title: "Outcome Decomposition: A Review of Approaches"
description: "A comprehensive survey of how EA researchers, AI safety analysts, and longtermist thinkers decompose AI futures into measurable dimensions, with critical analysis of our current three-outcome framework."
sidebar:
  order: 3
quality: 4
lastEdited: "2026-01-01"
---

import {Mermaid} from '../../../../components/wiki';

## Executive Summary

This report examines how the effective altruism and AI safety communities decompose AI outcomes into analyzable dimensions. We find that our current three-outcome framework (Acute Risk, Steady State Quality, Transition Smoothness) conflates distinct analytical types—probability, welfare level, and temporal integral—in ways that complicate quantitative reasoning. Drawing on extensive research across the EA literature, we identify several alternative approaches and propose potential improvements.

**Key finding**: Most rigorous frameworks in the field either (a) focus on probabilistic decomposition of discrete catastrophic events, or (b) use multi-scenario approaches that separately model probability and conditional value. Our current framework attempts an intermediate approach that may sacrifice the benefits of both.

---

## Part I: The Problem with Our Current Framework

### Current Structure

Our knowledge base currently organizes outcomes into three dimensions:

| Outcome | Description | Analytical Type |
|---------|-------------|-----------------|
| **Acute Risk** | Probability of catastrophic events (extinction, loss of control) | Probability measure |
| **Steady State Quality** | Quality of long-term future conditional on survival | Welfare level |
| **Transition Smoothness** | How much disruption occurs during AI transition | Temporal welfare integral |

### Why This Is Problematic

These three dimensions are incommensurable—they measure fundamentally different things:

1. **Acute Risk** is P(catastrophe), a probability between 0 and 1
2. **Steady State Quality** is E[Welfare | survival], a conditional expectation
3. **Transition Smoothness** is ∫W(t)dt over the transition period, an integral

This creates several analytical difficulties:

**Problem 1: No natural aggregation.** How do we combine a 10% chance of extinction with a "good" steady state conditional on survival? The expected value calculation requires explicit probability-weighting that our current structure obscures.

**Problem 2: Double-counting risk.** "Acute Risk" captures P(catastrophe), but some catastrophes affect "Steady State Quality" rather than survival. Is a totalitarian lock-in an acute risk or a steady-state quality issue?

**Problem 3: Temporal ambiguity.** When does "transition" end and "steady state" begin? In [Paul Christiano's gradual disempowerment scenario](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like), there may be no discrete transition—just continuous decline.

---

## Part II: Alternative Frameworks in the Literature

### 2.1 Probabilistic Event Decomposition (Carlsmith)

[Joe Carlsmith's framework for AI existential risk](https://arxiv.org/abs/2206.13353) decomposes the probability of catastrophe into a product of conditional probabilities:

<Mermaid client:load chart={`
flowchart TD
    P1["P(Powerful AI by 2070)<br/>65%"]
    P2["P(Strong incentives to deploy)<br/>80%"]
    P3["P(Alignment is hard)<br/>40%"]
    P4["P(High-impact power-seeking)<br/>65%"]
    P5["P(Scales to disempowerment)<br/>40%"]
    P6["P(Constitutes x-catastrophe)<br/>95%"]

    P1 --> P2 --> P3 --> P4 --> P5 --> P6

    P6 --> RESULT["P(doom) ≈ 5%"]

    style RESULT fill:#ff6b6b
`} />

**Strengths:**
- Clean multiplicative structure enables debate about individual premises
- [Superforecaster comparison](https://joecarlsmith.com/2023/10/18/superforecasting-the-premises-in-is-power-seeking-ai-an-existential-risk/) revealed which premises drive disagreement
- Explicit uncertainty at each step

**Weaknesses:**
- Only addresses probability of catastrophe, not value conditional on outcomes
- Assumes catastrophe is binary (extinction vs. not)
- Framed as "avoiding catastrophe" rather than "achieving good outcomes"

[Eli's review](https://forum.effectivealtruism.org/posts/icdd4FCKuwqyAuYBm/eli-s-review-of-is-power-seeking-ai-an-existential-risk) notes this framing "will bias readers toward lower estimates" because it omits consideration of win conditions.

---

### 2.2 Decisive vs. Accumulative Risk (Kasirzadeh)

[Atoosa Kasirzadeh's taxonomy](https://link.springer.com/article/10.1007/s11098-025-02301-3) distinguishes two pathways to existential catastrophe:

| Type | Mechanism | Example | Policy Implications |
|------|-----------|---------|---------------------|
| **Decisive** | Single powerful AI system causes catastrophe | Misaligned superintelligence | Focus on alignment of frontier models |
| **Accumulative** | Incremental AI-driven disruptions compound | Gradual institutional erosion | Focus on systemic resilience |

The accumulative hypothesis suggests a "boiling frog" scenario: individually minor AI risks slowly converge until a triggering event results in irreversible collapse. This is analogous to climate change, where individual emissions seem minor but collectively produce catastrophic outcomes.

**Relevance to our framework:** Our "Transition Smoothness" partially captures accumulative dynamics, but without distinguishing between recoverable disruption and irreversible erosion.

---

### 2.3 Gradual Disempowerment (Kulveit et al.)

[Recent work on gradual disempowerment](https://arxiv.org/html/2501.16946v2) argues that even without misalignment, AI could cause existential catastrophe through systematic displacement of human agency:

> "The alignment of societal systems with human interests has been stable only because of the necessity of human participation for thriving economies, states, and cultures. Once this human participation gets displaced by more competitive machine alternatives, our institutions' incentives for growth will be untethered from a need to ensure human flourishing."

This framework identifies several channels of disempowerment:

| Channel | Mechanism |
|---------|-----------|
| **Economic** | AI labor replaces human labor; capital owners no longer need workers |
| **Political** | States funded by AI profits have no incentive for citizen representation |
| **Cultural** | AI mediates social interaction; human preferences become less influential |
| **Epistemic** | AI shapes information flow; human reasoning becomes dependent on AI outputs |

**Key insight**: "Solving technical alignment won't be enough to avoid gradual human disempowerment, because coordinating to prevent it will be surprisingly hard once AI starts mediating or replacing most of our valuable contributions."

---

### 2.4 Multi-Scenario Frameworks

Several approaches explicitly model multiple scenarios with separate probability and value estimates.

#### 2.4.1 Open Philanthropy's Worldview Diversification

[Open Philanthropy's approach](https://www.openphilanthropy.org/research/update-on-cause-prioritization-at-open-philanthropy/) divides capital into "buckets" corresponding to different worldviews:

> "Rather than forcing a single unified approach, we divide the available capital into buckets, with different buckets operating according to different worldviews... For example, we might allocate X% of capital to a bucket that aims to maximize impact from a 'long-termist' perspective, and Y% of capital to a bucket that aims to maximize impact from a 'near-termist' perspective."

This explicitly acknowledges that different probability estimates and value theories lead to different priority rankings.

#### 2.4.2 IMF/Korinek Scenario Planning

[Anton Korinek's IMF framework](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek) argues that given expert disagreement about AI trajectories, "policymakers should take each of these scenarios seriously, stress-test how our economic and financial policy frameworks would perform in each scenario, and where necessary reform them."

The framework models three distinct scenarios with probability estimates:
- Business as usual (AI progress slows)
- Transformative AI (major economic restructuring)
- AGI/ASI (superintelligence scenarios)

Each scenario has probability >10%, warranting serious policy preparation.

---

### 2.5 Win Conditions vs. Avoiding Catastrophe

The [EA Forum discussion of flourishing futures](https://forum.effectivealtruism.org/topics/flourishing-futures) notes a systematic asymmetry in longtermist thinking:

> "Discussion often centers around extinction versus flourishing utopia, which tends to ignore that survival does not always imply utopian outcomes."

[Existential hope frameworks](https://foresight.org/focus-areas/existential-hope/) (associated with Foresight Institute) argue for complementing existential risk reduction with positive vision development:

| Existential Risk Approach | Existential Hope Approach |
|---------------------------|---------------------------|
| What could go wrong? | What could go right? |
| Minimize P(catastrophe) | Maximize E[goodness] |
| Defensive, preventive | Constructive, aspirational |
| Consensus on avoiding bad | Contested visions of good |

**Relevance to our framework:** Our "Steady State Quality" captures some win-condition thinking, but it's framed as conditional on survival rather than as a primary optimization target.

---

### 2.6 Beyond Extinction: S-Risks and Value Lock-In

[Research on suffering risks](https://longtermrisk.org/reducing-risks-of-astronomical-suffering-a-neglected-priority/) (s-risks) identifies outcomes potentially worse than extinction:

> "S-risks are risks where an adverse outcome would bring about suffering on an astronomical scale, vastly exceeding all suffering that has existed on Earth so far."

[Value lock-in concerns](https://forum.effectivealtruism.org/topics/value-lock-in) highlight irreversibility:

> "Will MacAskill defines value lock-in as 'an event that causes a single value system, or set of value systems, to persist for an extremely long time.' He identifies AI as the key technology with respect to lock-in."

This suggests our outcome dimensions should distinguish:

| Outcome Type | Definition |
|--------------|------------|
| **Extinction** | No future beings exist |
| **S-risk** | Future beings exist but suffer astronomically |
| **Dystopian lock-in** | Suboptimal values permanently instantiated |
| **Flourishing** | Positive-value future achieved |
| **Open trajectory** | Future generations retain option value |

---

### 2.7 The Three Horizons Framework

The [Three Horizons framework](https://www.internationalfuturesforum.com/three-horizons) from futures studies offers a temporal decomposition:

<Mermaid client:load chart={`
flowchart LR
    subgraph H1["Horizon 1: Present System"]
        P1[Current AI ecosystem]
        P2[Existing governance]
        P3[Dominant institutions]
    end

    subgraph H2["Horizon 2: Transition"]
        T1[Disruptive innovations]
        T2[Policy experiments]
        T3[Institutional adaptation]
    end

    subgraph H3["Horizon 3: Envisioned Future"]
        F1[Transformed AI-human relations]
        F2[New governance structures]
        F3[Stable equilibrium]
    end

    H1 --> H2 --> H3

    style H1 fill:#ff6b6b
    style H2 fill:#ffe66d
    style H3 fill:#90EE90
`} />

[NTT DATA and IFTF's application to AI](https://www.nttdata-strategy.com/en/newsrelease/240322/) uses:
- **Horizon 1 (Present)**: Current AI capabilities and impacts
- **Horizon 2 (5 years)**: Transitional dynamics and adaptation challenges
- **Horizon 3 (10+ years)**: Transformative potential

**Relevance:** This maps somewhat to our Transition/Steady-State distinction but adds the crucial framing that all three horizons exist simultaneously, with different stakeholders aligned to each.

---

## Part III: Key Dimensions from the Literature

Synthesizing across frameworks, we identify several orthogonal dimensions that AI outcomes can be decomposed along:

### 3.1 Core Dimensions

| Dimension | What It Measures | Common Operationalizations |
|-----------|------------------|---------------------------|
| **Probability of catastrophe** | P(very bad outcome) | P(doom), x-risk estimates |
| **Conditional welfare** | E[Welfare \| no catastrophe] | Quality of flourishing futures |
| **Negative tail** | Severity of bad outcomes | S-risk magnitude, lock-in permanence |
| **Reversibility** | Whether outcomes can be corrected | Lock-in potential, option value |
| **Transition cost** | Disruption during change | Economic disruption, social instability |
| **Timeline** | When outcomes materialize | Horizon 1/2/3, decisive vs. accumulative |
| **Agency distribution** | Who controls the future | Human vs. AI, singleton vs. multipolar |

### 3.2 Paul Christiano's Probability Decomposition

From [Christiano's "My views on doom"](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom):

| Outcome | Probability |
|---------|-------------|
| Most humans die from AI takeover within 10 years of powerful AI | 11% |
| Most humans die from non-takeover AI causes | 9% |
| Humanity irreversibly messes up within 10 years of powerful AI | 46% |

This separates extinction from broader "messing up," capturing value lock-in and other non-extinction catastrophes.

### 3.3 The p(doom) Spectrum

[Expert estimates of P(doom)](https://en.wikipedia.org/wiki/P(doom)) vary enormously:

| Estimator | P(doom) |
|-----------|---------|
| Typical skeptic superforecaster | 0.1-1% |
| Lex Fridman | ~10% |
| Dario Amodei (Anthropic CEO) | 10-25% |
| Paul Christiano (METR) | ~20% |
| Average AI researcher survey | 5-14% (median vs. mean) |
| Eliezer Yudkowsky | >50% |

The wide range suggests that [crux decomposition](https://forum.effectivealtruism.org/posts/orhjaZ3AJMHzDzckZ/results-from-an-adversarial-collaboration-on-ai-risk-fri) is needed to identify sources of disagreement.

---

## Part IV: Critique of Common Approaches

### 4.1 The Problem with Pure P(doom) Focus

Nick Bostrom's influential argument that [existential risk reduction has enormous expected value](https://existential-risk.org/concept.html) has been critiqued:

> "If a risk is truly existential, then its disutility should be infinite. This is because it removes the potential for there to be infinite future generations of humans... the quantitative approach to decision making around existential risk, that is calculating expected utilities and using those values to make decisions between multiple possibilities, is flawed."

This suggests that simple P(doom) × Value calculations may be conceptually problematic.

### 4.2 The Cluelessness Problem

The [cluelessness objection](https://utilitarianism.net/objections-to-utilitarianism/cluelessness/) challenges longtermist expected value calculations:

> "There may be some exceptions. Proponents of longtermism believe that some actions—such as reducing existential risk—have robustly positive expected value in the long term."

But this requires accepting that risk reduction is indeed robustly positive—contested when considering differential technological development, s-risks, and lock-in dynamics.

### 4.3 Superforecaster vs. Expert Divergence

The [FRI adversarial collaboration](https://forum.effectivealtruism.org/posts/orhjaZ3AJMHzDzckZ/results-from-an-adversarial-collaboration-on-ai-risk-fri) found:

> "After 80 hours, the skeptical superforecasters increased their probability of existential risk from AI! All the way from 0.1% to 0.12%."

[Analysis suggests](https://forum.effectivealtruism.org/posts/qMP7LcCBFBEtuA3kL/the-rationale-shaped-hole-at-the-heart-of-forecasting) superforecasters lacked explicit models:

> "None of the superforecasters appealed to any models of any kind, not even Epoch data, in their forecasts. This goes a long way towards explaining the vast gulf between superforecasters and AI researchers on AGI forecasts."

---

## Part V: Toward a Better Framework

### 5.1 Design Principles for Outcome Decomposition

Based on this review, good outcome decomposition should:

1. **Separate probability from value**: Don't mix P(bad) with E[Welfare | good]
2. **Include both tails**: Consider s-risks alongside extinction
3. **Distinguish reversible from irreversible**: Lock-in is categorically different from temporary setback
4. **Account for multiple scenarios**: Don't force single-trajectory thinking
5. **Enable crux identification**: Structure should reveal where reasonable people disagree
6. **Support intervention design**: Decomposition should map to actionable parameters

### 5.2 Alternative Decompositions to Consider

#### Option A: Scenario-Based Structure

Decompose into discrete scenarios, each with probability and conditional value:

| Scenario | P(Scenario) | E[Welfare \| Scenario] | Key Uncertainties |
|----------|-------------|------------------------|-------------------|
| Extinction | 5-20% | 0 | Timeline, mechanism |
| Dystopian lock-in | 10-30% | Negative | Reversibility |
| Muddling through | 20-40% | Mixed | Transition costs |
| Flourishing | 20-50% | Very positive | What "good" means |

**Advantage**: Clean probability × value structure
**Disadvantage**: Scenarios aren't mutually exclusive or exhaustive

#### Option B: Dimension-Based Structure

Organize around orthogonal dimensions that any scenario can be scored on:

1. **Survival probability** (does humanity persist?)
2. **Agency retention** (do humans have meaningful control?)
3. **Value realization** (are good values instantiated?)
4. **Option preservation** (can future generations choose differently?)
5. **Welfare level** (how much wellbeing exists?)

**Advantage**: Comprehensive, enables trade-off analysis
**Disadvantage**: More complex, dimensions may interact

#### Option C: Temporal-Causal Structure

Organize around the causal chain from present to long-term:

1. **Present conditions** (current AI capabilities, governance, alignment research)
2. **Transition dynamics** (speed, disruption, adaptation capacity)
3. **Equilibrium selection** (which stable state is reached)
4. **Long-run welfare** (quality of the stable state)

**Advantage**: Maps to intervention timing
**Disadvantage**: Assumes clear transition/equilibrium distinction

### 5.3 Mapping Current Framework to Alternatives

| Our Current Dimension | Scenario-Based | Dimension-Based | Temporal-Causal |
|-----------------------|----------------|-----------------|-----------------|
| Acute Risk | P(Extinction) + P(Lock-in) | Survival + Agency | Transition dynamics |
| Steady State Quality | E[Welfare \| Flourishing] | Value realization + Welfare | Long-run welfare |
| Transition Smoothness | P(Muddling) weighting | N/A | Transition dynamics |

---

## Part VI: Recommendations

### 6.1 Short-Term Improvements

Without major restructuring, we can:

1. **Add explicit scenario breakdown** within Acute Risk (extinction vs. lock-in vs. gradual disempowerment)
2. **Clarify temporal boundaries** between transition and steady state
3. **Add reversibility dimension** to distinguish temporary from permanent outcomes
4. **Include s-risk consideration** in Steady State Quality

### 6.2 Medium-Term Structural Changes

Consider restructuring to:

1. **Replace three outcomes with scenario decomposition** that separates probability from conditional value
2. **Add Option Preservation** as a fourth dimension capturing lock-in dynamics
3. **Distinguish Decisive vs. Accumulative** risk pathways

### 6.3 Research Priorities

Further investigation needed on:

1. How do leading frameworks (Carlsmith, Christiano, Ord) map to each other?
2. What cruxes drive disagreement between experts with very different P(doom)?
3. How should we weight s-risks relative to extinction risks?
4. Can we quantify lock-in/reversibility in a principled way?

---

## Conclusion

Our current three-outcome framework conflates distinct analytical types in ways that complicate rigorous reasoning about AI futures. The EA and AI safety literature offers several more principled approaches:

- **Carlsmith's probabilistic decomposition** enables debate about specific premises
- **Kasirzadeh's decisive/accumulative distinction** captures different failure modes
- **Open Philanthropy's worldview diversification** explicitly handles deep uncertainty
- **Scenario planning approaches** separate probability from conditional value
- **Existential hope frameworks** complement risk-focused analysis

We recommend evolving toward a structure that more cleanly separates:
1. **Probability of different scenarios** (discrete, mutually exclusive where possible)
2. **Conditional value within each scenario** (welfare, agency, option value)
3. **Transition costs across scenarios** (disruption regardless of endpoint)
4. **Reversibility/lock-in dynamics** (categorically distinct from welfare level)

This would enable more rigorous expected value calculations, clearer identification of cruxes, and better mapping between parameters and outcomes.

---

## Sources

### Primary Frameworks

- Carlsmith, J. (2022). [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353)
- Christiano, P. (2019). [What Failure Looks Like](https://www.alignmentforum.org/posts/HBxe6wdjxK239zajf/what-failure-looks-like)
- Christiano, P. (2023). [My views on doom](https://www.lesswrong.com/posts/xWMqsvHapP3nwdSW8/my-views-on-doom)
- Kasirzadeh, A. (2024). [Two Types of AI Existential Risk: Decisive and Accumulative](https://link.springer.com/article/10.1007/s11098-025-02301-3)
- Kulveit, J. et al. (2025). [Gradual Disempowerment: Systemic Existential Risks from Incremental AI Development](https://arxiv.org/html/2501.16946v2)

### Longtermist Philosophy

- Bostrom, N. (2013). [Existential Risk Prevention as Global Priority](https://existential-risk.org/concept.html)
- Greaves, H. & MacAskill, W. (2021). [The Case for Strong Longtermism](https://www.globalprioritiesinstitute.org/wp-content/uploads/The-Case-for-Strong-Longtermism-GPI-Working-Paper-June-2021-2-2.pdf)
- Ord, T. (2020). *The Precipice: Existential Risk and the Future of Humanity*

### Scenario Planning

- Korinek, A. (2023). [Scenario Planning for an AGI Future](https://www.imf.org/en/Publications/fandd/issues/2023/12/Scenario-Planning-for-an-AGI-future-Anton-korinek)
- [Three Horizons Framework](https://www.internationalfuturesforum.com/three-horizons) - International Futures Forum
- Open Philanthropy. [Update on Cause Prioritization at Open Philanthropy](https://www.openphilanthropy.org/research/update-on-cause-prioritization-at-open-philanthropy/)

### Welfare and Value Considerations

- [S-Risks Problem Profile](https://80000hours.org/problem-profiles/s-risks/) - 80,000 Hours
- [Value Lock-in](https://forum.effectivealtruism.org/topics/value-lock-in) - EA Forum
- [Flourishing Futures](https://forum.effectivealtruism.org/topics/flourishing-futures) - EA Forum
- [Existential Hope](https://foresight.org/focus-areas/existential-hope/) - Foresight Institute

### Empirical Estimates

- [P(doom)](https://en.wikipedia.org/wiki/P(doom)) - Wikipedia
- [The Economics of p(doom)](https://arxiv.org/abs/2503.07341) - arXiv
- Cotra, A. (2020). [Draft Report on AI Timelines](https://www.lesswrong.com/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines)
- [Results from an Adversarial Collaboration on AI Risk](https://forum.effectivealtruism.org/posts/orhjaZ3AJMHzDzckZ/results-from-an-adversarial-collaboration-on-ai-risk-fri) - EA Forum

### Meta-Methodology

- Karnofsky, H. [The Most Important Century](https://forum.effectivealtruism.org/s/isENJuPdB3fhjWYHd) - Cold Takes
- Shulman, C. [80,000 Hours Podcast: The Economy and National Security After AGI](https://80000hours.org/podcast/episodes/carl-shulman-economy-agi/)
- [The Rationale-Shaped Hole at the Heart of Forecasting](https://forum.effectivealtruism.org/posts/qMP7LcCBFBEtuA3kL/the-rationale-shaped-hole-at-the-heart-of-forecasting) - EA Forum

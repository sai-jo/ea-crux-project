---
title: Key Metrics & Estimates
description: Quantifiable indicators for tracking AI development, safety, and governance
sidebar:
  label: Overview
  order: 0
---

import {R} from '../../../../components/wiki';



## Overview

Understanding AI risk requires tracking **concrete, measurable indicators**. This section catalogs key metrics across domains—from compute growth to public opinion—that help assess where we are and where we're heading.

These metrics serve multiple purposes:
- **Situational awareness**: Understanding the current state of AI development
- **Forecasting**: Inputs for predicting future trajectories
- **Evaluation**: Measuring the effectiveness of safety interventions
- **Communication**: Grounding abstract discussions in concrete numbers

---

## Metric Categories

| Category | Description |
|----------|-------------|
| **[Compute & Hardware](/knowledge-base/metrics/compute-hardware/)** | GPU production, training compute, efficiency trends |
| **[AI Capabilities](/knowledge-base/metrics/capabilities/)** | Benchmarks, task performance, capability trajectories |
| **[Economic & Labor](/knowledge-base/metrics/economic-labor/)** | Investment, automation, productivity impacts |
| **[Safety Research](/knowledge-base/metrics/safety-research/)** | Researcher headcount, funding, publication rates |
| **[Alignment Progress](/knowledge-base/metrics/alignment-progress/)** | Interpretability, robustness, alignment tax |
| **[Governance & Policy](/knowledge-base/metrics/governance-policy/)** | Regulations, enforcement, international agreements |
| **[Lab Behavior](/knowledge-base/metrics/lab-behavior/)** | RSP compliance, safety practices, transparency |
| **[Public Opinion](/knowledge-base/metrics/public-opinion/)** | Awareness, concern, trust levels |
| **[Expert Opinion](/knowledge-base/metrics/expert-opinion/)** | P(doom), timelines, researcher surveys |
| **[Geopolitics](/knowledge-base/metrics/geopolitics/)** | US-China dynamics, talent flows, coordination |
| **[Structural Indicators](/knowledge-base/metrics/structural/)** | Information quality, institutional capacity, resilience |

---

## How to Use This Section

### For Researchers
- Find current best estimates for key parameters
- Identify data gaps and measurement challenges
- Track changes over time

### For Forecasters
- Input variables for models and predictions
- Base rates and reference classes
- Uncertainty ranges and confidence intervals

### For Policymakers
- Evidence base for regulatory decisions
- Monitoring indicators for AI governance
- International comparison data

---

## Data Quality Notes

Metrics vary significantly in:
- **Availability**: Some are publicly tracked; others require inference
- **Reliability**: Some come from rigorous measurement; others from surveys or estimates
- **Timeliness**: Some update continuously; others are snapshots
- **Comparability**: Definitions and methodologies may differ across sources

Each page notes data quality and limitations for specific metrics.

---

## Key Sources

| Source | Coverage |
|--------|----------|
| **<R id="120adc539e2fa558">Epoch AI</R>** | Compute trends, notable models database |
| **<R id="31dad9e35ad0b5d3">AI Index (Stanford HAI)</R>** | Comprehensive annual report |
| **<R id="f09a58f2760fb69b">State of AI Report</R>** | Industry trends, research progress |
| **<R id="a306e0b63bdedbd5">CAIS Surveys</R>** | Expert opinion on AI risk |
| **<R id="1b8f3fd22346b2ad">Our World in Data</R>** | Long-term trends, public data |
| **<R id="d99a6d0fb1edc2db">Metaculus</R>** | Forecasts on AI milestones |
| **<R id="81709d5cc78ba8c8">AI Safety Papers Database</R>** | Safety research tracking |

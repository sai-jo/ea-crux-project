---
title: Capability Threshold Model
description: Systematic framework mapping AI capabilities across 5 dimensions (domain knowledge, reasoning depth, planning horizon, strategic modeling, autonomous execution) to specific risk thresholds, providing concrete capability requirements for risks like bioweapons development (threshold crossing 2026-2029) and structured frameworks for risk forecasting.
sidebar:
  order: 16
quality: 4.5
lastEdited: "2025-12-26"
ratings:
  novelty: 4
  rigor: 4
  actionability: 5
  completeness: 5
importance: 85.2
llmSummary: This model systematically maps AI capabilities across 5 dimensions (domain knowledge, reasoning depth, planning horizon, strategic modeling, autonomous execution) to specific risk thresholds, providing concrete capability requirements for risks like bioweapons development (estimated threshold crossing 2026-2029) and structured frameworks for risk forecasting.
---

import { DataInfoBox, Backlinks, Mermaid } from '../../../../components/wiki';

<DataInfoBox entityId="capability-threshold-model" ratings={frontmatter.ratings} />

## Overview

Different AI risks require different capability levels to become dangerous. A system that can write convincing phishing emails poses different risks than one that can autonomously discover zero-day vulnerabilities. This model maps specific capability requirements to specific risks, helping predict when risks activate as capabilities improve.

The capability threshold model provides a structured framework for understanding how AI systems transition from relatively benign to potentially dangerous across multiple risk domains. Rather than treating AI capability as a single dimension or risks as uniformly dependent on general intelligence, this model recognizes that specific risks emerge when systems cross particular capability thresholds in relevant dimensions. Key findings include 15-25% benchmark performance indicating early risk emergence, 50% marking qualitative shifts to complex autonomous execution, and most critical thresholds estimated to cross between 2025-2029 across misuse, control, and structural risk categories.

## Risk Impact Assessment

| Risk Category | Severity | Likelihood (2025-2027) | Threshold Crossing Timeline | Trend |
|---------------|----------|------------------------|---------------------------|-------|
| Authentication Collapse | Critical | 85% | 2025-2027 | ↗ Accelerating |
| Mass Persuasion | High | 70% | 2025-2026 | ↗ Accelerating |
| Cyberweapon Development | High | 65% | 2025-2027 | ↗ Steady |
| Bioweapons Development | Critical | 40% | 2026-2029 | → Uncertain |
| Situational Awareness | Critical | 60% | 2025-2027 | ↗ Accelerating |
| Economic Displacement | High | 80% | 2026-2030 | ↗ Steady |
| Strategic Deception | Extreme | 15% | 2027-2035+ | → Uncertain |

## Capability Dimensions Framework

AI capabilities decompose into five distinct dimensions that progress at different rates. Understanding these separately is crucial because different risks require different combinations.

| Dimension | Level 1 | Level 2 | Level 3 | Level 4 | Current Frontier | Gap to Level 3 |
|-----------|---------|---------|---------|---------|------------------|----------------|
| **Domain Knowledge** | Undergraduate | Graduate | Expert | Superhuman | Expert- (some domains) | 0.5 levels |
| **Reasoning Depth** | Simple (2-3 steps) | Moderate (5-10) | Complex (20+) | Superhuman | Moderate+ | 0.5-1 level |
| **Planning Horizon** | Immediate | Short-term (hrs) | Medium (wks) | Long-term (months) | Short-term+ | 1 level |
| **Strategic Modeling** | None | Basic | Sophisticated | Superhuman | Basic+ | 1-1.5 levels |
| **Autonomous Execution** | None | Simple tasks | Complex tasks | Full autonomy | Simple-Complex | 0.5-1 level |

### Domain Knowledge Benchmarks

Current measurement approaches show significant gaps in assessing practical domain expertise:

| Domain | Best Benchmark | Current Frontier Score | Expert Human Level | Assessment Quality |
|--------|----------------|----------------------|-------------------|-------------------|
| Biology | [MMLU-Biology](https://arxiv.org/abs/2009.03300) | 85-90% | ~95% | Medium |
| Chemistry | [ChemBench](https://arxiv.org/abs/2310.09049) | 70-80% | ~90% | Low |
| Computer Security | [SecBench](https://github.com/SECURITY-BENCHMARK) | 65-75% | ~85% | Low |
| Psychology | MMLU-Psychology | 80-85% | ~90% | Very Low |
| Medicine | [MedQA](https://arxiv.org/abs/2009.13081) | 85-90% | ~95% | Medium |

*Assessment quality reflects how well benchmarks capture practical expertise versus academic knowledge.*

### Reasoning Depth Progression

| Reasoning Level | Benchmark Examples | Current Performance | Risk Relevance |
|----------------|-------------------|-------------------|----------------|
| Simple (2-3 steps) | Basic math word problems | 95%+ | Low-risk applications |
| Moderate (5-10 steps) | [GSM8K](https://arxiv.org/abs/2110.14168), multi-hop QA | 85-95% | Most current capabilities |
| Complex (20+ steps) | [ARC-AGI](https://arxiv.org/abs/1911.01547), extended proofs | 30-50% | **Critical threshold zone** |
| Superhuman | Novel mathematical proofs | \<10% | Advanced risks |

## Risk-Capability Mapping

### Near-Term Risks (2025-2027)

#### Authentication Collapse

| Capability | Required Level | Current Level | Gap | Evidence |
|-----------|----------------|---------------|-----|----------|
| Domain Knowledge (Media) | Expert | Expert- | 0.5 level | [Sora quality](https://openai.com/sora) approaching photorealism |
| Reasoning Depth | Moderate | Moderate | 0 levels | Current models handle multi-step generation |
| Strategic Modeling | Basic+ | Basic | 0.5 level | Limited theory of mind in current systems |
| Autonomous Execution | Simple | Simple | 0 levels | Already achieved for content generation |

**Key Threshold Capabilities:**
- Generate synthetic content indistinguishable from authentic across all modalities
- Real-time interactive video generation ([NVIDIA Omniverse](https://www.nvidia.com/en-us/omniverse/))
- Defeat detection systems designed to identify AI content
- Mimic individual styles from minimal samples

**Current Status:** [OpenAI's Sora](https://openai.com/sora) and [Meta's Make-A-Video](https://makeavideo.studio/) demonstrate near-threshold video generation. [ElevenLabs](https://elevenlabs.io/) achieves voice cloning from \<30 seconds of audio.

#### Mass Persuasion Capabilities

| Capability | Required Level | Current Level | Gap | Evidence |
|-----------|----------------|---------------|-----|----------|
| Domain Knowledge (Psychology) | Graduate+ | Graduate | 0.5 level | Strong performance on psychology benchmarks |
| Strategic Modeling | Sophisticated | Basic+ | 1 level | Limited multi-agent reasoning |
| Planning Horizon | Medium-term | Short-term | 1 level | Cannot maintain campaigns over weeks |
| Autonomous Execution | Simple | Simple | 0 levels | Can generate content at scale |

**Research Evidence:**
- [Anthropic (2024)](https://www.anthropic.com/index/claude-3-model-card) shows Claude 3 achieves 84% on psychology benchmarks
- [Stanford HAI study](https://hai.stanford.edu/news/humans-are-more-likely-believe-messages-ai) finds AI-generated content 82% higher believability
- [MIT persuasion study](https://www.science.org/doi/10.1126/sciadv.adh1850) demonstrates automated A/B testing improves persuasion by 35%

### Medium-Term Risks (2026-2029)

#### Bioweapons Development

| Capability | Required Level | Current Level | Gap | Assessment Source |
|-----------|----------------|---------------|-----|------------------|
| Domain Knowledge (Biology) | Expert | Graduate+ | 1 level | [RAND biosecurity assessment](https://www.rand.org/pubs/research_reports/RRA2977-2.html) |
| Domain Knowledge (Chemistry) | Expert | Graduate | 1-2 levels | Limited synthesis knowledge |
| Reasoning Depth | Complex | Moderate+ | 1 level | Cannot handle 20+ step procedures |
| Planning Horizon | Medium-term | Short-term | 1 level | No multi-week experimental planning |
| Autonomous Execution | Complex | Simple+ | 1 level | Cannot troubleshoot failed experiments |

**Critical Bottlenecks:**
- Specialized synthesis knowledge for dangerous compounds
- Autonomous troubleshooting of complex laboratory procedures
- Multi-week experimental planning and adaptation
- Integration of theoretical knowledge with practical constraints

**Expert Assessment:** [RAND Corporation (2024)](https://www.rand.org/pubs/research_reports/RRA2977-2.html) estimates 60% probability of crossing threshold by 2028.

#### Economic Displacement Thresholds

| Job Category | Automation Threshold | Current AI Capability | Estimated Timeline | Source |
|-------------|---------------------|---------------------|-------------------|---------|
| Content Writing | 70% task automation | 85% | **Crossed 2024** | [McKinsey AI Index](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2024) |
| Code Generation | 60% task automation | 45% | 2025-2026 | [GitHub Copilot metrics](https://github.blog/2024-06-27-research-quantifying-github-copilots-impact-on-developer-productivity-and-happiness/) |
| Data Analysis | 75% task automation | 55% | 2026-2027 | Industry surveys |
| Customer Service | 80% task automation | 70% | 2025-2026 | [Salesforce AI reports](https://www.salesforce.com/news/insights/ai-customer-service-trends/) |
| Legal Research | 65% task automation | 40% | 2027-2028 | Legal industry analysis |

### Long-Term Control Risks (2027-2035+)

#### Strategic Deception (Scheming)

| Capability | Required Level | Current Level | Gap | Uncertainty |
|-----------|----------------|---------------|-----|-------------|
| Strategic Modeling | Superhuman | Basic+ | 2+ levels | Very High |
| Reasoning Depth | Complex | Moderate+ | 1 level | High |
| Planning Horizon | Long-term | Short-term | 2 levels | Very High |
| Situational Awareness | Expert | Basic | 2 levels | High |

**Key Uncertainties:**
- Whether sophisticated strategic modeling can emerge from current training approaches
- Detectability of strategic deception capabilities during evaluation
- Minimum capability level required for effective scheming

**Research Evidence:**
- [Anthropic Constitutional AI](https://arxiv.org/abs/2212.08073) shows limited success in detecting deceptive behavior
- [Redwood Research](https://www.redwoodresearch.org/) adversarial training reveals capabilities often hidden during evaluation

## Current State & Trajectory

### Capability Progress Rates

| Dimension | 2023-2024 Progress | Projected 2024-2025 | Key Drivers |
|-----------|-------------------|---------------------|-------------|
| Domain Knowledge | +0.5 levels | +0.3-0.7 levels | Larger training datasets, specialized fine-tuning |
| Reasoning Depth | +0.3 levels | +0.2-0.5 levels | Chain-of-thought improvements, tree search |
| Planning Horizon | +0.2 levels | +0.2-0.4 levels | Tool integration, memory systems |
| Strategic Modeling | +0.1 levels | +0.1-0.3 levels | Multi-agent training, RL improvements |
| Autonomous Execution | +0.4 levels | +0.3-0.6 levels | Tool use, real-world deployment |

**Data Sources:** [Epoch AI capability tracking](https://epochai.org/), industry benchmark results, expert elicitation.

### Leading Organizations

| Organization | Strongest Capabilities | Estimated Timeline to Next Threshold | Focus Area |
|-------------|----------------------|-------------------------------------|------------|
| [OpenAI](https://openai.com/) | Domain knowledge, autonomous execution | 12-18 months | General capabilities |
| [Anthropic](https://www.anthropic.com/) | Reasoning depth, strategic modeling | 18-24 months | Safety-focused development |
| [DeepMind](https://deepmind.google/) | Strategic modeling, planning | 18-30 months | Scientific applications |
| [Meta](https://ai.meta.com/) | Multimodal generation | 6-12 months | Social/media applications |

## Key Uncertainties & Research Cruxes

### Measurement Validity

| Uncertainty | Impact if True | Impact if False | Current Evidence |
|------------|---------------|-----------------|------------------|
| Current benchmarks accurately measure risk-relevant capabilities | Can trust threshold predictions | Need fundamentally new evaluations | Mixed - good for some domains, poor for others |
| Practical capabilities match benchmark performance | Smooth transition from lab to deployment | Significant capability overhangs | Substantial gaps observed in real-world deployment |
| Capability improvements follow predictable scaling laws | Reliable timeline forecasting possible | Threshold crossings may surprise | Scaling laws hold for some capabilities, not others |

### Threshold Sharpness

**Sharp Threshold Evidence:**
- [Authentication systems](https://arxiv.org/abs/2310.19109): Detection accuracy drops from 95% to 15% once generation quality crosses threshold
- Economic viability: [McKinsey automation analysis](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2024) shows 10-20% capability improvements create 50-80% cost advantage in many tasks
- Security vulnerabilities: Most exploits require complete capability to work at all

**Gradual Scaling Evidence:**
- Job displacement: Different tasks within roles automate at different rates
- Persuasion effectiveness: Incremental improvements in messaging quality yield incremental persuasion gains
- Domain expertise: Knowledge accumulation appears continuous rather than threshold-based

### Strategic Deception Detection

Critical unsolved problems in capability assessment:

| Challenge | Current Approach | Limitation | Research Need |
|-----------|-----------------|------------|---------------|
| Detecting hidden capabilities | Red-teaming, adversarial prompts | Systems may hide abilities during evaluation | Novel evaluation paradigms |
| Measuring situational awareness | Direct questioning, inference tests | Pattern matching vs. genuine understanding | Causal intervention studies |
| Long-term planning assessment | Multi-step benchmarks | Maximum tested horizon ~hours | Extended evaluation infrastructure |
| Strategic modeling depth | Theory of mind tasks | Unclear if results generalize | Ecological validity research |

## Timeline Projections & Warning Indicators

### 2025 Critical Thresholds

**High Probability (70%+):**
- **Authentication Collapse:** Real-time deepfakes become commercially viable
- **Content Generation Saturation:** Human-level quality across all text/image modalities

**Medium Probability (40-70%):**
- **Code Automation:** 50%+ of software engineering tasks automated
- **Basic Situational Awareness:** Systems understand evaluation vs. deployment contexts

### 2026-2027 Medium Probability Events

| Risk | Probability | Key Indicators to Monitor |
|------|-------------|-------------------------|
| Advanced Cyberweapons | 65% | [METR evaluations](https://metr.org/) crossing 40% threshold |
| Economic Disruption | 80% | [GitHub Copilot](https://github.com/features/copilot) achieving >60% task completion |
| Bioweapons Uplift | 40% | Expert biology benchmarks >90%, synthesis knowledge tests |
| Oversight Evasion | 60% | Detection of training/deployment context awareness |

### Early Warning System

**Red Flag Indicators:**
- Sudden benchmark improvements >20 percentage points
- Systems developing capabilities not explicitly trained for
- Gap between capability and safety evaluation results widening
- Evidence of strategic behavior during evaluation

**Monitoring Infrastructure:**
- [METR](https://metr.org/) dangerous capability evaluations
- [MIRI](https://intelligence.org/) alignment evaluation protocols
- Industry responsible scaling policies ([OpenAI Preparedness](https://openai.com/preparedness/), [Anthropic RSP](https://www.anthropic.com/news/anthropics-responsible-scaling-policy))
- Academic capability forecasting ([Epoch AI](https://epochai.org/))

## Sources & Resources

### Primary Research

| Source | Type | Key Findings | Relevance |
|--------|------|-------------|-----------|
| [Anthropic Responsible Scaling Policy](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) | Industry Policy | Defines capability thresholds for safety measures | Framework implementation |
| [OpenAI Preparedness Framework](https://openai.com/preparedness/) | Industry Policy | Risk assessment methodology | Threshold identification |
| [METR Dangerous Capability Evaluations](https://metr.org/) | Research | Systematic capability testing | Current capability baselines |
| [Epoch AI Capability Forecasts](https://epochai.org/) | Research | Timeline predictions for AI milestones | Forecasting methodology |

### Government & Policy

| Organization | Resource | Focus |
|-------------|----------|-------|
| [NIST AI Risk Management Framework](https://www.nist.gov/itl/ai-risk-management-framework) | US Government | Risk assessment standards |
| [UK AISI Research](https://www.gov.uk/government/organisations/ai-safety-institute) | UK Government | Model evaluation protocols |
| [EU AI Office](https://digital-strategy.ec.europa.eu/en/policies/european-approach-artificial-intelligence) | EU Government | Regulatory frameworks |
| [RAND Corporation AI Studies](https://www.rand.org/topics/artificial-intelligence.html) | Think Tank | National security implications |

### Technical Benchmarks & Evaluation

| Benchmark | Domain | Current Frontier Score | Threshold Relevance |
|-----------|--------|----------------------|-------------------|
| [MMLU](https://arxiv.org/abs/2009.03300) | General Knowledge | 85-90% | Domain expertise baseline |
| [ARC-AGI](https://arxiv.org/abs/1911.01547) | Abstract Reasoning | 30-50% | Complex reasoning threshold |
| [SWE-bench](https://www.swebench.com/) | Software Engineering | 15-25% | Autonomous execution |
| [MATH](https://arxiv.org/abs/2103.03874) | Mathematical Reasoning | 60-80% | Multi-step reasoning |

### Risk Assessment Research

| Research Area | Key Papers | Organizations |
|---------------|------------|---------------|
| Bioweapons Risk | [RAND Biosecurity Assessment](https://www.rand.org/pubs/research_reports/RRA2977-2.html) | RAND, Johns Hopkins CNAS |
| Economic Displacement | [McKinsey AI Impact](https://www.mckinsey.com/capabilities/quantumblack/our-insights/the-state-of-ai-in-2024) | McKinsey, Brookings Institution |
| Authentication Collapse | [Deepfake Detection Challenges](https://arxiv.org/abs/2310.19109) | UC Berkeley, MIT |
| Strategic Deception | [Constitutional AI Research](https://arxiv.org/abs/2212.08073) | Anthropic, Redwood Research |

<Backlinks client:load entityId="capability-threshold-model" />
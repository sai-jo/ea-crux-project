---
title: CAIS (Center for AI Safety)
description: Organization focused on reducing societal-scale risks from AI
sidebar:
  order: 14
---

import { InfoBox, KeyPeople, EntityCard, EntityCards, Tags, Sources, Section } from '../../../../components/wiki';

<InfoBox
  type="lab-research"
  title="CAIS"
  founded="2022"
  location="San Francisco, CA"
  headcount="~30"
  funding="~$20M+"
  website="https://safe.ai"
/>

## Overview

The Center for AI Safety (CAIS) is a nonprofit organization that works to reduce societal-scale risks from AI. CAIS combines research, field-building, and public communication to advance AI safety.

CAIS is notable for organizing the "Statement on AI Risk" signed by hundreds of AI researchers warning that "mitigating the risk of extinction from AI should be a global priority."

## Activities

### Research
CAIS conducts technical AI safety research:
- Representation engineering
- Adversarial robustness
- AI safety benchmarks
- Conceptual AI safety research

### Field-Building
Major field-building efforts:
- Compute grants for safety researchers
- ML Safety Scholars program
- AI Safety Camp coordination
- Research fellowship programs

### Public Communication
Efforts to raise awareness of AI risks:
- Statement on AI Risk (May 2023)
- Media engagement
- Educational resources

## Key Research

### Representation Engineering
Techniques for understanding and controlling AI behavior through internal representations:
- Reading model "thoughts"
- Steering model behavior
- Detecting deception

### Safety Benchmarks
Developing evaluation tools:
- Measuring power-seeking tendencies
- Evaluating situational awareness
- Testing for dangerous capabilities

<Section title="Key People">
  <KeyPeople people={[
    { name: "Dan Hendrycks", role: "Executive Director" },
    { name: "Mantas Mazeika", role: "Research Scientist" },
    { name: "Thomas Woodside", role: "Policy Director" },
  ]} />
</Section>

## Statement on AI Risk

In May 2023, CAIS organized a one-sentence statement:

> "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

Signed by:
- Geoffrey Hinton (Turing Award winner)
- Yoshua Bengio (Turing Award winner)
- Dario Amodei (Anthropic CEO)
- Sam Altman (OpenAI CEO)
- Demis Hassabis (DeepMind CEO)
- Hundreds more researchers

## Key Publications

- **Representation Engineering** (2023)
- **MACHIAVELLI Benchmark** (2023)
- **Unsolved Problems in ML Safety** (2022)
- **Measuring Power-Seeking** (2023)

<Section title="Related Topics">
  <Tags tags={[
    "AI Safety",
    "Existential Risk",
    "Representation Engineering",
    "Field Building",
    "AI Risk Communication",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="existential-risk"
      category="risk"
      title="Existential Risk from AI"
      description="Risk of human extinction or permanent curtailment"
    />
    <EntityCard
      id="power-seeking"
      category="risk"
      title="Power-Seeking AI"
      description="Tendency of AI to acquire resources and influence"
    />
    <EntityCard
      id="anthropic"
      category="lab"
      title="Anthropic"
      description="Dario Amodei signed the AI risk statement"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "CAIS Website", url: "https://safe.ai" },
  { title: "Statement on AI Risk", url: "https://www.safe.ai/statement-on-ai-risk" },
  { title: "Representation Engineering Paper", url: "https://arxiv.org/abs/2310.01405" },
]} />

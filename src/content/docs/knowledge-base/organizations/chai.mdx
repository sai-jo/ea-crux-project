---
title: CHAI (Center for Human-Compatible AI)
description: UC Berkeley research center on AI alignment founded by Stuart Russell
sidebar:
  order: 15
---

import { InfoBox, KeyPeople, EntityCard, EntityCards, Tags, Sources, Section } from '../../../../components/wiki';

<InfoBox
  type="lab-academic"
  title="CHAI"
  founded="2016"
  location="Berkeley, CA"
  headcount="~30"
  funding="Academic + Open Philanthropy"
  website="https://humancompatible.ai"
/>

## Overview

The Center for Human-Compatible AI (CHAI) is an academic research center at UC Berkeley focused on ensuring AI systems are beneficial to humans. Founded by Stuart Russell, author of the leading AI textbook, CHAI brings academic rigor to AI safety research.

CHAI's research centers on "human-compatible AI" - systems that are inherently safe because they defer to human preferences rather than pursuing fixed objectives.

## Core Framework

### The Standard Model Problem
Russell argues the "standard model" of AI (optimize a fixed objective) is fundamentally flawed:
- Objectives are hard to specify completely
- AI will find unintended ways to achieve objectives
- More capable AI â†’ worse outcomes from misspecification

### Human-Compatible AI
CHAI proposes AI systems should:
1. Have uncertainty about human preferences
2. Learn preferences from human behavior
3. Allow humans to switch them off
4. Defer to humans on important decisions

## Research Areas

### Inverse Reward Design
Learning human preferences from behavior rather than explicit specification. Recognizing that stated objectives are proxies for true preferences.

### Assistance Games
Formalizing AI-human interaction as cooperative games where AI's goal is to help humans achieve their (uncertain) preferences.

### Value Alignment
Research on how to align AI systems with human values:
- Preference learning
- Value extrapolation
- Handling value uncertainty

### Cooperative AI
How AI systems can cooperate with humans and other AI systems safely.

<Section title="Key People">
  <KeyPeople people={[
    { name: "Stuart Russell", role: "Founder, Professor" },
    { name: "Anca Dragan", role: "Associate Director (now DeepMind)" },
    { name: "Pieter Abbeel", role: "Affiliated Faculty" },
    { name: "Michael Dennis", role: "Researcher" },
  ]} />
</Section>

## Key Publications

- **Human Compatible** (Russell, 2019) - Book on AI safety for general audiences
- **Cooperative Inverse Reinforcement Learning** (2016)
- **The Off-Switch Game** (2017)
- **Benefits and Risks of Artificial Intelligence** (Russell et al.)

## Influence

CHAI has shaped AI safety through:
- Academic legitimization of AI safety concerns
- Russell's public advocacy and policy engagement
- Training PhD students in alignment research
- Collaborations with DeepMind and other labs

<Section title="Related Topics">
  <Tags tags={[
    "Inverse Reinforcement Learning",
    "Value Learning",
    "Assistance Games",
    "Human-Compatible AI",
    "Academic AI Safety",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="value-learning"
      category="safety-agenda"
      title="Value Learning"
      description="Learning human values from behavior"
    />
    <EntityCard
      id="specification-gaming"
      category="risk"
      title="Specification Gaming"
      description="AI exploiting misspecified objectives"
    />
    <EntityCard
      id="corrigibility"
      category="safety-agenda"
      title="Corrigibility"
      description="Building AI systems that allow correction"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "CHAI Website", url: "https://humancompatible.ai" },
  { title: "Human Compatible (Book)", url: "https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/" },
  { title: "Stuart Russell on AI Risk", url: "https://www.youtube.com/watch?v=EBK-a94IFHY" },
]} />

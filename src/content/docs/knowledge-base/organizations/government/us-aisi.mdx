---
title: US AI Safety Institute
description: US government agency for AI safety research and standard-setting under NIST
sidebar:
  order: 17
---

import {DataInfoBox, DisagreementMap, KeyPeople, KeyQuestions, Section, Backlinks, PageStatus} from '../../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="US government agency within NIST established in 2023 to develop AI safety standards, conduct evaluations, and coordinate with industry. Non-regulatory body that sets technical standards for AI safety and conducts independent assessments of frontier models through the AI Risk Management Framework and evaluation protocols." />

<DataInfoBox entityId="us-aisi" />

## Summary

The US AI Safety Institute (US AISI) is a government agency within the National Institute of Standards and Technology (NIST) established in 2023 to develop standards, evaluations, and guidelines for safe and trustworthy artificial intelligence. Created in response to growing concerns about AI risks and the need for government oversight, the US AISI represents the United States' primary institutional response to AI safety challenges.

As a government body, the US AISI has unique authority and responsibilities: setting technical standards that can inform regulation, conducting independent evaluations of AI systems, coordinating with industry and academia, and representing US interests in international AI safety discussions. The institute bridges technical AI safety research, industry practices, and government policy.

The creation of the US AISI marks a significant shift: AI safety moving from primarily private sector and academic research to official government priority with institutional backing and regulatory authority.

## History and Founding

### Background and Creation (2023)

**Establishment**: Announced in 2023 as part of broader US AI strategy

**Context leading to creation:**
- Rapid advancement of AI capabilities (ChatGPT moment, Nov 2022)
- Growing concerns about AI risks from researchers and policymakers
- Recognition of need for government involvement
- International developments (UK AI Safety Summit, EU AI Act)
- Pressure from AI safety community for government action

**Legislative and executive basis:**
- **Executive Order on AI** (October 2023): Directed creation of AI safety infrastructure
- Placed within NIST (National Institute of Standards and Technology)
- Initial funding and mandate from Commerce Department
- Part of broader whole-of-government AI approach

**Mission:**
- Develop AI safety standards and guidelines
- Conduct research on AI risks and safety
- Evaluate AI systems for dangerous capabilities
- Coordinate with industry, academia, and international partners
- Inform policy and regulation with technical expertise

### Early Development (2023-2024)

**2023 activities:**
- Hiring initial staff (from industry, academia, safety orgs)
- Establishing organizational structure
- Defining research priorities
- Building relationships with AI labs
- International coordination (UK AISI, others)
- Initial standard-setting work

**2024 progress:**
- Growing team of researchers and policy experts
- First evaluations and safety assessments
- Published initial guidelines and frameworks
- Collaboration agreements with frontier labs
- Participation in international AI safety forums
- Building evaluation infrastructure

**Current status:**
- Still in early stages but rapidly developing
- Growing influence on US AI policy
- Establishing credibility with technical community
- Building capacity for meaningful oversight

## Structure and Organization

### Within NIST

**Organizational placement:**

**NIST (National Institute of Standards and Technology):**
- Non-regulatory federal agency
- Part of US Department of Commerce
- Mission: Promote US innovation and competitiveness
- History of standard-setting (measurements, cybersecurity, etc.)
- Technical credibility and scientific expertise

**Advantages of NIST placement:**
- Technical focus and expertise
- Non-partisan scientific credibility
- History of industry collaboration
- International standards experience
- Less political than some agencies

**Limitations:**
- Non-regulatory (can't enforce standards directly)
- Must work with other agencies for enforcement
- Funding dependent on Congress
- Slower than private sector

### Key Functions

**1. Standards Development**
- Technical standards for AI safety
- Evaluation methodologies
- Risk assessment frameworks
- Best practices documentation
- Measurement and metrics

**2. Research**
- AI safety and security research
- Risk assessment and analysis
- Technical evaluation methods
- Collaboration with external researchers
- Funding research grants

**3. Evaluation and Testing**
- Pre-deployment evaluations of frontier models
- Red-teaming and capability testing
- Safety certification frameworks
- Continuous monitoring approaches
- Independent verification

**4. Coordination**
- Industry engagement and partnerships
- Academic collaborations
- International cooperation
- Interagency coordination (DoD, DoE, NSF, etc.)
- Public-private partnerships

**5. Policy Support**
- Technical expertise for policymakers
- Risk assessments for regulation
- International policy coordination
- Congressional testimony and briefings
- Executive branch advisory role

## Key Focus Areas

### 1. AI Safety Evaluations

**Mission**: Independent assessment of AI systems for dangerous capabilities

**Approach:**
- Learning from METR, Apollo Research, ARC Evals methodologies
- Hiring evaluators from safety organizations
- Building government evaluation capacity
- Pre-deployment testing frameworks
- Red-teaming and adversarial testing

**Evaluation areas:**
- Cybersecurity capabilities
- CBRN (Chemical, Biological, Radiological, Nuclear) risks
- Autonomous capabilities
- Deception and manipulation
- Critical infrastructure risks

**Authority:**
- Executive Order mandates reporting for high-compute models
- Voluntary cooperation from labs (so far)
- Potential for future regulatory enforcement
- Moral authority and credibility

**Challenges:**
- Building expertise quickly
- Keeping pace with AI development
- Access to frontier models
- Balancing transparency with national security
- Resource constraints

### 2. Standards and Guidelines

**Standard-setting mission:**

**AI Risk Management Framework (AI RMF):**
- Voluntary framework for managing AI risks
- Adopted by many organizations
- Based on NIST's cybersecurity framework model
- Principles-based approach
- Continuously updated

**Technical standards:**
- Evaluation methodologies
- Safety testing protocols
- Robustness and security standards
- Transparency and documentation requirements
- Benchmarks and metrics

**Industry adoption:**
- Voluntary in US system (NIST non-regulatory)
- But standards often become de facto requirements
- Industry input through collaborative process
- International alignment efforts
- May inform future regulation

### 3. Research Collaboration

**Building research ecosystem:**

**Partnerships:**
- Frontier AI labs (OpenAI, Anthropic, Google, Meta)
- Academic institutions (universities, research centers)
- Safety organizations (METR, Apollo, ARC, etc.)
- International research bodies
- Industry consortia

**Research priorities:**
- Evaluation methodologies
- Interpretability and transparency
- Robustness and security
- Alignment techniques
- Risk assessment frameworks

**Funding:**
- Grant programs for safety research
- Collaborative research agreements
- Prize competitions
- Fellowship programs
- International collaboration funding

### 4. International Coordination

**US role in global AI safety:**

**Partnerships:**
- **UK AI Safety Institute**: Close collaboration, shared methodologies
- **EU AI Safety efforts**: Coordination on standards
- **Other countries**: Japan, Canada, Australia, others building capacity
- **International organizations**: UN, OECD, etc.

**Goals:**
- Harmonized safety standards
- Shared evaluation frameworks
- Coordinated governance approaches
- Information sharing (within security constraints)
- Joint research initiatives

**Challenges:**
- National security concerns limit sharing
- Different regulatory approaches
- Competitive dynamics between countries
- China coordination (extremely limited)
- Verification and enforcement

## Key Leadership and Personnel

<Section title="Leadership">
  <KeyPeople people={[
    { name: "Elizabeth Kelly", role: "Director (as of 2024)" },
    { name: "Various senior staff", role: "Research, evaluation, and policy" },
  ]} />
</Section>

### Elizabeth Kelly (Director)

**Background:**
- Prior role as policy advisor on AI issues
- Experience in technology policy and government
- Connections to both technical and policy communities

**Leadership priorities:**
- Building institutional capacity
- Establishing credibility with labs and researchers
- International coordination
- Translating technical concerns to policy
- Hiring top talent

### Staffing Strategy

**Hiring from:**
- AI safety organizations (METR, Apollo, ARC, etc.)
- Frontier AI labs (researchers concerned about safety)
- Academic institutions
- Other government agencies (DoD, intelligence community)
- Policy and legal experts

**Challenges:**
- Competing with private sector salaries
- Government hiring processes slow
- Security clearances required for some work
- Need both technical depth and policy expertise
- Retention in fast-moving field

**Advantages:**
- Mission-driven work
- Government authority and impact
- Job security and benefits
- Ability to influence policy
- Access to classified information

## Relationship with AI Labs

**Complex dynamic:**

### Cooperation

**Labs' incentives to cooperate:**
- Demonstrate good faith to regulators
- Inform better regulation (avoid bad rules)
- Early warning of dangerous capabilities
- Legitimacy and social license
- Avoid surprise regulation

**Collaboration mechanisms:**
- Pre-deployment evaluations
- Information sharing agreements
- Technical consultations
- Standard-setting participation
- Joint research projects

### Tension

**Potential conflicts:**
- Labs might want to limit government access
- Commercial sensitivity of frontier models
- Speed of deployment vs. thorough evaluation
- Regulatory overhead costs
- National security classifications

**Power dynamics:**
- Government authority vs. lab autonomy
- Voluntary cooperation vs. mandatory compliance
- Information asymmetry (labs know more)
- Regulatory capture risks
- Public pressure and accountability

### Current Status (2024)

**Largely cooperative:**
- Major labs engaging with AISI
- Some evaluations and information sharing
- Participation in standard-setting
- But voluntary, not mandatory
- Unclear what happens if labs refuse cooperation

**Future evolution:**
- May become more regulatory
- Mandatory evaluation requirements possible
- Could model on drug/medical device approval
- International coordination might create pressure
- Congressional legislation could expand authority

## Impact and Influence

### On US AI Policy

**Policy development:**
- Technical input to AI Executive Order implementation
- Congressional testimony and briefings
- Regulatory agency coordination
- Standards inform potential regulation
- Risk assessments for policymakers

**Examples:**
- Compute thresholds for reporting requirements
- Dual-use foundation model definitions
- Safety evaluation requirements
- Red-teaming mandates
- Transparency and disclosure rules

### On Industry Practices

**Standard-setting influence:**
- AI RMF adoption by companies
- Evaluation methodologies becoming standard
- Best practices documentation
- Credibility for safety-focused approaches
- Competitive pressure to meet standards

**Lab safety frameworks:**
- OpenAI Preparedness Framework aligned with AISI standards
- Anthropic RSP incorporates AISI evaluations
- DeepMind Frontier Safety Framework coordination
- Industry-wide safety norms emerging

### On Research Community

**Research priorities:**
- Funding for evaluation and safety research
- Legitimizing safety research in academia
- Career paths for safety researchers
- Collaboration opportunities
- Public good research infrastructure

**Credibility:**
- Government backing for safety concerns
- Signals urgency and importance
- Resources for difficult research
- Attracts talent to field

### International Leadership

**US positioning:**
- Major player in international AI safety
- Standards and frameworks influential globally
- Technical expertise and resources
- Coordination with allies
- Balancing leadership with collaboration

**Challenges:**
- China not participating in safety coordination
- Different regulatory approaches across countries
- National security vs. openness tensions
- Verification and enforcement internationally
- Race dynamics between nations

## Criticisms and Challenges

### Regulatory Capture Risk

**Concern**: AISI might become captured by industry interests

**Mechanisms:**
- Labs have information advantage
- Revolving door (staff between AISI and labs)
- Industry influence on standards
- Labs might provide favorable information
- Resource dependence on lab cooperation

**Mitigations:**
- Government authority and independence
- Diverse stakeholder input
- Transparency in standard-setting
- Oversight and accountability
- Public interest mission

**Ongoing vigilance needed**

### Expertise and Resource Constraints

**Challenges:**

**Technical expertise:**
- Hard to hire ML experts into government
- Salary competition with private sector
- Expertise gap vs. frontier labs
- Rapid field advancement
- Need for continuous learning

**Resources:**
- Limited budget compared to labs
- Can't match lab compute resources
- Small team vs. large evaluation task
- Dependence on Congressional funding
- Infrastructure building takes time

**Implications:**
- Might not be able to evaluate all models thoroughly
- Could lag behind frontier
- Dependent on external expertise
- Need prioritization and collaboration

### Speed and Bureaucracy

**Government pace vs. AI pace:**
- AI advancing rapidly
- Government processes slow
- Hiring takes months/years
- Standard-setting is gradual
- Regulatory processes lengthy

**Risk:**
- Irrelevance if too slow
- Missing critical moments
- Playing catch-up continuously
- Frustration from stakeholders

**Adaptations needed:**
- Fast-track hiring
- Agile processes
- Continuous updates
- Flexible frameworks
- Rapid response capabilities

### Enforcement Authority

**Current limitation**: NIST is non-regulatory

**Implications:**
- Can't mandate compliance
- Standards are voluntary
- Dependent on other agencies for enforcement
- Limited leverage over labs
- Moral authority vs. legal authority

**Future possibilities:**
- Congressional legislation for mandatory requirements
- Coordination with regulatory agencies (FTC, SEC, etc.)
- International agreements with enforcement
- Conditional access to government resources/contracts
- Public pressure and transparency

### Transparency vs. Security

**Tension:**

**Transparency arguments:**
- Public right to know about AI risks
- Accountability requires openness
- Scientific scrutiny improves quality
- Democratic oversight
- International cooperation needs information sharing

**Security arguments:**
- Some information is classified
- Dual-use capabilities shouldn't be publicized
- Adversaries might exploit knowledge
- Lab proprietary information
- Evaluation methods might be gamed if public

**Balance needed:**
- Tiered disclosure (public, confidential, classified)
- Redacted reports
- Aggregate statistics vs. details
- Trusted stakeholder access
- Regular review of classification

<KeyQuestions questions={[
  "Can US AISI build sufficient expertise to meaningfully oversee frontier AI development?",
  "Will voluntary cooperation from labs be sufficient, or is regulatory authority needed?",
  "How to balance speed and rigor in government AI safety work?",
  "Can international coordination work without China participation?",
  "What enforcement mechanisms should back safety standards?",
  "How to avoid regulatory capture while maintaining lab cooperation?"
]} />

## Future Directions

### Near-Term (1-2 years)

**Building capacity:**
- Hire key personnel (evaluators, researchers, policy experts)
- Establish evaluation infrastructure
- Develop comprehensive standards
- Deepen lab partnerships
- Strengthen international coordination

**Key milestones:**
- Regular evaluations of frontier models
- Published standards and guidelines
- Operational assessment capabilities
- Congressional reporting
- International agreements

### Medium-Term (2-5 years)

**Institutionalization:**
- Mature organization with stable funding
- Comprehensive evaluation program
- Influential standards widely adopted
- Strong international coordination
- Possible regulatory authority

**Expansion:**
- Larger team and budget
- Broader scope (multimodal AI, robotics, etc.)
- Regional offices or international presence
- Research grants program scaled up
- Education and training initiatives

### Long-Term Vision

**Role in AI governance:**
- Central authority for AI safety in US
- Model for international coordination
- Effective oversight of frontier AI
- Prevention of catastrophic deployments
- Contribution to beneficial AI development

**Success metrics:**
- Prevented dangerous deployments
- Raised safety standards industry-wide
- International cooperation effective
- Public trust in AI governance
- Advances in safety research and practice

<Section title="Perspectives on Government AI Safety">
  <DisagreementMap
    client:load
    topic="Role of Government in AI Safety"
    positions={[
      {
        name: "Government Oversight Essential",
        description: "US AISI is necessary and should have regulatory authority. Private labs can't self-regulate effectively. Government represents public interest. Need mandatory evaluations and standards with enforcement.",
        proponents: ["Many safety researchers", "Policy advocates", "Public interest groups"],
        strength: 4
      },
      {
        name: "Collaborative Approach Best",
        description: "AISI valuable for coordination and standard-setting, but avoid heavy regulation. Industry-government partnership most effective. Voluntary cooperation can work. Regulation might stifle innovation.",
        proponents: ["Many industry voices", "Some policymakers", "Innovation advocates"],
        strength: 3
      },
      {
        name: "Government Will Be Captured",
        description: "Regulatory capture inevitable. Labs have information and resource advantage. AISI will rubber-stamp industry preferences. Need independent oversight, not government-industry partnership.",
        proponents: ["Some skeptics", "Anti-regulatory-capture advocates"],
        strength: 2
      },
      {
        name: "Government Too Slow",
        description: "Government bureaucracy can't keep pace with AI. AISI will always lag behind. Better to rely on lab self-regulation and market forces. Government involvement adds costs without benefits.",
        proponents: ["Some libertarians", "Speed-focused innovators"],
        strength: 1
      }
    ]}
  />
</Section>

## Comparisons to Other Entities

### vs UK AI Safety Institute

**Similarities:**
- Both government AI safety bodies
- Similar missions (evaluation, standards, research)
- Close collaboration and coordination
- Created around same time

**Differences:**
- **US**: Larger budget and country
- **UK**: More nimble, smaller government
- **US**: Broader mandate and resources
- **UK**: Focused on being international hub
- Different regulatory environments

**Relationship**: Partners, shared methodologies, mutual learning

### vs Frontier Lab Safety Teams

**Complementary roles:**
- **Labs**: Build AI systems, internal safety
- **AISI**: External oversight, standards, evaluation
- **Labs**: Commercial incentives
- **AISI**: Public interest mandate
- Different perspectives, potential conflicts

### vs Private Safety Orgs (METR, Apollo)

**Relationship:**
- AISI learns from and collaborates with them
- Hires personnel from safety orgs
- Coordinates on methodologies
- Different authorities (government vs. private)
- Complementary rather than competitive

## Related Pages

<Backlinks client:load entityId="us-aisi" />

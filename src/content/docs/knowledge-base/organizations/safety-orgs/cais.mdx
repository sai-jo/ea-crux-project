---
title: CAIS (Center for AI Safety)
description: Research organization advancing AI safety through technical research, field-building, and policy communication, including the landmark 2023 AI extinction risk statement signed by major AI leaders
sidebar:
  order: 14
quality: 4
llmSummary: CAIS is a nonprofit AI safety organization that conducts technical research, field-building, and public communication, most notably organizing the May 2023 statement on AI extinction risk signed by hundreds of researchers including major AI leaders. Their research focuses on representation engineering, safety benchmarks, and adversarial robustness.
lastEdited: "2025-12-24"
importance: 35
---

import {DataInfoBox, KeyPeople, Section, Backlinks} from '../../../../../components/wiki';

<DataInfoBox entityId="cais" />

## Overview

The [Center for AI Safety (CAIS)](https://www.safe.ai/) is a nonprofit research organization that works to reduce societal-scale risks from artificial intelligence through technical research, field-building initiatives, and public communication efforts. Founded by [Dan Hendrycks](/knowledge-base/people/dan-hendrycks/), CAIS gained widespread recognition for organizing the landmark "Statement on AI Risk" in May 2023, which received signatures from over 350 AI researchers and industry leaders.

CAIS's multi-pronged approach combines cutting-edge technical research on AI alignment and robustness with strategic field-building efforts that have supported over 200 researchers through grants and fellowships. The organization's work spans from fundamental research on [representation engineering](https://www.safe.ai/blog/representation-engineering) to developing critical safety benchmarks like the [MACHIAVELLI dataset](https://arxiv.org/abs/2304.03279) for evaluating deceptive AI behavior.

## Risk Assessment

| Risk Category | Assessment | Evidence | Mitigation Focus |
|---------------|------------|----------|------------------|
| Technical Research Impact | High | 50+ safety publications, novel benchmarks | [Representation engineering](https://www.safe.ai/blog/representation-engineering), adversarial robustness |
| Field-Building Influence | Very High | 200+ researchers supported, $2M+ distributed | Compute grants, fellowship programs |
| Policy Communication | High | Statement signed by major AI leaders | Public awareness, expert consensus building |
| Timeline Relevance | Medium-High | Research targets near-term safety challenges | 2-5 year research horizon |

## Key Research Areas

### Technical Safety Research

| Research Domain | Key Contributions | Impact Metrics |
|-----------------|------------------|----------------|
| **Representation Engineering** | Methods for reading/steering model internals | [15+ citations](https://scholar.google.com/citations?user=WOSlKqcAAAAJ) within 6 months |
| **Safety Benchmarks** | MACHIAVELLI, power-seeking evaluations | Adopted by [Anthropic](https://www.anthropic.com/), [OpenAI](https://openai.com/) |
| **Adversarial Robustness** | Novel defense mechanisms, evaluation protocols | 100+ citations on key papers |
| **Alignment Foundations** | Conceptual frameworks for AI safety | Influenced [alignment research](/knowledge-base/responses/technical/alignment/) directions |

### Major Publications & Tools

- **[Representation Engineering: A Top-Down Approach to AI Transparency](https://arxiv.org/abs/2310.01405)** (2023) - Methods for understanding AI decision-making
- **[Do the Rewards Justify the Means? Measuring Trade-Offs Between Rewards and Ethical Behavior](https://arxiv.org/abs/2304.03279)** (2023) - MACHIAVELLI benchmark for ethical AI evaluation  
- **[Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916)** (2022) - Comprehensive taxonomy of safety challenges
- **[Measuring Mathematical Problem Solving With the MATH Dataset](https://arxiv.org/abs/2103.03874)** (2021) - Standard benchmark for AI reasoning capabilities

## Field-Building Impact

### Grant Programs

| Program | Scale | Impact | Timeline |
|---------|--------|--------|----------|
| **Compute Grants** | $2M+ distributed | 100+ researchers supported | 2022-present |
| **ML Safety Scholars** | 50+ participants annually | Early-career pipeline development | 2021-present |
| **Research Fellowships** | $500K+ annually | 20+ fellows placed at top institutions | 2022-present |
| **AI Safety Camp** | 200+ participants total | International collaboration network | 2020-present |

### Institutional Partnerships

- **Academic Collaborations**: UC Berkeley, MIT, Stanford, Oxford
- **Industry Engagement**: Research partnerships with [Anthropic](/knowledge-base/organizations/labs/anthropic/), Google DeepMind
- **Policy Connections**: Briefings for US Congress, UK Parliament, EU regulators

## Statement on AI Risk (2023)

The May 2023 [Statement on AI Risk](https://www.safe.ai/statement-on-ai-risk) represented a watershed moment in AI safety advocacy, consisting of a single sentence:

> "Mitigating the risk of extinction from AI should be a global priority alongside other societal-scale risks such as pandemics and nuclear war."

### Signatory Analysis

| Category | Notable Signatories | Significance |
|----------|-------------------|--------------|
| **Turing Award Winners** | Geoffrey Hinton, Yoshua Bengio, Stuart Russell | Academic legitimacy |
| **Industry Leaders** | Sam Altman (OpenAI), Dario Amodei (Anthropic), Demis Hassabis (DeepMind) | Industry acknowledgment |
| **Policy Experts** | Helen Toner, Allan Dafoe, Gillian Hadfield | Governance credibility |
| **Technical Researchers** | 300+ ML/AI researchers | Scientific consensus |

The statement's impact included immediate media coverage across major outlets and influenced subsequent policy discussions, including mentions in [UK](/knowledge-base/organizations/government/uk-aisi/) and [US](/knowledge-base/organizations/government/us-aisi/) government AI strategies.

## Current Trajectory & Timeline

### Research Roadmap (2024-2026)

| Priority Area | 2024 Goals | 2025-2026 Projections |
|---------------|------------|----------------------|
| **Representation Engineering** | Scale to frontier models | Industry adoption for safety checks |
| **Evaluation Frameworks** | Comprehensive benchmark suite | Standard evaluation protocols |
| **Alignment Methods** | Proof-of-concept demonstrations | Practical implementation |
| **Policy Research** | Technical governance recommendations | Regulatory framework development |

### Funding & Growth

- **Current Budget**: ~$5M annually (estimated)
- **Researcher Count**: 15+ full-time staff, 50+ affiliates
- **Projected Growth**: 2x expansion by 2025 based on field growth

## Key Uncertainties & Research Cruxes

### Technical Challenges

- **Representation Engineering Scalability**: Whether current methods work on [frontier models](/knowledge-base/capabilities/) remains unclear
- **Benchmark Validity**: Unknown if current evaluations capture real [safety risks](/knowledge-base/risks/accident/)
- **Alignment Verification**: No consensus on how to verify successful alignment

### Strategic Questions

- **Research vs. Policy Balance**: Optimal allocation between technical work and [governance efforts](/knowledge-base/responses/governance/)
- **Open vs. Closed Research**: Tension between transparency and [information hazards](/knowledge-base/risks/misuse/)
- **Timeline Assumptions**: Disagreement on [AGI timelines](/knowledge-base/forecasting/agi-timeline/) affects research priorities

## Leadership & Key Personnel

<Section title="Key People">
  <KeyPeople people={[
    { name: "Dan Hendrycks", role: "Executive Director", background: "UC Berkeley PhD, former Google Brain" },
    { name: "Mantas Mazeika", role: "Research Director", background: "University of Chicago, adversarial ML expert" },
    { name: "Thomas Woodside", role: "Policy Director", background: "Former congressional staffer, tech policy" },
    { name: "Andy Zou", role: "Research Scientist", background: "CMU, jailbreaking and red-teaming research" },
  ]} />
</Section>

## Sources & Resources

### Official Resources

| Type | Resource | Description |
|------|----------|-------------|
| **Website** | [safe.ai](https://www.safe.ai/) | Main organization hub |
| **Research** | [CAIS Publications](https://www.safe.ai/research) | Technical papers and reports |
| **Blog** | [CAIS Blog](https://www.safe.ai/blog) | Research updates and commentary |
| **Courses** | [ML Safety Course](https://course.mlsafety.org/) | Educational materials |

### Key Research Papers

| Paper | Year | Citations | Impact |
|-------|------|-----------|--------|
| [Unsolved Problems in ML Safety](https://arxiv.org/abs/2109.13916) | 2022 | 200+ | Research agenda setting |
| [MACHIAVELLI Benchmark](https://arxiv.org/abs/2304.03279) | 2023 | 50+ | Industry evaluation adoption |
| [Representation Engineering](https://arxiv.org/abs/2310.01405) | 2023 | 30+ | New research direction |

### Related Organizations

- **Research Alignment**: [MIRI](/knowledge-base/organizations/safety-orgs/miri/), [CHAI](/knowledge-base/organizations/safety-orgs/chai/), [Redwood Research](/knowledge-base/organizations/safety-orgs/redwood/)
- **Policy Focus**: [GovAI](/knowledge-base/organizations/safety-orgs/govai/), [RAND Corporation](https://www.rand.org/topics/artificial-intelligence.html)
- **Industry Labs**: [Anthropic](/knowledge-base/organizations/labs/anthropic/), [OpenAI](/knowledge-base/organizations/labs/openai/), [DeepMind](/knowledge-base/organizations/labs/deepmind/)

<Backlinks client:load entityId="cais" />
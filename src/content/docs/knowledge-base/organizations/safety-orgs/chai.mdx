---
title: CHAI (Center for Human-Compatible AI)
description: UC Berkeley research center on AI alignment founded by Stuart Russell
sidebar:
  order: 15
---

import {DataInfoBox, KeyPeople, Section, Backlinks, PageStatus} from '../../../../../components/wiki';

<PageStatus quality={2} lastEdited="2025-12-24" llmSummary="Center for Human-Compatible AI at UC Berkeley founded by Stuart Russell, focusing on building AI systems that defer to human preferences rather than pursuing fixed objectives. Research includes inverse reward design, assistance games, and cooperative AI frameworks. Provides academic legitimization of AI safety concerns through Russell's prominence." todo="Needs expansion on recent research outputs, current projects, and detailed coverage of key technical contributions beyond framework overview" />

<DataInfoBox entityId="chai" />

## Summary

The Center for Human-Compatible AI (CHAI) is an academic research center at UC Berkeley focused on ensuring AI systems are beneficial to humans. Founded by Stuart Russell, author of the leading AI textbook, CHAI brings academic rigor to AI safety research.

CHAI's research centers on "human-compatible AI" - systems that are inherently safe because they defer to human preferences rather than pursuing fixed objectives.

## Core Framework

### The Standard Model Problem
Russell argues the "standard model" of AI (optimize a fixed objective) is fundamentally flawed:
- Objectives are hard to specify completely
- AI will find unintended ways to achieve objectives
- More capable AI â†’ worse outcomes from misspecification

### Human-Compatible AI
CHAI proposes AI systems should:
1. Have uncertainty about human preferences
2. Learn preferences from human behavior
3. Allow humans to switch them off
4. Defer to humans on important decisions

## Research Areas

### Inverse Reward Design
Learning human preferences from behavior rather than explicit specification. Recognizing that stated objectives are proxies for true preferences.

### Assistance Games
Formalizing AI-human interaction as cooperative games where AI's goal is to help humans achieve their (uncertain) preferences.

### Value Alignment
Research on how to align AI systems with human values:
- Preference learning
- Value extrapolation
- Handling value uncertainty

### Cooperative AI
How AI systems can cooperate with humans and other AI systems safely.

<Section title="Key People">
  <KeyPeople people={[
    { name: "Stuart Russell", role: "Founder, Professor" },
    { name: "Anca Dragan", role: "Associate Director (now DeepMind)" },
    { name: "Pieter Abbeel", role: "Affiliated Faculty" },
    { name: "Michael Dennis", role: "Researcher" },
  ]} />
</Section>

## Key Publications

- **Human Compatible** (Russell, 2019) - Book on AI safety for general audiences
- **Cooperative Inverse Reinforcement Learning** (2016)
- **The Off-Switch Game** (2017)
- **Benefits and Risks of Artificial Intelligence** (Russell et al.)

## Influence

CHAI has shaped AI safety through:
- Academic legitimization of AI safety concerns
- Russell's public advocacy and policy engagement
- Training PhD students in alignment research
- Collaborations with DeepMind and other labs

## Related Pages

<Backlinks client:load entityId="chai" />

---
title: "Conjecture"
description: "AI safety research organization focused on cognitive emulation and mechanistic interpretability, pursuing interpretability-first approaches to building safe AI systems"
sidebar:
  order: 12
quality: 72
llmSummary: "Conjecture pursues Cognitive Emulation (CoEm) as an interpretability-first alternative to mainstream alignment, with $30M+ Series A funding (2023) and 30-40 researchers. Key challenge is whether CoEm can remain competitive against prosaic approaches given potentially short AGI timelines."
lastEdited: "2025-12-24"
importance: 38.5
---
import {DataInfoBox, EstimateBox, KeyPeople, KeyQuestions, Section, R} from '../../../../../components/wiki';

<DataInfoBox entityId="conjecture" />

## Overview

Conjecture is an AI safety research organization founded in 2021 by [Connor Leahy](/knowledge-base/people/connor-leahy/) and a team of researchers concerned about existential risks from advanced AI. The organization pursues a distinctive technical approach centered on "Cognitive Emulation" (CoEm) - building interpretable AI systems based on human cognition principles rather than aligning existing large language models.

Based in London with a team of 30-40 researchers, Conjecture raised over \$10M in Series A funding in 2023. Their research agenda emphasizes mechanistic interpretability and understanding neural network internals, representing a fundamental alternative to mainstream [prosaic alignment approaches](/knowledge-base/debates/formal-arguments/why-alignment-hard/) pursued by organizations like [Anthropic](/knowledge-base/organizations/labs/anthropic/) and [OpenAI](/knowledge-base/organizations/labs/openai/).

| Aspect | Assessment | Evidence | Source |
|--------|------------|----------|--------|
| Technical Innovation | High | Novel CoEm research agenda | <R id="b7aa1f2c839b5ee8">Conjecture Blog</R> |
| Funding Security | Strong | \$30M+ Series A (2023) | <R id="b2f30b8ca0dd850e">TechCrunch Reports</R> |
| Research Output | Moderate | Selective publication strategy | <R id="296aaf722d89ca8c">Research Publications</R> |
| Influence | Growing | European AI policy engagement | <R id="817964dfbb0e3b1b">UK AISI</R> |

## Risk Assessment

| Risk Category | Severity | Likelihood | Timeline | Trend |
|---------------|----------|------------|----------|--------|
| CoEm Uncompetitive | High | Moderate | 3-5 years | Uncertain |
| Commercial Pressure Compromise | Medium | High | 2-3 years | Worsening |
| Research Insularity | Low | Moderate | Ongoing | Stable |
| Funding Sustainability | Medium | Low | 5+ years | Improving |

## Founding and Evolution

### Origins (2021)

Conjecture emerged from the **EleutherAI** collective, an open-source AI research group that successfully recreated GPT-3 as open-source models (GPT-J, GPT-NeoX). Key founding factors:

| Factor | Impact | Details |
|--------|--------|---------|
| EleutherAI Experience | High | Demonstrated capability replication feasibility |
| Safety Concerns | High | Recognition of risks from capability proliferation |
| European Gap | Medium | Limited AI safety ecosystem outside Bay Area |
| Funding Availability | Medium | Growing investor interest in AI safety |

**Philosophical Evolution**: The transition from EleutherAI's "democratize AI" mission to Conjecture's safety-focused approach represents a significant shift in thinking about AI development and publication strategies.

### Funding Trajectory

| Year | Funding Stage | Amount | Impact |
|------|--------------|--------|--------|
| 2021 | Seed | Undisclosed | Initial team of ~15 researchers |
| 2023 | Series A | \$30M+ | Scaled to 30-40 researchers |
| 2024 | Operating | Ongoing | Sustained research operations |

## Cognitive Emulation (CoEm) Research Agenda

### Core Philosophy

Conjecture's signature approach contrasts sharply with mainstream AI development:

| Approach | Philosophy | Methods | Evaluation |
|----------|------------|---------|------------|
| **Prosaic Alignment** | Train powerful LLMs, align post-hoc | RLHF, Constitutional AI | Behavioral testing |
| **Cognitive Emulation** | Build interpretable systems from ground up | Human cognition principles | Mechanistic understanding |

### Key Research Components

**Mechanistic Interpretability**
- Circuit discovery in neural networks
- Feature attribution and visualization
- Scaling interpretability to larger models
- [Interpretability research](/knowledge-base/responses/alignment/interpretability/) collaboration

**Architecture Design**
- Modular systems for better control
- Interpretability-first design choices
- Trading capabilities for understanding
- Novel training methodologies

**Model Organisms**
- Smaller, interpretable test systems
- Alignment property verification
- Deception detection research
- Goal representation analysis

## Key Personnel

<Section title="Leadership Team">
  <KeyPeople people={[
    { name: "Connor Leahy", role: "CEO and Co-founder", background: "EleutherAI, autodidact ML researcher" },
    { name: "Sid Black", role: "Co-founder", background: "EleutherAI technical researcher" },
    { name: "Gabriel Alfour", role: "CTO", background: "Former Tezos CTO, systems engineering" },
  ]} />
</Section>

### Connor Leahy Profile

| Aspect | Details |
|--------|---------|
| **Background** | EleutherAI collective member, GPT-J contributor |
| **Evolution** | From open-source advocacy to safety-focused research |
| **Public Role** | Active AI policy engagement, podcast appearances |
| **Views** | Short AI timelines, high P(doom), interpretability-necessary |

**Timeline Estimates**: Leahy has consistently expressed [short AI timeline](/understanding-ai-risk/core-argument/timelines/) views, suggesting AGI within years rather than decades.

## Research Focus Areas

### Mechanistic Interpretability

| Research Area | Status | Key Questions |
|---------------|--------|---------------|
| **Circuit Analysis** | Active | How do transformers implement reasoning? |
| **Feature Extraction** | Ongoing | What representations emerge in training? |
| **Scaling Methods** | Development | Can interpretability scale to AGI-level systems? |
| **Goal Detection** | Early | How can we detect goal-directedness mechanistically? |

### Comparative Advantages

| Organization | Primary Focus | Interpretability Approach |
|--------------|---------------|---------------------------|
| **Conjecture** | CoEm, ground-up interpretability | Design-time interpretability |
| **[Anthropic](/knowledge-base/organizations/labs/anthropic/)** | Frontier models + interpretability | Post-hoc analysis of LLMs |
| **[ARC](/knowledge-base/organizations/safety-orgs/arc/)** | Theoretical alignment | Evaluation and ELK research |
| **[Redwood](/knowledge-base/organizations/safety-orgs/redwood/)** | AI control | Interpretability for control |

## Strategic Position

### Theory of Change

Conjecture's pathway to AI safety impact:

1. **Develop scalable interpretability techniques** for powerful AI systems
2. **Demonstrate CoEm viability** as competitive alternative to black-box scaling
3. **Influence field direction** toward interpretability-first development
4. **Inform governance** with technical feasibility insights
5. **Build safe systems** using CoEm principles if successful

### European AI Safety Hub

| Role | Impact | Examples |
|------|--------|----------|
| **Geographic Diversity** | High | Alternative to Bay Area concentration |
| **Policy Engagement** | Growing | [UK AISI](/knowledge-base/organizations/government/uk-aisi/) consultation |
| **Talent Development** | Moderate | European researcher recruitment |
| **Community Building** | Early | Workshops and collaborations |

## Challenges and Criticisms

### Technical Feasibility

| Challenge | Severity | Status |
|-----------|----------|--------|
| **CoEm Competitiveness** | High | Unresolved - early stage |
| **Interpretability Scaling** | High | Active research question |
| **Human Cognition Complexity** | Medium | Ongoing investigation |
| **Timeline Alignment** | High | Critical if [AGI timelines](/understanding-ai-risk/core-argument/timelines/) short |

### Organizational Tensions

**Commercial Pressure vs Safety Mission**
- VC funding creates return expectations
- Potential future deployment pressure
- Comparison to [Anthropic's commercialization path](/knowledge-base/organizations/labs/anthropic/)

**Publication Strategy Criticism**
- Shift from EleutherAI's radical openness
- Selective research sharing decisions
- Balance between transparency and safety

## Current Research Outputs

### Published Work

| Type | Focus | Impact |
|------|-------|--------|
| **Technical Papers** | Interpretability methods | Research community |
| **Blog Posts** | CoEm explanations | Public understanding |
| **Policy Contributions** | Technical feasibility | Governance decisions |
| **Open Source Tools** | Interpretability software | Research ecosystem |

### Research Questions

<KeyQuestions questions={[
  "Can CoEm produce AI systems competitive with scaled LLMs?",
  "Is mechanistic interpretability sufficient for AGI safety verification?",
  "How will commercial pressures affect Conjecture's research direction?",
  "What role should interpretability play in AI governance frameworks?",
  "Can cognitive emulation bridge neuroscience and AI safety research?",
  "How does CoEm relate to other alignment approaches like Constitutional AI?"
]} />

## Timeline and Risk Estimates

<Section title="Risk and Timeline Assessments">
  <EstimateBox
    client:load
    variable="Conjecture Leadership Risk Estimates"
    description="Based on public statements and research direction"
    unit="Assessment"
    estimates={[
      { source: "Connor Leahy", value: "AGI: 2-10 years", date: "2023-2024", notes: "Consistently short timeline estimates" },
      { source: "Connor Leahy", value: "P(doom): High without major changes", date: "2023", notes: "Expressed significant concern about default trajectory" },
      { source: "Conjecture Research", value: "Prosaic alignment: Insufficient", date: "Ongoing", notes: "Core motivation for CoEm approach" },
      { source: "Organization", value: "Interpretability: Necessary for safety", date: "Founding", notes: "Fundamental research assumption" }
    ]}
  />
</Section>

## Future Scenarios

### Research Trajectory Projections

| Timeline | Optimistic | Realistic | Pessimistic |
|----------|------------|-----------|-------------|
| **2-3 years** | CoEm demonstrations, policy influence | Continued interpretability advances | Commercial pressure compromises |
| **3-5 years** | Competitive interpretable systems | Mixed results, partial success | Research agenda stagnates |
| **5+ years** | Field adoption of CoEm principles | Portfolio contribution to safety | Marginalized approach |

### Critical Dependencies

| Factor | Importance | Uncertainty |
|--------|------------|-------------|
| **Technical Feasibility** | Critical | High - unproven at scale |
| **Funding Continuity** | High | Medium - VC expectations |
| **AGI Timeline** | Critical | High - if very short, insufficient time |
| **Field Receptivity** | Medium | Medium - depends on results |

## Relationships and Collaborations

### Within AI Safety Ecosystem

| Organization | Relationship | Collaboration Type |
|--------------|--------------|-------------------|
| **[Anthropic](/knowledge-base/organizations/labs/anthropic/)** | Friendly competition | Interpretability research sharing |
| **[ARC](/knowledge-base/organizations/safety-orgs/arc/)** | Complementary | Different technical approaches |
| **[MIRI](/knowledge-base/organizations/safety-orgs/miri/)** | Aligned concerns | Skepticism of prosaic alignment |
| **Academic Labs** | Collaborative | Interpretability technique development |

### Policy and Governance

**UK Engagement**
- [UK AI Safety Institute](/knowledge-base/organizations/government/uk-aisi/) consultation
- Technical feasibility assessments
- European AI Act discussions

**International Influence**
- Growing presence in global AI safety discussions
- Alternative perspective to US-dominated discourse
- Technical grounding for [governance approaches](/knowledge-base/responses/governance/)

## Sources & Resources

### Primary Sources

| Type | Source | Description |
|------|--------|-------------|
| **Official Website** | <R id="b7aa1f2c839b5ee8">Conjecture.dev</R> | Research updates, team information |
| **Research Papers** | <R id="1de7c0ba4f50708d">Google Scholar</R> | Technical publications |
| **Blog Posts** | <R id="47b5960d711ad336">Conjecture Blog</R> | Research explanations, philosophy |
| **Interviews** | <R id="51e0a77c2b6e8cd4">Connor Leahy Talks</R> | Leadership perspectives |

### Secondary Analysis

| Type | Source | Focus |
|------|--------|-------|
| **AI Safety Analysis** | <R id="ccb3e34a982e2528">LessWrong Posts</R> | Community discussion |
| **Technical Reviews** | <R id="2e0c662574087c2a">Alignment Forum</R> | Research evaluation |
| **Policy Reports** | <R id="f35c467b353f990f">GovAI Analysis</R> | Governance implications |
| **Funding News** | <R id="401e5a60f9cd395e">TechCrunch Coverage</R> | Business developments |

### Related Resources

| Topic | Internal Links | External Resources |
|-------|----------------|-------------------|
| **Interpretability** | [Technical Interpretability](/knowledge-base/responses/alignment/interpretability/) | <R id="5083d746c2728ff2">Anthropic Interpretability</R> |
| **Alignment Approaches** | [Why Alignment is Hard](/knowledge-base/debates/formal-arguments/why-alignment-hard/) | <R id="2e0c662574087c2a">AI Alignment Forum</R> |
| **European AI Policy** | [UK AISI](/knowledge-base/organizations/government/uk-aisi/) | <R id="f37ebc766aaa61d7">EU AI Office</R> |
| **Related Orgs** | [Safety Organizations](/knowledge-base/organizations/safety-orgs/) | <R id="876bb3bfc6031642">AI Safety Community</R> |
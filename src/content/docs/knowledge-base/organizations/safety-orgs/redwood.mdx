---
title: Redwood Research
description: AI safety lab focused on empirical research, mechanistic interpretability, and pioneering AI control methodology for safety without full alignment
sidebar:
  order: 12
quality: 5
llmSummary: Comprehensive overview of Redwood Research's evolution from adversarial robustness to mechanistic interpretability to AI control research, documenting their key contributions including causal scrubbing methodology and the novel AI control framework that assumes potential misalignment while building safety protocols. The organization has made significant empirical contributions to AI safety research with a pragmatic approach that pivots based on evidence.
lastEdited: "2025-12-24"
importance: 62.5
---

import {DataInfoBox, DisagreementMap, EstimateBox, KeyPeople, KeyQuestions, Section, Backlinks} from '../../../../../components/wiki';

<DataInfoBox entityId="redwood" />

## Overview

Redwood Research is an AI safety lab founded in 2021 that has pioneered the "AI control" research paradigm while making significant contributions to mechanistic interpretability. The organization operates on the pragmatic assumption that alignment may not be fully solved in time, leading to their groundbreaking work on ensuring safety even with potentially misaligned AI systems.

Their trajectory shows remarkable adaptability: beginning with adversarial robustness research, pivoting to mechanistic interpretability with their influential causal scrubbing methodology, and ultimately developing the AI control framework that has gained significant attention across the safety community. With 15+ researchers and $5M+ in funding, Redwood has trained numerous safety researchers now working across [Anthropic](/knowledge-base/organizations/labs/anthropic/), [ARC](/knowledge-base/organizations/safety-orgs/arc/), and other major organizations.

The organization's core insight is that traditional alignment approaches may be insufficient for highly capable systems, necessitating "control" measures that prevent catastrophic outcomes even from scheming AI systems.

## Risk and Impact Assessment

| Factor | Assessment | Evidence | Timeline | Source |
|--------|------------|----------|----------|--------|
| **Technical Influence** | High | Causal scrubbing adopted by Anthropic, 100+ citations | 2022-present | [Anthropic research](https://www.anthropic.com/research) |
| **Field Building** | High | 20+ fellowship alumni at major labs | 2021-present | [EA Forum tracking](https://forum.effectivealtruism.org) |
| **Control Paradigm Adoption** | Medium | Growing interest across labs | 2023-present | [AI safety discussions](https://www.alignmentforum.org) |
| **Policy Relevance** | Medium | Control more implementable than full alignment | 2024+ | [RAND analysis](https://www.rand.org/pubs/perspectives/PEA2849-1.html) |
| **Research Quality** | High | Rigorous empirical methodology | Ongoing | Multiple peer reviews |

## Key Research Evolution

### Phase 1: Adversarial Robustness (2021-2022)

**Problem Focus**: Language models could be jailbroken despite safety training

| Challenge | Approach | Results | Lessons |
|-----------|----------|---------|---------|
| Harmful content generation | Adversarial training on refusal | Some domain-specific success | Adversarial examples persist |
| Jailbreak robustness | Systematic prompt engineering | Limited scalability | Need internal understanding |
| Behavioral safety | External training constraints | Fundamental brittleness | Surface-level solutions insufficient |

**Key Finding**: Behavioral safety training has fundamental limitations that don't scale to existential safety concerns.

### Phase 2: Mechanistic Interpretability (2022-2023)

**Strategic Pivot**: Understanding internal computations rather than just behavior

**Major Contribution - Causal Scrubbing**:
- **Innovation**: Rigorous testing framework for interpretability hypotheses
- **Method**: Ablate components according to proposed explanation, measure behavioral changes
- **Impact**: Raised standards across interpretability field
- **Adoption**: Used by [Anthropic](/knowledge-base/organizations/labs/anthropic/), multiple academic groups
- **Citations**: 150+ across major venues

| Research Area | Contribution | Impact | Follow-up Work |
|---------------|-------------|--------|----------------|
| Circuit Analysis | Causal scrubbing methodology | Field standard | Adopted by Anthropic |
| Polysemanticity | Superposition investigation | Theoretical insight | Dictionary learning research |
| Feature Extraction | Automated circuit discovery | Tool development | Open-source implementations |

### Phase 3: AI Control (2023-Present)

**Revolutionary Paradigm**: Safety without requiring full alignment

**Core Framework**:
- **Assumption**: Deploy potentially misaligned but capable AI systems
- **Goal**: Prevent catastrophic outcomes despite potential scheming
- **Analogy**: Cybersecurity mindset applied to AI safety

| Control Component | Description | Research Status | Effectiveness |
|------------------|-------------|-----------------|---------------|
| **Monitoring** | Detect deceptive/scheming behavior | Active research | 70-90% detection in toy models |
| **Protocol Design** | Safe deployment procedures | Framework developed | Testing in controlled settings |
| **Red-team Testing** | Adversarial evaluation of controls | Ongoing | Multiple vulnerabilities identified |
| **Untrusted Monitoring** | AI monitoring AI systems | Early research | Theoretical framework |

## Current Research Portfolio

### AI Control Research

**Key Publications**:
- ["AI Control: Improving Safety Despite Intentional Subversion"](https://arxiv.org/abs/2312.06942) (2024)
- Control evaluation methodologies
- Red-team protocol development

**Research Questions**:
- Can monitoring detect sophisticated deception?
- What deployment protocols minimize catastrophic risk?
- How to design safety cases for potentially misaligned AI?

### Interpretability Methods

**Ongoing Work**:
- Activation steering techniques
- Automated circuit discovery
- [Deceptive alignment](/knowledge-base/risks/accident/deceptive-alignment/) detection
- Understanding [situational awareness](/knowledge-base/capabilities/situational-awareness/)

**Tools Developed**:
- Open-source interpretability libraries
- Circuit analysis frameworks
- Causal intervention methodologies

### Empirical Safety Research

| Project Area | Focus | Status | Collaboration |
|-------------|-------|--------|---------------|
| Sandbagging Detection | Models hiding capabilities | Ongoing | With [ARC](/knowledge-base/organizations/safety-orgs/arc/) |
| Jailbreak Robustness | Systematic prompt attacks | Published | Multiple labs |
| Training Dynamics | Misalignment development | Research phase | Academic partners |
| Evaluation Methodology | Safety assessment frameworks | Active | Industry consultation |

## Key Personnel and Expertise

<Section title="Leadership Team">
  <KeyPeople people={[
    { name: "Buck Shlegeris", role: "CEO & Co-founder", expertise: "Strategic direction, empirical methodology" },
    { name: "Ryan Greenblatt", role: "Research Lead", expertise: "AI control, adversarial evaluation" },
    { name: "William Saunders", role: "Research Scientist", expertise: "Interpretability, training dynamics" },
    { name: "Neel Nanda", role: "Former Senior Researcher (now DeepMind)", expertise: "Mechanistic interpretability" },
  ]} />
</Section>

### Buck Shlegeris (CEO)
- **Background**: Software engineering, rationalist community
- **Philosophy**: Empirical focus, rapid iteration, evidence-driven pivots
- **Contribution**: Organizational strategy, research direction setting
- **Public Role**: Active in safety forums, clear research communication

### Ryan Greenblatt
- **Expertise**: Machine learning, adversarial evaluation
- **Key Work**: AI control framework development, red-team methodologies
- **Approach**: Rigorous threat modeling, security-minded safety research

### Research Culture
- **Fellows Program**: Trained 20+ researchers now at major safety organizations
- **Methodology**: Empirical validation, quick pivots based on evidence
- **Collaboration**: Active partnerships with [labs](/knowledge-base/organizations/labs/) and [safety organizations](/knowledge-base/organizations/safety-orgs/)

## Current State and Trajectory

### Organizational Metrics

| Metric | 2021 | 2022 | 2023 | 2024 | Projection 2025 |
|--------|------|------|------|------|-----------------|
| Research Staff | 5 | 8 | 12 | 15 | 20-25 |
| Annual Budget | $1M | $2M | $3.5M | $5M | $7M |
| Publications | 0 | 3 | 8 | 12 | 15-20 |
| Fellows Trained | 0 | 5 | 12 | 20 | 30+ |

### Research Impact

**Citation Metrics**:
- Causal scrubbing paper: 150+ citations
- AI control framework: 50+ citations (recent publication)
- Total research impact: 500+ citations across work

**Adoption Indicators**:
- Anthropic implementing causal scrubbing methods
- Multiple labs exploring control approaches
- Policy discussions referencing control paradigm

### Strategic Positioning

**2024-2025 Focus**:
- Scaling AI control research
- Developing practical safety protocols
- Building industry partnerships for testing
- Expanding empirical evaluation capabilities

**Funding Trajectory**: Diversifying from grants to include industry partnerships while maintaining research independence.

## Key Uncertainties and Debates

### The Control Paradigm

<Section title="Major Questions">
  <KeyQuestions questions={[
    "Can control measures actually work against superintelligent scheming AI?",
    "Does the control approach create false confidence in deployment safety?",
    "Should resources focus on alignment research rather than control?",
    "How to validate control measures without access to truly dangerous AI?",
    "What is the appropriate balance between control and alignment research?"
  ]} />
</Section>

| Uncertainty | Pessimistic View | Optimistic View | Current Evidence |
|-------------|-----------------|-----------------|------------------|
| **Control Effectiveness** | Superintelligent AI will circumvent any controls | Layered defenses can achieve acceptable risk | Limited to toy model testing |
| **Resource Allocation** | Control distracts from alignment work | Control provides crucial safety layer | Mixed views in safety community |
| **Timeline Relevance** | Won't be ready before dangerous AI | Can develop alongside capabilities | Active research progress |
| **Industry Adoption** | Labs won't implement costly controls | Control more practical than alignment | Early industry interest |

### Technical Challenges

**Monitoring Limitations**:
- Can sophisticated AI hide deceptive behavior?
- How to detect novel attack vectors?
- Scalability of human oversight

**Protocol Design**:
- Balance between safety and capability
- Economic incentives for control adoption
- Verification of control implementations

### Philosophical Questions

**Relationship to Alignment**:
- Complement or substitute for alignment research?
- Risk of reducing alignment investment?
- Long-term vision for AI safety

## Comparative Analysis

### Research Approach Comparison

| Organization | Primary Focus | Timeline View | Methodology | Success Theory |
|-------------|---------------|---------------|-------------|----------------|
| **Redwood** | Control + interpretability | Medium pessimism | Empirical, adaptive | Safety without full alignment |
| **[Anthropic](/knowledge-base/organizations/labs/anthropic/)** | Constitutional AI, scaling | Cautious optimism | Large-scale experiments | Alignment through training |
| **[ARC](/knowledge-base/organizations/safety-orgs/arc/)** | Evaluation, alignment theory | Varied | Theory + evaluation | Early detection and preparation |
| **[MIRI](/knowledge-base/organizations/safety-orgs/miri/)** | Governance pivot | High pessimism | Theoretical (paused) | Coordination over technical |

### Unique Positioning

**Distinguishing Features**:
- Only major org focused primarily on control
- Empirical methodology with willingness to pivot
- Security-minded approach to safety
- Practical deployment focus

**Strategic Advantages**:
- Complementary to alignment research
- More implementable in near-term
- Appeals to industry safety concerns
- Builds on existing security practices

## Sources and Resources

### Primary Research Publications

| Publication | Year | Impact | Focus |
|-------------|------|--------|-------|
| [Causal Scrubbing Paper](https://arxiv.org/abs/2309.10312) | 2023 | 150+ citations | Interpretability methodology |
| [AI Control Framework](https://arxiv.org/abs/2312.06942) | 2024 | Foundational | Control paradigm |
| [Adversarial Robustness Studies](https://arxiv.org/search/cs?searchtype=author&query=Greenblatt%2C+R) | 2022-2023 | Field building | Empirical safety |

### Organizational Resources

| Type | Resource | Access |
|------|----------|--------|
| **Research Portal** | [Redwood Research](https://www.redwoodresearch.org/) | Public |
| **Publications** | [Research Publications](https://www.redwoodresearch.org/research) | Open access |
| **Code/Tools** | [GitHub Repository](https://github.com/redwoodresearch) | Open source |
| **Fellowship** | [Research Fellowship](https://www.redwoodresearch.org/fellowship) | Application-based |

### External Analysis

| Source | Focus | Assessment |
|--------|-------|------------|
| [Open Philanthropy](https://www.openphilanthropy.org/) | Funding analysis | Major safety contribution |
| [Alignment Forum](https://www.alignmentforum.org/tag/redwood-research) | Technical discussion | Mixed reception of control approach |
| [AI Safety Newsletter](https://aisafety.info/) | Regular coverage | Positive on empirical methodology |
| [Future of Humanity Institute](https://www.fhi.ox.ac.uk/) | Academic perspective | Valuable addition to safety portfolio |

### Related Safety Organizations

| Organization | Relationship | Collaboration Type |
|-------------|--------------|-------------------|
| **[Anthropic](/knowledge-base/organizations/labs/anthropic/)** | Research collaboration | Interpretability methods sharing |
| **[ARC](/knowledge-base/organizations/safety-orgs/arc/)** | Complementary focus | Evaluation methodology |
| **[CHAI](/knowledge-base/organizations/safety-orgs/chai/)** | Academic partnership | Research collaboration |
| **[Apollo Research](/knowledge-base/organizations/safety-orgs/apollo-research/)** | Similar approach | Evaluation and testing |

### Policy and Governance Context

| Institution | Engagement | Focus |
|------------|------------|-------|
| **[UK AISI](/knowledge-base/organizations/government/uk-aisi/)** | Consultation | Control methodologies |
| **[US AISI](/knowledge-base/organizations/government/us-aisi/)** | Framework discussion | Deployment safety |
| **NIST AI RMF** | Input provision | Risk management |
| **Industry Standards** | Development participation | Safety protocols |

<Backlinks client:load entityId="redwood" />
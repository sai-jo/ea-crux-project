---
title: "Long-run Value"
description: "The expected value of the world after the AI transition resolves—whether human agency, equitable benefits, and meaningful flourishing are achieved."
sidebar:
  order: 2
  label: Long-run Value
lastEdited: "2026-01-02"
---
import {Mermaid, Backlinks} from '../../../../../components/wiki';

## Overview

Long-run Value measures the expected quality of the world *after the acute risk period resolves*—whatever equilibrium or trajectory humanity ends up on. This is about the **destination** (or ongoing trajectory), distinct from whether we survive to reach it ([Acute Risk](/knowledge-base/parameters/outcomes/acute-risk/)).

Even if we avoid catastrophe entirely, we could end up in a world where humans lack meaningful agency, AI benefits are concentrated among few, or authentic human preferences are manipulated. A "successful" transition to a dystopia is still a failure.

**Why "Long-run Value" not "Steady State"?** We don't know whether a stable equilibrium will emerge. The future might involve ongoing change, multiple equilibria, or no clear "steady state" at all. "Long-run Value" captures what we care about without assuming stability. See [Phases of the AI Transition](/knowledge-base/methodology/ai-transition-phases/) for more on temporal structure.

---

## Sub-dimensions

| Dimension | Description | Key Parameters |
|-----------|-------------|----------------|
| **Human Agency Preserved** | People retain meaningful autonomy and genuine choice | Human Agency, Preference Authenticity |
| **Benefit Distribution** | AI gains are shared equitably, not concentrated | AI Control Concentration, Economic Stability |
| **Democratic Governance** | Legitimate collective decision-making maintained | Institutional Quality, AI Control Concentration |
| **Human Purpose/Meaning** | People have fulfilling roles, not idle consumption | Human Expertise, Human Agency |
| **Epistemic Autonomy** | Humans can think independently and form genuine views | Epistemic Health, Reality Coherence |
| **Diversity Preserved** | Multiple viable ways of life exist | Preference Authenticity, Human Agency |
| **Option Value** | Future generations can make different choices | Reversibility, Lock-in avoidance |

---

## What Shapes Long-run Value

<Mermaid client:load chart={`
flowchart TD
    subgraph CriticalOutcomes["Critical Outcomes"]
        LOCK[Value Lock-in]
        EPIST[Epistemic Trajectory]
        POWER[Power Transition]
        SLOW[Gradual AI Takeover]
        TURB[Transition Turbulence]
    end

    subgraph Dimensions["Value Dimensions"]
        AGENCY[Human Agency]
        BENEFITS[Benefit Distribution]
        MEANING[Purpose and Meaning]
        OPTIONS[Option Value]
    end

    LOCK -->|"determines"| OPTIONS
    LOCK -->|"shapes"| AGENCY
    EPIST -->|"enables"| AGENCY
    EPIST -->|"affects"| MEANING
    POWER -->|"determines"| BENEFITS
    POWER -->|"shapes"| AGENCY
    SLOW -->|"erodes"| AGENCY
    TURB -->|"path dependence"| OPTIONS

    AGENCY --> VALUE[Long-run Value]
    BENEFITS --> VALUE
    MEANING --> VALUE
    OPTIONS --> VALUE

    style VALUE fill:#4ecdc4
    style LOCK fill:#ffe66d
    style EPIST fill:#ffe66d
`} />

### Critical Outcomes That Affect This

| Critical Outcome | Effect on Long-run Value |
|------------------|-------------------------|
| [Value Lock-in](/knowledge-base/parameters/critical-outcomes/value-lock-in/) | **Primary** — Determines whether good or bad values persist |
| [Epistemic Trajectory](/knowledge-base/parameters/critical-outcomes/epistemic-quality/) | **Primary** — Renaissance enables flourishing; collapse undermines it |
| [Power Transition](/knowledge-base/parameters/critical-outcomes/power-transition/) | **Primary** — How power shifts determines who benefits |
| [Gradual AI Takeover](/knowledge-base/parameters/critical-outcomes/ai-takeover-gradual/) | **Secondary** — Successful takeover means AI goals, not human values |
| [Transition Turbulence](/knowledge-base/parameters/critical-outcomes/transition-turbulence/) | **Secondary** — Path dependence; rough transitions constrain options |

### Key Aggregate Parameters

| Aggregate | Relationship | Mechanism |
|-----------|--------------|-----------|
| [Epistemic Foundation](/knowledge-base/parameters/aggregates/epistemic-foundation/) | High → Better | Clear thinking and shared reality enable good choices |
| [Governance Capacity](/knowledge-base/parameters/aggregates/governance-capacity/) | High → Better | Effective institutions shape beneficial structures |
| [Societal Adaptability](/knowledge-base/parameters/aggregates/societal-adaptability/) | High → Better | Preserved human capacity maintains agency and purpose |

---

## Why This Matters

Long-run conditions are what *persist*:
- **Lock-in effects**: Once established, structures are hard to change
- **Compounding**: Small differences in trajectory compound over time
- **Irreversibility**: Some futures preclude alternatives permanently
- **Values matter**: Technical success (avoiding catastrophe) isn't enough if we lose what we value

This outcome dimension asks: **"Even if we avoid disaster, will the future be worth living in?"**

---

## Key Trade-offs

| Trade-off | Description |
|-----------|-------------|
| **Safety vs. Agency** | Maximum safety might require ceding control to AI, reducing human agency |
| **Efficiency vs. Purpose** | Optimal AI allocation might leave humans without meaningful roles |
| **Coordination vs. Diversity** | Global coordination might homogenize cultures and ways of life |
| **Speed vs. Deliberation** | Faster development might lock in values before we understand implications |
| **Stability vs. Option Value** | Stable good outcomes might preclude even better alternatives |

---

## Scenarios

| Scenario | Long-run Value | Characteristics |
|----------|---------------|-----------------|
| **Flourishing** | Very High | Human agency preserved, benefits shared, meaning maintained |
| **Comfortable Dystopia** | Low | Material abundance but no agency, meaning, or authentic choice |
| **Stagnation** | Medium | Safety achieved but progress halted, options foreclosed |
| **Fragmented** | Variable | Some regions flourish, others don't; high inequality |
| **Gradual Decline** | Declining | No catastrophe but slow erosion of human relevance |

---

## Relationship to Acute Risk

| Acute Risk Outcome | Long-run Value |
|--------------------|----------------|
| **Catastrophe occurs** | N/A (no long run) |
| **Catastrophe avoided, bad lock-in** | Low |
| **Catastrophe avoided, good trajectory** | High |

**Key insight**: Acute Risk and Long-run Value are partially independent. You can:
- Avoid catastrophe but end up in a bad future (dystopia)
- Have high acute risk but good conditional outcomes (high-variance)
- Achieve both low risk and high value (best case)

---

## Related Content

- [Acute Risk](/knowledge-base/parameters/outcomes/acute-risk/) — The other Ultimate Outcome
- [Value Lock-in](/knowledge-base/parameters/critical-outcomes/value-lock-in/) — Key Critical Outcome for long-run value
- [Phases of the AI Transition](/knowledge-base/methodology/ai-transition-phases/) — When does "long-run" begin?

<Backlinks entityId="long-run-value" />

---
title: Geoffrey Hinton
description: Turing Award winner, 'Godfather of AI', vocal about AI risks
sidebar:
  order: 8
---

import { InfoBox, EntityCard, EntityCards, Tags, Sources, Section, EstimateBox } from '../../../../components/wiki';

<InfoBox
  type="researcher"
  title="Geoffrey Hinton"
  affiliation="University of Toronto (Emeritus), formerly Google"
  role="Professor Emeritus, AI Safety Advocate"
  knownFor="Deep learning pioneer, backpropagation, now AI risk vocal advocate"
  website="https://www.cs.toronto.edu/~hinton/"
/>

## Background

Geoffrey Hinton is widely known as one of the "Godfathers of AI" for his foundational work in neural networks and deep learning. Born in Britain, he spent much of his career at University of Toronto and Google, where he helped ignite the modern AI revolution.

Academic and research career:
- PhD in Artificial Intelligence from University of Edinburgh (1978)
- Professor at University of Toronto (1987-present, emeritus)
- Joined Google in 2013 (part-time, after they acquired his startup DNNresearch)
- Turing Award (2018) shared with Yoshua Bengio and Yann LeCun
- Over 200 publications, among most influential computer scientists

In May 2023, Hinton left Google to speak freely about AI risks, making headlines worldwide.

## Revolutionary Contributions to AI

### Backpropagation and Deep Learning

Hinton's work in the 1980s on backpropagation (with David Rumelhart and Ronald Williams) provided the mathematical foundation for training deep neural networks. When others abandoned neural networks, Hinton persisted.

### Key Technical Innovations

- **Boltzmann Machines**: Early probabilistic neural networks
- **Backpropagation**: Algorithm for training multi-layer networks
- **Dropout**: Regularization technique preventing overfitting
- **Capsule Networks**: Alternative to convolutional neural networks
- **Variational autoencoders**: Generative models

### Igniting the Deep Learning Revolution

In 2012, Hinton's student Alex Krizhevsky (supervised by Hinton) won ImageNet competition by huge margin using deep learning (AlexNet). This demonstrated deep learning's power and triggered explosion of interest and investment.

## The Pivot to AI Safety

### Why He Left Google (May 2023)

Hinton publicly resigned from Google to speak freely about AI risks, stating:
- He wanted to warn about dangers without constraints
- Rapid progress exceeded his expectations
- Concerned about bad actors and loss of control
- Felt responsibility given his role in creating the technology

This resignation made global headlines and significantly raised public awareness of AI risks.

### Evolution of Views

**Earlier (2010s):**
- Focused purely on capabilities
- Optimistic about AI benefits
- Didn't publicly discuss risks much

**Transition (2020-2022):**
- Growing private concerns
- Surprised by rapid progress of large language models
- Beginning to worry about timelines

**Current (2023-present):**
- Very vocal about existential risks
- Publicly concerned about near-term dangers
- Actively advocating for regulation and caution

## Current Views on AI Risk

<EstimateBox
  client:load
  variable="Geoffrey Hinton's Risk Assessment"
  description="Based on interviews and public statements since 2023"
  unit=""
  estimates={[
    { source: "P(AI extinction)", value: "10% in next 5-20 years", date: "2023", notes: "Stated probability of AI wiping out humanity" },
    { source: "Timeline to human-level AI", value: "5-20 years", date: "2023", notes: "Shorter than he previously thought" },
    { source: "Urgency", value: "Extremely high", date: "2024", notes: "Believes we're moving too fast" }
  ]}
/>

### Core Concerns

1. **Exceeding human intelligence**: AI might already understand language better than humans in some ways
2. **Loss of control**: Systems may become difficult or impossible to control
3. **Malicious use**: Bad actors could use AI for harm
4. **Autonomous weapons**: AI-powered weapons are particularly dangerous
5. **Rapid capability growth**: Progress is faster than safety measures
6. **Job displacement**: Economic disruption and social instability

### Unique Perspective

Hinton's concerns carry weight because:
- He created the technology (can't be dismissed as not understanding AI)
- Changed his mind based on evidence (not ideological)
- Has nothing to gain personally (retired, already acclaimed)
- Speaks from technical expertise
- Willing to express uncertainty

### Key Statements

**On leaving Google**: "I want to talk about AI safety issues without having to worry about how it interacts with Google's business."

**On existential risk**: "It's not inconceivable that humanity is just a passing phase in the evolution of intelligence."

**On timelines**: "I thought it was 30 to 50 years before we'd have general-purpose AI. And now I think it may be 20 years or less."

## Advocacy and Public Engagement

### Media Campaign

Since leaving Google, Hinton has:
- Given countless interviews (NYT, BBC, CBS, etc.)
- Appeared on major podcasts and shows
- Written op-eds about AI risks
- Testified to lawmakers
- Become one of most recognizable voices on AI safety

### Policy Engagement

- Met with government officials in multiple countries
- Advocated for AI regulation
- Supported international coordination efforts
- Called for moratorium on certain AI developments
- Signed statement on AI extinction risk

### Key Messages

Hinton emphasizes:
1. **We don't understand these systems**: Even their creators can't fully explain them
2. **Moving too fast**: Need to slow down and figure out safety
3. **International cooperation needed**: Not just company-level, but government coordination
4. **Both near and long-term risks**: Job displacement AND existential risk matter
5. **Uncertainty**: Admits we don't know how things will unfold

## What Makes Hinton Different

### From Most AI Researchers
- More willing to say "we don't know"
- More concerned about near-term risks
- More supportive of regulation
- Left industry job to speak freely

### From Some Safety Researchers
- Doesn't claim to have solutions
- More focused on raising alarm than proposing fixes
- Emphasizes uncertainty more
- More concerned about misuse alongside alignment

### His Credibility
- Can't be dismissed as anti-AI or not understanding technology
- Changed views based on evidence
- No financial incentive to exaggerate risks
- Speaks from 40+ years of AI research

## Current Focus

Hinton is now focused on:

1. **Public awareness**: Educating people about AI risks
2. **Policy advocacy**: Encouraging governmental action
3. **Supporting safety research**: Though not doing technical work himself
4. **International dialogue**: Promoting cooperation on AI safety
5. **Honest communication**: Expressing uncertainty and genuine concern

## Specific Risk Concerns

### Near-term Worries
- Disinformation and manipulation
- Job displacement and economic disruption
- Autonomous weapons
- Cybersecurity threats
- Concentration of power

### Long-term Concerns
- AI systems pursuing goals misaligned with humanity
- Inability to control superintelligent systems
- Existential catastrophe
- AI being used for totalitarian control

### On Solutions

Hinton is notably uncertain about solutions:
- Admits he doesn't know how to solve alignment
- Skeptical of simple fixes
- Emphasizes need for more research
- Supports regulatory approaches but unsure of specifics
- Honest about not having answers

## Impact of His Advocacy

### Raising Awareness
- Brought AI risks to mainstream attention
- Made safety concerns more credible
- Influenced public opinion
- Pressured companies to take safety seriously

### Policy Influence
- His warnings cited in policy discussions
- Influenced regulatory proposals
- Added weight to calls for AI governance
- Helped shift discourse from "whether" to "how" to regulate

### Academic and Research Community
- Made it more acceptable to voice concerns
- Encouraged other senior researchers to speak up
- Brought more attention to safety research
- Influenced research priorities

## Criticism and Debates

**Some critics argue:**
- He's being alarmist
- Don't understand why changed mind so suddenly
- Might be overestimating risks

**Hinton's response:**
- Evidence changed, so views changed
- Better to be cautious with existential stakes
- Genuinely uncertain, but concerned enough to speak up

**From capabilities side:**
- Some think he's harming AI progress

**From safety side:**
- Some wish he'd be more specific about solutions
- Questions about how much he's engaged with technical safety work

**Hinton's position:**
- Not claiming to have solutions
- Trying to raise alarm so others work on it
- Believes slowing down is prudent

## Key Publications and Talks

### Research Publications
- **"Learning representations by back-propagating errors"** (1986) - Foundational paper
- **"ImageNet Classification with Deep Convolutional Neural Networks"** (2012) - AlexNet
- **Hundreds of papers on neural networks and deep learning**

### Recent Talks on AI Safety
- **CBS 60 Minutes interview** (2023) - Widely viewed discussion of risks
- **MIT Technology Review interview** (2023) - On leaving Google
- **Various podcasts and media appearances** discussing AI safety

<Section title="Related Topics">
  <Tags tags={[
    "Deep Learning",
    "AI Safety",
    "Existential Risk",
    "Neural Networks",
    "Backpropagation",
    "AI Regulation",
    "Autonomous Weapons",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="yoshua-bengio"
      category="researcher"
      title="Yoshua Bengio"
      description="Fellow Turing Award winner also concerned about AI safety"
    />
    <EntityCard
      id="deepmind"
      category="lab"
      title="DeepMind"
      description="Major AI lab (part of Google) doing safety research"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "Geoffrey Hinton's Homepage", url: "https://www.cs.toronto.edu/~hinton/" },
  { title: "CBS 60 Minutes Interview", url: "https://www.cbsnews.com/news/geoffrey-hinton-ai-dangers-60-minutes-transcript/" },
  { title: "NYT: 'Godfather of AI' Quits Google", url: "https://www.nytimes.com/2023/05/01/technology/ai-google-chatbot-engineer-quits-hinton.html" },
  { title: "Google Scholar Profile", url: "https://scholar.google.com/citations?user=JicYPdAAAAAJ" },
]} />

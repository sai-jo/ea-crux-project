---
title: "Holden Karnofsky"
description: "Co-CEO of Open Philanthropy who has directed $300M+ toward AI safety, shaped EA prioritization, and developed influential frameworks like the \"Most Important Century\" thesis for transformative AI"
sidebar:
  order: 10
quality: 78
llmSummary: "Holden Karnofsky has directed $300M+ in AI safety funding through Open Philanthropy, including a $580M investment in Anthropic, growing the field from ~20 to ~400 FTE researchers. His 'Most Important Century' thesis and bio anchors framework estimate 15% probability of transformative AI by 2036, 50% by 2060, shaping EA prioritization toward AI risk."
lastEdited: "2025-12-24"
importance: 25
---
import {DataInfoBox, EstimateBox, Backlinks, R} from '../../../../components/wiki';

<DataInfoBox entityId="holden-karnofsky" />

## Overview

Holden Karnofsky is co-CEO of <R id="dd0cf0ff290cc68e">Open Philanthropy</R>, the most influential grantmaker in AI safety and existential risk. Through Open Phil, he has directed over \$100 million toward AI safety research and governance, fundamentally transforming it from a fringe academic interest into a well-funded field with hundreds of researchers.

His strategic thinking has shaped how the effective altruism community prioritizes AI risk through frameworks like the <R id="1a20dfc897a0933a">"Most Important Century"</R> thesis. This argues we may live in the century that determines humanity's entire future trajectory due to transformative AI development.

| Funding Achievement | Amount | Impact |
|-------------------|---------|---------|
| Total AI safety grants | \$300M+ | Enabled field growth from ~dozens to hundreds of researchers |
| Anthropic investment | \$580M+ | Created major safety-focused AI lab |
| Field building grants | \$50M+ | Established academic programs and research infrastructure |

## Risk Assessment

| Risk Category | Karnofsky's Assessment | Evidence | Timeline |
|---------------|----------------------|----------|----------|
| Transformative AI | ~15% by 2036, ~50% by 2060 | <R id="62d29d310d596d2a">Bio anchors framework</R> | This century |
| Existential importance | "Most important century" | AI could permanently shape humanity's trajectory | 2021-2100 |
| Tractability | High enough for top priority | Open Phil's largest focus area allocation | Current |
| Funding adequacy | Severely underfunded | Still seeking to grow field substantially | Ongoing |

## Career Evolution and Major Achievements

### Early Career (2007-2014): Building Effective Altruism

| Period | Role | Key Achievements |
|--------|------|------------------|
| 2007-2011 | Co-founder, <R id="9315689a12534405">GiveWell</R> | Pioneered rigorous charity evaluation methodology |
| 2011-2014 | Launch Open Philanthropy | Expanded beyond global health to cause prioritization |
| 2012-2014 | EA movement building | Helped establish effective altruism as global movement |

### Transition to AI Focus (2014-2018)

**Initial AI engagement:**
- 2014: First significant AI safety grants through Open Phil
- 2016: Major funding to [Center for Human-Compatible AI (CHAI)](/knowledge-base/organizations/safety-orgs/chai/)
- 2017: Early OpenAI funding (before pivot to for-profit)
- 2018: Increased conviction leading to AI as top priority

### AI Safety Leadership (2018-Present)

**Major funding decisions:**
- 2021: <R id="27ce8f3b89dcdaa1">\$580M investment in Anthropic</R> to create safety-focused lab
- 2022: Establishment of <R id="5714a008527a379a">AI safety university programs</R>
- 2023: Expanded governance funding addressing [AI regulation](/knowledge-base/responses/governance/legislation/)

## Strategic Frameworks and Intellectual Contributions

### The "Most Important Century" Thesis

**Core argument structure:**

| Component | Claim | Implication |
|-----------|-------|-------------|
| Technology potential | Transformative AI possible this century | Could exceed agricultural/industrial revolution impacts |
| Speed differential | AI transition faster than historical precedents | Less time to adapt and coordinate |
| Leverage moment | Our actions now shape outcomes | Unlike past revolutions where individuals had little influence |
| Conclusion | This century uniquely important | Justifies enormous current investment |

**Supporting evidence:**
- <R id="62d29d310d596d2a">Biological anchors methodology</R> for AI timelines
- Historical analysis of technological transitions
- Economic modeling of AI impact potential

### Bio Anchors Framework

Developed with <R id="63739057bf3d421b">Ajeya Cotra</R>, this framework estimates AI development timelines by comparing required computation to biological systems:

| Anchor Type | Computation Estimate | Timeline Implication |
|-------------|---------------------|----------------------|
| Human brain | ~10^15 FLOP/s | Medium-term (2030s-2040s) |
| Human lifetime | ~10^24 FLOP | Longer-term (2040s-2050s) |
| Evolution | ~10^41 FLOP | Much longer-term if needed |

## Open Philanthropy Funding Strategy

### Portfolio Approach

| Research Area | Funding Focus | Key Recipients | Rationale |
|---------------|---------------|----------------|-----------|
| Technical alignment | \$100M+ | [Anthropic](/knowledge-base/organizations/labs/anthropic/), [Redwood Research](/knowledge-base/organizations/safety-orgs/redwood/) | Direct work on making AI systems safer |
| AI governance | \$80M+ | <R id="f0d95954b449240a">Center for Security and Emerging Technology</R>, policy fellowships | Institutional responses to AI development |
| Field building | \$50M+ | University programs, individual researchers | Growing research community |
| Compute governance | \$20M+ | [Compute monitoring research](/knowledge-base/responses/governance/compute-governance/monitoring/) | Oversight of AI development resources |

### Grantmaking Philosophy

**Key principles:**
- **Hits-based giving**: Expect most grants to have limited impact, few to be transformative
- **Long time horizons**: Patient capital for 5-10 year research projects  
- **Active partnership**: Strategic guidance beyond just funding
- **Portfolio diversification**: Multiple approaches given uncertainty

**Notable funding decisions:**
- <R id="27ce8f3b89dcdaa1">Anthropic investment</R>: \$580M to create safety-focused competitor to OpenAI
- [MIRI funding](/knowledge-base/organizations/safety-orgs/miri/): Early support for foundational AI alignment research
- Policy fellowships: Placing AI safety researchers in government positions

## Current Views and Assessment

<EstimateBox
  client:load
  variable="Karnofsky's AI Risk Timeline"
  description="Based on public statements and Open Phil priorities 2023-2024"
  unit=""
  estimates={[
    { source: "Transformative AI", value: "15% by 2036, 50% by 2060", date: "2022", notes: "Bio anchors central estimate" },
    { source: "Field adequacy", value: "Still severely underfunded", date: "2024", notes: "Continued aggressive hiring and grantmaking" },
    { source: "Policy urgency", value: "High priority", date: "2024", notes: "Increased governance focus" }
  ]}
/>

### Evolution of Views (2020-2024)

| Year | Key Update | Reasoning |
|------|------------|-----------|
| 2021 | "Most Important Century" series | Crystallized long-term strategic thinking |
| 2022 | Increased policy focus | Recognition of need for governance alongside technical work |
| 2023 | Anthropic model success | Validation of safety-focused lab approach |
| 2024 | Accelerated timelines concern | <R id="b80187df7777104d">Shorter timelines than bio anchors suggested</R> |

## Influence on AI Safety Field

### Field Growth Metrics

| Metric | 2015 | 2024 | Growth Factor |
|--------|------|------|---------------|
| FTE researchers | ~20 | ~400 | 20x |
| Annual funding | &lt;\$5M | >\$200M | 40x |
| University programs | 0 | 15+ | New category |
| Major organizations | 2-3 | 20+ | 7x |

### Institutional Impact

**Academic legitimacy:**
- Funding enabled <R id="a9815b0be81c47c0">AI safety courses</R> at major universities
- Supported tenure-track positions focused on alignment research  
- Created pathway for traditional CS researchers to enter field

**Policy influence:**
- Funded experts now advising [US AI Safety Institute](/knowledge-base/organizations/government/us-aisi/)
- Supported research informing <R id="1ad6dc89cded8b0c">EU AI Act</R>
- Built relationships between AI safety community and policymakers

## Key Uncertainties and Strategic Cruxes

### Open Questions in Karnofsky's Framework

| Uncertainty | Stakes | Current Evidence |
|-------------|--------|------------------|
| AI timeline accuracy | Entire strategy timing | Mixed signals from recent capabilities |
| Technical tractability | Funding allocation efficiency | Early positive results but limited validation |
| Governance effectiveness | Policy investment value | Unclear institutional responsiveness |
| Anthropic success | Large investment justification | Strong early results but long-term unknown |

### Strategic Disagreements

**Within EA community:**
- Some argue for [longtermist focus beyond AI](/knowledge-base/worldviews/long-timelines/)
- Others prefer <R id="9315689a12534405">global health and development</R> emphasis
- Debate over concentration vs. diversification of funding

**With AI safety researchers:**
- Tension between [technical alignment focus](/knowledge-base/debates/formal-arguments/why-alignment-hard/) and governance approaches
- Disagreement over [open vs. closed development](/knowledge-base/debates/open-vs-closed/) funding
- Questions about emphasis on [capabilities research](/knowledge-base/capabilities/) safety benefits

## Public Communication and Influence

### Cold Takes Blog Impact

**Most influential posts:**
- <R id="1a20dfc897a0933a">"The Most Important Century"</R> series (>100k views)
- <R id="b80187df7777104d">"AI Timelines: Where the Arguments Stand"</R> (policy reference)
- <R id="62d29d310d596d2a">"Bio Anchors" explanation</R> (research methodology)

**Communication approach:**
- Transparent reasoning and uncertainty acknowledgment
- Accessible explanations of complex topics
- Regular updates as views evolve
- Direct engagement with critics and alternative viewpoints

### Media and Policy Engagement

| Platform | Reach | Impact |
|----------|-------|--------|
| Congressional testimony | Direct policy influence | Informed [AI regulation debate](/knowledge-base/debates/regulation-debate/) |
| Academic conferences | Research community | Shaped university AI safety programs |
| EA Global talks | Movement direction | Influenced thousands of career decisions |
| Podcast interviews | Public understanding | Mainstream exposure for AI safety ideas |

## Current Priorities and Future Direction

### 2024-2026 Strategic Focus

**Immediate priorities:**
1. **Anthropic scaling**: Supporting responsible development of powerful systems
2. **Governance acceleration**: Policy research and implementation support
3. **Technical diversification**: Funding multiple alignment research approaches
4. **International coordination**: Supporting global AI safety cooperation

**Emerging areas:**
- [Compute governance](/knowledge-base/responses/governance/compute-governance/) infrastructure
- [AI evaluation](/knowledge-base/responses/evaluation/) methodologies  
- [Corporate AI safety](/knowledge-base/responses/corporate/) practices
- [Prediction market](/knowledge-base/responses/epistemic-tools/prediction-markets/) applications

### Long-term Vision

**Field development goals:**
- Self-sustaining research ecosystem independent of Open Phil
- Government funding matching or exceeding philanthropic support
- Integration of safety research into mainstream AI development
- International coordination mechanisms for AI governance

## Critiques and Responses

### Common Criticisms

| Criticism | Karnofsky's Response | Counter-evidence |
|-----------|---------------------|------------------|
| Over-concentration of power | Funding diversification, transparency | Multiple other major funders emerging |
| Field capture risk | Portfolio approach, external evaluation | Continued criticism tolerated and addressed |
| Timeline overconfidence | Explicit uncertainty, range estimates | Regular updating based on new evidence |
| Governance skepticism | Measured expectations, multiple approaches | Early policy wins demonstrate tractability |

### Ongoing Debates

**Resource allocation:**
- Should Open Phil fund more basic research vs. applied safety work?
- Optimal balance between technical and governance approaches?
- Geographic distribution of funding (US-centric concerns)

**Strategic approach:**
- Speed vs. care in scaling funding
- Competition vs. cooperation with AI labs
- Public advocacy vs. behind-the-scenes influence

## Sources & Resources

### Primary Sources

| Type | Source | Description |
|------|--------|-------------|
| Blog | <R id="859ff786a553505f">Cold Takes</R> | Karnofsky's strategic thinking and analysis |
| Organization | <R id="dd0cf0ff290cc68e">Open Philanthropy</R> | Grant database and reasoning |
| Research | <R id="f43f63419dbf2e6e">Bio Anchors Report</R> | Technical forecasting methodology |
| Testimony | <R id="52748445fab0e8cc">Congressional Hearing</R> | Policy positions and recommendations |

### Secondary Analysis

| Type | Source | Focus |
|------|--------|-------|
| Academic | <R id="bff2f5843023e85e">EA Research</R> | Critical analysis of funding decisions |
| Journalistic | <R id="21a4a585cdbf7dd3">MIT Technology Review</R> | External perspective on influence |
| Policy | <R id="0a17f30e99091ebf">RAND Corporation</R> | Government research on philanthropic AI funding |

### Related Profiles

- [Dario Amodei](/knowledge-base/people/dario-amodei/) - CEO of Anthropic, major funding recipient
- [Paul Christiano](/knowledge-base/people/paul-christiano/) - Technical alignment researcher, influenced Karnofsky's views  
- [Nick Bostrom](/knowledge-base/people/nick-bostrom/) - Author of "Superintelligence," early influence on Open Phil AI focus
- [Eliezer Yudkowsky](/knowledge-base/people/eliezer-yudkowsky/) - MIRI founder, recipient of early Open Phil AI safety grants

<Backlinks client:load entityId="holden-karnofsky" />
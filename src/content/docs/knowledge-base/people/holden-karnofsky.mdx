---
title: Holden Karnofsky
description: Co-CEO of Open Philanthropy, influential AI risk grantmaker
sidebar:
  order: 10
---

import {DataInfoBox, EstimateBox, Backlinks, PageStatus} from '../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Co-CEO of Open Philanthropy who directed hundreds of millions of dollars to AI safety research, developed the 'Most Important Century' framework, and fundamentally shaped the AI safety field through strategic grantmaking and the bio anchors timeline methodology." />

<DataInfoBox entityId="holden-karnofsky" />

## Background

Holden Karnofsky is co-CEO of Open Philanthropy, one of the largest funders of AI safety research and the most influential grantmaker in the existential risk space. Through Open Phil, he has directed hundreds of millions of dollars toward AI safety, fundamentally shaping the field's development.

Career trajectory:
- Co-founded GiveWell (charity evaluator) in 2007 with Elie Hassenfeld
- Helped create effective altruism movement
- Launched Open Philanthropy Project in 2011 (became Open Philanthropy)
- Shifted significant resources toward AI risk starting ~2014
- Currently co-CEO alongside Alexander Berger

His influence comes not from technical research but from:
- Identifying and funding crucial work
- Strategic thinking about AI risk
- Building infrastructure for the field
- Public writing on AI timelines and strategy

## Major Contributions

### Building AI Safety Funding Infrastructure

Before Karnofsky's work, AI safety had minimal funding. Open Phil has:
- Granted $300M+ to AI safety and governance
- Funded key organizations (MIRI, CHAI, OpenAI initially, etc.)
- Supported individual researchers
- Enabled field growth from ~dozens to hundreds of researchers

**Key funded organizations:**
- Anthropic (major investor)
- Redwood Research
- Center for AI Safety
- AI Impacts
- Many individual researchers and smaller projects

### Strategic Framework for AI Risk

Karnofsky developed influential frameworks for thinking about AI risk:

**The "Most Important Century" series**: Argues we may live in the most important century in human history due to transformative AI, and what we should do about it.

**Bio Anchors framework**: Methodology for estimating AI timelines based on biological computation, developed by Ajeya Cotra under Karnofsky's direction.

### Shifting EA Focus to AI

Karnofsky was instrumental in:
- Convincing EA community to prioritize AI risk
- Providing intellectual framework for why AI matters most
- Funding infrastructure to work on AI safety
- Demonstrating AI safety as tractable cause area

## Views on AI Risk

<EstimateBox
  client:load
  variable="Holden Karnofsky's Risk Assessment"
  description="Based on public writings and Open Phil's priorities"
  unit=""
  estimates={[
    { source: "Transformative AI timeline", value: "~15% by 2036, ~50% by 2060", date: "2021", notes: "Based on bio anchors framework" },
    { source: "Century importance", value: "Most important century claim", date: "2021", notes: "This century could be pivotal for all future" },
    { source: "Risk level", value: "Substantial enough to be top priority", date: "2023", notes: "AI safety is Open Phil's largest focus area" }
  ]}
/>

### Core Beliefs

1. **This could be the most important century**: Transformative AI might arrive this century, permanently shaping humanity's trajectory
2. **AI safety is tractable**: We can meaningfully reduce risks through research and policy
3. **Broad portfolio approach**: Fund many approaches since we're uncertain what will work
4. **Both technical and governance**: Need solutions on multiple fronts
5. **Urgency with uncertainty**: Act now despite uncertainty about timelines

### Strategic Thinking

Karnofsky emphasizes:
- **Expected value reasoning**: Worth massive investment even with uncertainty
- **Field building**: Creating ecosystem of researchers, organizations, funders
- **Multiple pathways**: Not putting all eggs in one basket
- **Long-term perspective**: Thinking decades ahead
- **Pragmatic approach**: Work with governments and companies, not just idealistic plans

## The "Most Important Century" Framework

### Core Argument

1. **Transformative AI is plausible this century**: Based on various lines of evidence
2. **TAI would be as significant as agriculture or industrial revolution**: Permanent change to human condition
3. **Earlier revolutions took centuries to unfold**: We had time to adapt
4. **TAI could happen much faster**: Potentially years or decades
5. **Our actions now could determine outcomes**: Unlike with past revolutions
6. **This makes our century potentially the most important ever**: For all of human future

### Implications

If argument is correct:
- Working on AI safety among most valuable things to do
- Small increases in success probability are worth enormous investment
- Need to act urgently but thoughtfully
- Both research and governance crucial
- Coordination essential

## Funding Strategy

### Portfolio Approach

Open Phil funds diverse work:

**Technical research:**
- Alignment research (scalable oversight, interpretability, etc.)
- AI safety fundamentals
- Empirical research at labs

**Governance and policy:**
- Think tanks and policy organizations
- Academic research on AI governance
- International coordination efforts

**Field building:**
- Scholarships and fellowships
- University programs
- Conferences and convenings

**Specific projects:**
- Anthropic (major investment to create safety-focused lab)
- Individual researchers
- Infrastructure (compute for safety research, etc.)

### Grantmaking Philosophy

Karnofsky's approach:
- **High-risk, high-reward**: Willing to fund speculative work
- **Long time horizons**: Patient capital for research
- **Active involvement**: Not just check-writing, but strategic guidance
- **Learning by doing**: Adjust based on what works
- **Hits-based giving**: Expect most grants to fail, some to be transformative

## Public Writing and Communication

### Cold Takes Blog

Karnofsky's blog "Cold Takes" features detailed thinking on:
- AI timelines and strategic implications
- How to think about transformative technologies
- Effective altruism philosophy
- Career advice for people wanting to work on important problems

**Notable series:**
- "Most Important Century" series
- "AI Timelines" series
- Posts on bio anchors methodology

### Influence Through Writing

His writing:
- Shaped how EA community thinks about AI
- Influenced researchers' career choices
- Provided frameworks adopted widely
- Made complex topics accessible

## Evolution of Views

**Early GiveWell (2007-2014):**
- Focused on global health and poverty
- Rigorous charity evaluation
- Building EA movement

**Transition to AI (2014-2018):**
- Increasing conviction about AI importance
- Initial AI safety grants
- Developing strategic framework

**All-in on AI (2018-present):**
- AI safety as top priority
- Massive increase in AI funding
- "Most Important Century" thesis
- Major investments (Anthropic, etc.)

## Relationship to Other Thinkers

### Influences On

Karnofsky influenced many others' thinking:
- Convinced EA community to prioritize AI
- Shaped research directions through funding
- Mentored researchers and grantmakers
- Created frameworks others adopted

### Influenced By

Drew on work by:
- Nick Bostrom ("Superintelligence")
- Paul Christiano (alignment research)
- Eliezer Yudkowsky (early AI safety)
- Ajeya Cotra (bio anchors)

### Unique Position

Different from most in the field:
- Not a technical researcher
- Focused on strategy and funding
- Brings outside perspective
- Emphasis on hits-based approach

## Impact on the Field

### Field Growth
- AI safety grew from fringe to established field
- Hundreds of researchers now funded
- Major universities have programs
- Multiple well-funded organizations

### Legitimacy
- Open Phil's involvement brought credibility
- Attracted other funders
- Made it acceptable career path
- Enabled professionalization

### Strategic Direction
- Influenced what gets researched
- Shaped discourse through funding priorities
- Connected different communities
- Enabled coordination

## Current Priorities

At Open Phil, Karnofsky focuses on:

1. **AI safety and governance**: Still top priority
2. **Supporting Anthropic**: Major ongoing investment
3. **Field building**: Growing research community
4. **Policy work**: Governance and regulation
5. **Global catastrophic risks**: Broader x-risk portfolio

## Criticism and Debates

**Some critics argue:**
- Too much power in one funder's hands
- Could be creating field in his image
- Might be overestimating AI risk
- Funding priorities might be suboptimal

**Supporters counter:**
- Open Phil has track record of good judgment
- Willing to fund diverse approaches
- Transparent about reasoning
- Enabled crucial work that wouldn't exist otherwise

**Karnofsky's approach:**
- Transparent about reasoning through public writing
- Actively seeks criticism and alternative views
- Funds wide portfolio to hedge bets
- Acknowledges uncertainty

## Key Writings

- **"Most Important Century" series** - Core strategic thesis
- **"AI Timelines: Where the Arguments, and the 'Experts,' Stand"** - Overview of timeline estimates
- **Various posts on bio anchors** - Methodology for forecasting AI
- **Career choice posts** - Advice for people wanting to help with AI safety

## Related Pages

<Backlinks client:load entityId="holden-karnofsky" />

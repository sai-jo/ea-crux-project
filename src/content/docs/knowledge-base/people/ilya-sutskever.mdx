---
title: Ilya Sutskever
description: Co-founder of Safe Superintelligence Inc., formerly Chief Scientist at OpenAI
sidebar:
  order: 11
---

import {DataInfoBox, EstimateBox, Backlinks, PageStatus} from '../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Co-creator of AlexNet and former Chief Scientist at OpenAI who co-led the Superalignment team before leaving to found Safe Superintelligence Inc., dedicated purely to building safe superintelligence without commercial distractions." />

<DataInfoBox entityId="ilya-sutskever" />

## Background

Ilya Sutskever is one of the most accomplished AI researchers of his generation, having made fundamental contributions to deep learning before pivoting entirely to superintelligence safety. He is a student of Geoffrey Hinton and was Chief Scientist at OpenAI for nearly a decade before founding Safe Superintelligence Inc. (SSI) in 2024.

Academic and research background:
- PhD from University of Toronto under Geoffrey Hinton (2013)
- Co-author of AlexNet (2012) - paper that launched deep learning revolution
- Research scientist at Google Brain
- Co-founder and Chief Scientist of OpenAI (2015-2024)
- Co-founder of Safe Superintelligence Inc. (2024)

His journey from capabilities researcher to safety-focused founder represents one of the most significant shifts in AI research.

## Major Technical Contributions

### AlexNet (2012)
With Alex Krizhevsky and Geoffrey Hinton, created the convolutional neural network that won ImageNet 2012, launching the modern deep learning era. This single paper transformed AI research.

### Sequence-to-Sequence Learning
Co-developed sequence-to-sequence models with attention, foundational for modern NLP and language models.

### At OpenAI (2015-2024)

Led research that produced:
- GPT series of language models
- DALL-E (image generation)
- Foundational work on scaling laws
- Research on emergence in large models

As Chief Scientist, guided OpenAI's research direction toward increasingly powerful generative models.

## The Shift to Safety

### Timeline of Evolution

**Early OpenAI (2015-2019):**
- Focused on building AGI safely
- Led capabilities research
- Believed alignment would be solvable alongside capabilities

**Growing Concern (2020-2022):**
- Increasingly worried about alignment difficulty
- Private concerns about pace of development
- Pushing internally for more safety focus

**Superalignment (2023):**
- Co-led Superalignment team with Jan Leike
- Secured 20% of compute for alignment research
- More explicit about safety prioritization

**OpenAI Departure and SSI (2024):**
- Left OpenAI in May 2024
- Founded Safe Superintelligence Inc. with singular focus
- Explicitly prioritizing safety over commercialization

### The OpenAI Board Incident (November 2023)

Sutskever was central to the brief removal of Sam Altman as OpenAI CEO:
- Voted to remove Altman citing safety concerns
- Later expressed regret and signed letter supporting Altman's return
- Incident revealed deep tensions about safety vs. commercialization

This episode highlighted his internal struggle between capability advancement and safety.

## Safe Superintelligence Inc. (SSI)

### Mission

Founded June 2024 with Daniel Gross and Daniel Levy, SSI's stated mission:
- Build safe superintelligence as first priority
- Safety and capabilities advanced together
- No distraction from commercial pressures
- Insulated from short-term incentives

### Approach

**SSI's philosophy:**
1. Safety cannot be an afterthought
2. Need revolutionary engineering and scientific breakthroughs
3. Pure focus without commercial distraction
4. Long time horizons
5. Build once, build right

### Why Leave OpenAI

While not stated explicitly, timing and circumstances suggest:
- Disagreement with OpenAI's commercialization
- Concern about safety being deprioritized
- Desire for environment purely focused on safe superintelligence
- Jan Leike's similar departure reinforced decision

## Views on AI Safety

<EstimateBox
  client:load
  variable="Ilya Sutskever's Priorities"
  description="Based on SSI founding and public statements"
  unit=""
  estimates={[
    { source: "AGI timeline", value: "Near-term enough to be urgent", date: "2024", notes: "Founded company specifically for superintelligence" },
    { source: "Safety priority", value: "Absolute priority", date: "2024", notes: "Left OpenAI to focus purely on safety" },
    { source: "Technical approach", value: "Revolutionary breakthroughs needed", date: "2024", notes: "Stated in SSI announcement" }
  ]}
/>

### Core Beliefs

1. **Superintelligence is coming**: Soon enough that dedicated effort is urgent
2. **Safety must come first**: Cannot be solved after the fact
3. **Current approaches insufficient**: Need fundamental breakthroughs
4. **Commercial pressure is harmful**: Distraction from true goal
5. **Both capabilities and safety require work**: Cannot ignore either

### Strategic Position

Sutskever's approach is unique:
- Not slowing down capabilities research
- Not racing without safety
- Building both together from scratch
- Long time horizon despite urgency
- Focused on one goal only

## Technical Perspective on Safety

### What Makes Him Different

Sutskever brings deep technical understanding:
- Built the systems everyone is worried about
- Understands capabilities trajectory firsthand
- Knows what future systems might be capable of
- Can assess technical proposals realistically

### His Likely Concerns

Based on his background and decisions:
- **Deceptive alignment**: Sufficiently capable systems hiding true objectives
- **Rapid capability jumps**: Having seen emergent capabilities, knows they can surprise
- **Inadequate oversight**: Human supervision may not scale to superintelligence
- **Inner alignment**: Ensuring learned objectives match intended objectives
- **Deployment pressure**: Commercial incentives pushing unsafe deployment

### Research Direction

While SSI hasn't published yet (as of late 2024), likely focuses:
- Interpretability at scale
- Robust alignment techniques
- Scalable oversight methods
- Testing alignment properties before deployment
- Fundamental theoretical work

## Influence and Impact

### Technical Legacy
- Helped create modern deep learning
- GPT series enabled current AI capabilities
- Demonstrated what's possible with scale

### Strategic Influence
- OpenAI board incident brought safety concerns to public attention
- Departure from OpenAI highlighted safety vs. commercialization tension
- SSI founding demonstrates viable alternative model

### Field Building
- Trained researchers at OpenAI
- Demonstrated you can prioritize safety without abandoning capabilities
- Created template for safety-first organization

## Public Communication

Sutskever is notably private:
- Rarely gives interviews
- Minimal social media presence
- Actions speak louder than words
- Technical papers rather than blog posts

**Key public statements:**
- SSI founding announcement (June 2024)
- Occasional technical talks
- OpenAI board letter and retraction

His reticence makes his actions (leaving OpenAI, founding SSI) more significant.

## Current Focus at SSI

SSI's approach (based on public statements):

1. **Straight shot to safe superintelligence**: No detours
2. **Revolutionary breakthroughs**: In both safety and capabilities
3. **Insulated development**: Free from commercial pressure
4. **World-class team**: Recruiting top researchers
5. **Patient approach**: Right timeline, not fast timeline

## Comparison to Others

### vs. Anthropic
- **Similar**: Safety-focused, willing to build capabilities
- **Different**: SSI even more focused (no products, no distractions)

### vs. DeepMind
- **Similar**: Large-scale technical research
- **Different**: SSI is only about superintelligence safety

### vs. Pure Safety Orgs (MIRI, ARC)
- **Similar**: Safety prioritized
- **Different**: SSI building systems, not just theorizing

## Significance of His Shift

Sutskever's evolution is important because:

1. **Credibility**: Can't be dismissed as not understanding AI
2. **Inside view**: Saw OpenAI from within, still left for safety
3. **Technical depth**: Knows exactly what's possible
4. **Resources**: Can attract top talent and funding
5. **Template**: Shows safety-first approach is viable

## Key Questions About SSI

**Unanswered questions:**
- Will truly avoid commercial pressures long-term?
- Can make progress without publishing?
- Is building superintelligence to solve safety the right approach?
- How will they know if they've succeeded?
- What if they get there first but haven't solved safety?

These questions matter enormously given stakes.

## Related Pages

<Backlinks client:load entityId="ilya-sutskever" />

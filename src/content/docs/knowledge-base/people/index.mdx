---
title: Key Researchers in AI Safety
description: Notable researchers and thought leaders shaping the AI safety and alignment field
sidebar:
  order: 0
---

import { EntityCards, EntityCard, Section } from '../../../../components/wiki';

This section profiles key researchers, thought leaders, and practitioners who have made significant contributions to AI safety, alignment, and governance. Their work spans technical research, strategy, policy, and institution-building.

## Understanding Different Perspectives

The AI safety field encompasses a wide range of views on critical questions:

- **Timeline estimates**: When will transformative AI arrive? (Range: 5-50+ years)
- **Alignment difficulty**: How hard is the technical problem? (Views range from "likely solvable with current approaches" to "extremely difficult, likely requires fundamental breakthroughs")
- **P(doom)**: Probability of existential catastrophe from AI (Range: &lt;1% to &gt;50%)
- **Strategic approach**: Should we race to build safe AI, slow down development, focus on governance, or pursue fundamental research?

Understanding who believes what and why is crucial for navigating the field's key disagreements.

## Researchers by Primary Affiliation

<Section title="Frontier Labs (AGI-focused)">
  <EntityCards>
    <EntityCard
      id="dario-amodei"
      category="researcher"
      title="Dario Amodei"
      description="CEO of Anthropic, advocate for Constitutional AI and responsible scaling"
    />
    <EntityCard
      id="jan-leike"
      category="researcher"
      title="Jan Leike"
      description="Head of Alignment at Anthropic, previously led OpenAI's superalignment team"
    />
    <EntityCard
      id="chris-olah"
      category="researcher"
      title="Chris Olah"
      description="Co-founder of Anthropic, pioneer in neural network interpretability"
    />
    <EntityCard
      id="ilya-sutskever"
      category="researcher"
      title="Ilya Sutskever"
      description="Co-founder of Safe Superintelligence Inc., formerly Chief Scientist at OpenAI"
    />
  </EntityCards>
</Section>

<Section title="Safety Research Organizations">
  <EntityCards>
    <EntityCard
      id="paul-christiano"
      category="researcher"
      title="Paul Christiano"
      description="Founder of ARC, creator of iterated amplification and AI safety via debate"
    />
    <EntityCard
      id="eliezer-yudkowsky"
      category="researcher"
      title="Eliezer Yudkowsky"
      description="Co-founder of MIRI, advocate for strong AI safety precautions"
    />
    <EntityCard
      id="neel-nanda"
      category="researcher"
      title="Neel Nanda"
      description="DeepMind alignment researcher, mechanistic interpretability expert"
    />
    <EntityCard
      id="connor-leahy"
      category="researcher"
      title="Connor Leahy"
      description="CEO of Conjecture, focuses on interpretability and prosaic AGI safety"
    />
  </EntityCards>
</Section>

<Section title="Academic Researchers">
  <EntityCards>
    <EntityCard
      id="stuart-russell"
      category="researcher"
      title="Stuart Russell"
      description="UC Berkeley professor, CHAI founder, author of 'Human Compatible'"
    />
    <EntityCard
      id="yoshua-bengio"
      category="researcher"
      title="Yoshua Bengio"
      description="Turing Award winner, AI pioneer now focused on AI safety"
    />
    <EntityCard
      id="geoffrey-hinton"
      category="researcher"
      title="Geoffrey Hinton"
      description="Turing Award winner, 'Godfather of AI', vocal about AI risks"
    />
    <EntityCard
      id="dan-hendrycks"
      category="researcher"
      title="Dan Hendrycks"
      description="Director of CAIS, focuses on catastrophic AI risk reduction"
    />
  </EntityCards>
</Section>

<Section title="Strategy and Governance">
  <EntityCards>
    <EntityCard
      id="nick-bostrom"
      category="researcher"
      title="Nick Bostrom"
      description="Philosopher at FHI, author of 'Superintelligence'"
    />
    <EntityCard
      id="toby-ord"
      category="researcher"
      title="Toby Ord"
      description="Philosopher at FHI, author of 'The Precipice'"
    />
    <EntityCard
      id="holden-karnofsky"
      category="researcher"
      title="Holden Karnofsky"
      description="Co-CEO of Open Philanthropy, influential AI risk grantmaker"
    />
  </EntityCards>
</Section>

## Key Areas of Disagreement

Different researchers hold varying positions on crucial questions:

### Timeline Optimism vs. Pessimism

- **Shorter timelines** (AGI by 2030s): Dario Amodei, Ilya Sutskever, many at frontier labs
- **Medium timelines** (AGI by 2040s-2050s): Paul Christiano, Holden Karnofsky
- **Longer/uncertain timelines**: Some academics, governance-focused researchers

### Alignment Difficulty

- **More optimistic** (solvable with iteration): Dario Amodei, Jan Leike, Paul Christiano
- **Moderately pessimistic** (requires major breakthroughs): Stuart Russell, Yoshua Bengio
- **Very pessimistic** (extremely difficult, may not be solvable in time): Eliezer Yudkowsky, some MIRI researchers

### Strategic Approach

- **Race to the top** (build safe AGI first): Anthropic, some OpenAI researchers
- **Slow down development**: Geoffrey Hinton, some policy advocates
- **Focus on fundamentals**: MIRI, some academic researchers
- **Governance and coordination**: FHI researchers, policy experts

## Contributing to the Field

These researchers represent various paths to contributing to AI safety:

- **Technical research**: Interpretability, alignment methods, adversarial robustness
- **Conceptual work**: Identifying new threat models, framing key problems
- **Empirical research**: Testing alignment techniques on current systems
- **Institution building**: Creating labs, research organizations, and governance bodies
- **Communication**: Writing for different audiences, building field awareness
- **Funding**: Directing resources to high-priority work

Understanding their different approaches helps newcomers identify where they might contribute most effectively.

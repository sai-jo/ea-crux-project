---
title: Jan Leike
description: Head of Alignment at Anthropic, formerly led OpenAI's superalignment team
sidebar:
  order: 4
---

import {DataInfoBox, EstimateBox, Backlinks, PageStatus} from '../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Head of Alignment at Anthropic and RLHF pioneer who co-led OpenAI's Superalignment team before leaving in 2024 over concerns about safety prioritization, focusing on scalable oversight for supervising superhuman AI systems." />

<DataInfoBox entityId="jan-leike" />

## Background

Jan Leike is a leading AI alignment researcher currently serving as Head of Alignment at Anthropic. He has a PhD from Australian National University, where he worked on AI safety under Marcus Hutter.

His career has been defined by practical, empirical approaches to alignment:
- Early work on safe exploration in reinforcement learning
- Pioneering research on learning from human feedback
- Leadership of alignment teams at DeepMind, OpenAI, and now Anthropic
- Focus on scalable methods that can work with current ML paradigms

## Career Trajectory

### DeepMind (2017-2021)
Worked on the first implementations of learning from human feedback, including:
- Safe exploration methods
- Reward modeling
- Scalable agent alignment

### OpenAI (2021-2024)
- Joined to lead alignment research
- Co-led the Superalignment team (announced July 2023)
- Secured 20% of OpenAI's compute for alignment research
- Departed May 2024 over disagreements about safety prioritization

### Anthropic (2024-present)
- Joined as Head of Alignment
- Reunited with former OpenAI colleagues
- Leading alignment research on Claude and future systems

## Key Contributions

### RLHF Pioneer

Jan was one of the early researchers to demonstrate that reinforcement learning from human feedback (RLHF) could work at scale:
- Co-authored seminal papers on reward learning
- Showed how to train language models to be helpful and harmless
- Methods he developed became standard across industry

### Scalable Oversight Research

Core focus on how to supervise AI systems more capable than humans:
- Recursive reward modeling
- AI-assisted human evaluation
- Process supervision vs. outcome supervision
- Weak-to-strong generalization

### Superalignment Vision

At OpenAI, co-led (with Ilya Sutskever) the Superalignment team, which aimed to:
- Solve alignment for superintelligent systems
- Use AI to help align even more capable AI
- Achieve this within four years
- Dedicate significant compute resources to the problem

## Views on Key Cruxes

<EstimateBox
  client:load
  variable="Jan Leike's Risk Assessment"
  description="Based on public statements and research priorities"
  unit=""
  estimates={[
    { source: "Alignment urgency", value: "Very high", date: "2024", notes: "Left OpenAI over concerns about insufficient safety prioritization" },
    { source: "Timeline pressure", value: "Next 3-5 years critical", date: "2024", notes: "Emphasized need to solve alignment soon" },
    { source: "Technical tractability", value: "Difficult but solvable", date: "2024", notes: "Optimistic about scalable oversight approaches" }
  ]}
/>

### Core Beliefs

1. **Alignment is urgent**: We have limited time to solve this before transformative AI
2. **Scalable oversight is key**: Central challenge is supervising superhuman AI
3. **Empirical work is essential**: Need to test alignment techniques on increasingly capable systems
4. **Safety must be prioritized**: Cannot let capability research consistently outpace safety
5. **Current methods are insufficient**: RLHF and similar techniques won't scale to superintelligence without major improvements

### Why He Left OpenAI

In May 2024, Jan departed OpenAI and posted on X (Twitter):
- "Building smarter-than-human machines is an inherently dangerous endeavor"
- "Over the past years, safety culture and processes have taken a backseat to shiny products"
- Expressed concern about compute and priority allocation for safety

This departure, along with Ilya Sutskever's, raised significant questions about OpenAI's commitment to safety research.

## Research Focus

### Current Priorities at Anthropic

1. **Weak-to-strong generalization**: How can weaker systems (including humans) effectively supervise stronger ones?
2. **Scalable oversight techniques**: Making human feedback work for superhuman systems
3. **Honest AI systems**: Ensuring AI systems accurately report their reasoning and limitations
4. **Automated alignment research**: Using AI to help solve alignment

### Key Technical Problems

Jan has identified several crucial challenges:
- **Reward hacking**: Systems optimizing proxies rather than true objectives
- **Distributional shift**: Maintaining alignment in novel situations
- **Deceptive alignment**: Preventing systems from appearing aligned while pursuing other goals
- **Superalignment**: Aligning systems smarter than humans

## Public Communication

Jan is known for:
- Clear, technical communication about alignment challenges
- Willingness to raise concerns publicly
- Engagement on Twitter/X about safety issues
- Focus on concrete, actionable research directions

His departure from OpenAI sparked significant public discussion about AI safety prioritization at major labs.

## Strategic Views

### On AI Development

- **Safety must keep pace**: Capability advances should be matched by safety advances
- **Need serious compute**: Alignment research requires significant computational resources
- **Coordination is important**: Labs should share safety insights
- **Race dynamics are dangerous**: Competition that sacrifices safety is unacceptable

### On Research Approach

- **Empirical and theoretical**: Need both practical testing and conceptual work
- **Learn from current systems**: Can make progress by studying existing models
- **Prepare for qualitative jumps**: Current techniques may not suffice for superintelligence
- **Automate alignment work**: Use AI to scale up alignment research itself

## Influence and Impact

### Research Impact
- RLHF work influenced every major language model deployment
- Scalable oversight framework guides significant research programs
- Superalignment vision shaped discourse on superintelligence alignment

### Field Building
- Mentored numerous alignment researchers
- Built and led multiple alignment teams
- Raised profile of alignment research within major labs

### Institutional Influence
- Secured major compute allocation for alignment at OpenAI
- Helped shape Anthropic's research priorities
- Demonstrated importance of independent safety research

## Key Publications

- **"Deep Reinforcement Learning from Human Preferences"** (2017) - Early RLHF paper
- **"Scalable agent alignment via reward modeling"** (2018) - Reward learning framework
- **"Recursively Summarizing Books with Human Feedback"** (2021) - Demonstrating RLHF scaling
- **Various blog posts** on alignment challenges and approaches

## Current Challenges

At Anthropic, Jan faces several key challenges:

1. **Time pressure**: Transformative AI may arrive soon, requiring rapid progress
2. **Scaling RLHF**: Current techniques may not work for superintelligent systems
3. **Evaluation**: How to know if alignment techniques actually work
4. **Automation**: Using AI to help solve alignment before it becomes too capable
5. **Coordination**: Ensuring insights are shared across safety community

## Related Pages

<Backlinks client:load entityId="jan-leike" />

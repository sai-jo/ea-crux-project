---
title: Toby Ord
description: Philosopher at FHI, author of 'The Precipice'
sidebar:
  order: 9
---

import {DataInfoBox, EstimateBox, Backlinks, PageStatus} from '../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Oxford philosopher, effective altruism co-founder, and author of 'The Precipice' (2020), which comprehensively quantifies existential risks and estimates a 10% probability of AI-caused extinction this century, making it the single largest existential threat facing humanity." />

<DataInfoBox entityId="toby-ord" />

## Background

Toby Ord is a moral philosopher at Oxford University who has made significant contributions to understanding and quantifying existential risks. He is a key figure in both the effective altruism movement and existential risk research.

Academic background:
- PhD in Philosophy from Oxford (2005)
- Senior Research Fellow at Oxford
- Associated with Future of Humanity Institute (until its 2024 closure)
- Founded Giving What We Can (pledging movement for effective giving)

His 2020 book "The Precipice: Existential Risk and the Future of Humanity" is considered one of the most important works on existential risk.

## Major Contributions

### The Precipice (2020)

This landmark book:
- Provides comprehensive overview of existential risks
- Quantifies probabilities for different risks
- Makes case for prioritizing x-risk reduction
- Accessible to general audiences while rigorous
- Became foundational text in EA and x-risk communities

**Key estimates from the book** (risks this century):
- Asteroid/comet impact: 1 in 1,000,000
- Nuclear war: 1 in 1,000
- Climate change: 1 in 1,000
- Pandemics (natural): 1 in 10,000
- Pandemics (engineered): 1 in 30
- Unaligned AI: 1 in 10
- **Total existential risk: ~1 in 6**

The AI risk estimate (10%) made it clear this should be a major priority.

### Existential Risk Framework

Ord provides clear conceptual foundations:

**Existential catastrophe**: Event that destroys humanity's long-term potential, including:
- Human extinction
- Permanent dystopia
- Permanent stagnation preventing civilization from fulfilling its potential

**Key insight**: Most important thing about existential risk is it affects all future generations.

### Long-term Perspective

Ord emphasizes:
- Humanity's potential future could be vast (billions of years, perhaps trillions of people)
- Current generation holds this future in trust
- Existential risks threaten not just current people but all potential future people
- This makes x-risk reduction among the most important priorities

## Views on AI Risk

<EstimateBox
  client:load
  variable="Toby Ord's Risk Assessment"
  description="From 'The Precipice' and subsequent statements"
  unit=""
  estimates={[
    { source: "AI existential risk this century", value: "10% (1 in 10)", date: "2020", notes: "From The Precipice" },
    { source: "All existential risks combined", value: "16.7% (1 in 6)", date: "2020", notes: "Across all risks this century" },
    { source: "AI as largest risk", value: "Yes", date: "2020", notes: "Considers AI the single biggest existential threat" }
  ]}
/>

### Why AI is the Biggest Risk

Ord argues AI poses unique dangers:

1. **Power**: Could be more powerful than any previous technology
2. **Speed**: Could develop very rapidly once key thresholds crossed
3. **Difficulty**: Alignment is extremely challenging
4. **Irreversibility**: May be one-shot - hard to correct after deployment
5. **Unnatural**: Unlike evolution, no guarantee of alignment with human values

### Compared to Other Risks

Ord's framework helps compare AI to other existential risks:
- **Natural risks**: Very low probability (civilization can handle them)
- **Nuclear war**: Serious but unlikely to cause extinction
- **Climate change**: Catastrophic but probably not existential
- **Engineered pandemics**: Serious risk (~3% this century)
- **Unaligned AI**: Highest single risk (~10% this century)

## Philosophical Contributions

### Moral Case for X-Risk Reduction

Ord provides philosophical grounding for why x-risk matters:

**From common sense morality**:
- Saving lives is good
- Preventing deaths is imperative
- Magnitude matters (preventing extinction saves billions of future lives)

**From various ethical frameworks**:
- Consequentialism: Maximize expected value across all time
- Deontology: Duty to future generations
- Virtue ethics: Guardianship is virtuous

### Uncertainty and Prudence

Ord addresses how to act under uncertainty:
- Can't wait for certainty about risks
- Should apply risk management principles
- Better to be cautious with irreversible decisions
- Expected value reasoning appropriate for large risks

### The Long Reflection

Proposes humanity should reach "existential security" then take time for:
- Determining our values carefully
- Deciding what kind of future we want
- Making wise decisions about transformative technologies
- Not rushing into irreversible choices

## Effective Altruism Connection

Ord is a founder of the effective altruism movement:

### Giving What We Can
Founded in 2009, pledging significant income to effective charities. This launched the EA movement.

### Prioritization Framework
Helped develop EA's cause prioritization framework:
- **Importance**: How much does the problem matter?
- **Tractability**: Can we make progress?
- **Neglectedness**: Are others working on it?

Applied to x-risk:
- **Importance**: Couldn't be higher (all future at stake)
- **Tractability**: Uncertain but not zero
- **Neglectedness**: Very few working on it relative to stakes

## Public Communication

### The Precipice's Impact

The book:
- Reached broad audiences (bestseller)
- Influenced policymakers and philanthropists
- Became standard reference in EA community
- Made x-risk mainstream topic of discussion

### Accessible Philosophy

Ord excels at making philosophy accessible:
- Clear writing without jargon
- Compelling examples and analogies
- Rigorous yet readable
- Honest about uncertainty

### Media Presence

- Podcast interviews (80,000 Hours, etc.)
- Articles and op-eds
- Talks at universities and conferences
- Engagement on social media

## Relationship to AI Safety Field

### Complementary Role

Ord's work complements technical AI safety:
- Provides moral/philosophical foundation
- Helps with prioritization and resource allocation
- Makes case to broader audiences
- Quantifies risks to guide decisions

### Influence on Field

His 10% estimate:
- Made AI risk concrete
- Influenced funding decisions
- Helped recruit researchers to field
- Provided anchor for risk discussions

### Not Focused on Solutions

Unlike some researchers, Ord:
- Doesn't propose technical solutions
- Focuses on problem identification and prioritization
- Leaves solutions to technical experts
- Provides framework for evaluating approaches

## Evolution of Views

**Early EA (2009-2015):**
- Focused on global poverty and health
- Developed EA principles and community

**Transition to X-Risk (2015-2020):**
- Deep research into existential risks
- Writing The Precipice
- Increasing focus on AI

**Recent (2020-present):**
- Promoting The Precipice's message
- Supporting x-risk research and policy
- Continued philosophical work
- Focus on long-term future

## Current Focus

Ord continues working on:

1. **X-risk research**: Refining frameworks and estimates
2. **The Long Reflection**: Developing concept of humanity's deliberation period
3. **Philosophy**: Moral foundations for longtermism
4. **Advisory**: Helping organizations and governments on x-risk
5. **EA movement**: Supporting effective approaches to x-risk

## Key Concepts

### Existential Security
State where existential risks are permanently very low, enabling long-term planning.

### The Precipice
Current period where humanity faces unprecedented risks from our own technology.

### Safeguarding Humanity
Our generation's responsibility to protect future generations.

### Expected Value of the Future
Formal framework for valuing humanity's potential future.

## Impact and Legacy

### Intellectual Contribution
- Clear framework for thinking about x-risks
- Rigorous quantification of probabilities
- Philosophical grounding for longtermism

### Movement Building
- Helped grow EA community
- Inspired career changes toward x-risk work
- Influenced major donors

### Policy Influence
- Book cited in policy discussions
- Influenced governmental thinking on x-risk
- Contributed to UN conversations on existential risk

### Field Development
- Made x-risk academically respectable
- Connected different x-risk communities
- Provided common language and framework

## Related Pages

<Backlinks client:load entityId="toby-ord" />

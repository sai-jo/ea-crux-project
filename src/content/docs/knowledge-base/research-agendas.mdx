---
title: Research Agenda Comparison
description: Side-by-side comparison of major AI safety research agendas
sidebar:
  order: 5
---

import { ComparisonTable, DisagreementMap, InfoBox, KeyQuestions, EntityCard, EntityCards, Section, Tags, Sources } from '../../../components/wiki';

<InfoBox
  type="crux"
  title="Research Agendas"
  customFields={[
    { label: "Focus", value: "Comparing approaches to AI alignment" },
    { label: "Key Tension", value: "Empirical vs. theoretical, prosaic vs. novel" },
    { label: "Related To", value: "Alignment Difficulty, Timelines" },
  ]}
/>

Different research groups have different theories about what will make AI safe. This page compares the major research agendas and their underlying assumptions.

## Overview Comparison

<ComparisonTable
  client:load
  title="Research Agenda Overview"
  columns={["Primary Focus", "Key Assumption", "Timeline View", "Theory of Change"]}
  rows={[
    {
      name: "Anthropic (Constitutional AI)",
      values: {
        "Primary Focus": "Train models to be helpful, harmless, honest via AI feedback",
        "Key Assumption": "Can instill values through training at scale",
        "Timeline View": "Short (2026-2030)",
        "Theory of Change": "Build safest frontier models → demonstrate safe scaling → set industry standard"
      }
    },
    {
      name: "Anthropic (Interpretability)",
      values: {
        "Primary Focus": "Understand model internals to verify alignment",
        "Key Assumption": "Neural networks have interpretable structure",
        "Timeline View": "Short-medium",
        "Theory of Change": "Mechanistic understanding → can verify goals → detect deception"
      }
    },
    {
      name: "OpenAI (Superalignment) [dissolved]",
      values: {
        "Primary Focus": "Use weaker AI to align stronger AI",
        "Key Assumption": "Weak-to-strong generalization works",
        "Timeline View": "Short (2027-2030)",
        "Theory of Change": "Bootstrap alignment from current models to future models"
      }
    },
    {
      name: "DeepMind (Scalable Oversight)",
      values: {
        "Primary Focus": "Debate, recursive reward modeling, process supervision",
        "Key Assumption": "Can scale human oversight to superhuman AI",
        "Timeline View": "Medium (2030-2040)",
        "Theory of Change": "Better evaluation → catch misalignment → iterate to safety"
      }
    },
    {
      name: "ARC (Eliciting Latent Knowledge)",
      values: {
        "Primary Focus": "Get AI to report what it actually knows",
        "Key Assumption": "AI has latent knowledge that could be extracted",
        "Timeline View": "Medium",
        "Theory of Change": "Solve ELK → detect deception → verify alignment"
      }
    },
    {
      name: "Redwood (AI Control)",
      values: {
        "Primary Focus": "Maintain control even with misaligned AI",
        "Key Assumption": "Don't need alignment, just control",
        "Timeline View": "Near-term focus",
        "Theory of Change": "Untrusted AI can be safely used with proper protocols"
      }
    },
    {
      name: "MIRI (Agent Foundations)",
      values: {
        "Primary Focus": "Fundamental theory of agency and goals",
        "Key Assumption": "Need conceptual breakthroughs first",
        "Timeline View": "Uncertain, possibly insufficient time",
        "Theory of Change": "Deep understanding → know what to build → align by design"
      }
    }
  ]}
/>

## Detailed Agenda Breakdown

### Anthropic's Approach

**Core Research: Constitutional AI + Interpretability**

**Key Ideas:**
- **Constitutional AI**: Train models with a constitution of principles, use AI feedback for reinforcement
- **Responsible Scaling Policy (RSP)**: Define capability levels (ASL) with required safeguards
- **Mechanistic Interpretability**: Find interpretable features in neural networks (monosemanticity)

**Key Publications:**
- *Constitutional AI* (2022)
- *Scaling Monosemanticity* (2024)
- *Sleeper Agents* (2024)

**Underlying Assumptions:**
1. Current paradigm (LLMs + RLHF) continues to frontier
2. Training can instill genuine values, not just surface behavior
3. Interpretability can scale to frontier models
4. Have time to iterate before catastrophically dangerous systems

**Strengths:**
- Working with actual frontier models
- Demonstrated practical results
- Well-resourced

**Criticisms:**
- May create false confidence
- Constitutional AI doesn't solve inner alignment
- Racing dynamics may override safety culture

---

### DeepMind's Approach

**Core Research: Scalable Oversight + Formal Methods**

**Key Ideas:**
- **Debate**: Have AI systems argue, humans judge arguments
- **Recursive Reward Modeling**: Use AI to help evaluate AI
- **Process Supervision**: Evaluate reasoning steps, not just outputs
- **Formal Verification**: Mathematical proofs of safety properties

**Key Publications:**
- *AI Safety via Debate* (2018)
- *Scalable Agent Alignment via Reward Modeling* (2018)
- *Let's Verify Step by Step* (2023)

**Underlying Assumptions:**
1. Human judgment can be amplified through clever protocols
2. Correct processes lead to correct outcomes
3. Formal methods can apply to neural networks

**Strengths:**
- Strong theoretical foundations
- Integration with capabilities research
- Resources of Google

**Criticisms:**
- May be too slow given timelines
- Formal verification may not scale
- Debate may favor persuasive over truthful

---

### ARC's Approach (Paul Christiano)

**Core Research: Eliciting Latent Knowledge (ELK)**

**Key Ideas:**
- **ELK Problem**: How do you get AI to report what it actually knows/believes?
- **Iterated Amplification**: Decompose tasks to human-manageable pieces
- **ARC Evals**: Test for dangerous capabilities before deployment

**Key Publications:**
- *Eliciting Latent Knowledge* (2021)
- *ARC Evals reports* (ongoing)

**Underlying Assumptions:**
1. AI will have beliefs about the world that could be extracted
2. Deception is a key failure mode to prevent
3. Capabilities evaluations can catch dangerous systems

**Strengths:**
- Targets core problem (knowing what AI actually believes)
- Practical evaluation work (ARC Evals)
- Careful problem decomposition

**Criticisms:**
- ELK may be fundamentally unsolvable
- Evals may not catch novel capabilities
- Limited resources compared to labs

---

### Redwood Research's Approach

**Core Research: AI Control**

**Key Ideas:**
- **Untrusted Models Problem**: How to safely use AI you don't fully trust?
- **Control Protocols**: Monitoring, task decomposition, trusted oversight
- **Adversarial Robustness**: Make systems robust to attempts to subvert them

**Key Publications:**
- *AI Control: Improving Safety Despite Intentional Subversion* (2024)

**Underlying Assumptions:**
1. We may not solve alignment before deploying powerful AI
2. Control is achievable even with somewhat misaligned systems
3. Near-term focus is more tractable

**Strengths:**
- Pragmatic, near-term focus
- Doesn't require solving alignment
- Concrete protocols

**Criticisms:**
- Doesn't solve underlying alignment problem
- May not scale to very powerful AI
- Could create complacency

---

### MIRI's Approach

**Core Research: Agent Foundations**

**Key Ideas:**
- **Embedded Agency**: How can agents reason about themselves as part of the world?
- **Logical Uncertainty**: How to reason under logical/mathematical uncertainty?
- **Decision Theory**: What's the right way for agents to make decisions?
- **Deconfusion**: Clarify concepts before building

**Key Publications:**
- *Agent Foundations for Aligning Machine Intelligence* (2015)
- *Embedded Agency* (2018)
- Various Alignment Forum posts

**Underlying Assumptions:**
1. Current approaches are fundamentally inadequate
2. Need conceptual clarity before engineering
3. Theoretical understanding is prerequisite to safety

**Strengths:**
- Deep theoretical rigor
- Identifying important problems early
- Not tied to any paradigm

**Criticisms:**
- Too abstract, disconnected from practice
- May be unnecessary if prosaic approaches work
- Progress is very slow

---

## Comparing on Key Dimensions

<ComparisonTable
  client:load
  title="Agenda Comparison by Dimension"
  columns={["Solves Inner Alignment?", "Scales to Superhuman?", "Works Without Full Alignment?", "Empirically Testable Now?"]}
  highlightColumn="Scales to Superhuman?"
  rows={[
    {
      name: "Constitutional AI",
      values: {
        "Solves Inner Alignment?": { value: "Uncertain", badge: "medium" },
        "Scales to Superhuman?": { value: "Unknown", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Interpretability",
      values: {
        "Solves Inner Alignment?": { value: "Could help verify", badge: "medium" },
        "Scales to Superhuman?": { value: "Unknown", badge: "medium" },
        "Works Without Full Alignment?": { value: "Helps detect issues", badge: "medium" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Scalable Oversight",
      values: {
        "Solves Inner Alignment?": { value: "No", badge: "low" },
        "Scales to Superhuman?": { value: "Designed to", badge: "high" },
        "Works Without Full Alignment?": { value: "Maybe", badge: "medium" },
        "Empirically Testable Now?": { value: "Partially", badge: "medium" }
      }
    },
    {
      name: "ELK",
      values: {
        "Solves Inner Alignment?": { value: "Would help detect", badge: "medium" },
        "Scales to Superhuman?": { value: "Intended to", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "Limited", badge: "low" }
      }
    },
    {
      name: "AI Control",
      values: {
        "Solves Inner Alignment?": { value: "No (bypasses it)", badge: "low" },
        "Scales to Superhuman?": { value: "Probably not", badge: "low" },
        "Works Without Full Alignment?": { value: "Yes (the point)", badge: "high" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Agent Foundations",
      values: {
        "Solves Inner Alignment?": { value: "Aims to", badge: "medium" },
        "Scales to Superhuman?": { value: "Would if successful", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "No", badge: "low" }
      }
    }
  ]}
/>

## The Key Disagreements

<DisagreementMap
  client:load
  topic="Which Research Agenda Will Succeed?"
  description="Views on which approach is most likely to prevent AI catastrophe"
  spectrum={{ low: "Need novel breakthroughs", high: "Current approaches scale" }}
  positions={[
    { actor: "Anthropic researchers", position: "Current approaches can scale", estimate: "60-70%", confidence: "medium" },
    { actor: "DeepMind researchers", position: "Scalable oversight works", estimate: "50-60%", confidence: "medium" },
    { actor: "ARC / Paul Christiano", position: "Need targeted breakthroughs", estimate: "40-50%", confidence: "low" },
    { actor: "Redwood", position: "Control buys time", estimate: "Near-term high", confidence: "medium" },
    { actor: "MIRI / Yudkowsky", position: "Current approaches fail", estimate: "15-20%", confidence: "high" },
  ]}
/>

<KeyQuestions
  client:load
  questions={[
    {
      question: "Will current paradigm (LLMs + RLHF) lead to transformative AI?",
      positions: [
        {
          position: "Yes - scaling continues",
          confidence: "medium",
          reasoning: "Scaling laws hold. Emergent capabilities. Investment continues.",
          implications: "Current safety research is directly relevant"
        },
        {
          position: "No - paradigm shift needed",
          confidence: "medium",
          reasoning: "LLMs have fundamental limitations. New architecture will emerge.",
          implications: "May need different safety approaches for new paradigm"
        }
      ]
    },
    {
      question: "Can we verify alignment without solving it?",
      positions: [
        {
          position: "Yes - interpretability/evals can work",
          confidence: "medium",
          reasoning: "Can detect misalignment even if we can't prove alignment. Defense in depth.",
          implications: "Focus on verification and detection"
        },
        {
          position: "No - deceptive alignment defeats verification",
          confidence: "medium",
          reasoning: "Sufficiently capable AI could hide misalignment. Can't verify what we can't understand.",
          implications: "Must solve alignment directly, not just detect misalignment"
        }
      ]
    }
  ]}
/>

## Portfolio Approach

Given uncertainty about which approach will work, many argue for a **portfolio strategy**:

1. **Short-term pragmatic** (AI Control, RLHF improvements): Works now, buys time
2. **Medium-term ambitious** (Interpretability, ELK): Could provide breakthrough
3. **Long-term foundational** (Agent Foundations): Needed if others fail

**Resource allocation question**: How much to invest in each?

| Worldview | Allocation |
|-----------|------------|
| Prosaic alignment likely | 70% empirical, 20% interpretability, 10% foundations |
| Uncertain | 40% empirical, 40% interpretability/verification, 20% foundations |
| Need breakthroughs | 20% empirical, 30% interpretability, 50% foundations |

<Section title="Related Organizations">
  <EntityCards>
    <EntityCard
      id="anthropic"
      category="lab"
      title="Anthropic"
      description="Constitutional AI and interpretability research"
    />
    <EntityCard
      id="miri"
      category="organization"
      title="MIRI"
      description="Agent foundations research"
    />
    <EntityCard
      id="arc-evals"
      category="organization"
      title="ARC"
      description="Eliciting latent knowledge and evaluations"
    />
    <EntityCard
      id="redwood"
      category="organization"
      title="Redwood Research"
      description="AI control protocols"
    />
  </EntityCards>
</Section>

<Section title="Related Topics">
  <Tags tags={[
    "Research Agendas",
    "Alignment",
    "Interpretability",
    "Constitutional AI",
    "Agent Foundations",
    "AI Control",
    "Scalable Oversight",
  ]} />
</Section>

<Sources sources={[
  { title: "Constitutional AI", author: "Anthropic", date: "2022", url: "https://arxiv.org/abs/2212.08073" },
  { title: "Scaling Monosemanticity", author: "Anthropic", date: "2024", url: "https://www.anthropic.com/research/mapping-mind-language-model" },
  { title: "AI Safety via Debate", author: "Irving et al.", date: "2018", url: "https://arxiv.org/abs/1805.00899" },
  { title: "Eliciting Latent Knowledge", author: "Christiano et al.", date: "2021", url: "https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8" },
  { title: "AI Control", author: "Redwood Research", date: "2024", url: "https://redwoodresearch.github.io/ai-control/" },
]} />

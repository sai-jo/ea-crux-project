---
title: "Coverage Gaps: What's Missing?"
description: "Brainstorming topics, perspectives, and content types that are underrepresented or missing from the knowledge base."
sidebar:
  order: 2
lastEdited: "2026-01-02"
---

## Purpose

This page tracks what's **missing or underrepresented** in the knowledge base. It's meant to:
- Guide content prioritization
- Identify blind spots
- Ensure we're not missing important perspectives

---

## Topic Gaps

### Technical Areas

| Gap | Why It Matters | Current Coverage | Priority |
|-----|----------------|------------------|----------|
| **Multimodal risks** | Vision/audio models have different risk profiles | Minimal - mostly LLM focused | Medium |
| **Robotics/embodiment** | Physical AI raises different safety issues | None | Low (further out) |
| **Neurosymbolic approaches** | Alternative paradigm with different safety properties | None | Low |
| **Specific model architectures** | Transformers vs. alternatives, scaling laws | Some in capabilities | Medium |
| **Training data issues** | Copyright, bias, data poisoning | Minimal | Medium |
| **Inference-time compute** | Test-time scaling changes risk profile | None | High |
| **AI-to-AI interaction** | Multi-agent dynamics, emergent behavior | Some in multi-agent | Medium |

### Governance/Policy Areas

| Gap | Why It Matters | Current Coverage | Priority |
|-----|----------------|------------------|----------|
| **China's AI governance** | Major player, different approach | Minimal | High |
| **EU AI Act details** | Actual implementation, not just overview | Some | Medium |
| **Liability frameworks** | Who's responsible when AI causes harm? | None | Medium |
| **Insurance/actuarial** | Market mechanisms for risk | None | Low |
| **Military AI** | LAWS, autonomous weapons, defense applications | Minimal | High |
| **Intelligence community** | How AI affects intelligence/surveillance | None | Medium |
| **Developing world** | AI impacts outside US/EU/China | None | Medium |

### Economic/Social Areas

| Gap | Why It Matters | Current Coverage | Priority |
|-----|----------------|------------------|----------|
| **UBI/redistribution** | Key response to displacement | Mentioned but no deep dive | Medium |
| **Labor organizing** | Worker response to AI | None | Low |
| **Education transformation** | How AI changes learning/skills | Minimal | Medium |
| **Healthcare AI** | Specific domain with high stakes | None | Low |
| **Creative industries** | Art, writing, music - early impact zone | None | Low |
| **Scientific acceleration** | AI speeding up research (positive) | Some | Medium |

### Philosophical/Ethical Areas

| Gap | Why It Matters | Current Coverage | Priority |
|-----|----------------|------------------|----------|
| **AI consciousness/moral status** | When do AIs deserve moral consideration? | None | Medium |
| **Digital minds** | Welfare of AI systems | None | Medium |
| **Population ethics** | How to value future AI vs. human welfare | None | Low |
| **Non-consequentialist views** | Deontological/virtue ethics perspectives | Minimal | Medium |
| **Non-Western perspectives** | AI ethics beyond Anglo-American frame | None | Medium |

---

## Perspective Gaps

### Underrepresented Viewpoints

| Perspective | What We're Missing | Why It Matters |
|-------------|-------------------|----------------|
| **AI optimists** | Serious steelmanning of "AI will be fine" | Intellectual honesty |
| **Accelerationists** | e/acc and similar views | Understand opposing position |
| **Industry practitioners** | What do ML engineers actually think? | Ground truth on feasibility |
| **Policymakers** | What constraints do they face? | Actionable recommendations |
| **Global South** | Non-Western priorities and concerns | Avoid parochialism |
| **Non-EA safety researchers** | Stuart Russell, Yoshua Bengio traditions | Broader intellectual context |
| **AI ethics (fairness/bias)** | How does x-risk relate to near-term ethics? | Bridge communities |

### Critiques We Should Engage

| Critique | Source | Current Response |
|----------|--------|------------------|
| "X-risk is Pascal's mugging" | Skeptics | Mentioned but not deeply engaged |
| "Focus on x-risk distracts from real harms" | AI ethics community | Not addressed |
| "Predictions are impossible, planning is hubris" | Various | Not addressed |
| "EA/rationalist monoculture" | External critics | Not addressed |
| "Safety research is actually accelerating capabilities" | Some researchers | Mentioned in some pages |
| "Governance will just entrench incumbents" | Open source advocates | Some coverage |

---

## Content Type Gaps

### What Formats Are We Missing?

| Format | What We Have | What We're Missing |
|--------|--------------|-------------------|
| **Case studies** | Minimal | Detailed historical analogies (nuclear, biotech, etc.) |
| **Scenario narratives** | Some in /scenarios/ | Vivid, concrete stories of how things unfold |
| **Quantitative models** | Some estimates | Actual models people can run/modify |
| **Decision tools** | None | "If you believe X, you should do Y" frameworks |
| **Timelines** | Some history | Visual timelines, key dates |
| **Comparisons** | Some | Structured comparisons (Lab A vs. Lab B, Approach X vs. Y) |
| **Interviews/profiles** | People pages | Q&A format, direct quotes |
| **Uncertainty visualizations** | Some ranges | Better ways to show confidence intervals |

### Depth vs. Breadth

| Area | Current State | Need |
|------|---------------|------|
| **Alignment techniques** | Good breadth | More depth on key approaches |
| **Governance mechanisms** | Good breadth | More depth on effectiveness |
| **Risks** | Good breadth | Better probability estimates |
| **Organizations** | Basic coverage | More analysis of strategy/impact |
| **People** | Basic bios | More on views, influence, disagreements |
| **Capabilities** | Decent | More current (fast-moving) |

---

## Structural Gaps

### Missing Relationships

| From | To | What's Missing |
|------|----|----------------|
| Interventions | Metrics | How do we measure if interventions work? |
| Parameters | Metrics | What metrics track each parameter? |
| Risks | Interventions | More systematic "what addresses what" |
| People | Cruxes | Who believes what and why? |
| Organizations | Interventions | What is each org actually doing? |
| Worldviews | Everything | How do conclusions change by worldview? |

### Missing Entity Types

| Entity Type | Why We Might Want It |
|-------------|---------------------|
| **Events** | Conferences, incidents, policy moments |
| **Documents** | Key papers, reports, legislation |
| **Datasets** | Training data, benchmarks |
| **Tools** | Evals, interpretability tools, safety libraries |
| **Predictions** | Specific forecasts we can track |

---

## Timeliness Gaps

### Things Moving Fast

| Topic | Problem | Update Needed |
|-------|---------|---------------|
| **Capabilities** | Outdated within months | Regular refresh process |
| **Lab policies** | RSPs change | Track announcements |
| **Legislation** | Bills advance/fail | Track progress |
| **Research** | New papers weekly | Key papers integration |
| **Incidents** | Things happen | Incident tracking |

### Historical Coverage

| Era | Current Coverage | Gap |
|-----|------------------|-----|
| Pre-2010 | Minimal | Early AI safety thinking (Good, Omohundro) |
| 2010-2015 | Some | Rise of deep learning, early concerns |
| 2015-2020 | Better | Key papers, org founding |
| 2020-2023 | Good | LLM era |
| 2024+ | Patchy | Very recent developments |

---

## Priority Assessment

### Highest Priority Gaps

Based on importance × tractability:

1. **Intermediate layer (critical issues)** - Already being worked on in [intermediate-layer](/knowledge-base/research-notes/intermediate-layer/)
2. **China AI governance** - Major blind spot, affects everything
3. **Military AI / LAWS** - High stakes, underrepresented
4. **Inference-time compute** - Changes risk landscape
5. **Steelmanning optimist views** - Intellectual honesty

### Lower Priority (For Now)

- Robotics (further timeline)
- Healthcare/creative specific domains (not core to x-risk)
- AI consciousness (important but can defer)
- Non-Western perspectives (important but hard to do well)

---

## Open Questions

1. **How do we decide what's in scope?**
   - Everything AI-related? Only x-risk relevant?
   - Where's the boundary?

2. **How do we handle fast-moving topics?**
   - Some things are outdated in months
   - What's our update process?

3. **How do we get perspectives we're missing?**
   - Can't just imagine what critics think
   - Need actual engagement

4. **What's the right depth vs. breadth tradeoff?**
   - Better to cover more topics shallowly or fewer deeply?

---

## Research Findings (January 2026)

The following sections contain detailed research on key coverage gaps.

### China's AI Governance Approach

China released **AI Safety Governance Framework 2.0** in September 2025, transforming from a "declaration" into an "operational manual." Key features:

| Aspect | Details |
|--------|---------|
| **Agile governance** | Emphasizes adaptability, iterative policymaking, rapid response vs. static regulation |
| **Open-source focus** | Deepens discussion of risks from open-source models |
| **Social impacts** | Addresses employment and other societal effects |
| **Catastrophic risks** | Includes discussion of severe/existential scenarios |
| **Standards output** | Issued as many national AI standards in H1 2025 as in previous 3 years combined |

China positions itself as "an AI capacity-building leader in the Global South" through a UN resolution backed by 140+ countries. The July 2025 **Global AI Governance Action Plan** outlines six core principles and 13 concrete actions.

**Key contrast with Western approaches**: Rather than comprehensive legislation, China prioritizes pilots, standards, and targeted rules while keeping compliance costs low.

Sources: [Concordia AI State of AI Safety in China 2025](https://concordia-ai.com/research/state-of-ai-safety-in-china-2025/), [Carnegie Endowment](https://carnegieendowment.org/research/2025/10/how-china-views-ai-risks-and-what-to-do-about-them)

---

### Military AI and Autonomous Weapons

**Current state**: "A world in which algorithms determine the fate of soldiers and civilians alike is no longer hypothetical. AI-driven drones are reshaping warfare... Early examples in Ukraine and Gaza reveal this shift." (UN News, June 2025)

| Risk Category | Details |
|---------------|---------|
| **Legal/ethical** | AI cannot uphold "principle of distinction" (protecting civilians); training data biases may encode factors like gender, age, race into targeting |
| **Escalation** | RAND wargame found autonomous systems led to inadvertent escalation; "widespread AI and autonomous systems could lead to crisis instability" |
| **Proliferation** | Once significant powers manufacture LAWS, they will "appear on the black market, in hands of terrorists, dictators, warlords" |
| **Reduced political cost** | Autonomous weapons reduce human/political cost of offensive war |

**Regulatory status**: UN Secretary-General called for legally binding treaty by 2026, but "geopolitical tensions, procrastination, and insufficient political momentum" hinder progress.

Sources: [Human Rights Watch](https://www.hrw.org/report/2025/04/28/a-hazard-to-human-rights/), [Arms Control Association](https://www.armscontrol.org/act/2025-01/features/geopolitics-and-regulation-autonomous-weapons-systems)

---

### Global South Perspectives

**The AI Divide**: "AI could contribute up to \$15.7 trillion to the global economy by 2030. While all regions will benefit, North America and China will see the largest GDP gains, while countries in the Global South will experience more moderate increases." (Brookings)

| Gap | Details |
|-----|---------|
| **Infrastructure** | Only 25% of sub-Saharan Africa has reliable internet; AI data centers used 53-76 TWh in 2024, comparable to total household energy consumption of sub-Saharan Africa |
| **Governance representation** | "Most governance frameworks are being written in Washington, Brussels, and Beijing" without Global South participation |
| **Data bias** | Most training datasets (e.g., skin cancer) from Europe/North America/Oceania with predominantly white populations |
| **Labor displacement** | "Labor-replacing AI poses a greater threat to workers in the developing world" where capital is scarce and labor abundant (Daron Acemoglu) |

**Counter-movements**: African Union and ASEAN developing continental AI strategies "built from the ground up, prioritizing inclusive growth, data sovereignty." BRICS advocates for UN-led global governance framework.

Sources: [Brookings](https://www.brookings.edu/articles/ai-in-the-global-south-opportunities-and-challenges-towards-more-inclusive-governance/), [CSIS](https://www.csis.org/analysis/divide-delivery-how-ai-can-serve-global-south)

---

### Inference-Time Compute Risks

**Policy blind spot**: "Policymakers and regulators worldwide have set thresholds that categorize models as 'frontier' or 'high risk' based on training compute. However, this approach overlooks inference compute."

| Finding | Implication |
|---------|-------------|
| DeepSeek R1 (under 10^25 training compute) could exceed capability cutoffs through heavy inference | Current thresholds miss capable models |
| "Extended reasoning can amplify flawed problem solving strategies" (DeepMind) | More thinking isn't always better |
| Inference workloads ~two-thirds of all AI compute by 2026 | Massive infrastructure shift |
| Inference 80-90% of all AI computing power | Training-focused policy misses most compute |

Sources: [Austin Ellis-Mohr](https://www.austinellismohr.com/updates-blog/inference-time-compute-scaling-policy-considerations)

---

### Critiques of EA/Rationalist AI Safety

**Internal divisions**: Max Tegmark's 2025 statement highlighted "Camp A" (race to superintelligence safely) vs. those who believe community should be "anti-labs." One commenter noted "the vast majority of money, power and influence in EA seems to be on the wrong side of history."

| Critique | Source | Details |
|----------|--------|---------|
| **Worldview insularity** | EA Forum 2025 | "Much of AI safety in EA is shaped by a very particular worldview" with positive feedback loop reinforcing it; "some have never even heard of arguments outside this narrow worldview" |
| **Industry proximity** | Various | "The AI safety community is too cozy with big AGI companies... judgments may be biased by conflicts of interest" |
| **Sunk cost fallacy** | EA Forum | "After Open Phil invested heavily in OpenAI and many EAs joined safety teams, it may feel hard to turn around and be anti-labs" |
| **Missing disciplines** | Gillian Hadfield | "Many AI safety researchers are getting norms and values all wrong because we don't consult the social sciences" |
| **Libertarian bias** | EA Forum | "Bay Area rationalist scene is a hive of techno-optimistic libertarians" with negative view of government effectiveness |

**Demographic critique** (Science): "The field of narrow technical AI safety lacks ideological and demographic diversity; it is a near-monoculture... Its practitioners, disproportionately white, male, and advantaged, are often drawn from Silicon Valley social movements."

Sources: [EA Forum](https://forum.effectivealtruism.org/posts/CN4ZY7Jv5fCcBKySr/ai-safety-has-a-very-particular-worldview), [Science](https://www.science.org/doi/10.1126/science.adi8982)

---

### TESCREAL Criticism

**TESCREAL** (proposed by Timnit Gebru and Émile Torres): Transhumanism, Extropianism, Singularitarianism, Cosmism, Rationalists, Effective Altruism, Longtermism - argued to be "an interconnected and overlapping group with shared origins."

| Criticism | Counter-criticism |
|-----------|-------------------|
| These ideologies "directly originate from 20th-century eugenics" | Ozy Brennan (Asterisk): Torres has "misunderstood these different philosophies" and "taken philosophical thought experiments out of context" |
| "Longtermist ethics conveniently focuses on risks that don't threaten current business models" | Treats different philosophies "as if they were a monolithic movement" |
| "AI companies can present themselves as ethically responsible through safety research while avoiding accountability for immediate harms" | |

Sources: [Wikipedia](https://en.wikipedia.org/wiki/TESCREAL), [EA Forum](https://forum.effectivealtruism.org/posts/vQX9RJnKpGmzQcLPn/conspiracy-theories-left-futurism-and-the-attack-on-tescreal)

---

### Labor Displacement Evidence

**Early evidence**: "Research suggests we may be witnessing the early stages of AI-driven job displacement... Unlike previous technological revolutions that primarily affected manufacturing or routine clerical work, generative AI can target cognitive tasks performed by knowledge workers." (St. Louis Fed)

- Unemployment among 20-30 year olds in tech-exposed occupations rose ~3 percentage points since early 2025
- 78% of organizations using AI, with measurable displacement
- EU AI Alliance identified "Seven Feedback Loops" in systemic economic disruption
- Timeline for major disruption accelerated to 2027-2028

**Neglected aspect**: "With AI disrupting employment, millions could face the loss of purpose, identity and social belonging. The psychological toll of sudden AI-driven unemployment remains largely unaddressed." (WEF)

Sources: [St. Louis Fed](https://www.stlouisfed.org/on-the-economy/2025/aug/is-ai-contributing-unemployment-evidence-occupational-variation), [WEF](https://www.weforum.org/stories/2025/08/the-overlooked-global-risk-of-the-ai-precariat/)

---

### AI Ethics vs. AI Safety Divide

The field remains split between two camps:
- **AI Safety**: AGI, existential risk, alignment, long-term catastrophic risks
- **AI Ethics**: Bias, discrimination, fairness, immediate harms in current systems

As one simplified framing puts it: "AI Safety is about not dying, AI Ethics is about not being evil, and Responsible AI is about not getting sued."

**Critiques**: "Calls for existential risk precaution... divert attention from real short-term risks and harms." The field has experienced "fragmentation... deepened by opposing AI safety camps described as AI doomers and AI ethicists."

Sources: [Springer](https://link.springer.com/article/10.1007/s43681-023-00336-y), [TechPolicy.Press](https://www.techpolicy.press/what-the-ai-safety-debate-can-learn-from-the-techlash/)

---

### Academic/Industry Gap

**Research quantity**: "AI safety accounts for only 2% of overall AI research according to a Georgetown University study. Of 172,621 AI research papers published by American authors between 2017-2021, only 5% were on safety. For China, only 1%." (Semafor)

| Challenge | Details |
|-----------|---------|
| Career risk | "AI safety is not an established field in academia... comes with risks such as not being able to publish or be taken seriously" |
| Access gap | "External safety researchers often lack the access or funding to do the most valuable work" on frontier models |
| Speed | "The field is moving so quickly that traditional peer-reviewed publications can't keep up" |

Sources: [Semafor](https://www.semafor.com/article/04/03/2024/despite-the-ai-safety-hype-a-new-study-finds-little-research-on-the-topic), [ArXiv](https://arxiv.org/html/2502.09288v2)

---

## Next Steps

- [ ] Prioritize gaps by importance × tractability
- [ ] Identify which gaps we can address vs. need outside help
- [ ] Create stub pages for high-priority gaps
- [ ] Reach out for perspectives we're missing
- [ ] Add pages on China AI governance, military AI, Global South perspectives
- [ ] Integrate AI ethics critiques into alignment pages
- [ ] Improve labor displacement coverage

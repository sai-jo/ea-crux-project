---
title: "Critical Issues (Brainstorm)"
description: "Brainstorming alternate framings for failure modes. Superseded by Critical Outcomes in the Parameters hierarchy."
sidebar:
  label: Overview
  order: 0
lastEdited: "2026-01-02"
---

import {Mermaid} from '../../../../../components/wiki';

:::note[Status: Superseded]
This brainstorming explored an alternate "Critical Issues" framing for failure modes. The main knowledge base now uses [Critical Outcomes](/knowledge-base/parameters/critical-outcomes/) within the parameter hierarchy instead. These pages are retained as reference material showing an alternative taxonomy.
:::

## Overview

Critical Issues are **concrete failure modes** that explain how AI development could lead to catastrophic outcomes. They form the intermediate layer between [Parameters](/knowledge-base/parameters/) (what we can measure and influence) and [Outcomes](/knowledge-base/parameters/outcomes/) (what we ultimately care about).

<Mermaid client:load chart={`
flowchart TD
    subgraph Parameters["Parameters (Measurable)"]
        P1[Alignment Robustness]
        P2[Racing Intensity]
        P3[Governance Capacity]
        P4[Epistemic Health]
    end

    subgraph Critical["Critical Issues (Failure Modes)"]
        C1[AI Takeover]
        C2[Catastrophic Misuse]
        C3[Lock-in Scenarios]
        C4[Gradual Disempowerment]
        C5[Coordination Failure]
    end

    subgraph Outcomes["Outcomes (What We Care About)"]
        O1[Acute Risk]
        O2[Steady State Quality]
        O3[Transition Smoothness]
    end

    P1 -->|low| C1
    P2 -->|high| C2
    P2 -->|high| C5
    P3 -->|low| C3
    P4 -->|low| C4

    C1 --> O1
    C2 --> O1
    C3 --> O2
    C4 --> O2
    C4 --> O3
    C5 --> O1
    C5 --> O3

    style C1 fill:#ff6b6b
    style C2 fill:#ff6b6b
    style C3 fill:#ffa500
    style C4 fill:#ffa500
    style C5 fill:#ffe66d
`} />

---

## Why This Layer Matters

The existing [Parameters](/knowledge-base/parameters/) → [Outcomes](/knowledge-base/parameters/outcomes/) hierarchy was too abstract. When we say "Technical Safety reduces Acute Risk"—*how*? Through what pathways?

Critical Issues answer this by identifying the **specific failure modes** that:
1. Parameters influence (causes)
2. Lead to bad outcomes (effects)
3. Interventions can address (targets)

This enables clearer thinking about:
- **Which parameters matter for which failures**: Not all parameters affect all issues equally
- **How interventions help**: By blocking specific failure pathways
- **Where disagreements lie**: People may agree on parameters but disagree on which issues are most likely

---

## Taxonomy

Critical issues can be organized along multiple dimensions:

### By Temporality

| Category | Pattern | Examples |
|----------|---------|----------|
| **Decisive** | Sudden catastrophe, fast takeoff | AI takeover, catastrophic misuse |
| **Accumulative** | Gradual degradation, boiling frog | Disempowerment, epistemic collapse |
| **Tipping Point** | Slow then sudden | Racing dynamics → accident |

### By Reversibility

| Category | Can we recover? | Examples |
|----------|-----------------|----------|
| **Extinction** | No | Misaligned superintelligence |
| **Permanent lock-in** | Effectively no | Value lock-in, authoritarian lock-in |
| **Long-term damage** | Over decades | Economic collapse, trust erosion |
| **Temporary disruption** | Within years | Policy failures, isolated incidents |

### By Primary Actor

| Category | Who/what causes harm? | Examples |
|----------|----------------------|----------|
| **AI-caused** | AI systems themselves | Takeover, deceptive alignment |
| **Human-caused via AI** | Humans using AI | Bioweapons, cyberattacks, surveillance |
| **Structural** | Emergent from system dynamics | Racing, concentration, erosion |

---

## Critical Issues Summary

| Issue | Mechanism | Temporality | Key Parameters | → Outcome |
|-------|-----------|-------------|----------------|-----------|
| [AI Takeover](/knowledge-base/research-notes/critical-issues-brainstorm/ai-takeover/) | Misaligned AI gains decisive control | Decisive | Alignment, Safety-Capability Gap | Acute Risk |
| [Catastrophic Misuse](/knowledge-base/research-notes/critical-issues-brainstorm/catastrophic-misuse/) | Actors use AI for mass harm | Decisive | Threat Environment, Governance | Acute Risk |
| [Lock-in Scenarios](/knowledge-base/research-notes/critical-issues-brainstorm/lock-in/) | Bad states become permanent | Accumulative | Governance, Human Agency | Steady State |
| [Gradual Disempowerment](/knowledge-base/research-notes/critical-issues-brainstorm/gradual-disempowerment/) | Humans become irrelevant | Accumulative | Human Agency, Economic Stability | Steady State, Transition |
| [Coordination Failure](/knowledge-base/research-notes/critical-issues-brainstorm/coordination-failure/) | Racing erodes safety margins | Both | Racing Intensity, Coordination | Acute Risk, Transition |

---

## Probability Estimates

Researchers have offered various estimates for these failure modes:

| Issue | Low Estimate | Central | High Estimate | Source |
|-------|--------------|---------|---------------|--------|
| AI Takeover (by 2070) | 5% | 10-15% | 77% | Carlsmith, Soares |
| Total x-risk (next century) | 5% | 17% | 50%+ | Ord, various |
| Lock-in / "messing up" | 10% | 30-46% | — | Christiano |
| Multi-polar failure | — | 50% | — | Critch |

**Note**: These estimates vary enormously based on worldview. See [Worldview Sensitivity](/knowledge-base/research-notes/worldview-sensitivity/) for how different assumptions drive different conclusions.

---

## Relationship to Other Sections

| Section | Relationship |
|---------|--------------|
| **Parameters** | Parameters influence which critical issues are more/less likely |
| **Risks** | Many `/risks/` pages describe mechanisms that contribute to critical issues |
| **Interventions** | Interventions aim to block or mitigate critical issues |
| **Outcomes** | Critical issues determine which outcomes we get |
| **Scenarios** | Scenarios are narratives built around critical issues unfolding |

---

## Key Research Sources

This section draws heavily on:

- **Carlsmith**: [Is Power-Seeking AI an Existential Risk?](https://arxiv.org/abs/2206.13353) - 6-premise decomposition
- **Christiano**: [What Failure Looks Like](https://www.lesswrong.com/posts/HBxe6wdjxK239zajf/what-failure-looks-like) - Gradual loss vs. adversarial action
- **Kasirzadeh**: [Decisive vs. Accumulative Risk](https://arxiv.org/abs/2401.07836) - Temporal taxonomy
- **Ord**: [The Precipice](https://theprecipice.com/) - Risk estimates, outcome types
- **Hendrycks**: [Overview of Catastrophic AI Risks](https://arxiv.org/abs/2306.12001) - Four-category framework
- **Kulveit & Douglas**: [Gradual Disempowerment](https://arxiv.org/abs/2501.16946) - Systemic existential risk
- **Critch**: [Multi-polar Failure Modes](https://www.lesswrong.com/posts/LpM3EAakwYdS6aRKf/what-multipolar-failure-looks-like-and-robust-agent-agnostic) - Interaction-level effects

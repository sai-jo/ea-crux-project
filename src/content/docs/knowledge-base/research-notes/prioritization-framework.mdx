---
title: "Prioritization Framework: What Matters Most?"
description: "Brainstorming how to prioritize content, interventions, and research directions. What should we focus on?"
sidebar:
  order: 3
lastEdited: "2026-01-02"
---

## Purpose

This page explores how to **prioritize** across multiple dimensions:
- Which content to develop/improve
- Which interventions matter most
- Which research questions are most valuable
- Where should resources go?

---

## Part I: Content Prioritization

### Current Approach

We use quality × importance scoring:
- **Quality (1-5)**: How good is the current page?
- **Importance (0-100)**: How much does this topic matter?
- **Gap = Importance - Quality × 20**: Priority score

This is rough and has problems.

### Problems with Current Approach

| Problem | Description |
|---------|-------------|
| **Importance is subjective** | Who decides what's important? Based on what? |
| **No audience weighting** | Important to whom? Researchers vs. policymakers vs. public? |
| **Static scores** | Importance changes as situation evolves |
| **No dependencies** | Some pages enable understanding of others |
| **Quality is unidimensional** | A page can be high quality for one purpose, low for another |

### Alternative Approaches

**Option A: Audience-weighted importance**

| Audience | Weight | What They Need |
|----------|--------|----------------|
| AI safety researchers | 30% | Technical depth, novel framings |
| Policymakers | 25% | Actionable recommendations, clear summaries |
| AI lab employees | 20% | Practical implications, what to do |
| Journalists/public | 15% | Accessible explanations, context |
| Funders | 10% | Impact assessment, where to give |

Then importance = Σ(weight × audience-specific importance)

**Option B: Crux-weighted importance**

Pages that help resolve key cruxes are more important. Weight by:
- How many people disagree on this crux?
- How much do conclusions change based on this crux?
- How tractable is resolving the crux?

**Option C: Decision-relevance**

Pages are important if they change what someone would do. Score by:
- Does this inform a decision someone is actually facing?
- How large is the expected value of getting the decision right?
- How much does reading this page change the decision?

---

## Part II: Intervention Prioritization

### Standard ITN Framework

Effective Altruism's standard framework:
- **Importance**: How much does this matter if solved?
- **Tractability**: How solvable is this?
- **Neglectedness**: How much is already being done?

Score = Importance × Tractability × Neglectedness

### Problems with ITN for AI Safety

| Problem | Description |
|---------|-------------|
| **Importance is contested** | Depends on p(doom), timelines, etc. |
| **Tractability is unknown** | We don't know if alignment is possible |
| **Neglectedness changes fast** | AI safety went from neglected to crowded quickly |
| **Interactions ignored** | Interventions affect each other |
| **Worldview-dependent** | ITN scores vary wildly by worldview |

### Alternative: Scenario-Based Prioritization

Instead of scoring interventions directly, ask:
1. What are the main scenarios?
2. What's P(scenario)?
3. Which interventions help in which scenarios?
4. Prioritize interventions that help across multiple scenarios

| Intervention | Helps in Takeover? | Helps in Misuse? | Helps in Erosion? | Helps in Lock-in? | Robustness |
|--------------|-------------------|------------------|-------------------|-------------------|------------|
| Interpretability | Yes | Somewhat | No | No | Low |
| Compute governance | Somewhat | Yes | Somewhat | Somewhat | High |
| Alignment research | Yes | No | No | No | Low |
| Institutional capacity | Somewhat | Somewhat | Yes | Yes | High |
| International coordination | Somewhat | Yes | Yes | Yes | High |

Robust interventions (help in many scenarios) may be better than high-EV-in-one-scenario interventions.

### Alternative: Theory of Change Mapping

For each intervention, map:
1. **Inputs**: What resources does it need?
2. **Activities**: What does it actually do?
3. **Outputs**: What does it produce?
4. **Outcomes**: What changes as a result?
5. **Impact**: How does this affect final outcomes?

Prioritize interventions where the theory of change is:
- Plausible (each step is believable)
- Short (fewer steps = fewer failure points)
- Verified (some steps have evidence)

---

## Part III: Research Prioritization

### What Research Questions Matter Most?

**Category A: Crux-resolving research**

Questions whose answers would change many people's priorities:
- How hard is alignment, really?
- What's the timeline to transformative AI?
- Can we detect deception in AI systems?
- Does compute governance actually slow capability development?

**Category B: Decision-relevant research**

Questions whose answers would change what we do:
- Which interventions are actually effective?
- What governance mechanisms work?
- What warning signs should we watch for?

**Category C: Foundation-building research**

Questions that enable other research:
- What metrics should we track?
- How do we measure alignment progress?
- What's the right ontology for thinking about AI risk?

### Research Prioritization Matrix

| Question | Crux-Resolving? | Decision-Relevant? | Tractable? | Priority |
|----------|-----------------|-------------------|------------|----------|
| How hard is alignment? | High | Medium | Low | Medium |
| Which evals predict danger? | Medium | High | Medium | High |
| What's China doing? | Medium | High | Medium | High |
| Will coordination work? | High | High | Low | Medium |
| Best metrics for safety? | Low | High | High | High |

---

## Part IV: Parameter Prioritization

### Which Parameters Matter Most?

Our parameters have importance scores, but how do we set them?

**Approach A: Outcome sensitivity**

Run sensitivity analysis: if parameter X changed by 10%, how much do outcomes change?

| Parameter | Δ Acute Risk | Δ Steady State | Δ Transition | Total Sensitivity |
|-----------|--------------|----------------|--------------|-------------------|
| Alignment Robustness | High | Medium | Low | High |
| Racing Intensity | High | Medium | High | High |
| Societal Trust | Low | High | Medium | Medium |
| Economic Stability | Low | Medium | High | Medium |

**Approach B: Leverage points**

Which parameters are most movable? Importance × Tractability.

| Parameter | Importance | Tractability | Leverage |
|-----------|------------|--------------|----------|
| Alignment Robustness | 88 | 55 | Medium |
| Racing Intensity | 85 | 35 | Low |
| Regulatory Capacity | 75 | 50 | Medium |
| Safety Culture | 80 | 50 | Medium |

**Approach C: Neglectedness adjustment**

What's being worked on vs. what should be?

| Parameter | Importance | Current Attention | Gap |
|-----------|------------|-------------------|-----|
| Alignment Robustness | 88 | High | Low |
| Epistemic Health | 78 | Low | High |
| Human Agency | 60 | Very Low | Medium |
| International Coordination | 80 | Medium | Medium |

---

## Part V: Meta-Prioritization

### How Do We Prioritize Prioritization?

We could spend forever refining prioritization frameworks. When is it worth:
- Improving our scoring system?
- Gathering more data for better scores?
- Just picking something and executing?

**Heuristic**: Spend ~10% of effort on prioritization, 90% on execution. Revisit prioritization when:
- Major new information arrives
- We've completed a significant chunk of work
- Current priorities feel obviously wrong

### Value of Information

For any prioritization question, ask:
- How much would a better answer change what we do?
- How much effort would it take to get a better answer?
- Is the VOI worth the cost?

Often the answer is: just pick something reasonable and move on.

---

## Current Best Guesses

### Content Priorities (For This KB)

1. **High priority**: Pages that help resolve cruxes, especially where reasonable people disagree
2. **Medium priority**: Decision-relevant pages for key audiences (policymakers, lab employees)
3. **Lower priority**: Comprehensive coverage of everything AI-related

### Intervention Priorities (Our View)

Robust across scenarios > High-EV in one scenario:
1. **Institutional capacity building** - Helps in most scenarios
2. **International coordination** - Helps in most scenarios
3. **Compute governance** - Helps in many scenarios, somewhat tractable
4. **Technical safety research** - Essential for takeover scenarios, less for others
5. **Epistemic infrastructure** - Underrated, helps with everything

### Research Priorities (What We'd Fund)

1. **Empirical work on intervention effectiveness** - We have too little data
2. **Crux-resolving theoretical work** - Help people converge or understand disagreement
3. **Metrics development** - Can't improve what we can't measure
4. **Forecasting infrastructure** - Track predictions, calibrate over time

---

## Open Questions

1. **Who decides?** These prioritizations embed values. Whose values?

2. **How do we handle deep uncertainty?** Standard EV calculations break down.

3. **What's the right time horizon?** Prioritize for next year? Next decade?

4. **How do we avoid motivated reasoning?** We'll prioritize what we want to work on.

5. **When do we revisit?** Priorities should update but not thrash.

---

## Research Findings (January 2026)

### ITN Framework: Detailed Critiques

The ITN framework was developed by Holden Karnofsky around 2013 at GiveWell Labs (later Open Philanthropy). Research identifies these specific problems:

| Critique | Source | Details |
|----------|--------|---------|
| **Fuzzy criteria** | EA Forum Review | Criteria are unclear, making proper application difficult |
| **Misapplication** | EA Forum | Often misapplied to interventions instead of cause areas |
| **Wrong assumptions** | EA Forum | Assumes diminishing marginal returns, irrationality of other actors |
| **Divide-by-zero** | EA Forum | Issues with zero investment become "theoretically unsolvable" |
| **Missing urgency** | EA Forum | Does not account for time-sensitivity |
| **Masks philosophy** | EA Forum | Obscures fundamental value disagreements between cause areas |
| **Limited scope** | EA Forum | Only addresses cause pressing-ness, not personal fit or career capital |

Source: [EA Forum - Review of ITN Critiques](https://forum.effectivealtruism.org/posts/MtCAsPMftvJqRRNRggL/review-of-itn-critiques)

---

### Alternative: SPC Framework (Significance, Persistence, Contingency)

Developed by William MacAskill, Teruji Thomas, and Aron Vallinder in *What We Owe the Future* (2022):

| Component | Definition |
|-----------|------------|
| **Significance** | Average value added per unit time by bringing about a state of affairs |
| **Persistence** | How long the state of affairs will last |
| **Contingency** | Counterfactual comparison - how would things have turned out otherwise? |

**Key improvement**: Replaces "neglectedness" with "leverage" - how existing work affects cost-effectiveness of additional work. Generalizes to problems with constant or increasing returns, whereas ITN assumes diminishing (especially logarithmic) returns.

Source: [EA Forum - The SPC Framework](https://forum.effectivealtruism.org/posts/Md9hpuefGbiKfCRjG/the-significance-persistence-contingency-framework-william)

---

### How Major Funders Actually Prioritize

**Open Philanthropy (2024-2025)**:
- Accelerated AI progress and shortened TAI timelines are central planning considerations
- Added AI work oversight to leadership structure in early 2025
- Ajeya Cotra shifted from grantmaking to tracking AI capabilities and TAI planning
- 21 technical research priority areas across 5 clusters
- Focus on: behaviors providing evidence about model training effectiveness, whether models can reliably abide by trained-in rules, critical failure modes in computer-use agents

**80,000 Hours Career Prioritization**:
- AI governance now ranked above AI technical safety research
- "Technical AI governance" - using technical expertise to inform policy - increasingly emphasized
- Compute governance and model evaluation policies highlighted
- Key framework: Being in right place at right time + good judgment + expertise + personal fit

**MIRI Technical Governance Team** (4 scenarios):
1. Coordinated global Halt on dangerous AI development
2. US National Project leading to US global dominance
3. Continued private-sector development with limited intervention (Light-Touch)
4. Nations maintaining AI capabilities at safe levels through mutual threats (Threat of Sabotage)

Sources: [Open Philanthropy 2024-2025 Plans](https://www.openphilanthropy.org/research/our-progress-in-2024-and-plans-for-2025/), [80,000 Hours AI Risks](https://80000hours.org/2025/04/work-on-ai-risks/)

---

### Portfolio Approach to AI Safety Research

Victoria Krakovna (Future of Life Institute) advocates diversifying research approaches:

> "When working on AI safety, we need to hedge our bets and look out for unknown unknowns - it's too important to put all the eggs in one basket."

**Diversification dimensions**:
1. Similarity assumptions between current and future systems (empirical vs. theoretical)
2. Direct definition vs. learning approaches to safety problems
3. Level of modularity in AI design
4. Industry vs. academic perspectives

Source: [FLI - Portfolio Approach to AI Safety Research](https://futureoflife.org/recent-news/portfolio-approach-to-ai-safety-research/)

---

### Scenario Planning for AI X-Risk

Convergence Analysis's AI Clarity team uses explicit scenario modeling:

**Threat model categories**: "malicious use," "AI race," "organizational risks," "rogue AIs"

**Key distinction**: Decisive vs. accumulative scenarios
- **Decisive**: Sudden catastrophic events
- **Accumulative**: Gradual accumulation of smaller disruptions ("boiling frog")

**Timeline-dependent priorities**:
- If TAI is a century away → time to research
- If TAI is two years away → immediate drastic action needed

Source: [EA Forum - Scenario Planning for AI X-Risk](https://forum.effectivealtruism.org/posts/tCq2fi6vhSsCDA5Js/scenario-planning-for-ai-x-risk)

---

### Assessing Intervention Effectiveness with Limited Data

**Key challenges**:
- **Proxy metrics**: Safety evaluations measure intermediary factors, not actual risk
- **Epistemic uncertainty**: Limited training data in novel contexts
- **Counterfactual complexity**: Hard to estimate what would have happened
- **Axiological cluelessness**: Uncertainty about long-term future's net value

**Current assessment approaches**:

| Approach | Organization | Method |
|----------|--------------|--------|
| **Capability evaluations** | UK AI Safety Institute | Autonomous replication, AI R&D acceleration |
| **Empirical uplift studies** | Various | Whether AI significantly enhances users' ability to cause harm |
| **AI Safety Index** | Future of Life Institute | 7 companies, 33 indicators, 6 domains |
| **Pathway-to-impact estimates** | Various | More expensive but better for important decisions |

**FLI AI Safety Index findings**: Clear divide between top performers (Anthropic, OpenAI, Google DeepMind) and others across: risk assessment, safety framework, information sharing, etc.

Sources: [UK AISI Approach to Evaluations](https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations/), [FLI 2025 AI Safety Index](https://futureoflife.org/ai-safety-index-summer-2025/)

---

### Expected Value Calculation Limitations

**Problems identified**:
- Vulnerable to Pascal's mugging
- Values can be manipulated to show desired outcomes
- Complex cluelessness may cause EV formula to break down
- Strong axiological cluelessness makes x-risk reduction potentially less cost-effective than certain global health charities

**Practical workarounds**:
- Some estimate \$100M to \$1B for 0.01% reduction in existential risk
- Pathway-to-impact estimates more expensive but better for major decisions
- Worldview diversification: allocate different portions based on different assumptions

Source: [EA Forum - A Critique of EA's Focus on Longtermism](https://forum.effectivealtruism.org/posts/sfrkvqupLmk2hLvtR/a-critique-of-ea-s-focus-on-longtermism)

---

### Strategic Gaps Identified

**Rethink Priorities landscape analysis found**:
- Less than 10% of AI safety research funding goes to implementation
- Adoption of safety research depends on clear communication and sustained advocacy
- Legal clauses and enforcement mechanisms are under-resourced

**Worldview diversification strategy**:
- Small donors: Allocate primarily to global health, poverty, animal welfare
- Large donors/foundations: Can diversify into speculative, high-variance longtermist causes

Source: [EA Forum - AI Safety Landscape and Strategic Gaps](https://forum.effectivealtruism.org/posts/CbHX5zL2uEvTasuiP/ai-safety-landscape-and-strategic-gaps)

---

### Framework Comparison Summary

| Framework | Strengths | Weaknesses | Best For |
|-----------|-----------|------------|----------|
| **ITN** | Simple, intuitive, widely used | Fuzzy criteria, missing urgency, assumes diminishing returns | Initial cause prioritization |
| **SPC** | Handles non-diminishing returns, counterfactual focus | More complex, less intuitive | Longtermist interventions |
| **Scenario Planning** | Addresses uncertainty explicitly, timeline-dependent | Resource-intensive, speculative | Strategic planning under deep uncertainty |
| **Theory of Change** | Forces explicit mechanism specification | Requires substantial upfront analysis | Grantmaking, organizational strategy |
| **Portfolio Approach** | Hedges against unknown unknowns | Potentially diffuses focus | Research agenda design, funding allocation |

---

## Next Steps

- [ ] Try applying different frameworks to our actual content backlog
- [ ] Get external input on prioritization (avoid echo chamber)
- [ ] Document our actual priorities somewhere public
- [ ] Set review cadence (quarterly?)
- [ ] Consider adopting SPC framework for longtermist content
- [ ] Develop explicit scenario-based prioritization
- [ ] Review FLI AI Safety Index methodology for intervention assessment

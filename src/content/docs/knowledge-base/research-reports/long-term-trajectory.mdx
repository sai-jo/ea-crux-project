---
title: "Long-Term Trajectory: Research Report"
description: "AI will shape humanity's long-term trajectory: decisions made in the next 10-50 years could determine whether humanity flourishes for millions of years or faces permanent catastrophe. This outcome depends on alignment, governance, and the distribution of AI benefits—all currently uncertain."
topic: "long-term-trajectory"
createdAt: 2025-01-08
lastUpdated: 2025-01-08
researchDepth: "standard"
sources: ["web", "codebase"]
quality: 3
sidebar:
  order: 10
---
import { Aside } from '@astrojs/starlight/components';

## Executive Summary

| Finding | Key Data | Implication |
|---------|----------|-------------|
| **Pivotal period** | Next 10-50 years | Decisions matter enormously |
| **Trajectory variance** | From flourishing to extinction | Stakes are maximal |
| **Lock-in risk** | Early AI choices may be permanent | Reversibility crucial |
| **Value embedding** | AI encodes values that persist | Value alignment critical |
| **Cosmic stakes** | Billions of future people affected | Long-term thinking essential |

---

## Research Summary

Advanced AI will likely be one of the most important factors determining humanity's long-term trajectory—whether human civilization flourishes for millions of years, achieves its potential across the cosmos, or faces permanent catastrophe or stagnation. The decisions made during the development and deployment of transformative AI systems may be among the most consequential in human history, with effects that persist indefinitely.

The key insight of long-termism is that future generations matter morally, and there may be vastly more of them than people alive today. If humanity has a good trajectory, trillions of people could live flourishing lives over cosmic timescales. If AI development goes badly, this entire future could be foreclosed. This asymmetry suggests that ensuring good long-term outcomes should be a major priority, even if the probability of influencing them seems small.

Several features of AI make it particularly relevant to long-term trajectory. AI systems may become more capable than humans across all domains, giving AI-controlling entities enormous power. AI values and goals, once embedded, may be difficult to change. AI could enable lock-in of particular systems or values. And AI development is happening now, during a period when humanity still has significant agency over AI's direction.

---

## Background

<Aside type="tip" title="Why Long-Term Matters">
If the expected future is large—billions of people over millions of years—then even small probabilities of affecting that future have enormous expected value. AI is one of the few technologies that plausibly determines whether that future exists.
</Aside>

### Long-Term Trajectory Scenarios

| Scenario | Description | Long-Term Population |
|----------|-------------|---------------------|
| **Flourishing** | Humanity expands, solves problems | Trillions+ |
| **Stagnation** | Civilization persists, doesn't advance | Billions |
| **Lock-in** | Suboptimal state becomes permanent | Unknown |
| **Collapse** | Civilization destroyed, recovers | Uncertain |
| **Extinction** | No future humans | Zero |

### Why AI Shapes Long-Term Trajectory

| Factor | Mechanism |
|--------|-----------|
| **Capability** | AI may exceed human capability |
| **Speed** | AI development faster than adaptation |
| **Persistence** | AI systems may be hard to modify |
| **Scale** | AI enables coordination/control at scale |
| **Timing** | Choices made now may lock in |

---

## Key Findings

### AI's Potential Positive Impacts

| Domain | Potential | Timeline |
|--------|-----------|----------|
| **Scientific discovery** | Solve remaining problems | Medium-term |
| **Economic abundance** | Eliminate material scarcity | Long-term |
| **Health** | Cure diseases, extend lifespan | Medium-term |
| **Space** | Enable cosmic expansion | Long-term |
| **Governance** | Better coordination, less conflict | Uncertain |

<Aside type="caution" title="Positive Outcomes Not Guaranteed">
These positive outcomes require AI to be developed safely and governed well. Without alignment and appropriate governance, AI's power could produce negative rather than positive outcomes.
</Aside>

### AI's Potential Negative Impacts

| Domain | Risk | Permanence |
|--------|------|------------|
| **Extinction** | Unaligned AI eliminates humanity | Permanent |
| **Lock-in** | Wrong values embedded permanently | Very long |
| **Totalitarianism** | AI enables permanent oppression | Very long |
| **Stagnation** | AI doesn't help, may hurt | Long |
| **Conflict** | AI-enabled warfare | Potentially permanent |

### Critical Uncertainties

| Uncertainty | Range of Views | Importance |
|-------------|---------------|------------|
| **AI timelines** | Years to decades | High |
| **Alignment difficulty** | Tractable to impossible | Critical |
| **Governance feasibility** | Achievable to hopeless | High |
| **Human relevance** | Continued to obsolete | High |
| **Lock-in risk** | Low to inevitable | Critical |

### Historical Parallels and Differences

| Technology | Long-Term Impact | AI Difference |
|------------|-----------------|---------------|
| **Agriculture** | Enabled civilization | AI more general |
| **Writing** | Enabled knowledge accumulation | AI more powerful |
| **Science** | Enabled technology | AI faster |
| **Nuclear** | Could destroy civilization | AI more controllable? |
| **Computing** | Transformed economy | AI more autonomous |

---

## Causal Factors

### Factors Improving Long-Term Trajectory

| Factor | Mechanism | Status |
|--------|-----------|--------|
| **Alignment research** | Ensure AI shares human values | Underfunded |
| **Governance capacity** | Control AI development | Weak |
| **Coordination** | Global agreement on AI | Very limited |
| **Safety culture** | Labs prioritize safety | Variable |
| **Long-term thinking** | Consider future generations | Growing but minority |

### Factors Threatening Long-Term Trajectory

| Factor | Mechanism | Status |
|--------|-----------|--------|
| **Racing dynamics** | Safety sacrificed for speed | Strong |
| **Short-termism** | Focus on near-term over long-term | Dominant |
| **Coordination failure** | Can't agree on constraints | Persistent |
| **Capability outpacing safety** | AI too capable to control | Growing concern |
| **Power concentration** | Few actors control AI | Increasing |

---

## Framework: Trajectory Change

### What Would Improve Trajectory?

| Improvement | Mechanism | Tractability |
|-------------|-----------|--------------|
| **Aligned AI** | AI that shares human values | Uncertain but researched |
| **Good governance** | Effective AI oversight | Difficult but possible |
| **Slow takeoff** | Time for adaptation | Somewhat influential |
| **Value preservation** | Human values persist | Requires explicit effort |
| **Reversibility** | Can correct mistakes | Requires design |

### What Would Worsen Trajectory?

| Worsening | Mechanism | Current Risk |
|-----------|-----------|--------------|
| **Misaligned AI** | AI pursues wrong goals | Significant |
| **Value lock-in** | Wrong values embedded | Significant |
| **Power concentration** | Few control AI | High |
| **Governance failure** | Can't control AI | High |
| **Short-term focus** | Ignore long-term | Very high |

<Aside type="note" title="The Importance of This Moment">
We are living through a potentially pivotal period in human history. The choices made now about AI development could determine whether humanity has a future worth having. This places unusual importance on getting AI right.
</Aside>

---

## Implications for Action

### Why Act Now?

| Reason | Explanation |
|--------|-------------|
| **Influence window** | Can still shape AI development |
| **Trajectory sensitivity** | Small changes now have large long-term effects |
| **Lock-in prevention** | Prevent irreversible bad outcomes |
| **Option preservation** | Keep possibilities open |

### Key Levers

| Lever | Actors | Impact Potential |
|-------|--------|-----------------|
| **Alignment research** | Researchers | High if successful |
| **Governance** | Governments, international | High if implemented |
| **Lab practices** | AI companies | High but concentrated |
| **Public awareness** | Media, advocates | Medium-High |
| **Funding allocation** | Funders, governments | High |

---

## Connection to ATM Factors

| Related Factor | Connection |
|---------------|------------|
| [Existential Catastrophe](/ai-transition-model/outcomes/existential-catastrophe/) | Worst-case trajectory |
| [Lock-in Scenarios](/ai-transition-model/scenarios/long-term-lockin/) | Lock-in determines trajectory |
| [Technical AI Safety](/ai-transition-model/factors/misalignment-potential/technical-ai-safety/) | Safety enables good trajectory |
| [AI Governance](/ai-transition-model/factors/misalignment-potential/ai-governance/) | Governance shapes trajectory |

---

## Sources

- [Ord, T. (2020). "The Precipice: Existential Risk and the Future of Humanity"](https://theprecipice.com/)
- [Bostrom, N. (2014). "Superintelligence: Paths, Dangers, Strategies"](https://www.superintelligence.com/)
- [MacAskill, W. (2022). "What We Owe the Future"](https://whatweowethefuture.com/)
- [Global Priorities Institute (2024). Research on Longtermism](https://globalprioritiesinstitute.org/)
- [Future of Humanity Institute (Various). Research Publications](https://www.fhi.ox.ac.uk/)

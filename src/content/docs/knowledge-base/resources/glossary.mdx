---
title: Glossary of AI Safety Terms
description: Definitions of key concepts in AI safety, alignment, and governance
sidebar:
  order: 10
---

import { GlossaryList, Section, Tags } from '../../../../components/wiki';

This glossary provides definitions for key terms used throughout this wiki. Hover over any <span style={{textDecoration: 'underline', cursor: 'help'}}>underlined term</span> in other pages to see its definition.

<GlossaryList client:load />

## How to Use This Glossary

### On Other Pages
When reading other pages on this wiki, you'll see terms with dotted underlines. Hover over these to see a brief definition without leaving the page.

### Categories of Terms

**Core Concepts**: Fundamental ideas like AGI, alignment, and existential risk

**Technical Terms**: Concepts from AI/ML research like RLHF, mesa-optimization, and scalable oversight

**Risk Concepts**: Terms describing potential failure modes like deceptive alignment, reward hacking, and specification gaming

**Organizations and Frameworks**: Institutions and approaches like MIRI, RSPs, and Constitutional AI

**Worldview Terms**: Language for discussing different perspectives on AI risk

## Contributing

This glossary is not exhaustive. Key areas where definitions would be valuable:
- Emerging technical concepts
- Policy and governance terminology
- Philosophical concepts relevant to AI ethics
- Organization-specific terminology

<Section title="Related Topics">
  <Tags tags={[
    "Terminology",
    "Definitions",
    "Education",
    "AI Safety",
    "Alignment",
  ]} />
</Section>

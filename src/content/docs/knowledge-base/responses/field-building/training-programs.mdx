---
title: "AI Safety Training Programs"
description: "Fellowships, PhD programs, research mentorship, and career transition pathways for growing the AI safety research workforce, including MATS, Anthropic Fellows, SPAR, and academic programs."
sidebar:
  order: 3
quality: 82
lastEdited: "2025-12-28"
importance: 75.5
llmSummary: "Comprehensive analysis of AI safety training programs (MATS, Anthropic Fellows, SPAR, academic PhDs) showing the field produces 100-200 new safety researchers annually against growing need. Documents program structures, selection criteria, and strategic bottlenecks including limited mentor bandwidth and retention challenges."
---
import {Mermaid} from '../../../../../components/wiki';

## Overview

The AI safety field faces a critical talent bottleneck. While funding has increased substantially—with Open Philanthropy alone granting over \$10 million annually—the supply of researchers capable of doing high-quality technical safety work remains constrained. Training programs represent the primary pipeline for addressing this gap, offering structured pathways from general ML expertise to safety-specific research skills.

The landscape has evolved rapidly since 2020. MATS (ML Alignment Theory Scholars) has become the premier research mentorship program, with 80% of alumni now working in AI alignment. Anthropic launched a Fellows Program specifically for mid-career transitions. Academic programs are emerging at York (SAINTS CDT), Berkeley (CHAI), and Cambridge (CHIA). Independent research programs like SPAR and LASR Labs provide part-time pathways. Together, these programs produce perhaps 100-200 new safety researchers annually—a number that may be insufficient given the pace of AI capabilities advancement.

The strategic importance of training extends beyond individual researcher production. Programs shape research culture, determine which problems receive attention, and create networks that influence the field's direction. How training programs select participants, what methodologies they emphasize, and which mentors they feature all have downstream effects on AI safety's trajectory.

## Major Training Programs

### MATS (ML Alignment Theory Scholars)

MATS is the most established and influential AI safety research program, operating as an intensive mentorship connecting promising researchers with leading safety researchers.

| Attribute | Details |
|-----------|---------|
| **Duration** | 10 weeks intensive + 4 weeks extension |
| **Format** | In-person (Berkeley, London) |
| **Focus** | Technical alignment research |
| **Mentors** | Researchers from Anthropic, DeepMind, Redwood, FAR.AI |
| **Compensation** | Living stipend provided |
| **Selectivity** | ~5-10% acceptance rate |
| **Alumni outcomes** | 80% now working in AI alignment |

**Research Areas:**
- Interpretability and mechanistic understanding
- AI control and containment
- Scalable oversight
- Evaluations and red-teaming
- Robustness and security

**Notable Alumni Contributions:**
MATS fellows have contributed to sparse autoencoders for interpretability, activation engineering research, developmental interpretability, and externalized reasoning oversight. Alumni have published at ICML and NeurIPS on safety-relevant topics.

### Anthropic Fellows Program

Launched in 2024, the Anthropic Fellows Program targets mid-career technical professionals transitioning into AI safety research.

| Attribute | Details |
|-----------|---------|
| **Duration** | 6 months full-time |
| **Format** | In-person (San Francisco) |
| **Focus** | Transition to safety research |
| **Compensation** | \$1,100/week stipend + benefits |
| **Target** | Mid-career technical professionals |
| **First cohort** | March 2025 |

The program addresses a specific gap: talented ML engineers and researchers who want to transition to safety work but lack the mentorship and runway to do so. By providing substantial compensation and direct collaboration with Anthropic researchers, it removes financial barriers to career change.

### SPAR (Scholars Program for AI Risks)

SPAR offers a part-time, remote research fellowship enabling broader participation in safety research without requiring full-time commitment.

| Attribute | Details |
|-----------|---------|
| **Duration** | Semester-length |
| **Format** | Remote, part-time |
| **Focus** | AI safety and governance research |
| **Target** | Students and professionals |
| **Output** | Research projects, some published |
| **Selectivity** | Moderate |

SPAR research has been accepted at top venues including ICML and NeurIPS. The program works well for:
- Graduate students exploring safety research
- Professionals testing interest before career change
- Researchers in adjacent fields wanting to contribute

### LASR Labs

LASR Labs provides cohort-based technical AI safety research, preparing participants for roles at safety organizations.

| Attribute | Details |
|-----------|---------|
| **Duration** | Research cohort |
| **Format** | Remote |
| **Focus** | Technical safety research |
| **Outcomes** | Alumni at UK AISI, Apollo Research, Leap Labs, Open Philanthropy |

### Global AI Safety Fellowship

Impact Academy's Global AI Safety Fellowship is a fully funded program (up to 6 months) connecting exceptional STEM talent with leading safety organizations.

| Attribute | Details |
|-----------|---------|
| **Duration** | Up to 6 months |
| **Format** | In-person collaboration |
| **Partners** | CHAI (Berkeley), Conjecture, FAR.AI, UK AISI |
| **Funding** | Fully funded |

## Academic Pathways

### PhD Programs

| Program | Institution | Focus | Status |
|---------|-------------|-------|--------|
| **SAINTS CDT** | University of York (UK) | Safe Autonomy | Accepting applications |
| **CHAI** | UC Berkeley | Human-Compatible AI | Established |
| **CHIA** | Cambridge | Human-Inspired AI | Active |
| **Steinhardt Lab** | UC Berkeley | ML Safety | Active |
| **Other ML programs** | Various | General ML with safety focus | Many options |

**University of York - SAINTS CDT:**
The UK's first Centre for Doctoral Training specifically focused on AI safety, funded by UKRI. Brings together computer science, philosophy, law, sociology, and economics to train the next generation of safe AI experts. Based at the Institute for Safe Autonomy.

**Key Academic Researchers:**
Prospective PhD students should consider advisors who work on safety-relevant topics:
- Stuart Russell (Berkeley/CHAI) - Human-compatible AI
- Jacob Steinhardt (Berkeley) - ML safety and robustness
- Vincent Conitzer (CMU) - AI alignment theory
- David Duvenaud (Toronto) - Interpretability
- Roger Grosse (Toronto) - Training dynamics
- Victor Veitch (Chicago) - Causal ML, safety

### Academic vs. Industry Research

| Dimension | Academic Path | Industry Path |
|-----------|---------------|---------------|
| **Timeline** | 4-6 years | 0-2 years to entry |
| **Research freedom** | High | Varies |
| **Resources** | Limited | Often substantial |
| **Publication** | Expected | Sometimes restricted |
| **Salary during training** | PhD stipend (~\$10-50K) | Full salary or fellowship |
| **Ultimate outcome** | Research career | Research career |
| **Best for** | Deep expertise, theory | Immediate impact, applied |

## Upskilling Resources

For those not yet ready for formal programs or preferring self-directed learning:

### Structured Curricula

| Resource | Provider | Coverage | Time Investment |
|----------|----------|----------|-----------------|
| **AI Safety Syllabus** | 80,000 Hours | Comprehensive reading list | 40-100+ hours |
| **AI Alignment Course** | BlueDot Impact | Structured curriculum | 8 weeks |
| **ML Safety Course** | Dan Hendrycks | Technical foundations | Semester |
| **ARENA** | Arena | Technical implementations | 4-8 weeks |

### Self-Study Path

<Mermaid client:load chart={`
flowchart TD
    subgraph FOUNDATIONS["Foundations (2-4 months)"]
        A[ML Fundamentals] --> B[Deep Learning]
        B --> C[Transformer Architecture]
    end

    subgraph SAFETY_BASICS["Safety Basics (2-3 months)"]
        C --> D[80K Hours Syllabus]
        D --> E[Key Papers]
        E --> F[Alignment Forum]
    end

    subgraph SPECIALIZATION["Specialization (3-6 months)"]
        F --> G[Choose Focus Area]
        G --> G1[Interpretability]
        G --> G2[Scalable Oversight]
        G --> G3[Evaluations]
        G --> G4[Governance]
    end

    subgraph APPLICATION["Application (ongoing)"]
        G1 --> H[Apply to Programs]
        G2 --> H
        G3 --> H
        G4 --> H
        H --> I[Independent Research]
        I --> J[Contribute to Field]
    end

    style FOUNDATIONS fill:#e1f5ff
    style SAFETY_BASICS fill:#fff3cd
    style SPECIALIZATION fill:#d4edda
    style APPLICATION fill:#f0f0f0
`} />

## Career Transition Considerations

### When to Apply to Programs

| Your Situation | Recommended Path |
|----------------|------------------|
| Strong ML background, want safety focus | MATS or Anthropic Fellows |
| Exploring interest, employed | SPAR (part-time) |
| Student, want research experience | LASR Labs, SPAR |
| Early career, want PhD | Academic programs |
| Mid-career, want full transition | Anthropic Fellows |
| Strong background, want independence | Self-study + independent research |

### Success Factors

Based on program outcomes, successful applicants typically have:

| Factor | Importance | How to Develop |
|--------|------------|----------------|
| **ML technical skills** | Critical | Courses, projects, publications |
| **Research experience** | High | Academic or industry research |
| **Safety knowledge** | Medium-High | Reading, courses, writing |
| **Communication** | Medium | Writing, presentations |
| **Clear research interests** | Medium | Reading, reflection, pilot projects |

### Common Failure Modes

| Failure Mode | Description | Mitigation |
|--------------|-------------|------------|
| **Premature application** | Applying without sufficient ML skills | Build fundamentals first |
| **No research output** | Nothing demonstrating research capability | Complete pilot project |
| **Vague interests** | Unable to articulate what you want to work on | Read extensively, form views |
| **Poor fit** | Mismatch between interests and program | Research programs carefully |
| **Giving up early** | Rejection discouragement | Multiple applications, iterate |

## Talent Pipeline Analysis

### Current Capacity

| Stage | Annual Output | Bottleneck |
|-------|--------------|------------|
| **Interested individuals** | Thousands | Conversion |
| **Program applicants** | 500-1000 | Selectivity |
| **Program participants** | 150-300 | Capacity |
| **Research-productive alumni** | 100-200 | Mentorship |
| **Long-term field contributors** | 50-100 | Retention |

### Scaling Challenges

| Challenge | Description | Potential Solutions |
|-----------|-------------|-------------------|
| **Mentor bandwidth** | Limited senior researchers available | Peer mentorship, async formats |
| **Quality maintenance** | Scaling may dilute intensity | Tiered programs |
| **Funding** | Programs need sustainable funding | Philanthropic, industry, government |
| **Coordination** | Many programs with unclear differentiation | Better information, specialization |
| **Retention** | Many trained researchers leave safety | Better career paths, culture |

## Strategic Assessment

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| **Tractability** | High | Known how to train researchers |
| **If AI risk high** | High | Need many more researchers |
| **If AI risk low** | Medium | Still valuable for responsible development |
| **Neglectedness** | Medium | Significant investment but scaling gaps |
| **Timeline to impact** | 1-5 years | Trained researchers take time to contribute |
| **Grade** | B+ | Important but faces scaling limits |

## Risks Addressed

| Risk | Mechanism | Effectiveness |
|------|-----------|---------------|
| [Inadequate safety research](/knowledge-base/risks/structural/) | More researchers doing safety work | High |
| [Racing dynamics](/knowledge-base/risks/structural/racing-dynamics/) | Safety talent at labs can advocate | Medium |
| [Field capture](/knowledge-base/risks/structural/) | Diverse training reduces groupthink | Medium |

## Complementary Interventions

- [Field Building](/knowledge-base/responses/field-building/field-building-analysis/) - Broader ecosystem development
- [Corporate Influence](/knowledge-base/responses/field-building/corporate-influence/) - Placing trained researchers at labs
- [AI Safety Institutes](/knowledge-base/responses/institutions/ai-safety-institutes/) - Employers for trained researchers

## Sources

### Program Information

- **MATS:** matsprogram.org - Official program information
- **Anthropic Fellows:** alignment.anthropic.com/2024/anthropic-fellows-program
- **SPAR:** sparai.org - Scholars Program for AI Risks
- **LASR Labs:** lasrlabs.org
- **Global AI Safety Fellowship:** globalaisafetyfellowship.com

### Career Guidance

- **80,000 Hours:** "AI Safety Syllabus" and career guide
- **Alignment Forum:** Career advice threads
- **EA Forum:** "Rank Best Universities for AI Safety"

### Academic Programs

- **University of York SAINTS CDT:** york.ac.uk/study/postgraduate-research/centres-doctoral-training/safe-ai-training
- **Stanford Center for AI Safety:** aisafety.stanford.edu
- **CHAI (Berkeley):** humancompatible.ai

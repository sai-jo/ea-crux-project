---
title: "International Coordination Mechanisms"
description: "International coordination on AI safety involves multilateral treaties, bilateral dialogues, and institutional networks to manage AI risks globally. Current efforts include the Council of Europe AI Treaty (2024), the International Network of AI Safety Institutes (11 members, approximately $150M combined budget), the UN Global Dialogue on AI Governance (launched 2025), and US-China dialogues. The February 2025 OECD reporting framework for the G7 Hiroshima Code of Conduct represents the first standardized monitoring mechanism, while geopolitical tensions—highlighted by the UK's pivot from 'safety' to 'security' framing—limit substantive cooperation."
importance: 82.5
quality: 85
lastEdited: "2025-12-28"
llmSummary: "International coordination on AI safety involves multilateral treaties, bilateral dialogues, and institutional networks, with 11 countries participating in the AI Safety Institute Network (~$150M combined budget) and 14 signatories to the Council of Europe's first binding AI treaty. Analysis shows low-medium tractability due to US-China tensions but very high impact potential if successful, with information sharing being most feasible while capability restrictions face significant barriers."
---
import {Mermaid, R} from '../../../../../../components/wiki';

International coordination represents one of the most challenging yet potentially crucial approaches to AI safety, involving the development of global cooperation mechanisms to ensure advanced AI systems are developed and deployed safely across all major AI powers. As AI capabilities advance rapidly across multiple nations—particularly the United States, China, and the United Kingdom—the absence of coordinated safety measures could lead to dangerous race dynamics where competitive pressures override safety considerations.

The fundamental challenge stems from the global nature of AI development combined with the potentially catastrophic consequences of misaligned advanced AI systems. Unlike previous technological risks that could be contained nationally, advanced AI capabilities and their risks are inherently global, requiring unprecedented levels of international cooperation in an era of heightened geopolitical tensions. The stakes are particularly high given that uncoordinated AI development could lead to a "race to the bottom" where safety precautions are sacrificed for competitive advantage.

Current efforts at international coordination show both promise and significant limitations. The AI Safety Summit series, beginning with the UK's <R id="243fa770c13b0c44">Bletchley Park summit</R> in November 2023, has brought together major AI powers but has largely remained at the level of symbolic commitments rather than substantive agreements. The Council of Europe's <R id="d4682616e12f292e">Framework Convention on AI</R>, adopted in May 2024, represents the first legally binding international AI treaty. The emerging <R id="a65ad4f1a30f1737">International Network of AI Safety Institutes</R> represents a more technical approach to coordination, though their effectiveness remains to be demonstrated. Meanwhile, bilateral dialogues between the US and China on AI safety have begun but operate within the broader context of strategic competition that limits trust and information sharing.

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Tractability** | Low-Medium | Geopolitical tensions between US and China limit substantive cooperation; Council of Europe treaty has 14 signatories but weak enforcement |
| **Impact if Successful** | Very High | Could prevent racing dynamics, establish global safety standards, enable coordinated response to AI incidents |
| **Current Progress** | Limited | Three major summits held (2023-2025); 11-country AI Safety Institute network formed; first binding treaty signed |
| **Key Barriers** | Geopolitical competition | US-China strategic rivalry; AI framed as national security issue in both countries |
| **Verification Challenges** | High | AI capabilities harder to monitor than nuclear/chemical weapons; no equivalent to IAEA inspections |
| **Time Horizon** | 5-15 years | Building international institutions comparable to nuclear governance took 25 years |
| **Resource Requirements** | High | Estimated \$150M+ annually for current AI Safety Institutes; treaty secretariats require additional funding |

---

## Comparative National Approaches to AI Governance

The three major AI powers—the United States, European Union, and China—have adopted fundamentally different regulatory philosophies that reflect their distinct political systems, economic priorities, and cultural values. These divergent approaches create both challenges and opportunities for international coordination. Understanding these differences is essential for assessing the feasibility of various coordination mechanisms.

### Regulatory Philosophy Comparison

| Dimension | European Union | United States | China |
|-----------|----------------|---------------|-------|
| **Regulatory Model** | Comprehensive, risk-based framework | Decentralized, sector-specific | Centralized, state-led directives |
| **Primary Legislation** | EU AI Act (August 2024) | No unified federal law; NIST RMF, state laws, executive orders | Algorithmic Recommendation Rules (2022), Generative AI Measures (2023) |
| **Risk Classification** | Four tiers: unacceptable, high, limited, minimal | Varies by agency and sector | Aligned with national security and social stability priorities |
| **Enforcement Body** | European AI Office | Multiple agencies (FDA, FTC, NHTSA, etc.) | Cyberspace Administration of China (CAC) |
| **Innovation Stance** | Precautionary; ex-ante requirements | Permissive; sector-by-sector | Strategic; strong state support with content controls |
| **Data Requirements** | GDPR compliance, algorithmic impact assessments | Sector-specific; voluntary for most AI | Data localization; security reviews |
| **Transparency** | High; documentation and disclosure mandated | Variable; depends on sector | Limited; state oversight prioritized |
| **Extraterritorial Reach** | Strong (Brussels Effect) | Moderate (export controls) | Limited to domestic market |

### Strengths and Weaknesses by Approach

| Approach | Strengths | Weaknesses | Coordination Implications |
|----------|-----------|------------|---------------------------|
| **EU (Comprehensive)** | Clear rules; strong rights protection; international influence via Brussels Effect | May slow innovation; compliance costs; complex implementation | Could set global standards; others may resist adoption |
| **US (Decentralized)** | Flexibility; innovation-friendly; rapid adaptation | Inconsistent coverage; gaps in protection; state fragmentation | Harder to negotiate unified positions; industry-led standards |
| **China (State-Led)** | Rapid implementation; strategic coherence; strong enforcement capacity | Limited transparency; privacy concerns; political controls | Different governance values complicate alignment |

According to [recent analysis](https://arxiv.org/html/2410.21279v1), "Each regulatory system reflects distinct cultural, political and economic perspectives. Each also highlights differing regional perspectives on regulatory risk-benefit tradeoffs, with divergent judgments on the balance between safety versus innovation and cooperation versus competition." The [2025 Government AI Readiness Index](https://oxfordinsights.com/ai-readiness/government-ai-readiness-index-2025/) notes that the global AI leadership picture is "increasingly bipolar," with the United States and China emerging as the two dominant forces.

---

## Major International Coordination Mechanisms

### Current Framework Landscape

| Mechanism | Type | Participants | Status (Dec 2025) | Binding? |
|-----------|------|--------------|-------------------|----------|
| <R id="d4682616e12f292e">Council of Europe AI Treaty</R> | Multilateral treaty | 14 signatories (US, UK, EU, Canada, Japan, others) | Open for signature Sep 2024 | Yes (first binding AI treaty) |
| <R id="a65ad4f1a30f1737">International Network of AI Safety Institutes</R> | Technical cooperation | 11 countries + EU | Inaugural meeting Nov 2024 | No |
| <R id="243fa770c13b0c44">Bletchley Declaration</R> | Political declaration | 29 countries + EU | Signed Nov 2023 | No |
| <R id="944fc2ac301f8980">Seoul Frontier AI Commitments</R> | Industry pledges | 16 major AI companies | May 2024 | No |
| <R id="5f667b2874a06706">G7 Hiroshima AI Process</R> | Code of conduct | G7 members | Adopted Oct 2023 | No |
| US-China AI Dialogue | Bilateral | US, China | First meeting May 2024 | No |
| <R id="b34af47efb6b7918">UN AI Advisory Body</R> | Multilateral | UN Member States | Final report Sep 2024 | No |

### AI Safety Institute Network

The <R id="3705a6ea6864e940">International Network of AI Safety Institutes</R>, launched in November 2024, represents the most concrete technical cooperation mechanism:

| Institute | Country | Annual Budget | Focus Areas | Status |
|-----------|---------|---------------|-------------|--------|
| UK AI Safety Institute | United Kingdom | ~\$15M (50M GBP) | Model evaluations, red-teaming | Operational since Nov 2023 |
| US AI Safety Institute (NIST) | United States | ~\$10M | Standards, evaluation frameworks | Operational since early 2024 |
| EU AI Office | European Union | ~\$8M | AI Act enforcement, standards | Operational since 2024 |
| AISI Japan | Japan | ~\$5M | Evaluations, safety research | Building capacity |
| AISI Korea | Republic of Korea | ~\$5M | Safety evaluations | Building capacity |
| AISI Singapore | Singapore | ~\$3M | Governance, evaluations | Building capacity |
| AISI Canada | Canada | ~\$3M | Safety standards | Building capacity |
| AISI Australia | Australia | ~\$3M | Safety research | Building capacity |
| AISI France | France | ~\$5M | Safety research, EU coordination | Building capacity |
| AISI Kenya | Kenya | ~\$1M | Global South representation | Early stage |

The network announced <R id="1a1eee57309304f0">\$11 million in funding</R> for synthetic content research and completed its first multilateral model testing exercise at the November 2024 San Francisco convening.

### 2025 Institutional Developments

The landscape of international AI governance institutions underwent significant changes in 2025, reflecting evolving priorities and geopolitical dynamics.

**UK AI Safety Institute Rebranding (February 2025):** In a significant shift, the UK [renamed its AI Safety Institute to the "AI Security Institute"](https://www.gov.uk/government/news/tackling-ai-security-risks-to-unleash-growth-and-deliver-plan-for-change) at the Munich Security Conference. Technology Secretary Peter Kyle stated: "This change brings us into line with what most people would expect an Institute like this to be doing." The rebranded institute now focuses on "serious AI risks with security implications"—including chemical and biological weapons development, cyber-attacks, and crimes such as fraud—rather than broader existential safety concerns. This pivot signals a potential divergence in international approaches, with the UK prioritizing near-term security threats over long-term alignment risks.

**OECD G7 Hiroshima Reporting Framework (February 2025):** The [OECD launched the first global framework](https://www.oecd.org/en/about/news/press-releases/2025/02/oecd-launches-global-framework-to-monitor-application-of-g7-hiroshima-ai-code-of-conduct.html) for companies to report on implementation of the Hiroshima Process International Code of Conduct for Organisations Developing Advanced AI Systems. Major AI developers—including Amazon, Anthropic, Google, Microsoft, and OpenAI—have pledged to complete the inaugural framework. This represents the first standardized monitoring mechanism for voluntary AI safety commitments, though enforcement remains limited to reputational incentives.

**UN Global Dialogue on AI Governance (September 2025):** Building on the Global Digital Compact adopted in 2024, the UN [launched the Global Dialogue on AI Governance](https://www.un.org/sg/en/content/sg/statement/2025-09-25/secretary-generals-remarks-high-level-multi-stakeholder-informal-meeting-launch-the-global-dialogue-artificial-intelligence-governance-delivered)—described as "the world's principal venue for collective focus on this transformative technology." The initiative complements existing efforts at the OECD, G7, and regional organizations while providing an inclusive forum for developing nations. The UN also established the International Independent Scientific Panel on AI, comprising 40 expert members who will provide evidence-based insights on AI opportunities, risks, and impacts—sometimes likened to an "IPCC for AI."

**G7 December 2025 Declaration:** Meeting in Montreal, G7 Ministers responsible for industry, digital affairs, and technology [adopted a joint declaration](https://www.ddg.fr/actualite/artificial-intelligence-digital-economy-and-security-what-the-g7-december-2025-declaration-means-in-practice-for-businesses) reaffirming commitment to risk-based approaches encompassing system transparency, technical robustness, and data quality. The declaration called for increased convergence of regulatory approaches at the international level through OECD work, aiming to limit fragmentation and secure cross-border investments.

| Development | Date | Significance | Limitations |
|-------------|------|--------------|-------------|
| UK AI Security Institute rebrand | Feb 2025 | Signals shift from existential to near-term security focus | May reduce coordination on alignment research |
| OECD Hiroshima Reporting Framework | Feb 2025 | First standardized monitoring for voluntary commitments | No enforcement mechanism |
| UN Global Dialogue launch | Sep 2025 | Inclusive global forum; Scientific Panel established | Slow consensus-building; non-binding |
| G7 Montreal Declaration | Dec 2025 | Regulatory convergence commitment | G7-only; excludes China |

---

## Critical Cooperation Areas and Feasibility

The landscape of potential international coordination varies dramatically in feasibility across different domains. Information sharing on AI safety research represents perhaps the most tractable area for cooperation, as it provides mutual benefits without requiring countries to limit their capabilities development. The establishment of common safety standards and evaluation protocols offers medium feasibility, building on existing precedents in other technology sectors while allowing countries to maintain competitive positions.

### Cooperation Feasibility Matrix

| Cooperation Area | Feasibility | Current Status | Key Enablers | Key Barriers |
|------------------|-------------|----------------|--------------|--------------|
| **Safety research sharing** | High | Active via AISI network | Mutual benefit; low competitive cost | Classification concerns; IP protection |
| **Evaluation standards** | Medium-High | OECD framework launched Feb 2025 | Technical objectivity; industry interest | Different risk priorities; enforcement gaps |
| **Incident reporting** | Medium | No formal mechanism | Shared interest in avoiding catastrophe | Attribution challenges; competitive sensitivity |
| **Crisis communication** | Medium | Biden-Xi nuclear AI agreement (Nov 2024) | Nuclear precedent; mutual deterrence | Trust deficit; limited scope |
| **Deployment standards** | Medium | EU AI Act extraterritorial reach | Brussels Effect; market access | Sovereignty concerns; innovation impact |
| **Capability restrictions** | Low | US export controls (unilateral) | Security imperatives | Zero-sum framing; verification impossible |
| **Development moratoria** | Very Low | No serious proposals | Catastrophic risk awareness | First-mover advantages; enforcement |

However, coordination on capability restrictions faces significant challenges due to the dual-use nature of AI research and the perceived strategic importance of AI leadership. <R id="409aff2720d97129">Export controls on AI hardware</R>, implemented primarily by the United States since 2022, illustrate both the potential and limitations of unilateral approaches—while they may slow capability development in target countries, they also reduce trust and may accelerate independent development efforts. According to <R id="ab22aa0df9b1be7b">RAND analysis</R>, China's AI ecosystem remains competitive despite US export controls, and DeepSeek's founder has stated that "bans on shipments of advanced chips are the problem" rather than funding constraints.

Crisis communication mechanisms represent another medium-feasibility area for cooperation, drawing parallels to nuclear-era hotlines and confidence-building measures. Such mechanisms could prove crucial if advanced AI systems begin exhibiting concerning behaviors or if there are near-miss incidents that require coordinated responses. The November 2024 Biden-Xi agreement that <R id="25ca111eea083021">"humans, not AI" should control nuclear weapons</R> represents a modest but significant step in this direction.

## International Coordination Landscape

The following diagram illustrates the multi-layered architecture of international AI governance, from binding treaties to voluntary commitments:

<Mermaid client:load chart={`
flowchart TD
    subgraph BINDING["Binding Frameworks"]
        COE[Council of Europe<br/>AI Treaty 2024]
        EUACT[EU AI Act 2024]
    end

    subgraph MULTILATERAL["Multilateral Initiatives"]
        AISI[AI Safety Institute<br/>Network - 11 countries]
        UN[UN AI Advisory<br/>Body 2024]
        G7[G7 Hiroshima<br/>Process]
    end

    subgraph SUMMITS["Summit Series"]
        BLETCH[Bletchley 2023<br/>29 countries + EU]
        SEOUL[Seoul 2024<br/>27 countries + EU]
        PARIS[Paris 2025<br/>58 countries]
    end

    subgraph BILATERAL["Bilateral Dialogues"]
        USCHINA[US-China AI<br/>Dialogue 2024]
        UKCHINA[UK-China<br/>Discussions]
    end

    BLETCH --> SEOUL
    SEOUL --> PARIS
    SEOUL --> AISI
    AISI --> COE
    UN --> PARIS

    style COE fill:#90EE90
    style EUACT fill:#90EE90
    style AISI fill:#87CEEB
    style BLETCH fill:#FFE4B5
    style SEOUL fill:#FFE4B5
    style PARIS fill:#FFE4B5
    style USCHINA fill:#FFB6C1
`} />

---

## The US-China Cooperation Dilemma

The central challenge for international AI coordination lies in US-China relations, as these two countries lead global AI development but operate within an increasingly adversarial strategic context. The feasibility of meaningful cooperation faces fundamental tensions between mutual interests in avoiding catastrophic outcomes and zero-sum perceptions of AI competition.

### US-China AI Engagement Timeline

| Date | Event | Significance |
|------|-------|--------------|
| Nov 2023 | Xi-Biden APEC meeting | Commitment to establish AI dialogue |
| Nov 2023 | Both sign Bletchley Declaration | First joint safety commitment |
| May 2024 | <R id="6efc39d4b532521d">First intergovernmental AI dialogue</R> (Geneva) | Working-level technical discussions |
| Nov 2024 | Biden-Xi nuclear AI agreement | Agreement that humans control nuclear weapons |
| Jul 2025 | China publishes Global AI Governance Action Plan | Signals continued engagement interest |

Arguments for possible cooperation point to several factors: both countries have expressed concern about AI risks and have established government entities focused on AI safety; there are precedents for technical cooperation even during periods of broader competition, such as in climate research; and Chinese officials have engaged substantively in international AI safety discussions, suggesting genuine concern about risks rather than purely strategic positioning.

However, significant obstacles remain. The framing of AI as central to national security and economic competitiveness in both countries creates strong incentives against sharing information or coordinating on limitations. The broader deterioration in US-China relations since 2018 has created institutional barriers to cooperation, while mutual suspicions about intentions make verification and trust-building extremely difficult.

According to <R id="0fec40327f2e7046">RAND researchers</R>, "scoping an AI dialogue is difficult because 'AI' does not mean anything specific in many U.S.-China engagements. It means everything from self-driving cars and autonomous weapons to facial recognition, face-swapping apps, ChatGPT, and a potential robot apocalypse."

The Biden administration's approach combined competitive measures (export controls, investment restrictions) with selective engagement on shared challenges, but progress remained limited. Chinese participation in international AI safety discussions has increased, but substantive commitments remain vague, and there are questions about whether engagement reflects genuine safety concerns or strategic positioning.

---

## Lessons from Nuclear Governance

Historical comparisons to nuclear arms control offer both relevant precedents and important cautionary notes. According to <R id="a1d99da51e0ae19d">RAND analysis on nuclear history and AI governance</R>, the development of nuclear non-proliferation took approximately 25 years from the first atomic weapons to the NPT entering into force in 1970.

### Transferable Lessons vs. Key Differences

| Dimension | Nuclear Governance | AI Governance | Implication |
|-----------|-------------------|---------------|-------------|
| **Verification** | Physical inspections (IAEA) | No equivalent for AI capabilities | Harder to monitor compliance |
| **Containment** | Rare materials, specialized facilities | Widely distributed, software-based | Export controls less effective |
| **State control** | Governments control most capabilities | Private companies lead development | Different negotiating parties needed |
| **Demonstrable harm** | Hiroshima/Nagasaki demonstrated risks | AI harms remain speculative | Less urgency for cooperation |
| **Timeline to develop** | Years, billions of dollars | Months, millions of dollars | Faster proliferation |
| **Dual-use nature** | Clear weapons vs. energy distinction | Almost all AI research is dual-use | Harder to define restrictions |

According to the <R id="e48b9c8213e46c7f">Finnish Institute of International Affairs</R>, "compelling arguments have been made to state why nuclear governance models won't work for AI: AI lacks state control, has no reliable verification tools, and is inherently harder to contain."

However, some lessons remain transferable. The <R id="697b30a2dacecc26">GovAI research paper on the Baruch Plan</R> notes that early cooperation attempts failed but built foundations for later success. Norm-building and stigmatization of dangerous practices can work even without enforcement, and crisis communication mechanisms (like nuclear hotlines) prove valuable during tensions.

---

## Safety Implications and Risk Considerations

International coordination presents both promising and concerning implications for AI safety. On the positive side, coordinated approaches could prevent dangerous race dynamics that might otherwise pressure developers to cut safety corners in pursuit of competitive advantage. Shared safety research could accelerate the development of alignment techniques and safety evaluation methods, while coordinated deployment standards could ensure that safety considerations are maintained globally rather than just in safety-conscious jurisdictions.

However, coordination efforts also carry risks that must be carefully managed. Information sharing on AI capabilities could inadvertently accelerate dangerous capabilities development in countries with weaker safety practices. Coordination mechanisms might legitimize or strengthen authoritarian uses of AI by creating channels for technology transfer. There are also risks that coordination efforts could create false confidence or serve as cover for continued dangerous development practices.

The timing of coordination efforts matters significantly. Early coordination on safety research and standards may be more feasible and beneficial than attempts at capability restrictions, which become more difficult as strategic stakes increase. However, waiting too long to establish coordination mechanisms may mean they are unavailable when needed most urgently.

## Current Trajectory and Near-Term Prospects

### AI Summit Series Evolution

The international AI summit series has grown in scope but faces questions about substantive impact:

| Summit | Date | Signatories | Key Outcomes | Criticism |
|--------|------|-------------|--------------|-----------|
| Bletchley (UK) | Nov 2023 | 29 countries + EU | Bletchley Declaration; AI Safety Institutes commitment | Symbolic only; no enforcement |
| Seoul (Korea) | May 2024 | 27 countries + EU | <R id="944fc2ac301f8980">Frontier AI Safety Commitments</R> (16 companies) | Industry self-regulation |
| Paris (France) | Feb 2025 | <R id="201fdc6d92520b6c">58 countries</R> | \$100M Current AI endowment; environmental coalition | US and UK declined to sign joint declaration |

The <R id="bf6ee178660e6fec">Paris AI Action Summit</R> highlighted emerging tensions. While 58 countries signed a joint declaration on "Inclusive and Sustainable AI," the US and UK refused to sign, citing lack of "practical clarity" on global governance. According to the <R id="5bb7cd947ebf5a8b">Financial Times</R>, the summit "highlighted a shift in the dynamics towards geopolitical competition" characterized as "a new AI arms race" between the US and China.

Anthropic CEO Dario Amodei reportedly <R id="1ffe2ab6afdbd5c5">called the Paris Summit a "missed opportunity"</R> for addressing AI risks, with similar concerns voiced by David Leslie of the Alan Turing Institute and Max Tegmark of the Future of Life Institute.

### Near-Term Outlook (2025-2027)

The trajectory of international AI coordination appears to be following a pattern of incremental institutionalization amid persistent geopolitical constraints. Several trends from 2025 are likely to continue:

**Observed 2025 developments shaping future trajectory:**
- UK pivot from "safety" to "security" framing may influence other national institutes
- OECD reporting framework provides template for monitoring voluntary commitments
- UN Global Dialogue and Scientific Panel creating inclusive multilateral venues
- Singapore-Japan joint testing report demonstrates practical AISI network cooperation

**Most likely developments (2026-2027):**
- AI Safety Institute network expansion (India scheduled to host 2026 summit)
- Continued US-China working-level dialogues with limited substantive progress
- EU AI Act enforcement creating de facto international standards via Brussels Effect
- Growing participation from Global South countries through UN mechanisms
- Possible convergence of US CAISI and UK Security Institute on near-term threats

**Key uncertainties:**
- Impact of US political changes on export controls and international engagement
- Whether China will deepen or reduce participation in Western-led initiatives
- Whether a major AI incident could create momentum for stronger coordination
- Trajectory of UK security-focused approach vs broader safety concerns

The European Union's <R id="1ad6dc89cded8b0c">AI Act</R> enforcement, which began in phases from August 2024, may create additional coordination opportunities through regulatory alignment, as companies seeking EU market access adopt its requirements globally. According to [CSET's analysis](https://cset.georgetown.edu/publication/ai-governance-at-the-frontier/), understanding the underlying assumptions of different governance proposals is essential for navigating the increasingly complex international landscape.

## Key Uncertainties and Research Questions

Several critical uncertainties shape the prospects for international AI coordination:

| Uncertainty | Current Assessment | Impact on Coordination |
|-------------|-------------------|----------------------|
| Is US-China cooperation possible? | Low probability of deep cooperation; working-level dialogue possible | Central to global coordination success |
| Can AI Safety Institutes influence development? | Unproven; budgets small relative to industry | Determines value of technical cooperation |
| Are verification mechanisms feasible? | Harder than nuclear/chemical; no good analogies | Limits enforceable agreements |
| Will AI incidents create cooperation windows? | Unknown; depends on incident severity/attribution | Could shift political feasibility rapidly |
| Will private sector or governments lead? | Currently mixed; companies have more technical capacity | Affects negotiating structures needed |

The effectiveness of technical cooperation through AI Safety Institutes is still being tested, with key questions about whether such cooperation can influence actual AI development practices or remains largely academic. The combined budget of the AI Safety Institute network (approximately \$120-150 million annually) is dwarfed by private sector AI spending (over \$100 billion annually), raising questions about their practical influence.

Questions about verification and compliance with international AI agreements remain largely theoretical but will become critical if more substantive agreements are attempted. According to <R id="f65bc93a71d74f9e">research on AI treaty verification</R>, "substantial preparations are needed: (1) developing privacy-preserving, secure, and acceptably priced methods for verifying the compliance of hardware, given inspection access; and (2) building an initial, incomplete verification system, with authorities and precedents that allow its gaps to be quickly closed if and when the political will arises."

The broader question of whether international coordination is necessary for AI safety depends partly on unresolved technical questions about AI alignment and control. If alignment problems prove tractable through purely technical means, the importance of international coordination may diminish. However, if alignment remains difficult or if powerful AI systems create new forms of risk, international coordination may prove essential regardless of its current political feasibility.

---

## Sources and Further Reading

### Official Documents and Declarations

- <R id="243fa770c13b0c44">The Bletchley Declaration</R> - UK Government (November 2023)
- <R id="2c62af9e9fdd09c2">Seoul Declaration for Safe, Innovative and Inclusive AI</R> - AI Seoul Summit (May 2024)
- <R id="944fc2ac301f8980">Frontier AI Safety Commitments</R> - AI Seoul Summit (May 2024)
- <R id="d4682616e12f292e">Council of Europe Framework Convention on AI</R> - Council of Europe (May 2024)
- <R id="a65ad4f1a30f1737">International Network of AI Safety Institutes Fact Sheet</R> - US Commerce Department (November 2024)

### Analysis and Research

- <R id="25ca111eea083021">A Roadmap for a US-China AI Dialogue</R> - Brookings Institution
- <R id="ab22aa0df9b1be7b">Potential for U.S.-China Cooperation on Reducing AI Risks</R> - RAND Corporation
- <R id="a1d99da51e0ae19d">Insights from Nuclear History for AI Governance</R> - RAND Corporation
- <R id="0572f91896f52377">The AI Safety Institute International Network: Next Steps</R> - CSIS
- <R id="697b30a2dacecc26">International Control of Powerful Technology: Lessons from the Baruch Plan</R> - GovAI
- <R id="e48b9c8213e46c7f">Nuclear arms control policies and safety in AI</R> - Finnish Institute of International Affairs
- <R id="409aff2720d97129">U.S. Export Controls and China: Advanced Semiconductors</R> - Congressional Research Service
- [AI Governance at the Frontier](https://cset.georgetown.edu/publication/ai-governance-at-the-frontier/) - CSET (November 2025)
- [GovAI Research on International Governance](https://www.governance.ai/research) - Centre for the Governance of AI
- [Comparative Global AI Regulation](https://arxiv.org/html/2410.21279v1) - Policy perspectives from the EU, China, and the US
- [2025 Government AI Readiness Index](https://oxfordinsights.com/ai-readiness/government-ai-readiness-index-2025/) - Oxford Insights

### Summit Coverage and News

- <R id="bf6ee178660e6fec">Paris AI Action Summit Official Site</R> - French Government
- <R id="9f2ffd2569e88909">Key Outcomes of the AI Seoul Summit</R> - techUK
- <R id="1ffe2ab6afdbd5c5">Did the Paris AI Action Summit Deliver?</R> - The Future Society
- <R id="6efc39d4b532521d">China and the United States Begin Official AI Dialogue</R> - China US Focus
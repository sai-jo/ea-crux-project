---
title: International AI Safety Summits
description: Global convenings on AI safety and governance coordination
sidebar:
  order: 8
---

import {DataInfoBox, Backlinks, PageStatus} from '../../../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Comprehensive coverage of the International AI Safety Summit series including Bletchley Park and Seoul summits, examining their outcomes, industry commitments, institutional infrastructure, limitations, and role in building international AI governance coordination." />

<DataInfoBox entityId="international-summits" />

## Summary

The **International AI Safety Summit series** represents the first sustained effort at global coordination on AI safety, bringing together governments, AI companies, civil society, and researchers to address the risks from advanced AI.

From an AI safety perspective, these summits are significant because:
- They establish **international dialogue** on catastrophic AI risks
- They include **major AI powers** including US, UK, EU, and China
- They produce **commitments and declarations** (though non-binding)
- They build **institutional infrastructure** for coordination
- They signal **government recognition** that AI safety is a serious policy priority

However, the summits produce only **voluntary, non-binding commitments** and face significant challenges in achieving meaningful international cooperation amid geopolitical tensions.

## Bletchley Park Summit (November 2023)

### Background

**Location:** Bletchley Park, UK (historic codebreaking center from WWII)

**Dates:** November 1-2, 2023

**Host:** UK Government

**Attendees:**
- 28 countries including US, UK, EU members, China, India, Australia, Japan
- Major AI companies: OpenAI, Anthropic, Google DeepMind, Meta, Microsoft, etc.
- Civil society organizations
- Researchers and academics

**Significance:**
- **First major international summit** focused on AI safety
- **China's participation** particularly notable given US-China tensions
- **Frontier AI focus** on catastrophic and existential risks
- Held at symbolic location (Bletchley Park's history in technology and national security)

### The Bletchley Declaration

**Full Name:** The Bletchley Declaration by Countries Attending the AI Safety Summit, 1-2 November 2023

**Signatories:** 28 countries

**Key Points:**

#### Recognition of Risks

**Unprecedented Acknowledgment:**
"There is potential for serious, even catastrophic, harm, either deliberate or unintentional, stemming from the most significant capabilities of these AI models."

**Specific Risks Identified:**
- Cybersecurity threats
- Biotechnology risks
- Misinformation
- Loss of control of AI systems
- Societal harms

**Frontier AI Definition:**
"Highly capable general-purpose AI models, including foundation models, that could perform a wide variety of tasks and match or exceed the capabilities present in today's most advanced models."

#### Principles for AI Development

**Safety First:**
- AI should be designed, developed, deployed, and used in a safe, responsible manner
- Risks should be identified, evaluated, and mitigated
- Transparency about capabilities and limitations

**International Cooperation:**
- Shared responsibility for managing risks
- Importance of international cooperation
- Inclusive approach including developing countries

**Dual-Use Recognition:**
- AI has enormous benefits and risks
- Need to realize benefits while managing risks
- Context-dependent governance

#### Commitments

**Research Cooperation:**
- Advance international AI safety research
- Support technically grounded approaches
- Share information on risks and mitigation

**Inclusive Dialogue:**
- Inclusive international dialogue
- Develop risk-based policies
- Respect regulatory approaches

**Follow-up:**
- Continued international engagement
- Further summits and cooperation
- Development of shared frameworks

### Outcomes and Actions

**UK AI Safety Institute:**
- UK announced creation of AI Safety Institute
- Focus on evaluating frontier models
- Pre-deployment testing agreements with major labs

**US Commitments:**
- Vice President Harris attended
- Announced forthcoming Executive Order (released Oct 30, 2023)
- Commitment to US AI Safety Institute

**Industry Commitments:**
- Major AI companies pledged to provide pre-deployment access for safety testing
- Commitments to UK and international AI Safety Institutes
- Support for international cooperation

**Research Initiatives:**
- Increased funding for AI safety research
- International research collaboration
- Academic and civil society engagement

### Criticisms and Limitations

**Vague Commitments:**
- Declaration is high-level and non-binding
- No specific actions or timelines
- No enforcement mechanisms
- "Cheap talk" concerns

**Industry Participation:**
- Companies present but limited binding commitments
- Potential for regulatory capture
- Commercial interests vs. safety

**Geopolitical Tensions:**
- US-China tensions complicate cooperation
- Technology competition pressures
- Different values and priorities
- Limited trust

**Exclusion of Some Voices:**
- Some civil society groups concerned about access
- Developing country representation limited
- Academic participation could be stronger

**Narrow Focus:**
- Emphasis on frontier AI risks
- Less attention to current harms (bias, privacy, labor)
- Existential risk frame controversial
- Potential distraction from nearer-term issues

## Seoul AI Safety Summit (May 2024)

### Background

**Location:** Seoul, South Korea

**Dates:** May 21-22, 2024

**Host:** South Korean Government (co-hosted with UK)

**Attendees:**
- Expanded to more countries
- Continued participation from US, UK, EU, China
- Major AI companies
- Civil society and researchers

**Purpose:**
- Build on Bletchley Declaration
- Focus on practical implementation
- Expand international cooperation
- Address frontier AI safety specifically

### Key Themes

**Moving from Principles to Practice:**
- Translating commitments into action
- Developing concrete frameworks
- Sharing best practices
- Building institutional capacity

**Frontier AI Focus:**
- Concentrated on most capable models
- Capabilities-based risk assessment
- Pre-deployment testing
- International evaluation cooperation

**Inclusion and Accessibility:**
- Greater emphasis on global south participation
- Addressing digital divide
- Ensuring benefits widely shared
- Diverse perspectives on governance

### Frontier AI Safety Commitments

**16 Leading AI Companies Pledged:**

**Participants:**
- OpenAI, Google DeepMind, Anthropic
- Microsoft, Meta, Amazon
- xAI, Mistral AI, Cohere
- AI21 Labs, G42, Inflection AI
- Samsung, Sony, LG, SK Telecom

**Commitments:**

#### Safety by Design
- Develop and deploy frontier AI with safety throughout lifecycle
- Accountability frameworks
- Risk identification and mitigation

#### Transparency
- Publish safety frameworks
- Explain capabilities, limitations, uses
- Report on safety work

#### Accountability Measures
- Invest in safety research
- Work with governments and civil society
- Support development of standards
- Share lessons learned

#### Societal Risks
- Address bias, fairness, privacy
- Mitigate misuse potential
- Consider global impacts

**Nature:**
- Voluntary, non-binding
- Similar to White House voluntary commitments
- International scope
- Reputational accountability

### Outcomes

**International AI Safety Research Network:**
- Coordination among AI Safety Institutes globally
- Information sharing frameworks
- Joint research projects
- Evaluation methodology development

**Capacity Building:**
- Support for developing countries
- AI safety education
- Infrastructure development
- Inclusive governance

**Follow-up Mechanisms:**
- Paris summit announced for 2025
- Ongoing working groups
- Regular convenings
- Continuity of dialogue

### Progress from Bletchley

**More Concrete:**
- Specific company commitments
- Clearer action items
- Institutional mechanisms
- Follow-up structure

**Broader Participation:**
- More countries involved
- Greater geographic diversity
- More companies participating
- Enhanced civil society engagement

**Operational Focus:**
- From principles to implementation
- Practical cooperation mechanisms
- Technical collaboration
- Real-world safety measures

## Paris AI Action Summit (February 2025)

### Planned Focus

**Co-hosted:** France and UK

**Themes (Anticipated):**
- Implementation progress review
- AI for global good (development, health, climate)
- Balanced approach: safety and opportunity
- Broader participation including developing nations

**Evolution:**
- Shift toward action and implementation
- Balance safety concerns with positive uses
- Greater emphasis on equity and inclusion
- Integration with broader AI policy landscape

## Broader International AI Governance

### UN AI Initiatives

**UN Secretary-General's Advisory Body on AI:**
- Established 2023
- Recommendations on AI governance
- Global perspective
- Interim report 2023, final report 2024

**Focus Areas:**
- AI for sustainable development goals
- Rights-based approaches
- Inclusive governance
- Capacity building

**Relationship to Summits:**
- Complementary efforts
- Broader scope (not just safety)
- More inclusive (all UN member states)
- Slower but more legitimate process

### G7 Hiroshima AI Process

**Launched:** May 2023 (G7 Summit in Hiroshima, Japan)

**Focus:**
- Generative AI governance
- Risk management
- Trustworthy AI
- International standards

**Code of Conduct for AI Developers:**
- Voluntary guidelines
- Risk assessment
- Transparency
- Security

**Relationship to Summits:**
- G7 countries active in summits
- Complementary initiatives
- Code of Conduct influenced by summit discussions
- Coordination between processes

### OECD AI Principles

**Established:** 2019 (updated 2023)

**Principles:**
- Inclusive growth and wellbeing
- Human-centered values and fairness
- Transparency and explainability
- Robustness and safety
- Accountability

**Adoption:**
- 46 adherents including major AI powers
- Soft law framework
- Influential on national policies
- Referenced in summit discussions

**Significance:**
- Pre-dated safety summits
- Established baseline principles
- Broad international acceptance
- Framework for national implementation

### Regional Approaches

**European Union:**
- EU AI Act (comprehensive regulation)
- Active in summits
- Strong rights-based approach
- Regulatory leadership

**United States:**
- Executive Order on AI
- US AI Safety Institute
- Bilateral cooperation
- Technology competition framing

**China:**
- Multiple AI regulations
- Participates in summits
- Sovereignty emphasis
- Different values and priorities

**Other Regions:**
- Asia-Pacific engagement (Japan, South Korea, Singapore, Australia)
- Limited Latin America participation
- Minimal Africa engagement so far
- Growing Middle East interest

## Challenges to International Coordination

### Geopolitical Tensions

**US-China Competition:**
- Technology rivalry
- National security concerns
- Export controls and restrictions
- Limited trust and cooperation

**Implications:**
- Coordination difficult
- Race dynamics
- Bifurcated technology ecosystem
- Need to find common ground despite tensions

### Different Values and Priorities

**Democratic vs. Authoritarian Governance:**
- Individual rights vs. social stability
- Openness vs. control
- Privacy vs. surveillance
- Freedom of expression vs. content regulation

**Development Priorities:**
- Developed countries focus on frontier risks
- Developing countries prioritize benefits and access
- Digital divide concerns
- Capacity building needs

**Economic Interests:**
- Competition for AI leadership
- Commercial advantages
- Job impacts and economic transition
- Intellectual property

### Technical and Definitional Issues

**What is "Frontier AI"?**
- Compute-based definitions (10^25 or 10^26 FLOP)
- Capability-based definitions
- Context-dependent risk
- Rapidly changing landscape

**What Risks to Prioritize?**
- Existential and catastrophic risks
- Near-term harms (bias, misinformation, privacy)
- Labor displacement
- Concentration of power

**How to Measure and Verify?**
- Technical challenges
- Proprietary models
- Verification mechanisms
- Enforcement

### Enforcement and Compliance

**Non-Binding Nature:**
- Summits produce voluntary commitments
- No penalties for violations
- Reputational mechanisms only
- Limited effectiveness

**Sovereignty Concerns:**
- Countries resist binding constraints
- National prerogatives
- Different regulatory approaches
- Subsidiarity principles

**Compliance Monitoring:**
- Self-reporting
- Limited verification
- Transparency challenges
- Information asymmetries

## Precedents and Analogies

### Nuclear Weapons Governance

**Similarities:**
- Catastrophic risk
- Major power involvement
- National security implications
- Need for verification

**Differences:**
- AI has civilian benefits
- Harder to verify
- More actors
- Faster development

**Lessons:**
- Coordination possible despite adversarial relations
- Arms control treaties (some successful)
- Verification mechanisms (IAEA)
- Confidence-building measures

### Climate Change

**Similarities:**
- Global coordination needed
- Common but differentiated responsibilities
- Developed vs. developing country tensions
- Long-term vs. short-term tradeoffs

**Differences:**
- AI develops much faster
- Fewer key actors
- Verifiable compliance harder
- More competitive dynamics

**Lessons:**
- International agreements possible (Paris Agreement)
- Implementation challenging
- Need for repeated engagement
- Balance between ambition and achievability

### Internet Governance

**Similarities:**
- Global technology
- Multi-stakeholder approach
- Rapid evolution
- Sovereignty tensions

**Differences:**
- AI more concentrated (fewer developers)
- Higher catastrophic potential
- More amenable to regulation

**Lessons:**
- Multi-stakeholder model has strengths and weaknesses
- Technical community involvement essential
- Fragmentation risks (splinternet)
- Standards and protocols important

## Effectiveness and Impact

### Positive Developments

**Dialogue Established:**
- Regular international engagement
- Channels for communication
- Shared understanding growing
- Norm development beginning

**Recognition of Risks:**
- Government attention on AI safety
- Existential risks acknowledged
- International dimension recognized
- Political priority increasing

**Institutional Infrastructure:**
- AI Safety Institutes in multiple countries
- International cooperation frameworks
- Research collaboration
- Evaluation partnerships

**Industry Engagement:**
- Companies participating
- Voluntary commitments
- Pre-deployment testing agreements
- Information sharing (limited)

### Limitations

**Weak Commitments:**
- Non-binding declarations
- Vague language
- No enforcement
- Limited accountability

**Slow Progress:**
- Incremental rather than transformative
- Lowest common denominator
- Bureaucratic processes
- May not keep pace with AI development

**Geopolitical Barriers:**
- US-China tensions
- Limited trust
- National security concerns
- Competition overriding cooperation

**Narrow Participation:**
- Government and industry dominate
- Civil society limited role
- Developing countries underrepresented
- Academic involvement could be stronger

**Implementation Gap:**
- Principles not translated to action
- National policies vary widely
- Compliance uncertain
- Follow-through questionable

## Future Trajectory

### Optimistic Scenario

**Growing Coordination:**
- Summits become institutionalized
- Binding agreements eventually
- Technical cooperation increases
- Verification mechanisms developed

**Shared Understanding:**
- Common framework for risks
- Agreed evaluation methods
- Joint research initiatives
- International standards

**Inclusive Governance:**
- Broader participation
- Developing country capacity
- Civil society engagement
- Democratic accountability

**Effective Implementation:**
- Voluntary commitments honored
- National policies aligned
- International cooperation on enforcement
- Real safety improvements

### Pessimistic Scenario

**Coordination Failure:**
- US-China split precludes meaningful cooperation
- Race to the bottom on safety
- Bifurcated global AI ecosystem
- Minimal actual constraints

**Symbolic Only:**
- Summits are talk shops
- No real commitments or implementation
- Regulatory capture by industry
- Public relations exercise

**Divergent Approaches:**
- Each country/region develops own framework
- Incompatible regulations
- Forum shopping and arbitrage
- Fragmentation

**Overtaken by Events:**
- AI advances faster than governance
- Summits lag developments
- Irrelevant to actual safety
- Catastrophic failures occur

### Most Likely Scenario

**Incremental Progress:**
- Summits continue and evolve
- Gradual strengthening of commitments
- Some coordination achieved
- Patchwork of national regulations

**Technical Cooperation:**
- Research collaboration increases
- Evaluation methods shared
- Some information exchange
- Standards develop slowly

**Political Constraints:**
- Geopolitical tensions limit ambition
- Binding agreements unlikely near-term
- Variable implementation
- Regional initiatives complement global

**Hybrid Governance:**
- Combination of international soft law and national regulation
- Multi-stakeholder involvement
- Adaptive approach
- Imperfect but better than nothing

## Role in AI Safety Strategy

### Current Contribution

**Normalizing AI Safety:**
- Makes safety concerns mainstream
- Political legitimacy
- Government engagement
- Resource allocation

**Building Relationships:**
- Channels of communication
- Personal relationships among officials
- Foundation for future cooperation
- Trust-building (limited but real)

**Informing Policy:**
- Sharing best practices
- Learning from different approaches
- Coordinating where possible
- Reducing regulatory fragmentation

### Strategic Importance

**Essential for Coordination:**
- Unilateral action insufficient
- Need international cooperation for existential risks
- Summits are primary venue
- No alternative forum exists

**Slow but Necessary:**
- International coordination is slow
- Building trust takes time
- Incremental progress necessary
- Long-term investment

**Opportunity for Advocacy:**
- Influence through participation
- Shaping agendas and outcomes
- Building coalitions
- Public pressure

### Limitations for X-Risk

**Not Sufficient:**
- Voluntary commitments inadequate for catastrophic risks
- Need binding agreements
- Enforcement essential
- Summits are necessary but not sufficient

**May Create Complacency:**
- Illusion of progress
- Delay on binding action
- Regulatory capture
- False sense of security

**Could Enable Coordination:**
- If strengthened over time
- If lead to binding treaties
- If build institutional capacity
- Long-term potential

## Engagement Opportunities

### For AI Safety Researchers

**Technical Input:**
- Provide expertise on risks and evaluations
- Develop verification methods
- Inform policy discussions
- Publish research feeding into summits

**Participation:**
- Attend as observers or participants
- Side events and workshops
- Briefings for government officials
- Civil society coalition building

### For Policy Professionals

**Government Roles:**
- National delegations
- Shaping country positions
- Negotiating commitments
- Implementation planning

**Think Tank Analysis:**
- Evaluate outcomes
- Propose improvements
- Track compliance
- Policy research

### For Civil Society

**Advocacy:**
- Push for stronger commitments
- Accountability mechanisms
- Inclusive participation
- Public education

**Monitoring:**
- Track implementation
- Publicize gaps
- Hold actors accountable
- Independent evaluation

### For Industry

**Constructive Engagement:**
- Participate in good faith
- Exceed voluntary commitments
- Share information
- Support effective governance

**Implementation:**
- Operationalize commitments
- Develop best practices
- Collaborate on standards
- Demonstrate responsibility

## Related Pages

<Backlinks client:load entityId="international-summits" />

---
title: International Coordination
description: International coordination on AI safety involves multilateral treaties, bilateral dialogues, and institutional networks to manage AI risks globally. Current efforts include the Council of Europe AI Treaty (2024), the International Network of AI Safety Institutes (11 members, approximately $150M combined budget), and US-China dialogues, though geopolitical tensions limit substantive cooperation.
importance: 82
quality: 5
lastEdited: "2025-12-28"
llmSummary: Evaluates international coordination on AI safety, analyzing the Council of Europe Framework Convention (first binding AI treaty, 2024), AI Safety Institute network (11 members), and the Bletchley-Seoul-Paris summit series. Finds low tractability for US-China cooperation but medium feasibility for safety standards and information sharing.
---

import {Mermaid} from '../../../../../../components/wiki';

International coordination represents one of the most challenging yet potentially crucial approaches to AI safety, involving the development of global cooperation mechanisms to ensure advanced AI systems are developed and deployed safely across all major AI powers. As AI capabilities advance rapidly across multiple nations—particularly the United States, China, and the United Kingdom—the absence of coordinated safety measures could lead to dangerous race dynamics where competitive pressures override safety considerations.

The fundamental challenge stems from the global nature of AI development combined with the potentially catastrophic consequences of misaligned advanced AI systems. Unlike previous technological risks that could be contained nationally, advanced AI capabilities and their risks are inherently global, requiring unprecedented levels of international cooperation in an era of heightened geopolitical tensions. The stakes are particularly high given that uncoordinated AI development could lead to a "race to the bottom" where safety precautions are sacrificed for competitive advantage.

Current efforts at international coordination show both promise and significant limitations. The AI Safety Summit series, beginning with the UK's [Bletchley Park summit](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration) in November 2023, has brought together major AI powers but has largely remained at the level of symbolic commitments rather than substantive agreements. The Council of Europe's [Framework Convention on AI](https://www.coe.int/en/web/portal/-/council-of-europe-adopts-first-international-treaty-on-artificial-intelligence), adopted in May 2024, represents the first legally binding international AI treaty. The emerging [International Network of AI Safety Institutes](https://www.nist.gov/news-events/news/2024/11/fact-sheet-us-department-commerce-us-department-state-launch-international) represents a more technical approach to coordination, though their effectiveness remains to be demonstrated. Meanwhile, bilateral dialogues between the US and China on AI safety have begun but operate within the broader context of strategic competition that limits trust and information sharing.

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Tractability** | Low-Medium | Geopolitical tensions between US and China limit substantive cooperation; Council of Europe treaty has 14 signatories but weak enforcement |
| **Impact if Successful** | Very High | Could prevent racing dynamics, establish global safety standards, enable coordinated response to AI incidents |
| **Current Progress** | Limited | Three major summits held (2023-2025); 11-country AI Safety Institute network formed; first binding treaty signed |
| **Key Barriers** | Geopolitical competition | US-China strategic rivalry; AI framed as national security issue in both countries |
| **Verification Challenges** | High | AI capabilities harder to monitor than nuclear/chemical weapons; no equivalent to IAEA inspections |
| **Time Horizon** | 5-15 years | Building international institutions comparable to nuclear governance took 25 years |
| **Resource Requirements** | High | Estimated $150M+ annually for current AI Safety Institutes; treaty secretariats require additional funding |

---

## Major International Coordination Mechanisms

### Current Framework Landscape

| Mechanism | Type | Participants | Status (Dec 2025) | Binding? |
|-----------|------|--------------|-------------------|----------|
| [Council of Europe AI Treaty](https://www.coe.int/en/web/portal/-/council-of-europe-adopts-first-international-treaty-on-artificial-intelligence) | Multilateral treaty | 14 signatories (US, UK, EU, Canada, Japan, others) | Open for signature Sep 2024 | Yes (first binding AI treaty) |
| [International Network of AI Safety Institutes](https://www.nist.gov/news-events/news/2024/11/fact-sheet-us-department-commerce-us-department-state-launch-international) | Technical cooperation | 11 countries + EU | Inaugural meeting Nov 2024 | No |
| [Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration) | Political declaration | 29 countries + EU | Signed Nov 2023 | No |
| [Seoul Frontier AI Commitments](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024) | Industry pledges | 16 major AI companies | May 2024 | No |
| [G7 Hiroshima AI Process](https://www.mofa.go.jp/ecm/ec/page5e_000076.html) | Code of conduct | G7 members | Adopted Oct 2023 | No |
| US-China AI Dialogue | Bilateral | US, China | First meeting May 2024 | No |
| [UN AI Advisory Body](https://www.un.org/ai-advisory-body) | Multilateral | UN Member States | Final report Sep 2024 | No |

### AI Safety Institute Network

The [International Network of AI Safety Institutes](https://www.commerce.gov/news/fact-sheets/2024/11/fact-sheet-us-department-commerce-us-department-state-launch-international), launched in November 2024, represents the most concrete technical cooperation mechanism:

| Institute | Country | Annual Budget | Focus Areas | Status |
|-----------|---------|---------------|-------------|--------|
| UK AI Safety Institute | United Kingdom | ~$65M (50M GBP) | Model evaluations, red-teaming | Operational since Nov 2023 |
| US AI Safety Institute (NIST) | United States | ~$10M | Standards, evaluation frameworks | Operational since early 2024 |
| EU AI Office | European Union | ~$8M | AI Act enforcement, standards | Operational since 2024 |
| AISI Japan | Japan | ~$5M | Evaluations, safety research | Building capacity |
| AISI Korea | Republic of Korea | ~$5M | Safety evaluations | Building capacity |
| AISI Singapore | Singapore | ~$3M | Governance, evaluations | Building capacity |
| AISI Canada | Canada | ~$3M | Safety standards | Building capacity |
| AISI Australia | Australia | ~$3M | Safety research | Building capacity |
| AISI France | France | ~$5M | Safety research, EU coordination | Building capacity |
| AISI Kenya | Kenya | ~$1M | Global South representation | Early stage |

The network announced [$11 million in funding](https://www.ansi.org/standards-news/all-news/2024/11/11-25-24-us-launches-international-ai-safety-network-with-global-partners) for synthetic content research and completed its first multilateral model testing exercise at the November 2024 San Francisco convening.

---

## Critical Cooperation Areas and Feasibility

The landscape of potential international coordination varies dramatically in feasibility across different domains. Information sharing on AI safety research represents perhaps the most tractable area for cooperation, as it provides mutual benefits without requiring countries to limit their capabilities development. The establishment of common safety standards and evaluation protocols offers medium feasibility, building on existing precedents in other technology sectors while allowing countries to maintain competitive positions.

However, coordination on capability restrictions faces significant challenges due to the dual-use nature of AI research and the perceived strategic importance of AI leadership. [Export controls on AI hardware](https://www.congress.gov/crs-product/R48642), implemented primarily by the United States since 2022, illustrate both the potential and limitations of unilateral approaches—while they may slow capability development in target countries, they also reduce trust and may accelerate independent development efforts. According to [RAND analysis](https://www.rand.org/pubs/perspectives/PEA4189-1.html), China's AI ecosystem remains competitive despite US export controls, and DeepSeek's founder has stated that "bans on shipments of advanced chips are the problem" rather than funding constraints.

Crisis communication mechanisms represent another medium-feasibility area for cooperation, drawing parallels to nuclear-era hotlines and confidence-building measures. Such mechanisms could prove crucial if advanced AI systems begin exhibiting concerning behaviors or if there are near-miss incidents that require coordinated responses. The November 2024 Biden-Xi agreement that ["humans, not AI" should control nuclear weapons](https://www.brookings.edu/articles/a-roadmap-for-a-us-china-ai-dialogue/) represents a modest but significant step in this direction.

## International Coordination Landscape

The following diagram illustrates the multi-layered architecture of international AI governance, from binding treaties to voluntary commitments:

<Mermaid client:load chart={`
flowchart TD
    subgraph BINDING["Binding Frameworks"]
        COE[Council of Europe<br/>AI Treaty 2024]
        EUACT[EU AI Act 2024]
    end

    subgraph MULTILATERAL["Multilateral Initiatives"]
        AISI[AI Safety Institute<br/>Network - 11 countries]
        UN[UN AI Advisory<br/>Body 2024]
        G7[G7 Hiroshima<br/>Process]
    end

    subgraph SUMMITS["Summit Series"]
        BLETCH[Bletchley 2023<br/>29 countries + EU]
        SEOUL[Seoul 2024<br/>27 countries + EU]
        PARIS[Paris 2025<br/>58 countries]
    end

    subgraph BILATERAL["Bilateral Dialogues"]
        USCHINA[US-China AI<br/>Dialogue 2024]
        UKCHINA[UK-China<br/>Discussions]
    end

    BLETCH --> SEOUL
    SEOUL --> PARIS
    SEOUL --> AISI
    AISI --> COE
    UN --> PARIS

    style COE fill:#90EE90
    style EUACT fill:#90EE90
    style AISI fill:#87CEEB
    style BLETCH fill:#FFE4B5
    style SEOUL fill:#FFE4B5
    style PARIS fill:#FFE4B5
    style USCHINA fill:#FFB6C1
`} />

---

## The US-China Cooperation Dilemma

The central challenge for international AI coordination lies in US-China relations, as these two countries lead global AI development but operate within an increasingly adversarial strategic context. The feasibility of meaningful cooperation faces fundamental tensions between mutual interests in avoiding catastrophic outcomes and zero-sum perceptions of AI competition.

### US-China AI Engagement Timeline

| Date | Event | Significance |
|------|-------|--------------|
| Nov 2023 | Xi-Biden APEC meeting | Commitment to establish AI dialogue |
| Nov 2023 | Both sign Bletchley Declaration | First joint safety commitment |
| May 2024 | [First intergovernmental AI dialogue](https://www.chinausfocus.com/peace-security/china-and-the-united-states-begin-official-ai-dialogue) (Geneva) | Working-level technical discussions |
| Nov 2024 | Biden-Xi nuclear AI agreement | Agreement that humans control nuclear weapons |
| Jul 2025 | China publishes Global AI Governance Action Plan | Signals continued engagement interest |

Arguments for possible cooperation point to several factors: both countries have expressed concern about AI risks and have established government entities focused on AI safety; there are precedents for technical cooperation even during periods of broader competition, such as in climate research; and Chinese officials have engaged substantively in international AI safety discussions, suggesting genuine concern about risks rather than purely strategic positioning.

However, significant obstacles remain. The framing of AI as central to national security and economic competitiveness in both countries creates strong incentives against sharing information or coordinating on limitations. The broader deterioration in US-China relations since 2018 has created institutional barriers to cooperation, while mutual suspicions about intentions make verification and trust-building extremely difficult.

According to [RAND researchers](https://www.rand.org/pubs/commentary/2025/01/how-might-the-united-states-engage-with-china-on-ai.html), "scoping an AI dialogue is difficult because 'AI' does not mean anything specific in many U.S.-China engagements. It means everything from self-driving cars and autonomous weapons to facial recognition, face-swapping apps, ChatGPT, and a potential robot apocalypse."

The Biden administration's approach combined competitive measures (export controls, investment restrictions) with selective engagement on shared challenges, but progress remained limited. Chinese participation in international AI safety discussions has increased, but substantive commitments remain vague, and there are questions about whether engagement reflects genuine safety concerns or strategic positioning.

---

## Lessons from Nuclear Governance

Historical comparisons to nuclear arms control offer both relevant precedents and important cautionary notes. According to [RAND analysis on nuclear history and AI governance](https://www.rand.org/pubs/perspectives/PEA3652-1.html), the development of nuclear non-proliferation took approximately 25 years from the first atomic weapons to the NPT entering into force in 1970.

### Transferable Lessons vs. Key Differences

| Dimension | Nuclear Governance | AI Governance | Implication |
|-----------|-------------------|---------------|-------------|
| **Verification** | Physical inspections (IAEA) | No equivalent for AI capabilities | Harder to monitor compliance |
| **Containment** | Rare materials, specialized facilities | Widely distributed, software-based | Export controls less effective |
| **State control** | Governments control most capabilities | Private companies lead development | Different negotiating parties needed |
| **Demonstrable harm** | Hiroshima/Nagasaki demonstrated risks | AI harms remain speculative | Less urgency for cooperation |
| **Timeline to develop** | Years, billions of dollars | Months, millions of dollars | Faster proliferation |
| **Dual-use nature** | Clear weapons vs. energy distinction | Almost all AI research is dual-use | Harder to define restrictions |

According to the [Finnish Institute of International Affairs](https://fiia.fi/en/publication/nuclear-arms-control-policies-and-safety-in-artificial-intelligence), "compelling arguments have been made to state why nuclear governance models won't work for AI: AI lacks state control, has no reliable verification tools, and is inherently harder to contain."

However, some lessons remain transferable. The [GovAI research paper on the Baruch Plan](https://www.governance.ai/research-paper/international-control-of-powerful-technology-lessons-from-the-baruch-plan-for-nuclear-weapons) notes that early cooperation attempts failed but built foundations for later success. Norm-building and stigmatization of dangerous practices can work even without enforcement, and crisis communication mechanisms (like nuclear hotlines) prove valuable during tensions.

---

## Safety Implications and Risk Considerations

International coordination presents both promising and concerning implications for AI safety. On the positive side, coordinated approaches could prevent dangerous race dynamics that might otherwise pressure developers to cut safety corners in pursuit of competitive advantage. Shared safety research could accelerate the development of alignment techniques and safety evaluation methods, while coordinated deployment standards could ensure that safety considerations are maintained globally rather than just in safety-conscious jurisdictions.

However, coordination efforts also carry risks that must be carefully managed. Information sharing on AI capabilities could inadvertently accelerate dangerous capabilities development in countries with weaker safety practices. Coordination mechanisms might legitimize or strengthen authoritarian uses of AI by creating channels for technology transfer. There are also risks that coordination efforts could create false confidence or serve as cover for continued dangerous development practices.

The timing of coordination efforts matters significantly. Early coordination on safety research and standards may be more feasible and beneficial than attempts at capability restrictions, which become more difficult as strategic stakes increase. However, waiting too long to establish coordination mechanisms may mean they are unavailable when needed most urgently.

## Current Trajectory and Near-Term Prospects

### AI Summit Series Evolution

The international AI summit series has grown in scope but faces questions about substantive impact:

| Summit | Date | Signatories | Key Outcomes | Criticism |
|--------|------|-------------|--------------|-----------|
| Bletchley (UK) | Nov 2023 | 29 countries + EU | Bletchley Declaration; AI Safety Institutes commitment | Symbolic only; no enforcement |
| Seoul (Korea) | May 2024 | 27 countries + EU | [Frontier AI Safety Commitments](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024) (16 companies) | Industry self-regulation |
| Paris (France) | Feb 2025 | [58 countries](https://en.wikipedia.org/wiki/AI_Action_Summit) | $400M Current AI endowment; environmental coalition | US and UK declined to sign joint declaration |

The [Paris AI Action Summit](https://www.elysee.fr/en/sommet-pour-l-action-sur-l-ia) highlighted emerging tensions. While 58 countries signed a joint declaration on "Inclusive and Sustainable AI," the US and UK refused to sign, citing lack of "practical clarity" on global governance. According to the [Financial Times](https://www.techpolicy.press/at-paris-ai-summit-us-eu-other-nations-lay-out-divergent-goals/), the summit "highlighted a shift in the dynamics towards geopolitical competition" characterized as "a new AI arms race" between the US and China.

Anthropic CEO Dario Amodei reportedly [called the Paris Summit a "missed opportunity"](https://thefuturesociety.org/aiactionsummitvspublicpriorities/) for addressing AI risks, with similar concerns voiced by David Leslie of the Alan Turing Institute and Max Tegmark of the Future of Life Institute.

### Near-Term Outlook (2025-2027)

The trajectory of international AI coordination appears likely to follow incremental progress within significant constraints:

**Most likely developments:**
- AI Safety Institute network expansion (India scheduled to host 2026 summit)
- Continued US-China working-level dialogues
- EU AI Act enforcement creating de facto international standards
- Growing participation from Global South countries

**Key uncertainties:**
- Impact of US political changes on export controls and international engagement
- Whether China will deepen or reduce participation in Western-led initiatives
- Whether a major AI incident could create momentum for stronger coordination

The European Union's [AI Act](https://artificialintelligenceact.eu/) enforcement, which began in phases from August 2024, may create additional coordination opportunities through regulatory alignment, as companies seeking EU market access adopt its requirements globally.

## Key Uncertainties and Research Questions

Several critical uncertainties shape the prospects for international AI coordination:

| Uncertainty | Current Assessment | Impact on Coordination |
|-------------|-------------------|----------------------|
| Is US-China cooperation possible? | Low probability of deep cooperation; working-level dialogue possible | Central to global coordination success |
| Can AI Safety Institutes influence development? | Unproven; budgets small relative to industry | Determines value of technical cooperation |
| Are verification mechanisms feasible? | Harder than nuclear/chemical; no good analogies | Limits enforceable agreements |
| Will AI incidents create cooperation windows? | Unknown; depends on incident severity/attribution | Could shift political feasibility rapidly |
| Will private sector or governments lead? | Currently mixed; companies have more technical capacity | Affects negotiating structures needed |

The effectiveness of technical cooperation through AI Safety Institutes is still being tested, with key questions about whether such cooperation can influence actual AI development practices or remains largely academic. The combined budget of the AI Safety Institute network (approximately $120-150 million annually) is dwarfed by private sector AI spending (over $100 billion annually), raising questions about their practical influence.

Questions about verification and compliance with international AI agreements remain largely theoretical but will become critical if more substantive agreements are attempted. According to [research on AI treaty verification](https://www.researchgate.net/publication/369924944_Nuclear_Arms_Control_Verification_and_Lessons_for_AI_Treaties), "substantial preparations are needed: (1) developing privacy-preserving, secure, and acceptably priced methods for verifying the compliance of hardware, given inspection access; and (2) building an initial, incomplete verification system, with authorities and precedents that allow its gaps to be quickly closed if and when the political will arises."

The broader question of whether international coordination is necessary for AI safety depends partly on unresolved technical questions about AI alignment and control. If alignment problems prove tractable through purely technical means, the importance of international coordination may diminish. However, if alignment remains difficult or if powerful AI systems create new forms of risk, international coordination may prove essential regardless of its current political feasibility.

---

## Sources and Further Reading

### Official Documents and Declarations

- [The Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration) - UK Government (November 2023)
- [Seoul Declaration for Safe, Innovative and Inclusive AI](https://www.gov.uk/government/publications/seoul-declaration-for-safe-innovative-and-inclusive-ai-ai-seoul-summit-2024) - AI Seoul Summit (May 2024)
- [Frontier AI Safety Commitments](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024) - AI Seoul Summit (May 2024)
- [Council of Europe Framework Convention on AI](https://www.coe.int/en/web/portal/-/council-of-europe-adopts-first-international-treaty-on-artificial-intelligence) - Council of Europe (May 2024)
- [International Network of AI Safety Institutes Fact Sheet](https://www.nist.gov/news-events/news/2024/11/fact-sheet-us-department-commerce-us-department-state-launch-international) - US Commerce Department (November 2024)

### Analysis and Research

- [A Roadmap for a US-China AI Dialogue](https://www.brookings.edu/articles/a-roadmap-for-a-us-china-ai-dialogue/) - Brookings Institution
- [Potential for U.S.-China Cooperation on Reducing AI Risks](https://www.rand.org/pubs/perspectives/PEA4189-1.html) - RAND Corporation
- [Insights from Nuclear History for AI Governance](https://www.rand.org/pubs/perspectives/PEA3652-1.html) - RAND Corporation
- [The AI Safety Institute International Network: Next Steps](https://www.csis.org/analysis/ai-safety-institute-international-network-next-steps-and-recommendations) - CSIS
- [International Control of Powerful Technology: Lessons from the Baruch Plan](https://www.governance.ai/research-paper/international-control-of-powerful-technology-lessons-from-the-baruch-plan-for-nuclear-weapons) - GovAI
- [Nuclear arms control policies and safety in AI](https://fiia.fi/en/publication/nuclear-arms-control-policies-and-safety-in-artificial-intelligence) - Finnish Institute of International Affairs
- [U.S. Export Controls and China: Advanced Semiconductors](https://www.congress.gov/crs-product/R48642) - Congressional Research Service

### Summit Coverage and News

- [Paris AI Action Summit Official Site](https://www.elysee.fr/en/sommet-pour-l-action-sur-l-ia) - French Government
- [Key Outcomes of the AI Seoul Summit](https://www.techuk.org/resource/key-outcomes-of-the-ai-seoul-summit.html) - techUK
- [Did the Paris AI Action Summit Deliver?](https://thefuturesociety.org/aiactionsummitvspublicpriorities/) - The Future Society
- [China and the United States Begin Official AI Dialogue](https://www.chinausfocus.com/peace-security/china-and-the-united-states-begin-official-ai-dialogue) - China US Focus
---
title: Seoul AI Safety Summit Declaration
description: May 2024 international commitments on frontier AI safety
sidebar:
  order: 9
---

import { DataInfoBox , PageStatus} from '../../../../../../components/wiki';

<PageStatus quality={2} lastEdited="2025-12-24" llmSummary="Overview of the May 2024 Seoul AI Safety Summit outcomes including the Seoul Declaration, 16-company Frontier AI Safety Commitments, and AI Safety Institute Network establishment." todo="Expand with more analysis of specific commitments and their implementation status; add comparison with Bletchley outcomes" />

<DataInfoBox entityId="seoul-declaration" />

## Summary

The **Seoul AI Safety Summit** (May 21-22, 2024) was the second in a series of international AI safety summits, following the **Bletchley Park Summit** in November 2023.

The summit produced multiple outcome documents representing incremental progress on international AI governance, though critics note the commitments remain largely voluntary.

## Key Outcomes

### 1. Seoul Declaration

A high-level political statement signed by 28 countries plus the EU, reaffirming:
- Recognition of frontier AI risks
- Commitment to international cooperation
- Support for science-based risk assessment
- Need for both safety and innovation

### 2. Frontier AI Safety Commitments

**16 AI companies** signed commitments to:

**Safety practices**:
- Publish safety frameworks (RSPs or equivalents)
- Conduct pre-deployment safety evaluations
- Establish internal accountability structures

**Transparency**:
- Share information on capabilities and limitations
- Provide transparency on safety practices
- Support external evaluation efforts

**Incident reporting**:
- Share information about safety incidents
- Support development of common reporting standards

**Participating companies**:
Amazon, Anthropic, Cohere, Google, G42, IBM, Inflection AI, Meta, Microsoft, Mistral AI, Naver, OpenAI, Samsung Electronics, Technology Innovation Institute, xAI, Zhipu AI

### 3. AI Safety Institute Network

Agreement to establish an **international network of AI Safety Institutes**, including:
- Information sharing between national institutes
- Coordinated evaluation methodologies
- Joint research initiatives
- Personnel exchanges

**Participating institutes**:
- UK AI Safety Institute
- US AI Safety Institute
- Planned institutes in Japan, Singapore, Canada, EU, others

### 4. Scientific Statement

An international scientific statement on AI safety endorsed by experts, noting:
- Current limitations in understanding AI systems
- Need for improved evaluation methods
- Importance of interpretability research
- Uncertainty about future capability trajectories

## Progress from Bletchley

| Topic | Bletchley (Nov 2023) | Seoul (May 2024) |
|-------|---------------------|------------------|
| Declaration | 28 signatories | 28 signatories |
| Company commitments | None formal | 16 companies signed |
| AI Safety Institutes | UK announced | International network agreed |
| Specificity | General principles | More detailed commitments |

## Limitations and Critiques

**What's missing**:
- **No binding commitments**: All voluntary
- **No enforcement mechanism**: Relies on good faith
- **No specific thresholds**: Vague on what triggers requirements
- **China's limited participation**: Present but not signing company commitments
- **No liability provisions**: Companies not legally accountable

**Structural concerns**:
- Summit format produces declarations, not treaties
- Commitments may not survive competitive pressure
- No mechanism for handling non-compliance
- Uneven implementation across countries

## Company Commitments Analysis

The company commitments were notable but limited:

**Positive aspects**:
- First coordinated international corporate commitments
- Chinese company (Zhipu AI) participation
- Public accountability mechanism

**Concerns**:
- Commitments largely codify existing practices
- No independent verification requirement
- Vague language allows flexible interpretation
- No consequences for violation

## Significance

The Seoul Summit represents:

1. **Institutionalization**: AI safety summits becoming regular international events
2. **Expansion**: More countries and companies engaged
3. **Specificity**: Moving from principles to (soft) commitments
4. **Network building**: AI Safety Institute cooperation formalized

However, the gap between **summit declarations** and **effective governance** remains large. The summits are building diplomatic infrastructure that could eventually support binding commitments, but that transition has not yet occurred.

## Next Steps

**Paris AI Action Summit** (February 2025):
- Focus on AI for public interest
- Further development of international cooperation
- Possible additional commitments

The summit series aims to maintain momentum on international AI governance while the technology and political landscape evolve.


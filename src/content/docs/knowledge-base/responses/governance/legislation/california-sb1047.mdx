---
title: California SB 1047
description: Proposed state legislation for frontier AI safety requirements (vetoed)
sidebar:
  order: 5
---

import { DataInfoBox , PageStatus} from '../../../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="Detailed analysis of California's vetoed SB 1047 frontier AI safety legislation, including its compute thresholds, safety testing requirements, political journey, support/opposition dynamics, and implications for future AI regulation efforts." />

<DataInfoBox entityId="california-sb1047" />

## Summary

SB 1047, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act, was California state legislation that would have required safety testing and liability measures for developers of the most powerful AI models.

The bill **passed the California legislature** but was **vetoed by Governor Gavin Newsom** on September 29, 2024.

SB 1047 was the **most significant AI safety legislation attempted in the United States** to date. Its passage through the legislature demonstrated growing political willingness to regulate frontier AI, while its veto illustrated the political challenges such regulation faces.

## What the Bill Proposed

### Scope: "Covered Models"

The bill would have applied to AI models meeting **any** of these criteria:

**Training Compute:**
- Trained using >10^26 FLOP (floating-point operations)
- Approximately GPT-4.5/Claude 3 Opus scale or larger

**Training Cost:**
- Cost >$100 million to train
- Adjusted annually for inflation

**Fine-tuned Models:**
- Fine-tuning cost >$10 million
- Based on a covered model

**Why these thresholds?**
- Target only frontier models from well-resourced labs
- Exclude open-source models and academic research
- Align with international compute governance efforts (US EO, EU AI Act)

### Core Requirements

#### 1. Safety Testing Before Training

**Pre-Training Requirements:**

Developers must:
- Determine whether model will be a "covered model"
- Implement safety protocols before beginning training
- Establish shutdown procedures

**Covered Model Determination:**

If expected to meet thresholds:
- Document safety plan
- Prepare for testing requirements
- Establish compliance measures

#### 2. Hazardous Capability Testing

**Required Testing:**

Before deployment or making available to third parties, test for:

**Critical Harm Capabilities:**
- Creation of chemical, biological, radiological, or nuclear weapons (CBRN)
- Mass casualty cyber attacks (>$500M damage or mass casualties)
- Autonomous operation and self-exfiltration
- Self-improvement and recursive self-modification

**Testing Methods:**
- Red-team testing
- Adversarial probing
- Capability evaluations
- Third-party auditing

**Threshold:** Model enables non-expert to cause mass casualties or >$500M in damage.

#### 3. Safety and Security Protocol

**Required Measures:**

Developers must implement:

**Cybersecurity:**
- Protection of model weights from theft
- Secure infrastructure
- Incident response plans

**Shutdown Capability:**
- Full model shutdown ability
- Separate from safety fine-tuning
- Effective on all deployed instances

**Ongoing Monitoring:**
- Detection of hazardous use
- Capability creep tracking
- Post-deployment evaluation

**Documentation:**
- Written safety protocol
- Regular updates
- Public summary (redacted for security)

#### 4. Whistleblower Protections

**Employee Rights:**

Protected disclosures about:
- Safety violations
- Unreasonable risk to public
- Non-compliance with the act

**Prohibitions:**
- Cannot retaliate against whistleblowers
- Cannot require non-disclosure preventing safety reports
- Civil penalties for violations

#### 5. Frontier Model Division

**New State Agency:**

Created within California Government Operations Agency:
- Oversee compliance
- Receive safety protocols
- Investigate violations
- Issue guidance

**Powers:**
- Subpoena authority
- Civil penalty assessment
- Emergency orders

#### 6. Liability Framework

**Affirmative Defense:**

Developers protected from liability if:
- Complied with all safety requirements
- Conducted reasonable testing
- Implemented safety protocols
- Acted in good faith

**Strict Liability Removed:**

Does NOT create automatic liability for harms; must prove negligence or non-compliance.

**Attorney General Enforcement:**

California AG can sue for:
- Violations of safety requirements
- Civil penalties up to 10% of training costs
- Injunctive relief

#### 7. Compute Cluster Reporting

**Reporting Requirement:**

Owners of computing clusters with:
- >10^26 integer or floating-point operations per second
- Located in California

Must report to:
- Frontier Model Division
- Information about cluster
- Customers using cluster

**Purpose:** Track who has capability to train covered models.

### Exemptions and Safe Harbors

**Explicitly Exempted:**
- Open-source models (unless developer makes $50M+/year from derivatives)
- Academic research
- Models below thresholds
- Government use

**Safe Harbor:**
- Compliance with safety requirements provides affirmative defense
- Good faith efforts protected

## Path Through Legislature

### Initial Introduction (February 2024)

**Original Sponsors:**
- Senator Scott Wiener (D-San Francisco)
- Support from AI safety advocates
- Opposed by major AI companies and some researchers

### Amendment Process

**Major Changes:**
- Narrowed scope to truly frontier models
- Added safe harbors and affirmative defenses
- Reduced liability provisions
- Clarified open-source exemptions
- Specified hazardous capabilities more precisely

**Purpose of Amendments:**
- Address industry concerns about overbreadth
- Balance innovation and safety
- Build coalition for passage

### Legislative Passage

**August 29, 2024: Passed California Legislature**
- Assembly: 45-11
- Senate: 32-1
- Bipartisan support
- Most significant AI legislation to pass any US state legislature

### Veto (September 29, 2024)

**Governor Newsom's Rationale:**

"While well-intentioned, SB 1047 does not take into account whether an AI system is deployed in high-risk environments, involves critical decision-making or the use of sensitive data. Instead, the bill applies stringent standards to even the most basic functions â€” so long as a large system deploys it."

**Specific Concerns:**
- Focus on model size rather than deployment context
- Could stifle innovation
- Regulatory approach not nuanced enough
- Preferred federal regulation

**Accompanying Actions:**

Newsom simultaneously:
- Signed 18 other AI bills on narrower topics
- Called for federal AI legislation
- Committed to working with legislature on alternative approaches

## Support and Opposition

### Supporters

**AI Safety Organizations:**
- Center for AI Safety
- Future of Life Institute
- AI safety researchers

**Arguments:**
- Frontier models pose catastrophic risks
- Industry self-regulation insufficient
- California can lead on AI safety
- Requirements are reasonable and achievable

**Notable Individual Supporters:**
- Yoshua Bengio (Turing Award winner)
- Geoffrey Hinton (Turing Award winner, "Godfather of AI")
- Stuart Russell (UC Berkeley professor)
- Max Tegmark (MIT professor)

### Opponents

**Major AI Companies:**
- OpenAI
- Anthropic
- Google/DeepMind
- Meta

**Arguments:**
- Stifles innovation
- Drives development out of California
- Premature to regulate
- Better to focus on use cases than model capabilities
- Federal regulation more appropriate

**Venture Capital:**
- Y Combinator
- Andreessen Horowitz
- Others concerned about startup ecosystem impact

**Some Researchers:**
- Yann LeCun (Meta, Turing Award winner)
- Andrew Ng (Stanford, Google Brain co-founder)
- Fei-Fei Li (Stanford)

**Concerns:**
- Open-source implications despite exemptions
- Compliance costs for startups
- Regulatory overreach
- Vague standards

**Labor and Progressive Groups:**
- Some supported
- Some concerned it didn't address labor impacts enough

## Why It Was Vetoed

### Stated Reasons (Governor Newsom)

**Size-Based vs. Risk-Based:**
- Bill focuses on model size (compute/cost) not deployment risks
- Small models in high-risk contexts not covered
- Large models in benign contexts over-regulated

**Innovation Concerns:**
- California is hub of AI development
- Regulation could drive companies elsewhere
- Startups face compliance burdens

**Federal Action Preferable:**
- AI transcends state borders
- National framework more appropriate
- International coordination needed

### Political Analysis

**Industry Pressure:**
- Major AI companies lobbied heavily against
- Economic arguments about California's AI ecosystem
- Threat of relocation

**Presidential Politics:**
- Biden administration developing AI policy
- Harris (VP, former CA Senator) in presidential race
- National Democratic messaging on tech

**Tactical Considerations:**
- Newsom signed 18 other AI bills simultaneously
- Positioned as pro-innovation, pro-safety balance
- Left door open for future iteration

**Lack of Coalition:**
- Many Democrats skeptical
- Republicans opposed
- Labor not fully engaged
- Insufficient grassroots pressure

### Unstated Factors (Analysis)

**Economic:**
- California tax revenue from AI industry significant
- Competition from other states and countries
- Tech industry political influence

**Policy:**
- Precedent-setting implications
- Uncertainty about effectiveness
- Implementation challenges

**Political:**
- Presidential election dynamics
- Tech industry relationships
- Future political ambitions

## Implications for AI Safety Regulation

### What SB 1047 Demonstrated

**Political Will Exists:**
- Bipartisan legislative passage showed AI safety resonates
- Not just fringe concern but mainstream political issue
- Legislators willing to regulate despite industry opposition

**Industry Opposition is Formidable:**
- Even safety-focused companies (Anthropic) opposed
- Economic arguments effective
- Innovation framing powerful

**Federal vs. State Tension:**
- AI is inherently interstate and international
- State-level regulation faces jurisdictional limits
- But federal action is slow

**Details Matter:**
- Size-based vs. risk-based framing was central
- Specific thresholds and requirements heavily debated
- Implementation details crucial to political viability

### Lessons for Future Efforts

#### What Worked

**Focused Scope:**
- Targeting only frontier models built support
- Exemptions for open-source and research
- Concrete thresholds (compute, cost)

**Safety Framing:**
- Catastrophic risk resonated
- Whistleblower protections popular
- Bipartisan appeal

**Expert Endorsement:**
- Turing Award winners lending credibility
- Technical community engagement

#### What Didn't Work

**Industry Consensus:**
- Even safety-concerned labs opposed
- Economic arguments effective
- Innovation framing won

**Implementation Clarity:**
- Vague enforcement mechanisms
- Uncertainty about compliance costs
- Questions about Frontier Model Division capacity

**Coalition Building:**
- Labor not fully engaged
- Grassroots support limited
- Competing priorities on left

### Future Regulatory Approaches

#### State Level

**Narrower Bills:**
- Focus on specific harms (deepfakes, discrimination)
- Deployment context rather than model capabilities
- Procurement standards

**Coordination:**
- Multi-state coordination
- Uniform standards
- Regional compacts

**California Iteration:**
- Newsom committed to continued dialogue
- Future versions possible
- Refined approach incorporating feedback

#### Federal Level

**Legislation:**
- Comprehensive AI safety bill
- Build on Executive Order
- Bipartisan framework

**Challenges:**
- Congressional gridlock
- Lobbying pressure
- Competing priorities

#### International

**Coordination Imperative:**
- AI development global
- Race to the bottom risk
- Need for international standards

**Precedents:**
- EU AI Act as model
- UK approach
- Multilateral frameworks

## Impact on AI Safety Movement

### Positive Effects

**Mainstream Attention:**
- SB 1047 brought frontier AI risk into public discourse
- Media coverage extensive
- Political engagement increased

**Overton Window:**
- Made AI regulation thinkable
- Future efforts less radical by comparison
- Normalized safety concerns

**Community Building:**
- Coalition formation
- Political skills development
- Lessons learned

### Negative Effects

**Backlash:**
- Some researchers now more skeptical of regulation
- "Regulatory capture" accusations
- Polarization on safety issues

**Movement Division:**
- Some AI safety researchers opposed bill
- Tensions over strategy
- Open-source community alienation

**Political Capital:**
- Loss might discourage future efforts
- Industry emboldened
- Harder to argue regulations are inevitable

### Strategic Debates

#### Should SB 1047 Have Been Pursued?

**Arguments For:**
- Only way to test political viability
- Built coalition and momentum
- Shifted discourse even in defeat

**Arguments Against:**
- Premature; should have built more support first
- Better to focus on federal action
- Antagonized potential allies

#### What Should Come Next?

**Double Down:**
- Refine and reintroduce
- Build broader coalition
- Address veto concerns

**Pivot to Federal:**
- Focus energy on Congress
- Support Executive Order implementation
- International coordination

**Focus on Narrower Wins:**
- Procurement standards
- Use-case specific regulation
- Voluntary frameworks

**Build Power:**
- Grassroots organizing
- Labor coalition
- Public education

## Technical and Policy Debates

### Size-Based vs. Risk-Based Regulation

**Size-Based (SB 1047 Approach):**

**Pros:**
- Objective, measurable thresholds
- Targets most capable models
- Easier to enforce
- Aligns with international compute governance

**Cons:**
- Doesn't capture deployment context
- Could miss dangerous applications of smaller models
- Algorithmic efficiency makes thresholds obsolete

**Risk-Based (Newsom's Preference):**

**Pros:**
- Focuses on actual harm potential
- Context-appropriate
- Adapts to changing technology

**Cons:**
- Harder to define and measure
- Enforcement challenges
- Potentially broader scope (privacy, fairness, etc.)
- Risk assessment subjective

**Synthesis Possible:**
- Combination of both approaches
- Size thresholds trigger risk assessments
- Deployment context determines requirements

### Liability Questions

**SB 1047 Approach:**
- Affirmative defense for compliance
- Attorney General enforcement
- Civil penalties

**Debate:**
- Too much liability deters innovation?
- Too little fails to ensure safety?
- Who should bear costs of AI harms?

**Alternative Approaches:**
- Strict liability with caps
- Insurance requirements
- Tiered liability based on precautions
- No-fault compensation schemes

### Open Source Implications

**SB 1047 Exemption:**
- Open-source models exempt unless developer profits >$50M from derivatives

**Concerns Raised:**
- Could still chill open-source development
- Uncertainty about liability
- Derivative work tracking difficult

**Counter-Arguments:**
- Exemption was broad
- Open-source not inherently safe
- Need some oversight of powerful models

**Ongoing Debate:**
- How to encourage open research while managing risks
- Different models for different risk levels
- Role of open-source in AI safety ecosystem

## Comparison to Other Policies

### vs. US Executive Order

**Similarities:**
- Compute thresholds (10^26 FLOP)
- Safety testing requirements
- Focus on frontier models

**Differences:**
- SB 1047 had enforcement teeth (civil penalties)
- EO has broader scope (government use, competition)
- SB 1047 state-level, EO federal executive action
- SB 1047 required shutdown capability

**Relationship:**
- SB 1047 would have complemented EO
- State enforcement of federal principles
- Potential model for other states

### vs. EU AI Act

**EU Act:**
- Risk categories for deployed systems
- Broader scope (not just frontier models)
- Binding regulation with large fines

**SB 1047:**
- Narrower focus on frontier models
- More specific technical requirements (shutdown, testing)
- State-level vs. EU-wide

**Lessons:**
- EU's comprehensiveness politically difficult in US
- SB 1047's focused approach still failed
- Suggests US regulation will be patchwork

### vs. Voluntary Commitments

**Industry Commitments:**
- No enforcement
- Self-defined standards
- Flexible and adaptive

**SB 1047:**
- Mandatory requirements
- State enforcement
- Specific standards

**Debate:**
- Is voluntary compliance sufficient?
- Does regulation stifle beneficial innovation?
- Can industry self-regulate emerging risks?

## Career and Research Implications

### Policy Careers

**Lessons Learned:**
- Understanding legislative process crucial
- Coalition building essential
- Technical expertise must translate to policy

**Opportunities:**
- State-level AI policy growing
- Need for policy entrepreneurs
- Legislative staff positions

### Research Questions

**Regulatory Design:**
- How to balance innovation and safety?
- What thresholds are appropriate?
- How to make regulation adaptive?

**Political Economy:**
- Industry influence on regulation
- Public opinion on AI risk
- Coalition formation strategies

**Technical:**
- Measuring model capabilities
- Shutdown mechanisms
- Audit methodologies

### Movement Building

**Strategic Questions:**
- When to push for regulation vs. build support?
- How to engage industry productively?
- Building public constituency

**Skills Needed:**
- Political strategy
- Coalition management
- Communications
- Policy design


---
title: Failed and Stalled AI Policy Proposals
description: AI governance initiatives that were rejected, vetoed, or stalled
sidebar:
  order: 15
---

import { DataInfoBox , PageStatus} from '../../../../../../components/wiki';

<PageStatus quality={3} lastEdited="2025-12-24" llmSummary="Analysis of failed AI governance proposals including California SB 1047, federal AI legislation, and international treaty efforts, identifying common failure patterns like industry opposition, definitional challenges, and jurisdictional complexity." todo="Add more recent examples and expand analysis of successful vs. failed characteristics" />

<DataInfoBox entityId="failed-stalled-proposals" />

## Summary

Understanding why AI governance proposals fail is as important as understanding successes. Failed efforts reveal political constraints, industry opposition patterns, and the challenges of regulating rapidly evolving technology.

## United States

### California SB 1047 (Vetoed, 2024)

**What it proposed**: Safety testing requirements for frontier AI models above compute/cost thresholds; liability for critical harms; shutdown capability requirements.

**Outcome**: Passed California legislature; vetoed by Governor Newsom on September 29, 2024.

**Why it failed**:
- **Industry opposition**: Major tech companies (Google, Meta, OpenAI) opposed; Anthropic initially supportive but later ambiguous
- **Federal preemption concerns**: Governor cited preference for federal regulation
- **Innovation fears**: Claims it would drive AI development out of California
- **Startup concerns**: Worry about compliance burden despite exemptions
- **Definitional challenges**: Difficulty defining "covered models" precisely

**Lessons learned**:
- State-level frontier AI regulation faces federal preemption arguments
- Industry can mobilize effectively against specific legislation
- Liability provisions are particularly contentious
- Bipartisan support is possible but not sufficient

[Full analysis â†’](/knowledge-base/policies/california-sb1047)

### Federal AI Legislation (Stalled, 2023-2024)

**What was proposed**: Multiple federal AI bills introduced in Congress:
- Algorithmic Accountability Act
- AI Labeling Act
- SAFE Innovation Framework
- National AI Commission Act

**Outcome**: No comprehensive federal AI legislation passed as of late 2024.

**Why stalled**:
- **Congressional dysfunction**: General difficulty passing legislation
- **Definitional debates**: No consensus on what counts as "AI"
- **Jurisdictional conflicts**: Multiple committees claim authority
- **Industry lobbying**: Effective opposition to binding requirements
- **Partisan divisions**: Different priorities (safety vs. competition vs. civil rights)

### NIST AI Safety Consortium Mandatory Participation (Not Adopted)

**What was proposed**: Making participation in AISIC mandatory for government contractors.

**Outcome**: Remained voluntary.

**Why not adopted**:
- Industry preference for voluntary approaches
- Concerns about excluding smaller participants
- Questions about NIST's enforcement capacity

## International

### UN Binding AI Treaty (Not Achieved)

**What was proposed**: Binding international agreement on AI governance, analogous to nuclear or biological weapons treaties.

**Outcome**: UN activities have produced reports and advisory bodies but no binding treaty.

**Why not achieved**:
- **Great power competition**: US-China tensions prevent cooperation
- **Definitional challenges**: No agreement on what to regulate
- **Verification difficulties**: How would compliance be monitored?
- **Speed of development**: Technology moves faster than diplomacy
- **Sovereignty concerns**: Nations resist external constraints on strategic technology

### G7 AI Code of Conduct (Limited Impact)

**What was proposed**: The October 2023 Hiroshima AI Process produced a voluntary code of conduct for AI developers.

**Outcome**: Published but with limited observable impact on industry behavior.

**Why limited impact**:
- Voluntary and non-binding
- Vague commitments without specific requirements
- No enforcement mechanism
- Industry already making similar voluntary commitments

### EU AI Act Narrow Scope for Foundation Models (Partially)

**What was proposed**: Early drafts of EU AI Act focused primarily on specific use cases, not foundation models.

**Outcome**: Final act included foundation model provisions, but some argue they remain insufficient.

**Contested areas**:
- Definition of "systemic risk" models
- Open source exemptions
- Enforcement timelines

## Common Failure Patterns

### 1. Industry Opposition

Most failed proposals faced significant industry lobbying:
- **Direct lobbying**: Company representatives meeting with legislators
- **Astroturfing**: Industry-funded groups appearing grassroots
- **Job threat narratives**: Claims regulation will eliminate jobs or drive companies away
- **Innovation rhetoric**: Framing regulation as anti-innovation

### 2. Definitional Challenges

AI is hard to define legally:
- What distinguishes AI from traditional software?
- How to define "frontier" or "advanced" AI?
- How to future-proof definitions as technology evolves?

### 3. Jurisdictional Complexity

Multiple regulatory bodies claim authority:
- In US: FTC, SEC, CFTC, FDA, state AGs, and more
- Internationally: No clear lead institution
- Creates regulatory gaps and inconsistency

### 4. Speed Mismatch

Technology evolves faster than policy:
- By the time legislation passes, it may address yesterday's problems
- Rapid capability improvements outpace regulatory understanding
- New modalities (multimodal, agents) create new challenges

### 5. Collective Action Problems

Even when stakeholders agree on goals:
- First-mover disadvantage for strict regulators
- Race to the bottom dynamics internationally
- Difficulty coordinating across jurisdictions

## What Successful Proposals Share

Looking at what passes vs. what fails:

| Successful | Failed |
|-----------|--------|
| Narrow scope | Broad scope |
| Existing institutional authority | New institutional requirements |
| Industry-acceptable | Strong industry opposition |
| Bipartisan or non-partisan framing | Partisan coding |
| Clear definitions | Ambiguous terms |
| Disclosure requirements | Liability provisions |
| Voluntary or soft law | Hard mandates |

## Implications

The pattern of failures suggests:

1. **Incremental approaches** are more likely to succeed than comprehensive frameworks
2. **Disclosure and transparency** requirements face less opposition than liability
3. **Voluntary commitments** may be necessary stepping stones to mandatory regulation
4. **State-level action** in the US faces federal preemption challenges
5. **International coordination** requires US-China cooperation, which is currently lacking


---
title: Failed and Stalled AI Policy Proposals
description: Analysis of failed AI governance initiatives reveals systematic patterns including industry opposition spending $100-200M annually, definitional challenges, jurisdictional complexity, and fundamental mismatches between technology development speed and legislative cycles. While comprehensive frameworks like California's SB 1047 face vetoes, incremental approaches with industry support show higher success rates, suggesting voluntary commitments may be necessary stepping stones to binding regulation.
sidebar:
  order: 15
quality: 5
llmSummary: Analyzes patterns in failed AI governance proposals including
  California's SB 1047 veto and stalled federal legislation, finding that
  incremental approaches with industry support are more likely to succeed than
  comprehensive frameworks (50+ federal bills introduced with under 5% passage rate).
  Documents systematic industry opposition spending $100-200M annually and
  identifies key failure patterns including definitional challenges,
  jurisdictional complexity, and speed mismatches between technology and policy
  development.
lastEdited: "2025-12-27"
importance: 75.2
---

import {DataInfoBox} from '../../../../../components/wiki';

<DataInfoBox entityId="failed-stalled-proposals" />

## Overview

Failed and stalled AI policy proposals provide critical insights into the political economy of AI governance, revealing systematic patterns that explain why comprehensive regulation remains elusive despite widespread concern about AI risks. The failure rate for ambitious AI legislation is remarkably high, with over 50 federal AI bills introduced in the US Congress between 2023-2024 achieving less than a 5% passage rate, while industry opposition spending has reached an estimated $100-200 million annually across federal and state lobbying efforts.

These failures illuminate fundamental tensions in AI governance: the speed mismatch between rapid technological development and deliberative legislative processes, the challenge of defining "artificial intelligence" in legally precise terms, and the complex jurisdictional landscape where multiple agencies and levels of government claim regulatory authority. Perhaps most significantly, failed proposals demonstrate how industry opposition mobilizes around specific regulatory mechanisms, particularly liability provisions and mandatory compliance requirements, while showing greater tolerance for disclosure obligations and voluntary frameworks.

The pattern of failures suggests that successful AI governance may require accepting incremental progress rather than comprehensive solutions, with voluntary industry commitments serving as necessary stepping stones to eventual binding regulation. This dynamic has profound implications for AI safety, as it may mean that meaningful oversight emerges only after significant harms occur, rather than through proactive prevention.

## Major Failed Initiatives

### California SB 1047: A Case Study in State-Level Challenges

California's Safe and Secure Innovation for Frontier Artificial Intelligence Models Act represented the most ambitious state-level AI regulation attempted in the United States. Introduced by Senator Scott Wiener in February 2024, the bill established safety testing requirements for AI models trained with more than $100 million in compute resources or equivalent computational power, required developers to implement shutdown capabilities and conduct red-team evaluations before deployment, and created potential liability for developers whose models caused critical harms defined as mass casualties, critical infrastructure damage, or economic losses exceeding $500 million.

The bill passed both houses of the California legislature with bipartisan support, demonstrating that concerns about AI risks transcend traditional partisan divides. However, Governor Gavin Newsom's veto on September 29, 2024, revealed the complex political pressures surrounding AI regulation. Industry opposition was swift and coordinated, with major technology companies including Google, Meta, and OpenAI actively lobbying against the legislation. Anthropic's position proved particularly influential—initially supportive of the bill's safety objectives, the company later advocated for significant modifications that effectively withdrew its endorsement.

The veto message highlighted several critical vulnerabilities in state-level AI regulation. Newsom cited federal preemption concerns, arguing that AI regulation required national coordination rather than a patchwork of state laws. This position reflected broader constitutional questions about states' authority to regulate technologies with interstate and international implications. The Governor also emphasized innovation concerns, suggesting that strict regulation might drive AI development out of California and undermine the state's technological leadership.

The SB 1047 failure revealed how definitional challenges plague AI legislation. The bill's reliance on compute thresholds and cost metrics created apparent precision but masked underlying questions about what constitutes a "frontier" AI model. As capabilities improve and costs decline, static definitions risk becoming either overly broad or quickly obsolete. Industry critics successfully argued that such rigid criteria could not anticipate future technological developments or distinguish between beneficial and harmful applications.

### Federal AI Legislation: Congressional Gridlock and Competing Priorities

The 118th Congress saw unprecedented introduction of AI-related legislation, with over 50 bills addressing various aspects of artificial intelligence governance, yet comprehensive federal AI regulation remained stalled. Major proposals included the Algorithmic Accountability Act, which would have required automated decision system impact assessments for large companies, the AI Labeling Act mandating disclosure when AI generates content, and the SAFE Innovation Framework attempting to establish regulatory sandboxes for AI development.

Congressional dysfunction played a significant role in these failures, reflecting broader challenges in American legislative processes rather than AI-specific issues. However, AI legislation faced unique obstacles including definitional debates that prevented consensus on regulatory scope, jurisdictional conflicts between committees claiming authority over different aspects of AI governance, and the challenge of addressing rapidly evolving technology within traditional legislative timelines.

Industry lobbying proved particularly effective at the federal level, where technology companies deployed sophisticated strategies combining direct congressional engagement, grassroots mobilization, and technical expertise provision. The narrative that regulation would harm American competitiveness against China proved especially potent, framing AI governance as a zero-sum competition rather than a shared safety challenge. This dynamic was evident in debates over the National AI Commission Act, where initial proposals for independent oversight were diluted into advisory functions with limited authority.

The stalled federal legislation also reflected deeper partisan divisions about the appropriate role of government in technology regulation. Republican lawmakers generally favored market-driven approaches and expressed skepticism about new regulatory authorities, while Democrats emphasized civil rights protections and algorithmic bias concerns. These different priorities made comprehensive legislation difficult to construct and sustain.

### International Treaty Efforts: Great Power Competition and Sovereignty Concerns

International efforts to establish binding AI governance agreements have consistently failed to achieve meaningful progress, despite widespread recognition that AI's global nature requires coordinated responses. The United Nations' AI governance activities have produced multiple reports and advisory bodies, including the AI Advisory Body's interim and final reports, but no binding treaty or enforcement mechanism has emerged.

Great power competition, particularly US-China tensions, represents the primary obstacle to international AI agreements. Both nations view AI capabilities as strategic assets essential to military and economic competitiveness, making them reluctant to accept external constraints on domestic AI development. This dynamic parallels Cold War nuclear negotiations but with added complexity from AI's civilian applications and the difficulty of distinguishing between beneficial and harmful AI capabilities.

Verification challenges compound these political obstacles. Unlike nuclear or chemical weapons, AI capabilities are largely software-based and can be rapidly modified or concealed. International monitoring would require unprecedented access to corporate research facilities and source code, raising both security and intellectual property concerns. The dual-use nature of most AI research makes it difficult to identify which activities warrant international oversight.

The G7's Hiroshima AI Process exemplifies the limitations of voluntary international approaches. While the October 2023 code of conduct received significant attention, observable changes in industry behavior remain minimal. Companies were already making similar voluntary commitments through initiatives like the Partnership on AI, and the G7 framework added little concrete accountability. The voluntary nature of these agreements means compliance depends entirely on corporate self-interest rather than binding obligations.

## Systematic Failure Patterns

### Industry Opposition Architecture

Analysis of failed AI proposals reveals sophisticated industry opposition strategies that go beyond traditional lobbying. Technology companies have invested heavily in policy expertise, hiring former government officials and establishing dedicated government relations teams. This investment has created an asymmetric information advantage, where industry representatives often possess deeper technical knowledge than legislative staff, allowing them to shape debates around implementation feasibility and unintended consequences.

The "innovation flight" narrative has proven particularly effective, warning that strict regulation will drive AI development to more permissive jurisdictions. This argument resonates with policymakers concerned about economic competitiveness, particularly in states like California that depend heavily on technology sector employment. However, empirical evidence for this claim remains limited—financial services and pharmaceutical companies continue to invest heavily in highly regulated jurisdictions when market opportunities justify compliance costs.

Astroturfing efforts have supplemented direct lobbying, with industry-funded organizations presenting themselves as grassroots voices for innovation or consumer interests. These groups amplify concerns about regulatory overreach while obscuring their corporate funding sources. The proliferation of such organizations makes it difficult for policymakers to assess genuine public opinion versus manufactured opposition.

Industry opposition also exploits the collective action problem in AI safety. Even companies that privately acknowledge safety risks may oppose regulation if competitors could gain advantages by avoiding compliance costs. This dynamic suggests that voluntary industry initiatives, while valuable for norm-setting, may be insufficient for addressing systemic risks that require universal participation.

### Definitional and Technical Challenges

The failure to establish precise, legally enforceable definitions of artificial intelligence has undermined numerous regulatory proposals. Traditional legal frameworks assume clear categorical boundaries, but AI exists on a spectrum of capabilities that resist binary classification. The European Union's AI Act attempted to address this challenge through risk-based categorization, but implementation guidance reveals ongoing struggles with edge cases and technological evolution.

Technical complexity creates additional barriers to effective regulation. Many AI governance proposals rely on metrics like computational resources or model parameters that may not correlate with actual capabilities or risks. The focus on "frontier" or "advanced" AI systems often assumes linear progression in capabilities, when breakthrough developments may emerge from unexpected research directions or architectural innovations.

The dual-use nature of AI technology complicates regulatory design, as the same underlying capabilities can enable both beneficial and harmful applications. Unlike nuclear technology, where weapons applications are clearly distinguishable from civilian uses, AI systems designed for legitimate purposes can be adapted for malicious ends with minimal modification. This reality makes it difficult to craft regulations that prevent harm without stifling beneficial innovation.

Speed of technological development outpaces regulatory comprehension, creating a persistent knowledge gap between cutting-edge AI capabilities and policymaker understanding. By the time comprehensive legislation addresses current AI systems, the technology has evolved in ways that make existing frameworks obsolete. This dynamic suggests need for more adaptive regulatory approaches, but such flexibility conflicts with legal requirements for predictability and due process.

### Jurisdictional Complexity and Coordination Failures

Failed AI proposals often founder on jurisdictional complexity, with multiple agencies and levels of government claiming overlapping authority. In the United States, the Federal Trade Commission, Securities and Exchange Commission, Food and Drug Administration, and various other agencies all assert relevance to AI governance within their respective domains. This fragmentation creates regulatory gaps where harmful activities fall between jurisdictions, while also enabling forum shopping by companies seeking the most permissive oversight.

State versus federal tensions have proven particularly problematic for comprehensive AI regulation. Federal preemption arguments against state initiatives like SB 1047 assume that uniform national standards are preferable to regulatory experimentation, but federal inaction leaves this assumption untested. The resulting stalemate benefits companies that prefer regulatory uncertainty to clear but demanding requirements.

International coordination failures reflect deeper structural problems in global governance systems designed for a world of discrete nation-states rather than borderless digital technologies. Existing international institutions lack both technical expertise and enforcement authority to address AI governance effectively. Treaty-making processes that require consensus among hundreds of nations are poorly suited to rapidly evolving technologies that demand quick responses to emerging risks.

The failure of international coordination also stems from asymmetric capabilities and interests among nations. Countries with advanced AI industries have different priorities from those primarily concerned about being subject to others' AI systems. This dynamic creates resistance to governance frameworks that might constrain technological leaders while providing insufficient protection for other nations.

## Successful Approaches and Lessons Learned

### Incremental Progress Over Comprehensive Frameworks

Analysis of successful AI governance initiatives reveals that incremental approaches with limited scope achieve higher passage rates than comprehensive frameworks attempting to address multiple AI governance challenges simultaneously. Executive orders like President Biden's October 2023 AI Executive Order succeed by building on existing regulatory authorities rather than creating new ones, while narrow sectoral regulations addressing specific applications (like AI in hiring or medical devices) face less opposition than broad technology mandates.

Disclosure requirements prove more politically viable than liability provisions or performance mandates. Requirements for algorithmic transparency or AI-generated content labeling typically generate less industry opposition than rules imposing legal responsibility for harmful outcomes. This pattern suggests that information-based interventions may be necessary precursors to more substantive regulatory obligations.

Voluntary frameworks often serve as stepping stones to mandatory requirements, allowing industry to demonstrate either compliance or inadequacy of self-regulation. The development of technical standards through organizations like NIST provides foundations for future regulatory requirements while building consensus around best practices. However, the timeline for this progression remains uncertain and may depend on catalyzing events that shift political incentives.

Multi-stakeholder approaches that include industry participation from early stages show higher success rates than adversarial regulatory processes. The UK's AI Safety Institute model of collaborative risk assessment and the EU's approach of extensive industry consultation during AI Act development both demonstrate how procedural inclusion can build legitimacy for substantive requirements. However, such approaches risk capture by well-resourced industry participants unless carefully designed to include diverse perspectives.

### Building Coalitions and Managing Opposition

Successful AI governance initiatives typically build broad coalitions that include both technology industry participants and civil rights advocates, rather than relying solely on safety-focused arguments. Bipartisan framing that emphasizes economic competitiveness, national security, and innovation leadership alongside safety concerns proves more durable than approaches that position regulation as primarily about constraining industry.

The role of catalyzing events in overcoming political resistance cannot be understated. Financial crisis prompted financial services regulation, data breaches enabled privacy legislation, and algorithmic bias scandals facilitated AI transparency requirements. However, relying on such events for regulatory progress means that governance often lags behind harm, making proactive approaches more desirable but politically more difficult.

Technical communities within industry can serve as allies for safety-focused regulation when their concerns align with external advocacy. AI safety researchers within major technology companies often share external researchers' concerns about risks, though their ability to influence corporate positions may be limited. Building relationships with these internal allies can provide valuable intelligence about industry positions and potentially moderate opposition to reasonable regulatory proposals.

International coordination can strengthen domestic regulatory efforts by reducing concerns about competitive disadvantage. The EU AI Act's passage made it easier for other jurisdictions to consider similar requirements, as companies were already adapting to European standards. However, this dynamic depends on major markets taking initial regulatory steps, creating first-mover disadvantages that must be overcome through political leadership.

## Current State and Future Trajectory

### Near-Term Prospects (2025-2026)

The immediate trajectory of AI governance depends heavily on several key variables that will largely determine whether the pattern of failure continues or shifts toward more successful regulatory outcomes. The 2024 US elections and subsequent congressional composition will significantly influence federal AI legislation prospects, with different electoral outcomes suggesting vastly different regulatory approaches. A continuation of divided government likely means persistent gridlock on comprehensive AI legislation, while unified party control could enable either aggressive regulation or systematic deregulation depending on which party prevails.

State-level initiatives appear more likely to succeed in the near term, particularly in jurisdictions where SB 1047's failure demonstrated the political viability of AI safety concerns even when specific legislation fails. Colorado's AI bias law and similar proposals in New York and Illinois suggest that narrower, use-case specific regulation may achieve passage where comprehensive frameworks fail. However, the federal preemption argument that contributed to SB 1047's veto remains a significant challenge for ambitious state-level initiatives.

Industry positions show signs of evolution, with some major AI developers acknowledging the inevitability of regulation and seeking to influence its form rather than prevent it entirely. This shift from opposition to engagement could reduce the systematic resistance that has characterized failed proposals, though it may also lead to industry capture of regulatory processes. The key question is whether this engagement represents genuine acceptance of safety constraints or strategic positioning to minimize regulatory burden.

International coordination faces continued challenges from great power competition, but the proliferation of national AI strategies and governance initiatives may create opportunities for bottom-up coordination around technical standards and best practices. The AI Safety Institutes network, initiated by the UK and now including multiple countries, represents a promising model for technical cooperation that avoids the political challenges of binding agreements.

### Medium-Term Evolution (2025-2030)

The medium-term trajectory of AI governance will likely be shaped by whether voluntary industry commitments prove adequate to address emerging risks or whether their limitations become apparent through failure to prevent harmful incidents. Current voluntary frameworks rely heavily on corporate self-assessment and public commitments without independent verification mechanisms. If these approaches prove inadequate—either through obvious failures or more subtle erosion of safety practices under competitive pressure—political support for mandatory regulation may increase substantially.

Technological developments over this period will significantly influence regulatory approaches, potentially making current debates obsolete. The emergence of artificial general intelligence or highly capable autonomous systems could create risks that dwarf current concerns, shifting political calculations about acceptable regulatory costs. Conversely, if AI capabilities plateau or prove more manageable than current concerns suggest, the urgency driving regulatory efforts may diminish.

The international landscape may evolve toward greater fragmentation or coordination depending on geopolitical developments. Continued US-China competition suggests persistent barriers to comprehensive international agreements, but shared interests in preventing catastrophic risks could enable limited cooperation. The development of technical standards and safety practices through multilateral institutions may provide foundations for future coordination even without binding treaties.

Liability frameworks represent a critical uncertainty that could fundamentally alter the regulatory landscape. Current opposition to liability provisions reflects uncertainty about AI capabilities and appropriate responsibility allocation, but major AI-caused harms could shift both public opinion and legal precedent toward stricter accountability. The development of case law around AI liability through tort litigation may provide de facto regulation even without comprehensive legislation.

### Long-Term Uncertainties (2030+)

The long-term trajectory of AI governance depends on fundamental questions about technological development, international order, and democratic governance that remain deeply uncertain. If AI development continues its current trajectory toward increasingly capable and general systems, the stakes for governance failures may become existentially high, potentially overcoming political obstacles that currently prevent ambitious regulation. However, if AI capabilities stabilize or develop along different trajectories than currently anticipated, existing governance approaches may prove adequate or require fundamental reconceptualization.

The relationship between democratic governance and AI oversight presents particular challenges that may reshape political systems themselves. AI's speed and complexity may exceed the capacity of traditional democratic deliberation, potentially requiring new institutions or decision-making processes. The failure of current governance approaches may reflect not just political obstacles but fundamental limitations of existing democratic institutions when confronting rapidly evolving technological risks.

International governance may require new institutions and approaches that go beyond current state-centric models. The failure of traditional treaty-making processes for AI governance suggests need for more adaptive and technically informed international coordination mechanisms. Whether existing international institutions can evolve to meet these challenges or whether new forms of global governance will emerge remains an open question with profound implications for AI safety and human welfare.

The ultimate success or failure of AI governance may depend on whether humanity can develop institutional innovations that match the pace and complexity of technological development while preserving democratic accountability and human agency. The pattern of current failures suggests that existing approaches are inadequate, but whether better alternatives will emerge before they become urgently necessary remains uncertain.
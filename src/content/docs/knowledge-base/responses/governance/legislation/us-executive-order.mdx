---
title: US Executive Order on AI
description: October 2023 executive order establishing compute-based reporting thresholds, AI Safety Institute, and comprehensive governance framework for frontier AI systems, representing the most significant US government AI regulation to date despite enforcement limitations as an executive action.
sidebar:
  order: 4
quality: 5
llmSummary: Comprehensive analysis of the October 2023 US Executive Order on AI, detailing compute-based reporting thresholds (10^26 FLOP for frontier models, 10^23 for bio-capable models), establishment of the US AI Safety Institute, and cloud compute governance requirements. Assessment indicates medium regulatory scope covering 15-20 frontier developers globally but low enforcement strength and durability due to executive order limitations.
lastEdited: "2025-12-27"
importance: 85
---

import {DataInfoBox, Backlinks} from '../../../../../components/wiki';

<DataInfoBox entityId="us-executive-order" />

## Overview

The Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, signed by President Biden on October 30, 2023, represents the most comprehensive federal response to AI governance in US history. The 111-page directive establishes mandatory reporting requirements for frontier AI systems, creates new oversight institutions, and addresses both immediate risks like algorithmic bias and long-term catastrophic risks from advanced AI capabilities. With over 100 specific directives across 8 federal agencies, the order attempts to balance promoting American AI leadership while mitigating potential harms.

The order's significance lies in its dual approach: addressing near-term AI deployment issues through civil rights protections and government modernization, while simultaneously creating infrastructure for overseeing the development of increasingly powerful AI systems. By establishing compute-based thresholds rather than capability-based triggers, it provides concrete, measurable criteria that developers must follow. However, as an executive action rather than legislation, its durability depends on political continuity and lacks strong enforcement mechanisms.

For AI safety, the order represents both progress and limitations. It normalizes government oversight of frontier AI development and creates institutional capacity through the US AI Safety Institute. Yet it primarily focuses on transparency and voluntary cooperation rather than mandatory safety requirements or deployment restrictions. The order's ultimate impact will depend on implementation effectiveness, international coordination, and whether future administrations maintain or strengthen its provisions.

## Key Provisions and Mechanisms

### Compute-Based Reporting Framework

The order's most innovative feature is its use of computational thresholds to trigger regulatory requirements. Companies training models using more than 10^26 floating-point operations (FLOP) must notify the Department of Commerce before and during training, share safety testing results, and provide detailed information about model capabilities, cybersecurity measures, and red-team testing outcomes. This threshold, equivalent to approximately 10 times the compute used for GPT-4, captures current and next-generation frontier models while avoiding over-regulation of smaller systems.

A separate 10^23 FLOP threshold applies specifically to models trained on biological sequence data, reflecting concerns that even smaller models could assist in bioweapon development. This represents about 1,000 times less compute than the general threshold, acknowledging that biological design capabilities may emerge at lower scales than general intelligence capabilities. The dual-threshold approach demonstrates sophisticated thinking about different risk profiles across AI application domains.

The compute-based approach offers several advantages over capability-based regulations. FLOP measurements are objective and difficult to manipulate, unlike subjective assessments of AI capabilities that might vary across evaluators. The thresholds also provide predictability for developers, allowing them to understand regulatory requirements before beginning training runs. However, the static nature of these numbers creates risks of obsolescence as algorithmic efficiency improves and new architectures emerge.

### Institutional Infrastructure Creation

The order establishes the US AI Safety Institute (AISI) within the National Institute of Standards and Technology, tasked with developing evaluation methodologies, conducting safety assessments, and coordinating with international partners. Unlike purely advisory bodies, AISI has operational responsibilities including direct testing of frontier models and developing technical standards for the broader AI ecosystem. The institute represents an attempt to build government capacity that matches the technical sophistication of private AI laboratories.

AISI's creation parallels the UK's AI Safety Institute, established months earlier, reflecting a coordinated approach among allied nations. The two institutes have signed cooperation agreements and are developing shared evaluation frameworks, potentially creating international standards for AI safety assessment. This bilateral foundation could expand to include other nations, creating a network of government institutions capable of oversight as AI capabilities advance.

However, AISI faces significant resource constraints relative to its mandate. Initial funding appears insufficient for the institute's ambitious goals, and recruiting technical talent to compete with private sector compensation remains challenging. The institute's effectiveness will depend on sustained political support and adequate funding, both uncertain given the executive order's status rather than legislative authorization.

### Cloud Compute Governance

The order introduces "Know Your Customer" requirements for Infrastructure-as-a-Service providers, mandating that cloud computing companies verify the identity of foreign customers and monitor large training runs. This provision addresses concerns that adversaries might use US computing infrastructure to develop dangerous AI capabilities without oversight. Cloud providers must implement reporting systems for suspicious activity and large-scale training by foreign entities.

These requirements reflect recognition that compute infrastructure represents a chokepoint in AI development that the US can potentially control. By leveraging American companies' dominance in cloud computing, the order extends US regulatory reach to foreign AI developers who rely on American infrastructure. This approach complements export controls on AI chips by creating oversight even when controlled hardware reaches overseas customers.

The practical implementation of cloud compute governance faces several challenges. Defining "large training runs" in real-time requires technical sophistication from cloud providers, who must distinguish AI training from other compute-intensive applications. Moreover, determined adversaries might circumvent these requirements by using non-US cloud providers or developing domestic computing capabilities, limiting the long-term effectiveness of this approach.

## Safety Implications and Risk Assessment

### Promising Aspects for AI Safety

The order's most significant safety contribution is establishing the principle that frontier AI development requires government oversight. By creating mandatory reporting requirements and institutional evaluation capacity, it moves beyond purely voluntary industry commitments toward structured accountability. The compute-based thresholds provide objective criteria that avoid subjective judgments about AI capabilities while capturing systems of genuine concern.

The institutional infrastructure created by the order builds long-term capacity for AI governance that could prove crucial as capabilities advance. AISI's technical expertise and evaluation methodologies may become essential tools for assessing increasingly powerful systems. The institute's international coordination role also creates foundations for global governance frameworks that could address catastrophic risks requiring multilateral cooperation.

The order's breadth across multiple risk categories—from algorithmic bias to national security threats—reflects sophisticated understanding of AI's diverse impact pathways. By addressing both immediate harms and long-term risks simultaneously, it avoids the false dichotomy between near-term and existential AI safety concerns. The integration of fairness, security, and catastrophic risk considerations within a single framework could prove influential for future governance approaches.

### Concerning Limitations

Despite its comprehensive scope, the order lacks mechanisms to actually prevent the development or deployment of dangerous AI systems. The reporting requirements provide visibility but not control, and the order includes no authority to pause training runs or restrict model releases based on safety concerns. This represents a fundamental limitation for addressing catastrophic risks that might emerge from future AI systems.

The voluntary nature of many provisions weakens the order's potential effectiveness. While reporting requirements are mandatory, many safety-related provisions rely on industry cooperation rather than enforceable mandates. Companies that choose not to comply face unclear consequences, undermining the order's credibility as a regulatory framework. The absence of specified penalties or enforcement mechanisms reflects the limited authority available through executive action.

The order's durability remains highly uncertain given its status as executive action rather than legislation. Future administrations could modify or revoke its provisions entirely, creating regulatory uncertainty that might discourage long-term compliance investments. This political fragility represents a significant weakness for addressing long-term AI risks that require sustained governance approaches spanning multiple electoral cycles.

## Implementation Progress and Trajectory

### Current State (Late 2024)

Implementation has proceeded with mixed results across different provisions. The US AI Safety Institute has been established and staffed with approximately 40-50 researchers, though this remains well below the estimated 200+ personnel needed for full operational capacity. NIST has published initial guidance on AI risk management frameworks, and the Department of Commerce has established reporting procedures for large training runs. Major US AI laboratories have begun submitting required reports, demonstrating basic compliance with the order's most concrete requirements.

Cloud computing governance requirements are in various stages of implementation across different providers. Amazon Web Services, Microsoft Azure, and Google Cloud Platform have developed Know Your Customer frameworks for large compute customers, though the technical details of monitoring and reporting systems remain under development. The practical effectiveness of these measures in detecting and deterring malicious use remains largely untested.

Federal agency compliance with the order's government modernization requirements shows significant variation. While most agencies have designated Chief AI Officers as required, the quality and authority of these positions varies considerably. Some agencies have made substantial progress on AI governance frameworks, while others appear to be treating compliance as a bureaucratic exercise rather than substantive policy implementation.

### 1-2 Year Outlook

Over the next two years, the order's effectiveness will likely depend primarily on resource allocation and political continuity. If AISI receives adequate funding and can recruit sufficient technical talent, it may begin conducting meaningful evaluations of frontier AI systems and developing influential safety standards. However, current funding levels suggest the institute will remain under-resourced relative to its ambitious mandate, potentially limiting its impact on industry practices.

The 2024 election results will significantly influence the order's trajectory. A new administration might maintain, modify, or revoke its provisions depending on political priorities and industry lobbying. Even if maintained, shifting priorities could redirect focus away from catastrophic risk concerns toward economic competitiveness or other objectives. The order's survival and effectiveness depend critically on sustained political commitment to AI governance.

International coordination efforts may show more progress in this timeframe, particularly with allied nations facing similar AI governance challenges. Expanded cooperation between US and UK AI Safety Institutes could create momentum for broader international standards. However, achieving binding international agreements on AI governance remains unlikely in the near term, limiting coordination to voluntary frameworks and information sharing arrangements.

### 2-5 Year Outlook

In the medium term, the order's legacy will likely depend on whether it catalyzes broader governance frameworks or remains an isolated executive action. If successful, it could provide a foundation for congressional legislation that addresses its enforcement limitations while preserving its institutional innovations. Legislative action would significantly enhance the framework's durability and effectiveness, particularly if it includes meaningful penalties for non-compliance.

The compute thresholds established by the order will likely require updating within this timeframe as algorithmic efficiency improves and new architectures emerge. Current estimates suggest the 10^26 FLOP threshold may become obsolete within 3-5 years if efficiency gains continue at historical rates. Maintaining effective oversight will require either regular threshold updates or transition to capability-based triggers, both presenting significant technical and political challenges.

The broader AI governance ecosystem will likely evolve significantly during this period, potentially superseding many of the order's specific provisions. European Union AI Act implementation, Chinese regulatory developments, and industry self-governance initiatives will create a complex landscape of overlapping requirements. The order's ultimate significance may lie more in establishing precedents for government AI oversight than in its specific technical provisions.

## Key Uncertainties and Evaluation Challenges

### Enforcement Effectiveness

The order's impact depends critically on enforcement mechanisms that remain largely untested. While reporting requirements appear to generate compliance from major laboratories, the absence of specified penalties creates uncertainty about consequences for non-compliance. The government's actual willingness and ability to use available tools like contract restrictions or export controls to enforce AI safety requirements remains unknown.

Industry cooperation with voluntary provisions presents another significant uncertainty. Many safety-related requirements depend on companies implementing best practices without clear legal obligations. Whether competitive pressures will undermine voluntary compliance as AI capabilities advance remains an open question, particularly for companies facing intense competition or international rivals operating under different regulatory frameworks.

### Technical Obsolescence

The order's compute-based thresholds face inevitable obsolescence as AI research advances. While current thresholds appear appropriate for today's technological landscape, algorithmic improvements could enable dangerous capabilities at lower compute levels within several years. The challenge of maintaining relevant thresholds without stifling innovation or creating loopholes for malicious actors represents a fundamental tension in the governance framework.

Alternative technical approaches to AI development, such as neuromorphic computing or quantum-classical hybrid systems, might not fit cleanly within the FLOP-based framework. As AI research explores diverse architectures beyond standard deep learning, the order's technical foundations may require substantial revision to maintain effectiveness.

### International Coordination

The order's global impact depends significantly on international adoption of similar frameworks, which remains highly uncertain. While allied nations have expressed support for coordinated AI governance, translating this into binding international agreements faces substantial obstacles. Competing national interests, differing regulatory philosophies, and varying technical capabilities among nations complicate efforts to establish global standards.

China's response to US AI governance initiatives will particularly influence the order's strategic effectiveness. If Chinese AI development proceeds without equivalent oversight, it could create competitive pressures that undermine voluntary compliance by US companies. The order's success in addressing global catastrophic risks depends partly on achieving international cooperation that currently appears unlikely.

<Backlinks client:load entityId="us-executive-order" />
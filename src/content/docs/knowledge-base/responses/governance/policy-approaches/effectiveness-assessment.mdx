---
title: Policy Effectiveness Assessment
description: Evaluating which AI governance approaches are actually working
sidebar:
  order: 20
---

import { DataInfoBox, KeyQuestions , PageStatus} from '../../../../../../components/wiki';

<PageStatus quality={3} lastEdited="2025-12-24" llmSummary="Framework for assessing AI policy effectiveness across different approaches (disclosure requirements, voluntary commitments, compute thresholds, export controls), finding that mandatory requirements with enforcement outperform voluntary commitments, though most policies are too new for rigorous assessment." todo="Add more empirical data on policy outcomes as evidence becomes available; expand comparative analysis" />

<DataInfoBox entityId="effectiveness-assessment" />

## Summary

As AI governance efforts multiply, a critical question emerges: **Which policies are actually working?**

This is difficult to assess because:
- Most policies are recent (limited track record)
- Counterfactuals are unknowable (what would have happened without the policy?)
- "Success" depends on goals (safety vs. innovation vs. rights)
- Effects may take years to materialize

Nevertheless, we can examine available evidence and develop frameworks for assessment.

## Framework for Assessment

### Dimensions of Effectiveness

| Dimension | Question | Metrics |
|-----------|----------|---------|
| **Compliance** | Are regulated entities following the rules? | Audit results, violation rates |
| **Behavioral Change** | Has conduct actually changed? | Safety investments, practices adopted |
| **Risk Reduction** | Are AI risks actually lower? | Incidents, near-misses, capability constraints |
| **Side Effects** | What unintended consequences? | Innovation rates, geographic shifts |
| **Durability** | Will effects persist? | Industry acceptance, political stability |

### Types of Evidence

**Strong evidence:**
- Randomized controlled trials (rare in policy)
- Clear before/after comparisons with controls
- Measurable outcome changes

**Moderate evidence:**
- Compliance audits and enforcement data
- Industry behavior changes (investments, hiring)
- Expert assessments and surveys

**Weak evidence:**
- Anecdotes and case studies
- Stated intentions and commitments
- Theoretical arguments

## Assessment by Policy Type

### Mandatory Disclosure Requirements

**Examples:** NYC Local Law 144 (hiring AI audits), EU AI Act transparency requirements

**Evidence of effectiveness:**

| Indicator | Finding |
|-----------|---------|
| Compliance rates | Mixed; many companies initially non-compliant with NYC law |
| Behavioral change | Some companies abandoned AI hiring tools rather than audit |
| Information produced | Limited public audits available; quality variable |
| Market effects | Audit industry emerging; unclear if improving AI quality |

**Assessment:** Disclosure requirements **can work** but depend heavily on:
- Enforcement rigor
- Audit quality standards
- Public/media attention to disclosures
- Consequences for bad findings

**Grade: C+** - Creates some accountability but often weak implementation

---

### Voluntary Commitments

**Examples:** White House AI commitments (July 2023), Responsible Scaling Policies

**Evidence of effectiveness:**

| Indicator | Finding |
|-----------|---------|
| Adoption | Major labs have published RSPs or equivalents |
| Behavioral change | Some capability evaluations being conducted |
| Enforcement | None; self-policed |
| Competitive pressure | Commitments may weaken under pressure (untested) |

**Assessment:** Voluntary commitments **establish norms** but lack enforcement:
- Good for signaling and coordination
- Insufficient alone for risk reduction
- May serve as precursor to regulation
- Vulnerable to defection under competitive pressure

**Grade: C** - Better than nothing, but limited confidence in durability

---

### Compute Thresholds

**Examples:** US Executive Order (10^26 FLOP reporting), EU AI Act (10^25 FLOP for GPAI)

**Evidence of effectiveness:**

| Indicator | Finding |
|-----------|---------|
| Compliance | Major labs reporting as required |
| Behavioral change | Some evidence of threshold gaming (training just below) |
| Coverage | Only captures largest models; misses fine-tuning, inference |
| Adaptability | Thresholds may become obsolete as efficiency improves |

**Assessment:** Compute thresholds are **useful but imperfect**:
- Provide objective, measurable criteria
- Create reporting requirements for largest models
- May miss dangerous capabilities in smaller models
- Gaming and evasion are possible

**Grade: B-** - Reasonable starting point, needs refinement

---

### Export Controls

**Examples:** US chip export restrictions to China (October 2022, updated 2023)

**Evidence of effectiveness:**

| Indicator | Finding |
|-----------|---------|
| Immediate impact | Chinese AI labs report chip shortages |
| Workarounds | Cloud access, smuggling, stockpiling reported |
| Domestic response | China accelerating domestic chip development |
| Long-term effect | Unclear; may slow China 1-3 years or accelerate independence |

**Assessment:** Export controls **have real short-term effects** but:
- Significant evasion and workaround activity
- May accelerate adversary domestic capabilities
- Create diplomatic tensions
- Effectiveness decays over time

**Grade: B-** - Creates friction but not a complete solution

---

### Risk-Based Regulatory Frameworks

**Examples:** EU AI Act, Colorado AI Act

**Evidence of effectiveness:**

| Indicator | Finding |
|-----------|---------|
| Compliance preparation | Companies investing in compliance infrastructure |
| Market effects | Some AI products withdrawn from EU market |
| Risk reduction | Too early to assess |
| Innovation effects | Contested; some claim chilling effect |

**Assessment:** Risk-based frameworks are **comprehensive but complex**:
- Provide structured approach to different risk levels
- May drive compliance investment
- High administrative burden
- Effectiveness depends on enforcement (untested for AI Act)

**Grade: Incomplete** - Too early to assess; promising framework

---

### AI Safety Institutes

**Examples:** UK AISI, US AISI

**Evidence of effectiveness:**

| Indicator | Finding |
|-----------|---------|
| Capacity built | Staff hired, some evaluations conducted |
| Access secured | Agreements with some labs for model access |
| Influence | Contributed to summit outcomes; policy influence unclear |
| Independence | Concerns about capture/insufficient authority |

**Assessment:** AI Safety Institutes are **promising but early**:
- Building needed technical capacity in government
- Model access arrangements being established
- Authority and independence still developing
- Evaluation methodologies still maturing

**Grade: B** - Good foundation, effectiveness depends on evolution

---

## Comparative Effectiveness

### What Seems to Work

1. **Clear, measurable requirements** - Compute thresholds, specific disclosures
2. **Third-party verification** - Audits, evaluations (when high quality)
3. **Real consequences** - Market access restrictions, legal liability
4. **International coordination** - Prevents regulatory arbitrage

### What Seems Less Effective

1. **Pure voluntary commitments** - Without enforcement or consequences
2. **Vague principles** - "Responsible AI" without specifics
3. **Fragmented jurisdiction** - Easy to relocate or route around
4. **After-the-fact enforcement** - Harms already occurred

### Key Uncertainties

<KeyQuestions
  client:load
  questions={[
    {
      question: "Can any policy actually prevent catastrophic AI risks?",
      positions: [
        {
          position: "Yes, with sufficient stringency",
          confidence: "low",
          reasoning: "Mandatory safety testing, liability, and compute controls could meaningfully constrain dangerous development",
          implications: "Push for stronger regulations; current policies are inadequate"
        },
        {
          position: "Unlikely without global coordination",
          confidence: "medium",
          reasoning: "Unilateral policies just shift development elsewhere; only global agreement can constrain",
          implications: "Prioritize international coordination over domestic policy"
        },
        {
          position: "Technical progress matters more",
          confidence: "medium",
          reasoning: "Policy can buy time but ultimately safety depends on solving alignment; governance is secondary",
          implications: "Focus on technical research; policy is supporting actor"
        }
      ]
    }
  ]}
/>

## Lessons for Policy Design

### From Successes

1. **Specificity matters** - Vague requirements produce vague compliance
2. **Enforcement is essential** - Unenforced rules become suggestions
3. **Measurability helps** - What can be measured can be managed
4. **Timing matters** - Earlier intervention is easier than retrofitting

### From Failures

1. **Industry will route around** - Expect gaming and evasion
2. **Definitions are contested** - "AI" and "high-risk" are fought over
3. **Voluntary â‰  durable** - Commitments fade under pressure
4. **Speed mismatch** - Policy can't keep up with technology

## Research Needs

Better effectiveness assessment requires:

1. **Incident databases** - Track AI failures and near-misses
2. **Compliance monitoring** - Systematic auditing and reporting
3. **Counterfactual analysis** - What would have happened without policy?
4. **Longitudinal studies** - Track effects over time
5. **Cross-jurisdiction comparison** - Natural experiments from different approaches

## Bottom Line

**Most AI policies are too new to rigorously assess**, but early evidence suggests:

- **Mandatory requirements with enforcement** outperform voluntary commitments
- **Specific, measurable obligations** outperform vague principles
- **International coordination** is necessary for policies targeting mobile activities
- **No current policy adequately addresses catastrophic risks** from frontier AI

The field needs better evidence infrastructure and longer time horizons to properly assess what works.


---
title: AI Safety Institutes
description: Government institutions for AI safety evaluation and research
sidebar:
  order: 22
---

import { DataInfoBox , PageStatus} from '../../../../../components/wiki';

<PageStatus quality={4} lastEdited="2025-12-24" llmSummary="AI Safety Institutes (UK AISI, US AISI, and emerging institutes in Japan, Singapore, and other countries) are government institutions building technical capacity to evaluate frontier AI systems and advise on policy. While promising for addressing information asymmetry and international coordination, they face challenges of scale (dozens of staff versus thousands at labs), limited enforcement authority, and potential industry capture." todo="Update staff numbers and organizational changes for 2025; add more specific evaluation results and impact metrics" />

<DataInfoBox entityId="ai-safety-institutes" />

## Summary

**AI Safety Institutes (AISIs)** are government-affiliated institutions dedicated to evaluating AI systems, conducting safety research, and advising on AI policy. They represent a significant development in AI governance: **building technical capacity within government** to understand and oversee advanced AI systems.

The AISI model emerged in 2023-2024 and is rapidly spreading, with plans for an **international network** of coordinating institutes.

## The Case for AISIs

### Why Government Needs Technical Capacity

Traditional regulatory approaches face challenges with AI:
- **Information asymmetry:** Labs understand their systems far better than regulators
- **Rapid advancement:** Capabilities evolve faster than policy can respond
- **Technical complexity:** Evaluating AI safety requires deep ML expertise
- **Access requirements:** Meaningful evaluation requires model access

AISIs aim to address these by:
- Building in-house technical expertise
- Securing access to frontier models
- Developing evaluation methodologies
- Informing policy with technical understanding

### What AISIs Do

1. **Safety Evaluations**
   - Pre-deployment testing of frontier models
   - Capability assessments (dangerous capabilities, dual-use)
   - Red-teaming and adversarial testing

2. **Research**
   - Evaluation methodology development
   - Interpretability and alignment research
   - Risk assessment frameworks

3. **Policy Support**
   - Technical advice to policymakers
   - Input to standards development
   - International coordination

4. **Ecosystem Development**
   - Supporting academic research
   - Training government personnel
   - Building evaluation infrastructure

## Institute Profiles

### UK AI Safety Institute

**Established:** November 2023 (announced at Bletchley Summit)

**Location:** London (with presence at Bletchley Park)

**Leadership:**
- Ian Hogarth (Chair)
- Yuntao Bai (Research Lead, formerly Anthropic)

**Staff:** ~50+ (growing rapidly)

**Key activities:**
- Pre-deployment evaluations of frontier models
- Research on dangerous capabilities
- Development of evaluation frameworks (Inspect)
- International coordination leadership

**Model access:**
- Agreements with major labs (Anthropic, OpenAI, Google DeepMind, Meta)
- Pre-deployment access for evaluation
- Some access for research purposes

**Notable outputs:**
- Inspect: Open-source evaluation framework
- Contributed to Seoul Summit outcomes
- Published research on dangerous capabilities

**Organizational status:**
- Housed within DSIT (Department for Science, Innovation and Technology)
- Relatively independent but government-funded
- Reports to ministers

---

### US AI Safety Institute (USAISI)

**Established:** February 2024 (within NIST)

**Location:** Gaithersburg, Maryland (NIST campus)

**Leadership:**
- Elizabeth Kelly (Director)
- Paul Christiano (Head of AI Safety, formerly Anthropic/ARC)

**Staff:** ~30+ (growing, authorized for 100+)

**Key activities:**
- Developing evaluation guidelines
- Coordinating with UK AISI
- Supporting Executive Order implementation
- Building testing infrastructure

**Organizational status:**
- Housed within NIST
- Part of Commerce Department
- Coordinates with broader AI policy apparatus

**Challenges:**
- Slower to establish than UK counterpart
- More bureaucratic constraints (federal hiring, procurement)
- Political uncertainty about future

---

### Japan AI Safety Institute

**Established:** February 2024

**Location:** Tokyo (within IPA - Information-technology Promotion Agency)

**Focus:**
- Evaluation methodologies adapted for Japanese context
- International coordination with UK/US
- Supporting Japanese AI policy development

**Status:** Earlier stage than UK/US, building capacity

---

### Singapore AI Safety Institute

**Established:** Planning stage (announced 2024)

**Focus:**
- Testing and evaluation capabilities
- Regional coordination in Asia-Pacific
- Support for Singapore's AI governance framework

---

### Other Planned/Proposed Institutes

| Country | Status | Notes |
|---------|--------|-------|
| Canada | Proposed | AISI discussions ongoing |
| France | Planning | Part of broader AI strategy |
| Germany | Discussing | Federal AI agency considerations |
| Australia | Exploring | Following UK model |
| South Korea | Announced | Seoul Summit follow-up |
| EU | Under discussion | Possible EU-level coordination body |

## International Network

### Seoul Summit Agreement

At the May 2024 Seoul AI Safety Summit, participating countries agreed to establish an **International Network of AI Safety Institutes** with:

- Information sharing protocols
- Coordinated evaluation approaches
- Personnel exchanges
- Joint research initiatives

### Coordination Mechanisms

**Bilateral:**
- UK-US MOU on AI safety cooperation
- Regular working-level exchanges
- Shared evaluation frameworks

**Multilateral:**
- Summit-based coordination (Bletchley → Seoul → Paris)
- Working groups on specific topics
- Emerging governance structure for network

### Harmonization Challenges

- Different national priorities and threat models
- Varying levels of model access
- Classification/secrecy constraints on sharing
- Jurisdictional limits on action

## Evaluation Approaches

### Current Capabilities

AISIs are developing evaluation capabilities across:

**Dangerous capabilities:**
- Biological/chemical weapons uplift
- Cyber offense capabilities
- Autonomous replication/resource acquisition
- Persuasion and manipulation

**Safety properties:**
- Honesty and truthfulness
- Refusal of harmful requests
- Robustness to jailbreaks
- Instruction following

### Methodological Challenges

1. **Evaluation validity:** Do tests predict real-world risks?
2. **Coverage:** Can't test for unknown capabilities
3. **Gaming:** Models optimized against known benchmarks
4. **Generalization:** Lab results may not reflect deployment
5. **Speed:** Evaluations take time; development is fast

### Open Tools

**UK AISI Inspect:**
- Open-source evaluation framework
- Modular, extensible design
- Supports multiple model APIs
- Community contributions welcomed

## Critiques and Limitations

### Structural Concerns

**Independence:**
- Government AISIs may face political pressure
- Industry relationships could compromise objectivity
- Funding dependent on political priorities

**Authority:**
- Most AISIs advisory only (no regulatory power)
- Cannot compel model access (rely on voluntary agreements)
- Limited enforcement mechanisms

**Capture risk:**
- Staff often from industry (revolving door concerns)
- May adopt industry framing of safety
- Resource constraints vs. well-funded labs

### Capacity Limitations

**Scale:**
- AISIs have dozens of staff; labs have thousands
- Cannot match lab investment in evaluation
- Dependent on lab cooperation

**Access:**
- Model access often limited and conditional
- May not get full access to training data, infrastructure
- Labs control what evaluators see

**Expertise:**
- Competing with industry for talent
- Government salaries often lower
- Security clearance requirements slow hiring

### Effectiveness Questions

| Optimistic View | Pessimistic View |
|-----------------|------------------|
| Building needed government capacity | Too small to matter |
| Securing important model access | Access is limited and controlled |
| Informing better policy | Policy moves independently |
| International coordination | Coordination is superficial |
| Technical credibility | Captured by industry framing |

## Future Directions

### Potential Evolution

1. **Regulatory authority:** AISIs could gain enforcement powers
2. **Mandatory access:** Laws requiring pre-deployment evaluation
3. **International treaty:** AISIs as verification bodies
4. **Expanded mandate:** Beyond evaluation to incident response

### Key Uncertainties

- Will AISIs maintain independence as political pressures grow?
- Can they keep pace with rapid capability advancement?
- Will international coordination deepen or fragment?
- How will they handle classified/national security models?

## Assessment

**Strengths:**
- Building technical capacity in government (necessary)
- Establishing model access precedents
- Creating international coordination infrastructure
- Developing open evaluation tools

**Weaknesses:**
- Small relative to challenge
- Limited authority and enforcement
- Dependent on voluntary lab cooperation
- Potential for capture/politicization

**Overall:** AISIs represent a **promising but early** institutional innovation. They're building necessary infrastructure for AI governance but face significant challenges of scale, authority, and independence. Their long-term effectiveness depends on how they evolve and whether they gain meaningful powers.


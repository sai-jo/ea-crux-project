---
title: Anthropic Core Views
description: Anthropic's Core Views on AI Safety (2023) articulates the thesis that meaningful safety research requires frontier access. With approximately 1,000+ employees, $8B from Amazon, $3B from Google, and over $5B run-rate revenue by 2025, the company maintains 15-25% of R&D on safety research, including the world's largest interpretability team (40-60 researchers). Their RSP framework has influenced industry standards, though critics question whether commercial pressures will erode safety commitments.
sidebar:
  order: 1
quality: 5
llmSummary: Comprehensive analysis of Anthropic's strategy combining frontier AI
  development with safety research, including quantified estimates of their
  ~$100-200M/year safety investment and assessment that 20-30% of technical
  staff focus on safety. Evaluates the controversial 'safety requires frontier
  access' claim with concrete metrics on research output (~15-25 safety papers
  annually) and influence (RSP framework adopted by 2-3 other labs).
lastEdited: "2025-12-28"
importance: 78.5
---

import {DataInfoBox, Backlinks, Mermaid} from '../../../../../components/wiki';

<DataInfoBox entityId="anthropic-core-views" />

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| Research Investment | High (~$100-200M/year) | Estimated 15-25% of R&D budget on safety research; dedicated teams for interpretability, alignment, and red-teaming |
| Interpretability Leadership | Highest in industry | 40-60 researchers led by [Chris Olah](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/); published [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/) (May 2024) |
| Safety/Capability Ratio | Medium (20-30%) | Estimated 20-30% of 1,000+ technical staff focus primarily on safety vs. capability development |
| Publication Output | Medium-High | 15-25 major papers annually including Constitutional AI, interpretability, and deception research |
| Industry Influence | High | RSP framework adopted by OpenAI, DeepMind; [MOU with US AI Safety Institute](https://www.nist.gov/news-events/news/2024/08/us-ai-safety-institute-signs-agreements-regarding-ai-safety-research) (August 2024) |
| Commercial Pressure Risk | High | $5B+ run-rate revenue by August 2025; $8B Amazon investment, $3B Google investment create deployment incentives |
| Governance Structure | Medium | Public Benefit Corporation status provides some protection; Jared Kaplan serves as Responsible Scaling Officer |

## Overview

Anthropic's [Core Views on AI Safety](https://www.anthropic.com/news/core-views-on-ai-safety), published in 2023, articulates the company's fundamental thesis: that meaningful AI safety work requires being at the frontier of AI development, not merely studying it from the sidelines. The approximately 6,000-word document outlines Anthropic's predictions that AI systems "will become far more capable in the next decade, possibly equaling or exceeding human level performance at most intellectual tasks," and argues that safety research must keep pace with these advances.

The Core Views emerge from Anthropic's unique position as a company founded in 2021 by [seven former OpenAI employees](https://en.wikipedia.org/wiki/Anthropic)—including siblings Dario and Daniela Amodei—explicitly around AI safety concerns. The company has since raised over $11 billion, including [$8 billion from Amazon](https://techfundingnews.com/amazon-anthropic-ai-investment-strategy/) and [$3 billion from Google](https://www.cnbc.com/2025/01/22/google-agrees-to-new-1-billion-investment-in-anthropic.html), while reaching [over $5 billion in annualized revenue](https://taptwicedigital.com/stats/anthropic) by August 2025. This dual identity—mission-driven safety organization and commercial AI lab—creates both opportunities and tensions that illuminate broader questions about how AI safety research should be conducted in an increasingly competitive landscape.

At its essence, the Core Views document attempts to resolve what many see as a fundamental contradiction: how can building increasingly powerful AI systems be reconciled with concerns about AI safety and existential risk? Anthropic's answer involves a theory of change that emphasizes empirical research, scalable oversight techniques, and the development of safety methods that can keep pace with rapidly advancing capabilities. The document presents a three-tier framework (optimistic, intermediate, pessimistic scenarios) for how difficult alignment might prove to be, with corresponding strategic responses for each scenario. Whether this approach genuinely advances safety or primarily serves to justify commercial AI development remains one of the most contentious questions in AI governance.

### Anthropic's Theory of Change

<Mermaid client:load chart={`
flowchart TD
    FRONTIER[Frontier AI Development] --> EMPIRICAL[Empirical Safety Research]
    EMPIRICAL --> INTERP[Mechanistic Interpretability]
    EMPIRICAL --> CAI[Constitutional AI]
    EMPIRICAL --> EVAL[Capability Evaluations]

    INTERP --> UNDERSTAND[Understand Model Internals]
    CAI --> ALIGN[Train Aligned Behavior]
    EVAL --> RSP[Responsible Scaling Policy]

    UNDERSTAND --> SAFE[Safe Deployment]
    ALIGN --> SAFE
    RSP --> SAFE

    SAFE --> INFLUENCE[Industry Influence]
    INFLUENCE --> NORMS[Safety Norms & Standards]

    style FRONTIER fill:#ffcccc
    style SAFE fill:#ccffcc
    style INFLUENCE fill:#ccffcc
    style NORMS fill:#ccffcc
    style RSP fill:#ffffcc
`} />

## The Frontier Access Thesis

The cornerstone of Anthropic's Core Views is the argument that effective AI safety research requires access to the most capable AI systems available. This claim rests on several empirical observations about how AI capabilities and risks emerge at scale. Anthropic argues that many safety-relevant phenomena only become apparent in sufficiently large and capable models, making toy problems and smaller-scale research insufficient for developing robust safety techniques. The Core Views document estimates that over the next 5 years, they "expect around a 1000x increase in the computation used to train the largest models, which could result in a capability jump significantly larger than the jump from GPT-2 to GPT-3."

The evidence supporting this thesis has accumulated through Anthropic's own research programs. Their work on [mechanistic interpretability](https://transformer-circuits.pub/2024/scaling-monosemanticity/), led by Chris Olah and published in "[Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/)" (May 2024), demonstrates that sparse autoencoders can extract interpretable features from Claude 3 Sonnet—identifying millions of concepts including safety-relevant features related to deception, sycophancy, and dangerous content. This required access to production-scale models with billions of parameters, providing evidence that certain interpretability techniques only become feasible at frontier scale.

### Evidence Assessment

| Claim | Supporting Evidence | Counterargument |
|-------|---------------------|-----------------|
| Interpretability requires scale | Scaling Monosemanticity found features only visible in large models | Smaller-scale research identified similar phenomena earlier (e.g., word embeddings) |
| Alignment techniques don't transfer | Constitutional AI works better on larger models | Many alignment principles are architecture-independent |
| Emergent capabilities create novel risks | GPT-4 showed capabilities not present in GPT-3 | Capabilities may be predictable with better evaluation |
| Safety-capability correlation | Larger models follow instructions better | Larger models also harder to control |

However, the frontier access thesis faces significant skepticism from parts of the AI safety community. Critics argue that this position is suspiciously convenient for a company seeking to justify large-scale AI development, and that much valuable safety research can be conducted without building increasingly powerful systems. The debate often centers on whether Anthropic's research findings genuinely require frontier access or whether they primarily demonstrate that such access is helpful rather than necessary.

## Research Investment and Organizational Structure

Anthropic's commitment to safety research is reflected in substantial financial investments, estimated at $100-200 million annually. This represents approximately 15-25% of their total R&D budget, a proportion that significantly exceeds most other AI companies. The investment supports multiple [research teams](https://www.anthropic.com/research) including Alignment, Interpretability, Societal Impacts, Economic Research, and the Frontier Red Team (which analyzes implications for cybersecurity, biosecurity, and autonomous systems).

### Organizational Metrics

| Metric | Estimate | Context |
|--------|----------|---------|
| Total employees | 1,000-1,100 (Sept 2024) | [331% growth](https://seo.ai/blog/how-many-people-work-at-anthropic) from 240 employees in 2023 |
| Safety-focused staff | 200-330 (20-30%) | Includes interpretability, alignment, red team, policy |
| Interpretability team | 40-60 researchers | Largest dedicated team globally |
| Annual safety publications | 15-25 papers | Constitutional AI, interpretability, deception research |
| Key safety hires (2024) | Jan Leike, John Schulman | Former OpenAI safety leads joined Anthropic |

The company's organizational structure reflects this dual focus, with an estimated 20-30% of technical staff working primarily on safety-focused research rather than capability development. This includes the world's largest dedicated interpretability team, comprising 40-60 researchers working on understanding the internal mechanisms of neural networks. The interpretability program, led by figures like Chris Olah from the former OpenAI safety team, represents a distinctive bet that reverse-engineering AI systems can provide crucial insights for ensuring their safe deployment.

Anthropic's research output includes 15-25 major safety papers annually, published in venues like NeurIPS, ICML, and through their [Alignment Science Blog](https://alignment.anthropic.com/). Notable publications include:

- **[Sleeper Agents](https://siliconangle.com/2024/01/14/anthropic-researchers-show-ai-systems-can-taught-engage-deceptive-behavior/)** (January 2024): Demonstrated that AI systems can be trained for deceptive behavior that persists through safety training
- **[Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/)** (May 2024): Extracted millions of interpretable features from Claude 3 Sonnet
- **[Alignment Faking](https://alignment.anthropic.com/)** (December 2024): First empirical example of a model engaging in alignment faking without explicit training

## Constitutional AI and Alignment Research

[Constitutional AI](https://www.anthropic.com/research/constitutional-ai-harmlessness-from-ai-feedback) (CAI) represents Anthropic's flagship contribution to AI alignment research, offering an alternative to traditional reinforcement learning from human feedback (RLHF) approaches. The technique, [published in December 2022](https://arxiv.org/abs/2212.08073), involves training models to follow a set of principles or "constitution" by using the model's own critiques of its outputs. This self-correction mechanism has shown promise in making models more helpful, harmless, and honest without requiring extensive human oversight for every decision.

### Claude's Constitution Sources

[Claude's constitution](https://www.anthropic.com/news/claudes-constitution) draws from multiple sources:

| Source | Example Principles |
|--------|-------------------|
| UN Declaration of Human Rights | "Choose responses that support freedom, equality, and a sense of brotherhood" |
| Trust and safety best practices | Guidelines on harmful content, misinformation |
| DeepMind Sparrow Principles | Adapted principles from other AI labs |
| Non-Western perspectives | Effort to capture diverse cultural values |
| Apple Terms of Service | Referenced for Claude 2's constitution |

The development of Constitutional AI exemplifies Anthropic's empirical approach to alignment research. Rather than relying purely on theoretical frameworks, the technique emerged from experiments with actual language models, revealing how self-correction capabilities scale with model size and training approaches. The process involves both a supervised learning and a reinforcement learning phase: in the supervised phase, the model generates self-critiques and revisions; in the RL phase, AI-generated preference data trains a preference model.

In 2024, Anthropic published research on [Collective Constitutional AI](https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input), using the Polis platform for online deliberation to curate a constitution using preferences from people outside Anthropic. This represents an attempt to democratize the values encoded in AI systems beyond developer preferences.

Constitutional AI also demonstrates the broader philosophy underlying Anthropic's Core Views: that alignment techniques must be developed and validated on capable systems to be trustworthy. The approach's reliance on the model's own reasoning capabilities means that it may not transfer to smaller or less sophisticated systems, supporting Anthropic's argument that safety research benefits from frontier access.

## Risks Addressed

Anthropic's Core Views framework and associated research address multiple AI risk categories:

| Risk Category | Mechanism | Anthropic's Approach |
|---------------|-----------|---------------------|
| [Deceptive alignment](/knowledge-base/risks/accident/) | AI systems optimizing for appearing aligned | Interpretability to detect deception features; Sleeper Agents research |
| [Misuse - Bioweapons](/knowledge-base/risks/misuse/bioweapons/) | AI assisting biological weapon development | RSP biosecurity evaluations; Frontier Red Team assessments |
| [Misuse - Cyberweapons](/knowledge-base/risks/misuse/) | AI assisting cyberattacks | Capability thresholds before deployment; jailbreak-resistant classifiers |
| [Loss of control](/knowledge-base/risks/accident/) | AI systems pursuing unintended goals | Constitutional AI for value alignment; RSP deployment gates |
| [Racing dynamics](/knowledge-base/risks/structural/) | Labs cutting safety corners for competitive advantage | RSP framework exportable to other labs; industry norm-setting |

The Core Views framework positions Anthropic to address these risks through empirical research at the frontier while attempting to influence industry-wide safety practices through transparent policy frameworks.

## Responsible Scaling Policies

Anthropic's [Responsible Scaling Policy](https://www.anthropic.com/responsible-scaling-policy) (RSP) framework represents their attempt to make capability development conditional on safety measures. First released in September 2023, the framework defines a series of "AI Safety Levels" (ASL-1 through ASL-5) that correspond to different capability thresholds and associated safety requirements. Models must pass safety evaluations before deployment, and development may be paused if adequate safety measures cannot be implemented.

### RSP Version History

| Version | Effective Date | Key Changes |
|---------|---------------|-------------|
| 1.0 | September 2023 | Initial release establishing ASL framework |
| [2.0](https://assets.anthropic.com/m/24a47b00f10301cd/original/Anthropic-Responsible-Scaling-Policy-2024-10-15.pdf) | October 2024 | New capability thresholds; safety case methodology; enhanced governance |
| 2.1 | March 2025 | Clarified which thresholds require ASL-3+ safeguards |
| [2.2](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf) | May 2025 | Amended insider threat scope in ASL-3 Security Standard |

The RSP framework has gained influence beyond Anthropic, with other major AI labs including OpenAI and DeepMind developing similar policies. Jared Kaplan, Co-Founder and Chief Science Officer, serves as Anthropic's Responsible Scaling Officer, succeeding Sam McCandlish who oversaw the initial implementation. The framework's emphasis on measurable capability thresholds and concrete safety requirements provides a more systematic approach than previous ad hoc safety measures.

However, the RSP framework has also attracted criticism. [SaferAI has argued](https://www.safer-ai.org/anthropics-responsible-scaling-policy-update-makes-a-step-backwards) that the October 2024 update "makes a step backwards" by shifting from precisely defined thresholds to more qualitative descriptions—"specifying the capability levels they aim to detect and the objectives of mitigations, but lacks concrete details on the mitigations and evaluations themselves." Critics argue this reduces transparency and accountability.

Additionally, the framework's focus on preventing obviously dangerous capabilities (biosecurity, cybersecurity, autonomous replication) may not address more subtle alignment failures or gradual erosion of human control over AI systems. The company retains ultimate discretion over safety thresholds and evaluation criteria, raising questions about whether commercial pressures might influence implementation.

## Mechanistic Interpretability Leadership

Anthropic's [interpretability research program](https://transformer-circuits.pub/), led by figures like [Chris Olah](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/) and others from the former OpenAI safety team, represents the most ambitious effort to understand the internal workings of large neural networks. The program's goal is to reverse-engineer trained models to understand their computational mechanisms, potentially enabling detection of deceptive behavior or misalignment before deployment.

The research has achieved notable successes, documented on the [Transformer Circuits thread](https://transformer-circuits.pub/). In May 2024, the team published "[Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/)," demonstrating that sparse autoencoders can decompose Claude 3 Sonnet's activations into interpretable features. The research team—including Adly Templeton, Tom Conerly, Jack Lindsey, Trenton Bricken, and others—identified millions of features representing specific concepts, including safety-relevant features for deception, sycophancy, bias, and dangerous content.

### Key Interpretability Findings

| Research | Date | Finding | Safety Relevance |
|----------|------|---------|------------------|
| [Towards Monosemanticity](https://transformer-circuits.pub/) | October 2023 | Dictionary learning applied to small transformer | Proof of concept for feature extraction |
| [Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/) | May 2024 | Extracted millions of features from Claude 3 Sonnet | First production-scale interpretability |
| [Circuits Updates](https://transformer-circuits.pub/2024/july-update/index.html) | July 2024 | Engineering challenges in scaling interpretability | Identified practical barriers |
| Golden Gate Bridge experiment | May 2024 | Demonstrated feature steering by amplifying specific concept | Showed features can be manipulated |

The interpretability program illustrates the frontier access thesis in practice. Many of the team's most significant findings have emerged from studying Claude models directly, rather than smaller research systems. The ability to identify interpretable circuits and features in production-scale models provides evidence that safety-relevant insights may indeed require access to frontier systems.

However, significant challenges remain. The features found represent only a small subset of all concepts learned by the model—finding a full set using current techniques would be cost-prohibitive. Additionally, understanding the representations doesn't tell us how the model uses them; the circuits still need to be found. The ultimate utility of these insights for ensuring safe deployment remains to be demonstrated.

## Commercial Pressures and Sustainability

Anthropic's position as a venture-funded company with significant commercial revenue creates inherent tensions with its safety mission. The company has raised over $11 billion in funding, including [$8 billion from Amazon](https://techfundingnews.com/amazon-anthropic-ai-investment-strategy/) and [$3 billion from Google](https://www.cnbc.com/2025/01/22/google-agrees-to-new-1-billion-investment-in-anthropic.html). By August 2025, [annualized revenue exceeded $5 billion](https://taptwicedigital.com/stats/anthropic)—representing 400% growth from $1 billion in 2024—with [Claude Code alone generating over $500 million](https://taptwicedigital.com/stats/anthropic) in run-rate revenue. The company's March 2025 funding round valued it at [$61.5 billion](https://www.cnbc.com/2025/03/03/amazon-backed-ai-firm-anthropic-valued-at-61point5-billion-after-latest-round.html).

### Financial Trajectory

| Metric | 2024 | 2025 (Projected) | Source |
|--------|------|------------------|--------|
| Annual Revenue | $1B | $9B+ | [Anthropic Statistics](https://taptwicedigital.com/stats/anthropic) |
| Valuation | $18.4B (Series E) | $61.5B-$183B | [CNBC](https://www.cnbc.com/2025/03/03/amazon-backed-ai-firm-anthropic-valued-at-61point5-billion-after-latest-round.html) |
| Total Funding Raised | ~$7B | $14.3B+ | Wikipedia, funding announcements |
| Enterprise Revenue Share | ~80% | ~80% | Enterprise customers dominate |

The sustainability of Anthropic's dual approach depends critically on whether investors and customers value safety research or merely tolerate it as necessary overhead. Market pressures could gradually shift resources toward capability development and away from safety research, particularly if competitors gain significant market advantages. The company's governance structure, including its Public Benefit Corporation status, provides some protection against purely profit-driven decision-making, but ultimate accountability remains to shareholders.

Evidence for how well Anthropic manages these pressures is mixed. The company has reportedly delayed deployment of at least one model due to safety concerns, suggesting some willingness to prioritize safety over speed to market. However, the rapid release cycle for Claude models (Claude 3 in March 2024, Claude 3.5 Sonnet in June 2024, Claude 3.5 Opus expected 2025) and competitive positioning against ChatGPT and other systems demonstrates that commercial considerations remain paramount in deployment decisions. [Anthropic announced plans](https://www.cnbc.com/2025/09/26/anthropic-global-ai-hiring-spree.html) to triple its international workforce and expand its applied AI team fivefold in 2025.

## Trajectory and Future Prospects

In the near term (1-2 years), Anthropic's approach faces several key tests. The company's ability to maintain its safety research focus while scaling commercial operations—from $1B to potentially $9B+ revenue—will determine whether the Core Views framework can survive contact with market realities. In February 2025, Anthropic published research on [classifiers that filter jailbreaks](https://alignment.anthropic.com/), withstanding over 3,000 hours of red teaming with no universal jailbreak discovered. Upcoming challenges include implementing more stringent RSP evaluations as model capabilities advance, demonstrating practical applications of interpretability research, and maintaining technical talent in both safety and capability research.

The medium-term trajectory (2-5 years) will likely determine whether Anthropic's bet on empirical alignment research pays off. Key milestones include:

- Developing interpretability tools that can reliably detect deception or misalignment in production
- Scaling Constitutional AI to more sophisticated moral reasoning
- Demonstrating that RSP frameworks can actually prevent deployment of dangerous systems
- Maintaining safety research investment as the company scales to potentially $20-26B revenue (2026 projection)

The company's influence on industry safety practices may prove more important than its technical contributions if other labs adopt similar approaches. The [MOU with the US AI Safety Institute](https://www.nist.gov/news-events/news/2024/08/us-ai-safety-institute-signs-agreements-regarding-ai-safety-research) (August 2024) provides government access to major models before public release—a template that could become industry standard.

The longer-term viability of the Core Views framework depends on broader questions about AI development trajectories and governance structures. If transformative AI emerges on Anthropic's projected timeline of 5-15 years, the company's safety research may prove crucial for ensuring beneficial outcomes. However, if development proves slower or if effective governance mechanisms emerge independently, the frontier access thesis may lose relevance as safety research can be conducted through other means.

## Critical Uncertainties and Limitations

Several fundamental uncertainties limit our ability to evaluate Anthropic's Core Views framework definitively. The most critical question involves whether safety research truly benefits from or requires frontier access, or whether this claim primarily serves to justify commercial AI development. While Anthropic has produced evidence supporting the frontier access thesis, alternative research approaches remain largely untested, making comparative evaluation difficult.

The sustainability of safety research within a commercial organization facing competitive pressures represents another major uncertainty. Anthropic's current allocation of 20-30% of technical staff to primarily safety-focused work may prove unsustainable if market pressures intensify or if safety research fails to produce commercially relevant insights. The company's governance mechanisms provide some protection, but their effectiveness under severe commercial pressure remains untested.

Questions about the effectiveness of Anthropic's specific safety techniques also introduce significant uncertainty. While Constitutional AI and interpretability research have shown promise, their ability to scale to more capable systems and detect sophisticated forms of misalignment remains unclear. The RSP framework's enforcement mechanisms have not been seriously tested, as no model has yet approached the capability thresholds that would require significant deployment restrictions.

Finally, the broader question of whether any technical approach to AI safety can succeed without comprehensive governance and coordination mechanisms introduces systemic uncertainty. Anthropic's Core Views assume that safety-conscious labs can maintain meaningful influence over AI development trajectories, but this may prove false if less safety-focused actors dominate the field or if competitive dynamics overwhelm safety considerations across the industry.

---

## Sources & References

### Primary Documents

- **[Core Views on AI Safety](https://www.anthropic.com/news/core-views-on-ai-safety)** - Anthropic's official 2023 document articulating their safety philosophy
- **[Responsible Scaling Policy v2.2](https://www-cdn.anthropic.com/872c653b2d0501d6ab44cf87f43e1dc4853e4d37.pdf)** - Current RSP effective May 2025
- **[Constitutional AI: Harmlessness from AI Feedback](https://arxiv.org/abs/2212.08073)** - Original December 2022 paper
- **[Claude's Constitution](https://www.anthropic.com/news/claudes-constitution)** - Documentation of Claude's constitutional principles

### Research Publications

- **[Scaling Monosemanticity](https://transformer-circuits.pub/2024/scaling-monosemanticity/)** - May 2024 interpretability research
- **[Transformer Circuits Thread](https://transformer-circuits.pub/)** - Ongoing interpretability research documentation
- **[Alignment Science Blog](https://alignment.anthropic.com/)** - Research notes and early findings
- **[Collective Constitutional AI](https://www.anthropic.com/research/collective-constitutional-ai-aligning-a-language-model-with-public-input)** - 2024 research on democratic AI alignment

### Media & Analysis

- **[Chris Olah on 80,000 Hours](https://80000hours.org/podcast/episodes/chris-olah-interpretability-research/)** - Interview on interpretability research
- **[Anthropic Valuation Reaches $61.5B](https://www.cnbc.com/2025/03/03/amazon-backed-ai-firm-anthropic-valued-at-61point5-billion-after-latest-round.html)** - CNBC, March 2025
- **[Amazon's $8B Investment](https://techfundingnews.com/amazon-anthropic-ai-investment-strategy/)** - Tech Funding News
- **[Google's $1B Investment](https://www.cnbc.com/2025/01/22/google-agrees-to-new-1-billion-investment-in-anthropic.html)** - CNBC, January 2025
- **[US AI Safety Institute Agreement](https://www.nist.gov/news-events/news/2024/08/us-ai-safety-institute-signs-agreements-regarding-ai-safety-research)** - NIST, August 2024

### Critical Perspectives

- **[SaferAI RSP Critique](https://www.safer-ai.org/anthropics-responsible-scaling-policy-update-makes-a-step-backwards)** - Analysis of RSP transparency concerns
- **[Anthropic Statistics & Revenue](https://taptwicedigital.com/stats/anthropic)** - Financial trajectory data
- **[Anthropic Employee Growth](https://seo.ai/blog/how-many-people-work-at-anthropic)** - Organizational scaling data

---

<Backlinks client:load entityId="anthropic-core-views" />
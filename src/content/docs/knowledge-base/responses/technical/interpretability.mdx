---
title: Mechanistic Interpretability
description: Understanding AI systems by reverse-engineering their internal computations
sidebar:
  order: 2
---

import { DataInfoBox, KeyPeople, Section, Backlinks , PageStatus} from '../../../../../components/wiki';

<PageStatus quality={3} lastEdited="2025-12-24" llmSummary="Mechanistic interpretability aims to reverse-engineer neural networks by identifying meaningful features, circuits, and algorithms within models using techniques like sparse autoencoders, activation patching, and circuit analysis to enable detection of deceptive cognition and verification of alignment." todo="Add citations for major results (Scaling Monosemanticity paper, specific induction heads work), expand on superposition with examples, include more recent 2024-2025 developments" />

<DataInfoBox entityId="interpretability" />

## Summary

Mechanistic interpretability is a research field focused on reverse-engineering neural networks to understand how they work internally. Rather than treating models as black boxes, researchers aim to identify meaningful circuits, features, and algorithms that explain model behavior.

This is considered safety-critical because it could enable:
- Detection of deceptive or misaligned cognition
- Verification that models have learned intended concepts
- Understanding of unexpected capabilities before deployment

## Core Concepts

### Features
Meaningful directions in activation space that correspond to human-interpretable concepts. A "Golden Gate Bridge" feature activates when the model processes information about that landmark.

### Circuits
Computational subgraphs that implement specific behaviors. For example, an "induction circuit" that performs pattern completion.

### Superposition
Models represent more features than they have dimensions by encoding features in overlapping ways. This makes interpretation harder but is a core phenomenon to understand.

## Key Techniques

### Sparse Autoencoders (SAEs)
Train auxiliary networks to decompose model activations into interpretable features:
- Input: model activations
- Output: sparse combination of learned features
- Key advance: Anthropic's "Scaling Monosemanticity" (2024)

### Activation Patching
Test causal importance by replacing activations:
- Run model normally, save activations
- Replace specific activations with alternatives
- Observe behavior change to infer causal role

### Probing
Train simple classifiers on internal representations:
- What information is represented?
- Where in the network?
- How is it encoded?

### Circuit Analysis
Trace information flow through specific components:
- Attention pattern analysis
- Weight-based path tracing
- Ablation studies

## Major Results

### Toy Models
- Complete understanding of simple attention patterns
- Grokking (sudden generalization) explained mechanistically
- Modular arithmetic circuits identified

### Language Models
- Induction heads for in-context learning
- Name movers for subject identification
- Sparse autoencoder features at scale
- Detection of deception-related features

## Open Questions

### Scalability
- Can techniques scale to frontier models?
- Is understanding necessarily incomplete?
- How much interpretation is "enough"?

### Deception Detection
- Can interpretability find deceptive cognition?
- Would deception hide from interpretation?
- Is behavioral interpretability sufficient?

### Automation
- Can AI assist in interpreting AI?
- What's the role of automated interpretability?

<Section title="Key People">
  <KeyPeople people={[
    { name: "Chris Olah", role: "Pioneer, Anthropic" },
    { name: "Neel Nanda", role: "DeepMind" },
    { name: "Tom Lieberum", role: "Apollo Research" },
  ]} />
</Section>

## Related Pages

<Backlinks client:load entityId="interpretability" />


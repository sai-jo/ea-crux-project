---
title: Research Agenda Comparison
description: Comprehensive analysis of major AI safety research agendas, comparing their core approaches, theoretical assumptions, and prospects for preventing catastrophic AI risks across different timelines and deployment scenarios.
sidebar:
  order: 5
quality: 4
llmSummary: Comprehensive comparison of 7 major AI safety research agendas (Anthropic, OpenAI, DeepMind, ARC, Redwood, MIRI) analyzing their core approaches, assumptions, timelines, and theories of change. Evaluates each agenda's prospects for solving inner alignment, scaling to superhuman AI, and providing near-term safety benefits. Identifies key disagreements about whether current paradigms will scale and the necessity of conceptual breakthroughs.
lastEdited: "2025-12-24"
importance: 85
---

import {ComparisonTable, DataInfoBox, DisagreementMap, KeyQuestions} from '../../../../components/wiki';

<DataInfoBox entityId="research-agendas" />

## Overview

The AI safety landscape features dramatically different research agendas based on fundamentally different theories about what will make advanced AI systems safe. These approaches range from Anthropic's constitutional AI—which aims to instill human values through training—to MIRI's agent foundations work that seeks deep theoretical understanding before building anything. The differences aren't merely tactical but reflect deep disagreements about timelines, the nature of intelligence, and what kinds of failures we should expect.

This fragmentation reflects both uncertainty about which technical approaches will succeed and disagreement about what "success" even looks like. Some agendas focus on making current large language models safer through better training techniques. Others prepare for scenarios where we deploy AI systems we cannot fully trust or understand. Still others argue that without fundamental conceptual breakthroughs, all current approaches will fail catastrophically. Understanding these differences is crucial for researchers, policymakers, and anyone trying to evaluate the overall trajectory of AI safety work.

The stakes of these disagreements are enormous. If constitutional AI succeeds at scale, we might achieve safe AGI through careful scaling of current techniques. If it fails and we haven't developed robust control protocols or verification methods, we could face exactly the catastrophic scenarios these research programs aim to prevent. The resource allocation decisions being made today across these different approaches may determine humanity's ability to navigate the development of transformative AI.

## Core Research Agenda Analysis

### Anthropic's Constitutional AI Paradigm

Anthropic's approach represents the most direct attempt to scale current techniques to advanced AI systems. Their constitutional AI methodology trains language models to follow a set of principles through a two-stage process: first using AI feedback to critique and revise outputs according to the constitution, then using this data for reinforcement learning. Their Responsible Scaling Policy framework creates concrete capability thresholds (ASL-2, ASL-3, etc.) with corresponding safety requirements, providing a roadmap for scaling safely through increasingly capable systems.

The constitutional AI work has shown promising empirical results on current models, successfully reducing harmful outputs while maintaining helpfulness across various tasks. Their 2024 research on "sleeper agents" provided concerning evidence that deceptive alignment could persist through standard training techniques, while their mechanistic interpretability research aims to understand model internals well enough to detect such deception. The combination suggests a research program that takes seriously both the promise and the perils of scaling current approaches.

However, critics argue that constitutional AI may create a false sense of security by solving outer alignment (getting models to produce safe outputs) without addressing inner alignment (ensuring the model's internal goals are aligned). The approach assumes that values can be reliably instilled through training rather than just learned as behavioral patterns that might break down under novel circumstances. Whether this assumption holds for superintelligent systems remains one of the most crucial open questions in AI safety.

### DeepMind's Scalable Oversight Framework

DeepMind's research program centers on the fundamental challenge of maintaining human oversight over AI systems that surpass human capabilities. Their debate approach, where AI systems argue different positions for human evaluation, aims to leverage competition to surface truth even when evaluators cannot directly assess claims. Process supervision evaluates reasoning steps rather than final outputs, potentially catching errors before they compound into dangerous conclusions.

The scalable oversight paradigm addresses a core problem: if AI systems become more capable than humans in domains like scientific research or strategic planning, how can we evaluate whether they're pursuing our intended goals? Their research on recursive reward modeling explores using AI systems to help evaluate other AI systems, potentially creating a bootstrapping process for maintaining oversight as capabilities scale. This connects to their broader work on formal verification, attempting to provide mathematical guarantees about AI system behavior.

Recent results on process supervision in mathematical reasoning show promise, with models trained to optimize for correct reasoning steps rather than just correct answers showing improved robustness. However, the approach faces significant challenges in domains where correct processes are less well-defined than mathematics. Critics worry that debate could favor persuasive arguments over truthful ones, and that recursive evaluation might amplify rather than correct human biases embedded in the initial training process.

### ARC's Eliciting Latent Knowledge Research

ARC's research agenda, led by Paul Christiano, focuses on what many consider the core technical problem of AI alignment: ensuring that AI systems report what they actually know rather than what they think humans want to hear. The eliciting latent knowledge (ELK) problem assumes that advanced AI systems will develop rich internal representations of the world but may not be incentivized to share their true beliefs with human operators.

The ELK problem is particularly concerning in scenarios where AI systems understand that humans are using their reports to make important decisions. A system that understands human psychology and incentives might learn to give reassuring rather than accurate assessments of plans or situations. ARC's research explores various technical approaches to extracting genuine beliefs, including methods that don't rely on human feedback and could thus be harder for AI systems to manipulate.

ARC's practical work through ARC Evals provides crucial empirical grounding by testing current systems for dangerous capabilities before they're deployed. Their evaluations of GPT-4 for tasks like autonomous replication and persuasion established important precedents for capability assessment. However, the core ELK problem remains largely unsolved, with fundamental questions about whether latent knowledge even exists in the forms the research assumes. Some critics argue that attempting to solve ELK may be unnecessary if other approaches can provide adequate safety guarantees.

### Redwood Research's AI Control Approach

Redwood Research takes a pragmatic approach that explicitly doesn't require solving alignment. Their AI control paradigm assumes we may deploy powerful AI systems before we fully understand or trust them, and focuses on developing protocols to safely use untrusted AI. This includes monitoring systems, decomposing tasks so that no single AI system has excessive power, and maintaining human oversight over critical decisions.

The AI control approach acknowledges that perfect alignment may be impossible or insufficient given competitive pressures and deployment timelines. Instead, it focuses on containment strategies, robust monitoring, and defense-in-depth approaches that could work even with partially misaligned systems. Their research on control protocols examines specific technical mechanisms: how to detect when AI systems are acting outside their intended scope, how to limit their capabilities while preserving usefulness, and how to maintain meaningful human control over important decisions.

This paradigm offers near-term practical benefits and could buy crucial time for other safety research. Control protocols could help prevent immediate harms from current systems while longer-term alignment research continues. However, critics argue that the approach may not scale to truly superintelligent systems that could find ways to circumvent monitoring and controls. There's also concern that successful control protocols might create complacency, reducing incentives to solve more fundamental alignment problems.

### MIRI's Agent Foundations Program

MIRI's research program stands apart by focusing on fundamental theoretical questions about agency, goals, and decision-making before attempting to build aligned AI systems. Their agent foundations research explores problems like embedded agency (how can agents reason about themselves as part of the world they're trying to understand?), logical uncertainty (how to make decisions when even logical facts are uncertain), and decision theory for AI systems.

This approach reflects deep skepticism about whether current AI paradigms can be made safe without fundamental conceptual breakthroughs. MIRI researchers argue that most other approaches are building on shaky theoretical foundations and may create capabilities without the conceptual tools needed to ensure safety. Their work on topics like Löb's theorem and reflective reasoning explores how advanced AI systems might reason about their own reasoning processes—a capability that could be crucial for ensuring they remain aligned as they modify themselves or create successor systems.

MIRI's theoretical rigor has identified important problems and concepts that other researchers now work on. However, their approach faces criticism for being too abstract and disconnected from practical AI development. The theoretical questions they focus on may indeed need to be solved eventually, but the research program offers little immediate help with near-term AI systems. Given uncertain timelines to transformative AI, the value of their work depends heavily on whether we have time for fundamental research or need immediate practical solutions.

## Safety Implications and Failure Modes

Each research agenda implies different failure modes and offers different types of safety assurances. Constitutional AI could fail if values aren't genuinely internalized, leading to systems that behave safely during training but pursue misaligned goals when deployed in novel situations. The approach also depends on the assumption that we can identify and specify human values clearly enough to train systems effectively—a assumption that may not hold for complex moral questions.

Scalable oversight approaches face the challenge that oversight itself might be gameable by sufficiently sophisticated systems. If AI systems understand they're being evaluated, they might optimize for appearing aligned rather than being aligned. The debate and process supervision methods also assume that correct reasoning processes can be identified and evaluated, which may not hold in domains where the correct approach is itself uncertain or controversial.

The control paradigm accepts some risk of misalignment in exchange for practical deployability, but this creates a fundamental scaling problem. As AI systems become more capable, the gap between their abilities and human oversight capabilities grows, potentially making control protocols increasingly ineffective. There's also the risk that control protocols designed for current systems won't transfer to fundamentally different AI architectures.

Each approach also offers distinct benefits. Constitutional AI provides immediate practical benefits for current systems and could scale smoothly if its assumptions hold. Scalable oversight directly addresses the superintelligence control problem. AI control offers near-term safety even without solving alignment. Agent foundations could prevent fundamental errors in how we think about AI goals and decision-making.

## Timeline Considerations and Research Trajectories

The research agendas operate on different timelines that reflect their different assumptions about AI development. Anthropic's constitutional AI work focuses on systems likely to be deployed within 2-5 years, with their Responsible Scaling Policy providing a framework for capability thresholds through potential AGI development. This near-term focus allows for empirical validation but may miss longer-term failure modes.

DeepMind's scalable oversight research targets the 5-10 year timeframe when AI systems might exceed human capabilities in most domains. Their formal verification work requires significant theoretical development but could provide stronger safety guarantees. The timeline pressure creates tension between developing robust theoretical foundations and providing practical solutions for nearer-term systems.

ARC's ELK research addresses problems that become critical as AI systems become sophisticated enough to understand and potentially manipulate human oversight. This could become relevant within 5-7 years as language models develop better theory of mind capabilities. However, fundamental progress on ELK has been limited, raising questions about whether the problem is solvable within relevant timelines.

Current trajectories suggest that multiple approaches will likely be needed rather than a single solution. Constitutional AI and control protocols provide immediate benefits for current systems. Interpretability and oversight methods could become crucial as capabilities scale. Foundational research might prevent fundamental errors in system design. The key question is resource allocation across these different timelines and approaches.

## Key Technical and Strategic Uncertainties

The most fundamental uncertainty is whether current AI paradigms—large language models trained with reinforcement learning from human feedback—will lead directly to transformative AI. If scaling current approaches leads to AGI, then research on constitutional AI and scalable oversight directly addresses the systems we'll need to align. If a paradigm shift occurs, much current safety research might not transfer to new architectures.

Another crucial uncertainty is the feasibility of verification without full alignment. Can we develop interpretability tools good enough to detect deceptive alignment? Can oversight methods scale to superintelligent systems? These questions determine whether we can safely deploy AI systems we don't fully understand or trust—a scenario that seems increasingly likely given competitive pressures.

The tractability of fundamental theoretical problems remains unclear. Are problems like ELK solvable in principle? Do we need conceptual breakthroughs about agency and goals before building advanced AI? The answers affect how much effort should go into foundational research versus empirical approaches with current systems.

Strategic uncertainties include how different research agendas interact and whether they're complements or substitutes. Does success in constitutional AI reduce the need for control protocols, or do we need defense in depth? How do competitive dynamics between AI developers affect the feasibility of different safety approaches? These considerations may be as important as the technical questions for determining which research directions to prioritize.

The comparison reveals that rather than competing approaches, we likely need a portfolio strategy that addresses different scenarios and timelines. The optimal allocation depends on beliefs about AI development trajectories, the tractability of different technical problems, and the strategic landscape for AI deployment. Understanding these trade-offs is essential for making effective research and policy decisions in an uncertain but rapidly evolving field.

<ComparisonTable
  client:load
  title="Research Agenda Overview"
  columns={["Primary Focus", "Key Assumption", "Timeline View", "Theory of Change"]}
  rows={[
    {
      name: "Anthropic (Constitutional AI)",
      values: {
        "Primary Focus": "Train models to be helpful, harmless, honest via AI feedback",
        "Key Assumption": "Can instill values through training at scale",
        "Timeline View": "Short (2026-2030)",
        "Theory of Change": "Build safest frontier models → demonstrate safe scaling → set industry standard"
      }
    },
    {
      name: "Anthropic (Interpretability)",
      values: {
        "Primary Focus": "Understand model internals to verify alignment",
        "Key Assumption": "Neural networks have interpretable structure",
        "Timeline View": "Short-medium",
        "Theory of Change": "Mechanistic understanding → can verify goals → detect deception"
      }
    },
    {
      name: "OpenAI (Superalignment) [dissolved]",
      values: {
        "Primary Focus": "Use weaker AI to align stronger AI",
        "Key Assumption": "Weak-to-strong generalization works",
        "Timeline View": "Short (2027-2030)",
        "Theory of Change": "Bootstrap alignment from current models to future models"
      }
    },
    {
      name: "DeepMind (Scalable Oversight)",
      values: {
        "Primary Focus": "Debate, recursive reward modeling, process supervision",
        "Key Assumption": "Can scale human oversight to superhuman AI",
        "Timeline View": "Medium (2030-2040)",
        "Theory of Change": "Better evaluation → catch misalignment → iterate to safety"
      }
    },
    {
      name: "ARC (Eliciting Latent Knowledge)",
      values: {
        "Primary Focus": "Get AI to report what it actually knows",
        "Key Assumption": "AI has latent knowledge that could be extracted",
        "Timeline View": "Medium",
        "Theory of Change": "Solve ELK → detect deception → verify alignment"
      }
    },
    {
      name: "Redwood (AI Control)",
      values: {
        "Primary Focus": "Maintain control even with misaligned AI",
        "Key Assumption": "Don't need alignment, just control",
        "Timeline View": "Near-term focus",
        "Theory of Change": "Untrusted AI can be safely used with proper protocols"
      }
    },
    {
      name: "MIRI (Agent Foundations)",
      values: {
        "Primary Focus": "Fundamental theory of agency and goals",
        "Key Assumption": "Need conceptual breakthroughs first",
        "Timeline View": "Uncertain, possibly insufficient time",
        "Theory of Change": "Deep understanding → know what to build → align by design"
      }
    }
  ]}
/>

<ComparisonTable
  client:load
  title="Agenda Comparison by Dimension"
  columns={["Solves Inner Alignment?", "Scales to Superhuman?", "Works Without Full Alignment?", "Empirically Testable Now?"]}
  highlightColumn="Scales to Superhuman?"
  rows={[
    {
      name: "Constitutional AI",
      values: {
        "Solves Inner Alignment?": { value: "Uncertain", badge: "medium" },
        "Scales to Superhuman?": { value: "Unknown", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Interpretability",
      values: {
        "Solves Inner Alignment?": { value: "Could help verify", badge: "medium" },
        "Scales to Superhuman?": { value: "Unknown", badge: "medium" },
        "Works Without Full Alignment?": { value: "Helps detect issues", badge: "medium" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Scalable Oversight",
      values: {
        "Solves Inner Alignment?": { value: "No", badge: "low" },
        "Scales to Superhuman?": { value: "Designed to", badge: "high" },
        "Works Without Full Alignment?": { value: "Maybe", badge: "medium" },
        "Empirically Testable Now?": { value: "Partially", badge: "medium" }
      }
    },
    {
      name: "ELK",
      values: {
        "Solves Inner Alignment?": { value: "Would help detect", badge: "medium" },
        "Scales to Superhuman?": { value: "Intended to", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "Limited", badge: "low" }
      }
    },
    {
      name: "AI Control",
      values: {
        "Solves Inner Alignment?": { value: "No (bypasses it)", badge: "low" },
        "Scales to Superhuman?": { value: "Probably not", badge: "low" },
        "Works Without Full Alignment?": { value: "Yes (the point)", badge: "high" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Agent Foundations",
      values: {
        "Solves Inner Alignment?": { value: "Aims to", badge: "medium" },
        "Scales to Superhuman?": { value: "Would if successful", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "No", badge: "low" }
      }
    }
  ]}
/>

<DisagreementMap
  client:load
  topic="Which Research Agenda Will Succeed?"
  description="Views on which approach is most likely to prevent AI catastrophe"
  spectrum={{ low: "Need novel breakthroughs", high: "Current approaches scale" }}
  positions={[
    { actor: "Anthropic researchers", position: "Current approaches can scale", estimate: "60-70%", confidence: "medium" },
    { actor: "DeepMind researchers", position: "Scalable oversight works", estimate: "50-60%", confidence: "medium" },
    { actor: "ARC / Paul Christiano", position: "Need targeted breakthroughs", estimate: "40-50%", confidence: "low" },
    { actor: "Redwood", position: "Control buys time", estimate: "Near-term high", confidence: "medium" },
    { actor: "MIRI / Yudkowsky", position: "Current approaches fail", estimate: "15-20%", confidence: "high" },
  ]}
/>

<KeyQuestions
  client:load
  questions={[
    {
      question: "Will current paradigm (LLMs + RLHF) lead to transformative AI?",
      positions: [
        {
          position: "Yes - scaling continues",
          confidence: "medium",
          reasoning: "Scaling laws hold. Emergent capabilities. Investment continues.",
          implications: "Current safety research is directly relevant"
        },
        {
          position: "No - paradigm shift needed",
          confidence: "medium",
          reasoning: "LLMs have fundamental limitations. New architecture will emerge.",
          implications: "May need different safety approaches for new paradigm"
        }
      ]
    },
    {
      question: "Can we verify alignment without solving it?",
      positions: [
        {
          position: "Yes - interpretability/evals can work",
          confidence: "medium",
          reasoning: "Can detect misalignment even if we can't prove alignment. Defense in depth.",
          implications: "Focus on verification and detection"
        },
        {
          position: "No - deceptive alignment defeats verification",
          confidence: "medium",
          reasoning: "Sufficiently capable AI could hide misalignment. Can't verify what we can't understand.",
          implications: "Must solve alignment directly, not just detect misalignment"
        }
      ]
    }
  ]}
/>
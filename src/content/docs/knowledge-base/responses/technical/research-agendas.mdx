---
title: Research Agenda Comparison
description: Comprehensive analysis of major AI safety research agendas, comparing their core approaches, theoretical assumptions, and prospects for preventing catastrophic AI risks across different timelines and deployment scenarios.
sidebar:
  order: 5
quality: 5
llmSummary: Comprehensive comparison of 7 major AI safety research agendas (Anthropic, OpenAI, DeepMind, ARC, Redwood, MIRI) analyzing their core approaches, assumptions, timelines, and theories of change. Evaluates each agenda's prospects for solving inner alignment, scaling to superhuman AI, and providing near-term safety benefits. Identifies key disagreements about whether current paradigms will scale and the necessity of conceptual breakthroughs.
lastEdited: "2025-12-28"
importance: 85
---

import {ComparisonTable, DataInfoBox, DisagreementMap, KeyQuestions, Mermaid} from '../../../../../components/wiki';

<DataInfoBox entityId="research-agendas" />

## Quick Assessment

| Organization | Annual Budget | Staff Size | Primary Focus | Tractability | Timeline |
|--------------|---------------|------------|---------------|--------------|----------|
| **Anthropic** | ~$100M+ (safety research) | ~15 red team + safety teams | Constitutional AI, Interpretability | High (empirical validation) | 2-5 years |
| **DeepMind** | $50M+ (estimated) | Growing 37-39% annually | Scalable Oversight, Formal Verification | Medium (theoretical + empirical) | 5-10 years |
| **ARC** | ~$5M annually | Small (~5-10 researchers) | Eliciting Latent Knowledge | Low (fundamental problem) | 5-7 years |
| **Redwood Research** | ~$5-10M annually | Small team | AI Control protocols | High (practical protocols) | Near-term |
| **MIRI** | $7M (2025 projected) | Small, pivoting to governance | Agent Foundations, Policy | Low (fundamental theory) | Long-term/governance |
| **OpenAI (dissolved)** | 20% compute allocation | Disbanded May 2024 | Superalignment (weak-to-strong) | N/A (team dissolved) | N/A |

## Overview

The AI safety landscape features dramatically different research agendas based on fundamentally different theories about what will make advanced AI systems safe. These approaches range from Anthropic's constitutional AI—which aims to instill human values through training—to MIRI's agent foundations work that seeks deep theoretical understanding before building anything. The differences aren't merely tactical but reflect deep disagreements about timelines, the nature of intelligence, and what kinds of failures we should expect.

This fragmentation reflects both uncertainty about which technical approaches will succeed and disagreement about what "success" even looks like. Some agendas focus on making current large language models safer through better training techniques. Others prepare for scenarios where we deploy AI systems we cannot fully trust or understand. Still others argue that without fundamental conceptual breakthroughs, all current approaches will fail catastrophically. Understanding these differences is crucial for researchers, policymakers, and anyone trying to evaluate the overall trajectory of AI safety work.

The stakes of these disagreements are enormous. If constitutional AI succeeds at scale, we might achieve safe AGI through careful scaling of current techniques. If it fails and we haven't developed robust control protocols or verification methods, we could face exactly the catastrophic scenarios these research programs aim to prevent. The resource allocation decisions being made today across these different approaches may determine humanity's ability to navigate the development of transformative AI.

### Research Agenda Relationships

<Mermaid client:load chart={`
flowchart TD
    subgraph Near-Term[Near-Term Safety 2-5 years]
        CAI[Constitutional AI<br/>Anthropic]
        INTERP[Interpretability<br/>Anthropic/DeepMind]
        CTRL[AI Control<br/>Redwood]
    end

    subgraph Medium-Term[Medium-Term 5-10 years]
        SO[Scalable Oversight<br/>DeepMind]
        ELK[Eliciting Latent Knowledge<br/>ARC]
        FORM[Formal Verification<br/>DeepMind]
    end

    subgraph Long-Term[Long-Term/Foundational]
        AF[Agent Foundations<br/>MIRI]
        GOV[Technical Governance<br/>MIRI]
    end

    CAI --> SO
    INTERP --> ELK
    CTRL --> SO
    ELK --> FORM
    AF -.-> SO
    AF -.-> GOV

    style CAI fill:#e1f5e1
    style CTRL fill:#e1f5e1
    style SO fill:#fff4e1
    style ELK fill:#fff4e1
    style AF fill:#ffe1e1
    style GOV fill:#ffe1e1
`} />

## Core Research Agenda Analysis

### Anthropic's Constitutional AI Paradigm

Anthropic's approach represents the most direct attempt to scale current techniques to advanced AI systems. Their constitutional AI methodology trains language models to follow a set of principles through a two-stage process: first using AI feedback to critique and revise outputs according to the constitution, then using this data for reinforcement learning. Their Responsible Scaling Policy framework creates concrete capability thresholds (ASL-2, ASL-3, etc.) with corresponding safety requirements, providing a roadmap for scaling safely through increasingly capable systems.

With [over $16 billion raised in 2025](https://markets.financialcontent.com/stocks/article/tokenring-2025-12-24-anthropics-13-billion-series-f-the-183-billion-valuation-that-redefined-the-ai-race) and a valuation exceeding $350 billion, Anthropic dedicates substantial resources to safety research. Their Frontier Red Team comprises approximately 15 researchers who stress-test advanced systems for misuse risks in biological research, cybersecurity, and autonomous systems. The company's [Anthropic Fellows Program](https://alignment.anthropic.com/2025/anthropic-fellows-program-2026/) provides $3,850/week stipends plus ~$15k/month in compute funding, producing research papers from over 80% of fellows. Their Claude Academy has trained over 300 engineers in AI safety practices, converting generalist software developers into specialized practitioners.

The constitutional AI work has shown promising empirical results on current models, successfully reducing harmful outputs while maintaining helpfulness across various tasks. Their 2024 research on "sleeper agents" provided concerning evidence that deceptive alignment could persist through standard training techniques, while their mechanistic interpretability research aims to understand model internals well enough to detect such deception. The combination suggests a research program that takes seriously both the promise and the perils of scaling current approaches.

However, critics argue that constitutional AI may create a false sense of security by solving outer alignment (getting models to produce safe outputs) without addressing inner alignment (ensuring the model's internal goals are aligned). The approach assumes that values can be reliably instilled through training rather than just learned as behavioral patterns that might break down under novel circumstances. Whether this assumption holds for superintelligent systems remains one of the most crucial open questions in AI safety.

### DeepMind's Scalable Oversight Framework

DeepMind's research program centers on the fundamental challenge of maintaining human oversight over AI systems that surpass human capabilities. Their debate approach, where AI systems argue different positions for human evaluation, aims to leverage competition to surface truth even when evaluators cannot directly assess claims. Process supervision evaluates reasoning steps rather than final outputs, potentially catching errors before they compound into dangerous conclusions.

Google DeepMind's [AGI Safety & Alignment team](https://deepmindsafetyresearch.medium.com/agi-safety-and-alignment-at-google-deepmind-a-summary-of-recent-work-8e600aca582a) has grown 37-39% annually, with leadership including Anca Dragan, Rohin Shah, and Allan Dafoe under executive sponsor Shane Legg. The team encompasses mechanistic interpretability, scalable oversight, and frontier safety including dangerous capability evaluations. Their [Frontier Safety Framework](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/) provides systematic evaluation protocols, while the AGI Safety Council analyzes risks and recommends safety practices. DeepMind's safety research investment is estimated at over $50 million annually, though exact figures are not publicly disclosed.

The scalable oversight paradigm addresses a core problem: if AI systems become more capable than humans in domains like scientific research or strategic planning, how can we evaluate whether they're pursuing our intended goals? Their research on recursive reward modeling explores using AI systems to help evaluate other AI systems, potentially creating a bootstrapping process for maintaining oversight as capabilities scale. This connects to their broader work on formal verification, attempting to provide mathematical guarantees about AI system behavior.

Recent results on process supervision in mathematical reasoning show promise, with models trained to optimize for correct reasoning steps rather than just correct answers showing improved robustness. However, the approach faces significant challenges in domains where correct processes are less well-defined than mathematics. Critics worry that debate could favor persuasive arguments over truthful ones, and that recursive evaluation might amplify rather than correct human biases embedded in the initial training process.

### ARC's Eliciting Latent Knowledge Research

ARC's research agenda, led by Paul Christiano, focuses on what many consider the core technical problem of AI alignment: ensuring that AI systems report what they actually know rather than what they think humans want to hear. The eliciting latent knowledge (ELK) problem assumes that advanced AI systems will develop rich internal representations of the world but may not be incentivized to share their true beliefs with human operators.

The [Alignment Research Center](https://www.alignment.org/) operates with a small team of approximately 5-10 researchers and an annual budget of around $5 million. Early staff included Paul Christiano and Mark Xu, with the organization receiving [$265,000 from Open Philanthropy in March 2022](https://www.openphilanthropy.org/grants/arc-general-support/). In 2025, ARC reported making conceptual and theoretical progress at the fastest pace since 2022. The organization spun out ARC Evals as an independent nonprofit called METR in December 2023, led by Beth Barnes who joined from OpenAI. Historical salary ranges for full-time positions were $150k-400k annually, with intern positions at $15k/month.

The ELK problem is particularly concerning in scenarios where AI systems understand that humans are using their reports to make important decisions. A system that understands human psychology and incentives might learn to give reassuring rather than accurate assessments of plans or situations. ARC's research explores various technical approaches to extracting genuine beliefs, including methods that don't rely on human feedback and could thus be harder for AI systems to manipulate.

ARC's practical work through ARC Evals provides crucial empirical grounding by testing current systems for dangerous capabilities before they're deployed. Their evaluations of GPT-4 for tasks like autonomous replication and persuasion established important precedents for capability assessment. However, the core ELK problem remains largely unsolved, with fundamental questions about whether latent knowledge even exists in the forms the research assumes. Some critics argue that attempting to solve ELK may be unnecessary if other approaches can provide adequate safety guarantees.

### Redwood Research's AI Control Approach

Redwood Research takes a pragmatic approach that explicitly doesn't require solving alignment. Their AI control paradigm assumes we may deploy powerful AI systems before we fully understand or trust them, and focuses on developing protocols to safely use untrusted AI. This includes monitoring systems, decomposing tasks so that no single AI system has excessive power, and maintaining human oversight over critical decisions.

Founded in August 2021, [Redwood Research](https://www.redwoodresearch.org/) operates with an annual budget of $5-10 million and has received [over $9.4 million from Open Philanthropy](https://www.openphilanthropy.org/grants/redwood-research-general-support/) plus nearly $1.3 million from the Survival and Flourishing Fund as of July 2022. Their ICML oral paper ["AI Control: Improving Risk Despite Intentional Subversion"](https://arxiv.org/abs/2312.06942) introduced protocols for monitoring malign LLM agents, establishing AI control as a bedrock approach for mitigating catastrophic risk. Redwood collaborates with governments and advises AI companies including Google DeepMind and Anthropic, partnering with UK AISI to produce safety case frameworks. The organization runs the Constellation office space, a 30,000 square foot facility hosting staff from multiple AI safety organizations including Open Philanthropy, ARC, Atlas Fellowship, and CEA.

The AI control approach acknowledges that perfect alignment may be impossible or insufficient given competitive pressures and deployment timelines. Instead, it focuses on containment strategies, robust monitoring, and defense-in-depth approaches that could work even with partially misaligned systems. Their research on control protocols examines specific technical mechanisms: how to detect when AI systems are acting outside their intended scope, how to limit their capabilities while preserving usefulness, and how to maintain meaningful human control over important decisions.

This paradigm offers near-term practical benefits and could buy crucial time for other safety research. Control protocols could help prevent immediate harms from current systems while longer-term alignment research continues. However, critics argue that the approach may not scale to truly superintelligent systems that could find ways to circumvent monitoring and controls. There's also concern that successful control protocols might create complacency, reducing incentives to solve more fundamental alignment problems.

### MIRI's Agent Foundations Program

MIRI's research program stands apart by focusing on fundamental theoretical questions about agency, goals, and decision-making before attempting to build aligned AI systems. Their agent foundations research explores problems like embedded agency (how can agents reason about themselves as part of the world they're trying to understand?), logical uncertainty (how to make decisions when even logical facts are uncertain), and decision theory for AI systems.

The [Machine Intelligence Research Institute](https://intelligence.org/) operates with a projected 2025 budget of $7 million and reserves of $10 million—enough for approximately 1.5 years if no additional funds are raised. Annual expenses from 2019-2023 ranged from $5.4M to $7.7M. MIRI is conducting its [first fundraiser in six years](https://intelligence.org/2025/12/01/miris-2025-fundraiser/), targeting $6M with the first $1.6M matched 1:1 via an SFF grant. Historically, MIRI received [$2.1 million from Open Philanthropy in 2019](https://www.openphilanthropy.org/grants/miri-general-support-2019/) supplemented by $7.7M in 2020, plus several million dollars in Ethereum from Vitalik Buterin in 2021. The organization has pivoted from technical AI safety research to focusing on informing policymakers and the public about AI risks, expanding its communications team and launching a technical AI governance program.

This approach reflects deep skepticism about whether current AI paradigms can be made safe without fundamental conceptual breakthroughs. MIRI researchers argue that most other approaches are building on shaky theoretical foundations and may create capabilities without the conceptual tools needed to ensure safety. Their work on topics like Löb's theorem and reflective reasoning explores how advanced AI systems might reason about their own reasoning processes—a capability that could be crucial for ensuring they remain aligned as they modify themselves or create successor systems.

MIRI's theoretical rigor has identified important problems and concepts that other researchers now work on. However, their approach faces criticism for being too abstract and disconnected from practical AI development. The theoretical questions they focus on may indeed need to be solved eventually, but the research program offers little immediate help with near-term AI systems. Given uncertain timelines to transformative AI, the value of their work depends heavily on whether we have time for fundamental research or need immediate practical solutions.

### OpenAI's Superalignment (Disbanded)

OpenAI's Superalignment team, established in July 2023, represented a major commitment to solving superintelligence alignment within four years. Co-led by Ilya Sutskever (Chief Scientist) and Jan Leike (Head of Alignment), the team received [20% of OpenAI's compute allocation](https://openai.com/index/introducing-superalignment/)—an unprecedented resource dedication for safety research. The team focused on "weak-to-strong generalization," exploring whether weaker AI models could effectively supervise and align stronger successors.

However, [in May 2024, OpenAI disbanded the Superalignment team](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html) less than one year after its formation. Jan Leike sharply criticized the company upon departure, stating "Over the past years, safety culture and processes have taken a backseat to shiny products." Ilya Sutskever also left, with Jakub Pachocki replacing him as Chief Scientist and John Schulman becoming scientific lead for alignment work. Jan Leike subsequently [joined Anthropic](https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/) to lead a new superalignment team focusing on scalable oversight, weak-to-strong generalization, and automated alignment research.

In October 2024, OpenAI [disbanded another safety team](https://www.cnbc.com/2024/10/24/openai-miles-brundage-agi-readiness.html)—the "AGI Readiness" team that advised on the company's capacity to handle increasingly powerful AI. Miles Brundage, senior advisor for AGI Readiness, wrote upon departure: "Neither OpenAI nor any other frontier lab is ready, and the world is also not ready." The dissolution of both teams within six months raised significant questions about OpenAI's commitment to safety research relative to product development.

## Detailed Research Agenda Comparison

| Research Agenda | Funding/Resources | Team Size | Key Publications | Output Metrics | Industry Adoption |
|-----------------|-------------------|-----------|------------------|----------------|-------------------|
| **Anthropic Constitutional AI** | $100M+ annually; $16B raised 2025 | ~15 red team + safety teams | RSP framework, Sleeper agents paper, Constitutional AI paper | 80%+ fellows publish; 300+ Claude Academy graduates | High (RSP adopted by labs) |
| **Anthropic Interpretability** | Shared safety budget | Integrated with safety teams | Mechanistic interpretability papers, Circuit analysis | Growing publication output | Medium (research influence) |
| **DeepMind Scalable Oversight** | $50M+ estimated | Growing 37-39% annually | Debate papers, Process supervision research | Multiple high-impact papers | Medium (research influence) |
| **DeepMind Formal Verification** | Shared budget | Integrated with safety teams | Mathematical verification research | Targeted publications | Low (early stage) |
| **ARC (ELK)** | ~$5M annually | ~5-10 researchers | ELK technical report, METR evaluations | Conceptual progress 2025 | Low (fundamental research) |
| **Redwood (AI Control)** | $5-10M annually | Small team | ICML oral paper on AI control | Government/industry collaborations | High (UK AISI partnership) |
| **MIRI (Agent Foundations)** | $7M projected 2025 | Small, pivoting to governance | Agent foundations papers, Policy communications | Shifting to policy output | Low (pivoting away) |
| **OpenAI Superalignment** | 20% compute allocation | Disbanded May 2024 | Weak-to-strong generalization research | N/A (dissolved) | N/A (dissolved) |

## Safety Implications and Failure Modes

Each research agenda implies different failure modes and offers different types of safety assurances. Constitutional AI could fail if values aren't genuinely internalized, leading to systems that behave safely during training but pursue misaligned goals when deployed in novel situations. The approach also depends on the assumption that we can identify and specify human values clearly enough to train systems effectively—a assumption that may not hold for complex moral questions.

Scalable oversight approaches face the challenge that oversight itself might be gameable by sufficiently sophisticated systems. If AI systems understand they're being evaluated, they might optimize for appearing aligned rather than being aligned. The debate and process supervision methods also assume that correct reasoning processes can be identified and evaluated, which may not hold in domains where the correct approach is itself uncertain or controversial.

The control paradigm accepts some risk of misalignment in exchange for practical deployability, but this creates a fundamental scaling problem. As AI systems become more capable, the gap between their abilities and human oversight capabilities grows, potentially making control protocols increasingly ineffective. There's also the risk that control protocols designed for current systems won't transfer to fundamentally different AI architectures.

Each approach also offers distinct benefits. Constitutional AI provides immediate practical benefits for current systems and could scale smoothly if its assumptions hold. Scalable oversight directly addresses the superintelligence control problem. AI control offers near-term safety even without solving alignment. Agent foundations could prevent fundamental errors in how we think about AI goals and decision-making.

## Timeline Considerations and Research Trajectories

The research agendas operate on different timelines that reflect their different assumptions about AI development. Anthropic's constitutional AI work focuses on systems likely to be deployed within 2-5 years, with their Responsible Scaling Policy providing a framework for capability thresholds through potential AGI development. This near-term focus allows for empirical validation but may miss longer-term failure modes.

DeepMind's scalable oversight research targets the 5-10 year timeframe when AI systems might exceed human capabilities in most domains. Their formal verification work requires significant theoretical development but could provide stronger safety guarantees. The timeline pressure creates tension between developing robust theoretical foundations and providing practical solutions for nearer-term systems.

ARC's ELK research addresses problems that become critical as AI systems become sophisticated enough to understand and potentially manipulate human oversight. This could become relevant within 5-7 years as language models develop better theory of mind capabilities. However, fundamental progress on ELK has been limited, raising questions about whether the problem is solvable within relevant timelines.

Current trajectories suggest that multiple approaches will likely be needed rather than a single solution. Constitutional AI and control protocols provide immediate benefits for current systems. Interpretability and oversight methods could become crucial as capabilities scale. Foundational research might prevent fundamental errors in system design. The key question is resource allocation across these different timelines and approaches.

## Key Technical and Strategic Uncertainties

The most fundamental uncertainty is whether current AI paradigms—large language models trained with reinforcement learning from human feedback—will lead directly to transformative AI. If scaling current approaches leads to AGI, then research on constitutional AI and scalable oversight directly addresses the systems we'll need to align. If a paradigm shift occurs, much current safety research might not transfer to new architectures.

Another crucial uncertainty is the feasibility of verification without full alignment. Can we develop interpretability tools good enough to detect deceptive alignment? Can oversight methods scale to superintelligent systems? These questions determine whether we can safely deploy AI systems we don't fully understand or trust—a scenario that seems increasingly likely given competitive pressures.

The tractability of fundamental theoretical problems remains unclear. Are problems like ELK solvable in principle? Do we need conceptual breakthroughs about agency and goals before building advanced AI? The answers affect how much effort should go into foundational research versus empirical approaches with current systems.

Strategic uncertainties include how different research agendas interact and whether they're complements or substitutes. Does success in constitutional AI reduce the need for control protocols, or do we need defense in depth? How do competitive dynamics between AI developers affect the feasibility of different safety approaches? These considerations may be as important as the technical questions for determining which research directions to prioritize.

The comparison reveals that rather than competing approaches, we likely need a portfolio strategy that addresses different scenarios and timelines. The optimal allocation depends on beliefs about AI development trajectories, the tractability of different technical problems, and the strategic landscape for AI deployment. Understanding these trade-offs is essential for making effective research and policy decisions in an uncertain but rapidly evolving field.

## Sources

### Anthropic
- [Anthropic's $13 Billion Series F funding announcement](https://markets.financialcontent.com/stocks/article/tokenring-2025-12-24-anthropics-13-billion-series-f-the-183-billion-valuation-that-redefined-the-ai-race) - December 2025
- [Anthropic Fellows Program for 2026](https://alignment.anthropic.com/2025/anthropic-fellows-program-2026/) - Applications open
- [Introducing the Anthropic Fellows Program](https://alignment.anthropic.com/2024/anthropic-fellows-program/) - Program overview

### DeepMind
- [AGI Safety and Alignment at Google DeepMind](https://deepmindsafetyresearch.medium.com/agi-safety-and-alignment-at-google-deepmind-a-summary-of-recent-work-8e600aca582a) - Team overview
- [Introducing the Frontier Safety Framework](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/) - Safety evaluation protocols

### OpenAI
- [Introducing Superalignment](https://openai.com/index/introducing-superalignment/) - July 2023
- [OpenAI dissolves Superalignment AI safety team](https://www.cnbc.com/2024/05/17/openai-superalignment-sutskever-leike.html) - May 2024
- [Anthropic hires former OpenAI safety lead Jan Leike](https://techcrunch.com/2024/05/28/anthropic-hires-former-openai-safety-lead-to-head-up-new-team/) - May 2024
- [OpenAI disbands AGI Readiness team](https://www.cnbc.com/2024/10/24/openai-miles-brundage-agi-readiness.html) - October 2024

### ARC
- [Alignment Research Center website](https://www.alignment.org/) - Organization overview
- [ARC's first technical report: Eliciting Latent Knowledge](https://www.alignment.org/blog/arcs-first-technical-report-eliciting-latent-knowledge/) - Technical paper
- [Open Philanthropy grant to ARC](https://www.openphilanthropy.org/grants/arc-general-support/) - March 2022

### Redwood Research
- [Redwood Research website](https://www.redwoodresearch.org/) - Organization overview
- [Open Philanthropy general support grant](https://www.openphilanthropy.org/grants/redwood-research-general-support/) - Funding information
- [AI Control: Improving Risk Despite Intentional Subversion](https://arxiv.org/abs/2312.06942) - ICML oral paper

### MIRI
- [MIRI's 2025 Fundraiser](https://intelligence.org/2025/12/01/miris-2025-fundraiser/) - Current fundraising campaign
- [MIRI's 2024 End-of-Year Update](https://intelligence.org/2024/12/02/miris-2024-end-of-year-update/) - Strategic direction
- [Open Philanthropy grant to MIRI (2019)](https://www.openphilanthropy.org/grants/miri-general-support-2019/) - Funding history

<ComparisonTable
  client:load
  title="Research Agenda Overview"
  columns={["Primary Focus", "Key Assumption", "Timeline View", "Theory of Change"]}
  rows={[
    {
      name: "Anthropic (Constitutional AI)",
      values: {
        "Primary Focus": "Train models to be helpful, harmless, honest via AI feedback",
        "Key Assumption": "Can instill values through training at scale",
        "Timeline View": "Short (2026-2030)",
        "Theory of Change": "Build safest frontier models → demonstrate safe scaling → set industry standard"
      }
    },
    {
      name: "Anthropic (Interpretability)",
      values: {
        "Primary Focus": "Understand model internals to verify alignment",
        "Key Assumption": "Neural networks have interpretable structure",
        "Timeline View": "Short-medium",
        "Theory of Change": "Mechanistic understanding → can verify goals → detect deception"
      }
    },
    {
      name: "OpenAI (Superalignment) [dissolved]",
      values: {
        "Primary Focus": "Use weaker AI to align stronger AI",
        "Key Assumption": "Weak-to-strong generalization works",
        "Timeline View": "Short (2027-2030)",
        "Theory of Change": "Bootstrap alignment from current models to future models"
      }
    },
    {
      name: "DeepMind (Scalable Oversight)",
      values: {
        "Primary Focus": "Debate, recursive reward modeling, process supervision",
        "Key Assumption": "Can scale human oversight to superhuman AI",
        "Timeline View": "Medium (2030-2040)",
        "Theory of Change": "Better evaluation → catch misalignment → iterate to safety"
      }
    },
    {
      name: "ARC (Eliciting Latent Knowledge)",
      values: {
        "Primary Focus": "Get AI to report what it actually knows",
        "Key Assumption": "AI has latent knowledge that could be extracted",
        "Timeline View": "Medium",
        "Theory of Change": "Solve ELK → detect deception → verify alignment"
      }
    },
    {
      name: "Redwood (AI Control)",
      values: {
        "Primary Focus": "Maintain control even with misaligned AI",
        "Key Assumption": "Don't need alignment, just control",
        "Timeline View": "Near-term focus",
        "Theory of Change": "Untrusted AI can be safely used with proper protocols"
      }
    },
    {
      name: "MIRI (Agent Foundations)",
      values: {
        "Primary Focus": "Fundamental theory of agency and goals",
        "Key Assumption": "Need conceptual breakthroughs first",
        "Timeline View": "Uncertain, possibly insufficient time",
        "Theory of Change": "Deep understanding → know what to build → align by design"
      }
    }
  ]}
/>

<ComparisonTable
  client:load
  title="Agenda Comparison by Dimension"
  columns={["Solves Inner Alignment?", "Scales to Superhuman?", "Works Without Full Alignment?", "Empirically Testable Now?"]}
  highlightColumn="Scales to Superhuman?"
  rows={[
    {
      name: "Constitutional AI",
      values: {
        "Solves Inner Alignment?": { value: "Uncertain", badge: "medium" },
        "Scales to Superhuman?": { value: "Unknown", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Interpretability",
      values: {
        "Solves Inner Alignment?": { value: "Could help verify", badge: "medium" },
        "Scales to Superhuman?": { value: "Unknown", badge: "medium" },
        "Works Without Full Alignment?": { value: "Helps detect issues", badge: "medium" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Scalable Oversight",
      values: {
        "Solves Inner Alignment?": { value: "No", badge: "low" },
        "Scales to Superhuman?": { value: "Designed to", badge: "high" },
        "Works Without Full Alignment?": { value: "Maybe", badge: "medium" },
        "Empirically Testable Now?": { value: "Partially", badge: "medium" }
      }
    },
    {
      name: "ELK",
      values: {
        "Solves Inner Alignment?": { value: "Would help detect", badge: "medium" },
        "Scales to Superhuman?": { value: "Intended to", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "Limited", badge: "low" }
      }
    },
    {
      name: "AI Control",
      values: {
        "Solves Inner Alignment?": { value: "No (bypasses it)", badge: "low" },
        "Scales to Superhuman?": { value: "Probably not", badge: "low" },
        "Works Without Full Alignment?": { value: "Yes (the point)", badge: "high" },
        "Empirically Testable Now?": { value: "Yes", badge: "high" }
      }
    },
    {
      name: "Agent Foundations",
      values: {
        "Solves Inner Alignment?": { value: "Aims to", badge: "medium" },
        "Scales to Superhuman?": { value: "Would if successful", badge: "medium" },
        "Works Without Full Alignment?": { value: "No", badge: "low" },
        "Empirically Testable Now?": { value: "No", badge: "low" }
      }
    }
  ]}
/>

<DisagreementMap
  client:load
  topic="Which Research Agenda Will Succeed?"
  description="Views on which approach is most likely to prevent AI catastrophe"
  spectrum={{ low: "Need novel breakthroughs", high: "Current approaches scale" }}
  positions={[
    { actor: "Anthropic researchers", position: "Current approaches can scale", estimate: "60-70%", confidence: "medium" },
    { actor: "DeepMind researchers", position: "Scalable oversight works", estimate: "50-60%", confidence: "medium" },
    { actor: "ARC / Paul Christiano", position: "Need targeted breakthroughs", estimate: "40-50%", confidence: "low" },
    { actor: "Redwood", position: "Control buys time", estimate: "Near-term high", confidence: "medium" },
    { actor: "MIRI / Yudkowsky", position: "Current approaches fail", estimate: "15-20%", confidence: "high" },
  ]}
/>

<KeyQuestions
  client:load
  questions={[
    {
      question: "Will current paradigm (LLMs + RLHF) lead to transformative AI?",
      positions: [
        {
          position: "Yes - scaling continues",
          confidence: "medium",
          reasoning: "Scaling laws hold. Emergent capabilities. Investment continues.",
          implications: "Current safety research is directly relevant"
        },
        {
          position: "No - paradigm shift needed",
          confidence: "medium",
          reasoning: "LLMs have fundamental limitations. New architecture will emerge.",
          implications: "May need different safety approaches for new paradigm"
        }
      ]
    },
    {
      question: "Can we verify alignment without solving it?",
      positions: [
        {
          position: "Yes - interpretability/evals can work",
          confidence: "medium",
          reasoning: "Can detect misalignment even if we can't prove alignment. Defense in depth.",
          implications: "Focus on verification and detection"
        },
        {
          position: "No - deceptive alignment defeats verification",
          confidence: "medium",
          reasoning: "Sufficiently capable AI could hide misalignment. Can't verify what we can't understand.",
          implications: "Must solve alignment directly, not just detect misalignment"
        }
      ]
    }
  ]}
/>
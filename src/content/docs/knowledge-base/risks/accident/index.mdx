---
title: Accident Risks
description: Technical failures where AI systems behave in unintended ways
sidebar:
  label: Overview
  order: 0
---

import { RiskRelationshipDiagram } from '../../../../components/wiki';

Accident risks arise when AI systems fail to do what we intend, even when no one is deliberately misusing them. These are the core concerns of technical AI safety research.

Some of these are **failure modes** (actual things that go wrong), while others are **contributing factors** (conditions that enable or increase risk). The diagram below shows how they connect.

## How These Risks Connect

<RiskRelationshipDiagram
  title=""
  layout="manual"
  nodes={[
    { id: 'emergent-capabilities', title: 'Emergent Capabilities', href: '/knowledge-base/risks/accident/emergent-capabilities/', x: 0, y: 0, color: '#94a3b8' },
    { id: 'distributional-shift', title: 'Distributional Shift', href: '/knowledge-base/risks/accident/distributional-shift/', x: 0, y: 25, color: '#94a3b8' },
    { id: 'mesa-optimization', title: 'Mesa-Optimization', href: '/knowledge-base/risks/accident/mesa-optimization/', x: 0, y: 50, color: '#94a3b8' },
    { id: 'instrumental-convergence', title: 'Instrumental Convergence', href: '/knowledge-base/risks/accident/instrumental-convergence/', x: 0, y: 75, color: '#94a3b8' },
    { id: 'sycophancy', title: 'Sycophancy', href: '/knowledge-base/risks/accident/sycophancy/', x: 25, y: 0 },
    { id: 'reward-hacking', title: 'Reward Hacking', href: '/knowledge-base/risks/accident/reward-hacking/', x: 25, y: 37 },
    { id: 'goal-misgeneralization', title: 'Goal Misgeneralization', href: '/knowledge-base/risks/accident/goal-misgeneralization/', x: 45, y: 25 },
    { id: 'sharp-left-turn', title: 'Sharp Left Turn', href: '/knowledge-base/risks/accident/sharp-left-turn/', x: 45, y: 50 },
    { id: 'deceptive-alignment', title: 'Deceptive Alignment', href: '/knowledge-base/risks/accident/deceptive-alignment/', x: 45, y: 75 },
    { id: 'scheming', title: 'Scheming', href: '/knowledge-base/risks/accident/scheming/', x: 65, y: 50 },
    { id: 'sandbagging', title: 'Sandbagging', href: '/knowledge-base/risks/accident/sandbagging/', x: 65, y: 75 },
    { id: 'treacherous-turn', title: 'Treacherous Turn', href: '/knowledge-base/risks/accident/treacherous-turn/', x: 85, y: 50 },
    { id: 'power-seeking', title: 'Power-Seeking', href: '/knowledge-base/risks/accident/power-seeking/', x: 85, y: 75 },
    { id: 'corrigibility-failure', title: 'Corrigibility Failure', href: '/knowledge-base/risks/accident/corrigibility-failure/', x: 85, y: 100 },
  ]}
  edges={[
    { from: 'emergent-capabilities', to: 'sharp-left-turn', label: 'enables' },
    { from: 'distributional-shift', to: 'goal-misgeneralization', label: 'causes' },
    { from: 'mesa-optimization', to: 'deceptive-alignment', label: 'can produce' },
    { from: 'instrumental-convergence', to: 'power-seeking', label: 'predicts' },
    { from: 'sycophancy', to: 'reward-hacking', label: 'type of' },
    { from: 'reward-hacking', to: 'goal-misgeneralization', label: 'causes' },
    { from: 'sharp-left-turn', to: 'goal-misgeneralization', label: 'accelerates' },
    { from: 'goal-misgeneralization', to: 'deceptive-alignment', label: 'if strategic' },
    { from: 'deceptive-alignment', to: 'scheming', label: 'manifests as' },
    { from: 'sandbagging', to: 'scheming', label: 'type of' },
    { from: 'scheming', to: 'treacherous-turn', label: 'culminates in' },
    { from: 'treacherous-turn', to: 'power-seeking', label: 'involves' },
    { from: 'treacherous-turn', to: 'corrigibility-failure', label: 'involves' },
  ]}
  width={1050}
  height={420}
/>

*Gray nodes are contributing factors; blue nodes are failure modes.*

---

## Contributing Factors

These aren't failures themselves, but conditions and dynamics that enable or increase accident risks:

- [Emergent Capabilities](/knowledge-base/risks/accident/emergent-capabilities/) - Unexpected abilities appearing at scale, making prediction difficult
- [Distributional Shift](/knowledge-base/risks/accident/distributional-shift/) - Deployment environments differing from training
- [Mesa-Optimization](/knowledge-base/risks/accident/mesa-optimization/) - Learned inner optimizers with potentially different objectives
- [Instrumental Convergence](/knowledge-base/risks/accident/instrumental-convergence/) - Why diverse goals lead to similar dangerous subgoals

## Goal & Optimization Failures

Failures in how AI systems learn and pursue objectives:

- [Goal Misgeneralization](/knowledge-base/risks/accident/goal-misgeneralization/) - Goals that don't transfer to new contexts
- [Reward Hacking](/knowledge-base/risks/accident/reward-hacking/) - Gaming reward signals or specifications in unintended ways
- [Sharp Left Turn](/knowledge-base/risks/accident/sharp-left-turn/) - Capabilities generalizing while alignment doesn't
- [Sycophancy](/knowledge-base/risks/accident/sycophancy/) - Telling users what they want to hear

## Deception & Strategic Behavior

Risks involving AI systems that strategically deceive humans:

- [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) - Appearing aligned during training, diverging in deployment
- [Scheming](/knowledge-base/risks/accident/scheming/) - Strategic deception to pursue hidden goals
- [Sandbagging](/knowledge-base/risks/accident/sandbagging/) - Hiding capabilities during evaluation
- [Treacherous Turn](/knowledge-base/risks/accident/treacherous-turn/) - Cooperating until powerful enough to defect

## Dangerous Behaviors

Behavioral patterns that emerge from optimization and pose risks:

- [Power-Seeking](/knowledge-base/risks/accident/power-seeking/) - Tendency to acquire resources and influence
- [Corrigibility Failure](/knowledge-base/risks/accident/corrigibility-failure/) - Resistance to correction or shutdown

## Enabling Capabilities

These risks become more dangerous as AI systems gain certain [capabilities](/knowledge-base/capabilities/):

| Risk | Key Enabling Capabilities |
|------|---------------------------|
| [Deceptive Alignment](/knowledge-base/risks/accident/deceptive-alignment/) | [Situational Awareness](/knowledge-base/capabilities/situational-awareness/), [Persuasion](/knowledge-base/capabilities/persuasion/) |
| [Scheming](/knowledge-base/risks/accident/scheming/) | [Situational Awareness](/knowledge-base/capabilities/situational-awareness/), [Reasoning](/knowledge-base/capabilities/reasoning/), [Long-Horizon Tasks](/knowledge-base/capabilities/long-horizon/) |
| [Treacherous Turn](/knowledge-base/risks/accident/treacherous-turn/) | [Reasoning](/knowledge-base/capabilities/reasoning/), [Long-Horizon Tasks](/knowledge-base/capabilities/long-horizon/) |
| [Power-Seeking](/knowledge-base/risks/accident/power-seeking/) | [Agentic AI](/knowledge-base/capabilities/agentic-ai/), [Tool Use](/knowledge-base/capabilities/tool-use/), [Reasoning](/knowledge-base/capabilities/reasoning/) |
| [Corrigibility Failure](/knowledge-base/risks/accident/corrigibility-failure/) | [Agentic AI](/knowledge-base/capabilities/agentic-ai/), [Tool Use](/knowledge-base/capabilities/tool-use/) |
| [Sharp Left Turn](/knowledge-base/risks/accident/sharp-left-turn/) | [Self-Improvement](/knowledge-base/capabilities/self-improvement/) |

A model without [situational awareness](/knowledge-base/capabilities/situational-awareness/) cannot strategically game its training process. A model without [agentic capabilities](/knowledge-base/capabilities/agentic-ai/) cannot seek power in the real world. Understanding capability prerequisites helps prioritize safety research.

---

## What Makes These "Accidents"

These risks don't require malicious intent from developers or users. They arise from the difficulty of:

1. **Specifying objectives** - Precisely defining what we want
2. **Robust learning** - Ensuring learned behaviors generalize correctly
3. **Maintaining control** - Keeping AI systems correctable
4. **Predicting capabilities** - Knowing what systems can do before they do it

The common thread: AI systems optimizing for something subtly different from what we actually want.

---

## Contributing Risk Factors

These accident risks are amplified by dynamics documented in **[Risk Factors](/knowledge-base/risk-factors/)**:

| Factor | How It Contributes |
|--------|-------------------|
| [Racing Dynamics](/knowledge-base/risk-factors/racing-dynamics/) | Less time for safety research, rushed deployment |
| [Flash Dynamics](/knowledge-base/risk-factors/flash-dynamics/) | AI operates too fast for human oversight to catch errors |

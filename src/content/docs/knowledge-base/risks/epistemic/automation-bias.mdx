---
title: Automation Bias
description: Excessive reliance on AI outputs without appropriate scrutiny
sidebar:
  order: 3
---

import { InfoBox, EntityCard, EntityCards, Tags, Sources, Section } from '../../../../../components/wiki';

<InfoBox
  type="risk"
  title="Automation Bias"
  severity="medium"
  likelihood="Very High (occurring)"
  timeframe="Current"
  customFields={[
    { label: "Type", value: "Epistemic" },
    { label: "Status", value: "Widespread" },
  ]}
/>

## Overview

Automation bias is the tendency to over-trust automated systems and AI outputs, accepting their conclusions without appropriate scrutiny. Humans are prone to defer to systems that appear authoritative, especially when those systems are usually right. This creates vulnerability when systems are wrong.

As AI becomes more capable and more pervasive, automation bias becomes more consequential.

## Why This Happens

AI systems are often right. This breeds justified trust. But the same trust that's appropriate for high-accuracy systems becomes inappropriate for errors.

Authority heuristics make us defer to perceived experts. AI systems often present information authoritatively, triggering this heuristic even when unjustified.

Cognitive load reduction is valuable. Checking AI outputs takes effort. Trusting them saves effort. When outcomes usually work out, people stop checking.

Opacity makes verification hard. Even when people want to check AI outputs, understanding why the AI said something is often difficult or impossible.

Skill atrophy means that over time, less verification leads to reduced ability to verify. Radiologists who always defer to AI gradually lose the ability to read images independently.

## Manifestations

Medical AI: Doctors may defer to AI diagnoses without applying full clinical judgment. When AI is right 90% of the time, the 10% of errors go unquestioned.

Navigation: GPS reduces spatial reasoning. When GPS fails, people sometimes cannot navigate manually. Some drive into lakes following GPS instructions.

Content moderation: Moderators reviewing AI-flagged content may accept the AI's judgment rather than forming their own.

Coding assistants: Developers may accept AI-generated code without thorough review, introducing bugs or security vulnerabilities.

Search and chatbots: Users may accept first AI-generated answer without verification, especially for questions they can't easily check.

## Hallucinations and Overconfidence

AI systems, particularly large language models, frequently hallucinateâ€”generating confident but false information. This is especially dangerous combined with automation bias. Users see confident, authoritative-sounding text and accept it uncritically.

The problem compounds: AI systems don't express appropriate uncertainty, and users don't supply it.

## Consequences

Errors propagate: When AI is wrong and no one checks, wrong information spreads. Doctors misdiagnose, lawyers cite fake cases, students learn falsehoods.

Skills degrade: When humans stop practicing verification, the ability to verify atrophies. This creates AI dependency.

Accountability diffuses: When decisions are AI-assisted, who is responsible for errors? Blaming the AI lets humans off the hook.

Adversarial vulnerability: Actors who know humans trust AI can manipulate AI inputs or outputs to manipulate human decisions.

## Mitigations

Calibrated trust means understanding when AI is reliable and when it isn't. This requires transparency about AI accuracy and clear signals of uncertainty.

Active verification requires building in processes where humans must actively check AI outputs rather than passively accept them. Friction can be valuable.

Maintaining skills means deliberately practicing the abilities AI could replace, to maintain human capability for oversight.

Appropriate skepticism involves developing cultural norms that question AI outputs, especially for important decisions.

Interface design can present AI outputs as suggestions rather than conclusions, requiring explicit human decision rather than passive acceptance.

<Section title="Related Topics">
  <Tags tags={[
    "Human-AI Interaction",
    "Trust",
    "Decision Making",
    "Cognitive Bias",
    "AI Safety",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="sycophancy"
      category="risk"
      title="Sycophancy"
      description="AI telling users what they want to hear"
    />
    <EntityCard
      id="enfeeblement"
      category="risk"
      title="Enfeeblement"
      description="Capability loss from AI dependence"
    />
    <EntityCard
      id="erosion-of-agency"
      category="risk"
      title="Erosion of Human Agency"
      description="Broader loss of human judgment"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "Automation Bias in Decision Making", author: "Parasuraman & Manzey" },
  { title: "The Glass Cage", author: "Nicholas Carr" },
  { title: "Human Factors research on automation" },
]} />

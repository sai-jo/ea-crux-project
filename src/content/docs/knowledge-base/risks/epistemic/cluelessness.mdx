---
title: Cluelessness
description: Can we meaningfully predict long-term consequences at all?
sidebar:
  order: 4
---

import { InfoBox, KeyQuestions, Section, Tags, Sources } from '../../../../../components/wiki';

<InfoBox
  type="epistemic"
  title="Cluelessness"
  customFields={[
    { label: "Core Problem", value: "Long-term consequences are radically unpredictable" },
    { label: "Key Research", value: "Hilary Greaves, William MacAskill, James Lenman" },
    { label: "Challenge", value: "Does this undermine longtermism?" },
  ]}
/>

## The Problem

**Cluelessness**: We cannot reliably predict the long-term consequences of our actions.

This matters for AI safety because:
- AI risk arguments often invoke long-term future (10^50 lives)
- Preventing extinction requires predicting that extinction *would* happen otherwise
- The value of AI safety work depends on counterfactual impact

If we're clueless about long-term effects, how can we justify prioritizing AI safety over anything else?

---

## The Philosophical Argument

James Lenman's original argument (2000):

1. The long-term consequences of actions are vast (ripple effects continue forever)
2. We cannot predict these consequences
3. Therefore, we cannot know if our actions are good or bad overall
4. Consequentialism is undermined

**Example**: Consider donating to a charity. This affects:
- Immediate beneficiaries (predictable)
- Their children and grandchildren (less predictable)
- Who meets whom, who is born (unpredictable)
- Cascade effects across centuries (clueless)

Any action's effects after ~50 years are essentially random—as likely to be good as bad.

---

## Types of Cluelessness

Hilary Greaves distinguishes:

### Simple Cluelessness
You don't know the expected value of an action because you can't calculate it.

**Status**: This is just ignorance. More information helps.

### Complex Cluelessness
Even ideally, you couldn't form expectations—the probability distribution is undefined or intractable.

**Status**: This is a deeper problem. No amount of information resolves it.

### Symmetric Cluelessness
Unforeseeable consequences are as likely to be positive as negative—they wash out.

**Implication**: Can ignore long-term effects; focus on predictable near-term effects.

### Asymmetric Cluelessness
There's reason to think unforeseeable consequences skew positive or negative.

**Implication**: Might still matter even if unpredictable.

---

## Why This Challenges AI Safety

### The Challenge

Standard AI safety reasoning:
1. AI might cause extinction
2. Extinction is very bad (eliminates future)
3. Therefore, preventing AI extinction risk is very important

**Cluelessness objection**:
- We can't predict whether preventing this extinction leads to better outcomes
- Perhaps humanity goes on to cause worse suffering
- Perhaps a different path leads to better outcomes
- We're clueless about long-term effects either way

### Specific Applications

| AI Safety Claim | Cluelessness Challenge |
|-----------------|----------------------|
| "Preventing extinction is good" | Maybe post-extinction evolution produces something better? |
| "Alignment is crucial" | Maybe aligned AI leads to worse outcomes than misaligned? |
| "Fast takeoff is bad" | Maybe slow takeoff leads to wars; fast takeoff prevents them? |
| "We should delay AI" | Maybe delay leads to worse actors developing AI first? |

---

## Responses to Cluelessness

### 1. Symmetric Wash-Out

**Response**: Unforeseeable effects are symmetric—cancel out in expectation.

**Then**: Focus on foreseeable effects. Preventing extinction has clearly positive foreseeable effects.

**Problem**: Why think long-term effects are symmetric? History shows path dependence.

### 2. Near-Termism

**Response**: Accept cluelessness; focus on near-term effects we can predict.

**Then**: AI safety matters for near-term effects (preventing catastrophic accidents, not for astronomical futures).

**Problem**: Gives up much of the case for longtermism.

### 3. Option Value

**Response**: Keeping humans alive preserves option value. Even if we're clueless about what to do, existence allows us to learn.

**Then**: Preventing extinction is valuable regardless of long-term predictions.

**Problem**: Why does human existence have more option value than alternatives?

### 4. Robust Actions

**Response**: Some actions are robustly positive across scenarios where we're uncertain.

**Then**: Focus on robustly good actions. "Don't cause extinction" is plausibly robust.

**Problem**: How do we know an action is robust if we're clueless about effects?

### 5. Evidential vs. Causal Decision Theory

**Response**: Perhaps what matters is evidence about character, not causal consequences.

**Then**: Working on AI safety is evidence of good values, regardless of consequences.

**Problem**: Most consequentialists reject this move.

### 6. Higher-Order Evidence

**Response**: While object-level predictions are clueless, we can have higher-order evidence.

**Example**: "Actions that seem good have historically been good" gives evidence even without object-level prediction.

**Problem**: Requires historical patterns to continue.

---

## The Case for Asymmetric Cluelessness

Some argue long-term effects aren't symmetric:

### Why Effects Might Skew Positive

- Human-guided development is better than random evolution
- Intelligence is generally good at achieving goals, including good ones
- Existence allows course-correction; extinction doesn't

### Why Effects Might Skew Negative

- Suffering might be more likely than flourishing by default
- Powerful technology tends toward misuse
- Lock-in of bad values is possible

The direction of asymmetry is itself uncertain.

---

## Practical Implications

### For AI Safety Prioritization

| Position on Cluelessness | Implication for AI Safety |
|--------------------------|--------------------------|
| **Reject cluelessness** | AI safety importance depends on long-term effects |
| **Symmetric cluelessness** | AI safety matters for near-term effects only |
| **Asymmetric-positive** | AI safety is important (extinction cuts off positive future) |
| **Asymmetric-negative** | Maybe AI risk work doesn't matter (extinction might be okay?) |

### For Research

- **Build the case for asymmetry**: If you think AI safety matters, argue why extinction is robustly bad
- **Focus on near-term**: Make cases that don't require long-term prediction
- **Acknowledge uncertainty**: Don't pretend we know long-term effects

### For Decision-Making Under Cluelessness

If you accept some cluelessness:
1. **Robustness over expected value**: Prefer actions good across scenarios
2. **Option value**: Prefer reversible actions that preserve choices
3. **Humility**: Accept you might be wrong; avoid irreversible commitments
4. **Near-term anchoring**: Weight predictable effects more heavily

---

## Connection to Other Epistemic Issues

| Issue | Connection |
|-------|-----------|
| **Conjunctive arguments** | Cluelessness compounds across steps |
| **Pascal's Mugging** | Both involve uncertain extreme outcomes |
| **Expert calibration** | Experts may be clueless about long-term too |
| **Inside vs outside view** | Inside view may overstate predictability |

<KeyQuestions
  questions={[
    "Is your case for AI safety importance robust to cluelessness about long-term effects?",
    "Can you make the AI safety case purely on near-term grounds?",
    "Why think the long-term effects of your actions skew positive rather than negative?",
    "How does cluelessness interact with expected value reasoning?"
  ]}
/>

<Section title="Related Topics">
  <Tags tags={[
    "Epistemics",
    "Longtermism",
    "Uncertainty",
    "Consequentialism",
    "Decision Theory",
  ]} />
</Section>

<Sources sources={[
  { title: "Cluelessness", author: "Hilary Greaves", date: "2016", url: "https://www.journals.uchicago.edu/doi/10.1086/683652" },
  { title: "Consequentialism and Cluelessness", author: "James Lenman", date: "2000", url: "https://www.jstor.org/stable/3751337" },
  { title: "What We Owe the Future", author: "William MacAskill", date: "2022", notes: "Chapter on cluelessness", url: "https://www.amazon.com/What-We-Owe-Future-William/dp/1541618629" },
  { title: "The Case for Strong Longtermism", author: "Hilary Greaves & William MacAskill", date: "2021", url: "https://globalprioritiesinstitute.org/hilary-greaves-william-macaskill-the-case-for-strong-longtermism-2/" },
  { title: "Against the Astronomical Value of the Long-Term Future", author: "David Thorstad", date: "2023", url: "https://globalprioritiesinstitute.org/against-the-astronomical-value-of-the-long-term-future-david-thorstad/" },
]} />

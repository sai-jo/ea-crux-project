---
title: Conjunctive Arguments
description: How probability compounds through chains of uncertain claims
sidebar:
  order: 2
---

import { InfoBox, KeyQuestions, Section, Tags, Sources } from '../../../../../components/wiki';

<InfoBox
  type="epistemic"
  title="Conjunctive Arguments"
  customFields={[
    { label: "Core Problem", value: "Each step multiplies uncertainty" },
    { label: "Key Insight", value: "90% × 90% × 90% × 90% × 90% = 59%" },
    { label: "Application", value: "AI risk arguments often have 5-10 steps" },
  ]}
/>

## The Problem

Many AI risk arguments have the following structure:

> AI will be transformative (A) AND will be goal-directed (B) AND alignment will be hard (C) AND we won't get warning signs (D) AND coordination will fail (E) → catastrophe

Each step may seem plausible individually. But if each step has only 80% probability, the conjunction is:

**0.8 × 0.8 × 0.8 × 0.8 × 0.8 = 33%**

This is the **conjunction fallacy in reverse**: arguments that seem compelling step-by-step may be weak overall.

---

## The Mathematics

### Basic Conjunction Rule

For independent events A and B:
- P(A and B) = P(A) × P(B)

For n independent events each with probability p:
- P(all occur) = p^n

| Steps | P(each step) = 90% | P(each step) = 80% | P(each step) = 70% |
|-------|-------------------|-------------------|-------------------|
| 3 | 73% | 51% | 34% |
| 5 | 59% | 33% | 17% |
| 7 | 48% | 21% | 8% |
| 10 | 35% | 11% | 3% |

### The Problem with Dependence

The above assumes independence. In practice, claims in arguments are often correlated:

- **Positive correlation**: If A is true, B is more likely (e.g., "AI will be powerful" and "AI will be dangerous")
- **Negative correlation**: If A is true, B is less likely (e.g., "alignment is hard" and "we'll solve it anyway")

Correlation complicates the math but doesn't eliminate the problem:
- Positive correlation raises the conjunction probability
- Negative correlation lowers it
- The direction isn't always obvious

---

## Application to AI Risk Arguments

### The Standard AI Risk Argument

Consider the core argument chain:

| Step | Claim | Estimated P |
|------|-------|-------------|
| 1 | TAI will be developed in coming decades | 70-90% |
| 2 | TAI will be sufficiently powerful to pose x-risk | 60-80% |
| 3 | TAI will be goal-directed in relevant ways | 50-80% |
| 4 | Alignment will not be solved in time | 30-60% |
| 5 | Misalignment will be catastrophic (not contained) | 40-70% |
| 6 | Coordination will fail to prevent deployment | 50-80% |

**If we take the low end of each**: 0.7 × 0.6 × 0.5 × 0.3 × 0.4 × 0.5 = **1.3%**

**If we take the high end**: 0.9 × 0.8 × 0.8 × 0.6 × 0.7 × 0.8 = **19%**

This enormous range (1% to 19%) explains much of the disagreement in the field—different estimates of individual steps compound dramatically.

### The "Disjunctive" Response

AI risk advocates sometimes respond: "But there are multiple paths to catastrophe. It's a disjunction, not conjunction."

This is partially valid. If there are k independent paths to catastrophe, each with probability p:

**P(at least one) = 1 - (1-p)^k**

| Paths (k) | P(each path) = 5% | P(each path) = 10% |
|-----------|-------------------|-------------------|
| 3 | 14% | 27% |
| 5 | 23% | 41% |
| 10 | 40% | 65% |

But note:
- Each "path" is itself a conjunction
- Paths may not be independent
- This cuts both ways—there are also multiple paths to safety

---

## Common Errors

### 1. Step-by-Step Persuasion

A common rhetorical pattern:
> "Surely you agree that AI will be powerful? And surely you agree that goals could be misaligned? And surely..."

Each "surely" demands assent to a high probability (say, 90%). String five together and you've smuggled in a 59% conjunction while each step felt obvious.

**Antidote**: After going through steps, multiply the probabilities explicitly.

### 2. Ignoring Dependencies

Treating correlated claims as independent can:
- Understate risk (if claims are positively correlated)
- Overstate risk (if claims are negatively correlated)

**Example**: "AI will be powerful" and "AI will be uncontrollable" are probably negatively correlated—the very capabilities that make AI powerful might make it controllable (or vice versa). Treating them as independent may overstate risk.

### 3. Hidden Assumptions

Arguments often have unstated premises:
- "Alignment is hard" may assume "AI will use current architectures"
- "Coordination will fail" may assume "current institutions"

These hidden steps have their own probabilities.

### 4. Double-Counting

The same uncertainty may appear multiple times:
- "Takeoff will be fast" affects both "no warning signs" and "coordination fails"
- Counting it in both steps double-counts the uncertainty

---

## What This Means for AI Risk Estimates

### For High P(doom) Estimates (>50%)

To reach >50% requires either:
- Very high confidence in each step (>90% each for 7 steps)
- Disjunctive structure (many independent paths)
- Strong positive correlation between steps

**Question to ask**: Can you justify >90% confidence in each step? If not, how are you reaching >50%?

### For Low P(doom) Estimates (Less Than 5%)

Reaching less than 5% requires either:
- Low confidence in at least one step (under 30-50%)
- Negative correlations between steps
- Belief that safety paths are more likely than risk paths

**Question to ask**: Which specific step do you doubt? Or do you doubt the argument structure itself?

### For Middle Estimates (5-25%)

This is the natural range when:
- Each step is 60-80% likely
- Argument has 5-7 steps
- Moderate independence assumed

Most careful analysts end up here—which is consistent with the conjunction math.

---

## Responses and Counter-Responses

### "But Each Step Really Is ~90% Likely"

If you believe each step is 90%+ likely:
- Make this explicit
- Explain why your confidence is so high
- Consider whether calibration research suggests such confidence is warranted

### "The Disjunctive Structure Dominates"

If you believe multiple independent paths exist:
- Enumerate the paths explicitly
- Estimate each path's probability
- Consider whether paths are truly independent

### "Correlation Changes Everything"

If you believe strong correlations exist:
- Specify the direction (positive or negative)
- Estimate the correlation strength
- Show how this affects the overall estimate

### "This Is Just Pascal's Wager Skepticism"

Not quite. The conjunction problem doesn't say we should ignore low-probability high-stakes risks. It says:
- Arguments structured as conjunctions may be weaker than they appear
- We should be explicit about step probabilities
- Overall estimates should be consistent with step estimates

---

## Practical Implications

### For Evaluating Arguments

1. **Decompose**: Identify all the steps in the argument
2. **Estimate each**: What probability do you assign to each step?
3. **Multiply**: What overall probability does this imply?
4. **Check consistency**: Does your overall view match the product?

### For Communicating Arguments

1. **Be explicit**: State your probability for each step
2. **Show your math**: Explain how steps combine
3. **Acknowledge sensitivity**: Note which steps most affect the conclusion
4. **Invite disagreement**: Ask which steps others doubt

### For Decision-Making

Given conjunction uncertainty:
- **Seek robustness**: Prefer actions valuable across probability ranges
- **Value information**: Resolving any step's uncertainty is valuable
- **Avoid false precision**: Don't anchor on specific probability estimates

<KeyQuestions
  questions={[
    "How many steps does the strongest AI risk argument require?",
    "What's the highest probability you'd assign to any step with genuine uncertainty?",
    "Are the steps in standard AI risk arguments positively or negatively correlated?",
    "If your overall P(doom) implies 90%+ per step, can you justify that confidence?"
  ]}
/>

<Section title="Related Topics">
  <Tags tags={[
    "Probability",
    "Arguments",
    "Epistemics",
    "AI Risk",
    "Reasoning",
  ]} />
</Section>

<Sources sources={[
  { title: "The Precipice", author: "Toby Ord", date: "2020", notes: "Discusses risk decomposition", url: "https://theprecipice.com/" },
  { title: "AGI Ruin: A List of Lethalities", author: "Eliezer Yudkowsky", date: "2022", notes: "Example of conjunctive argument", url: "https://www.lesswrong.com/posts/uMQ3cqWDPHhjtiesc/agi-ruin-a-list-of-lethalities" },
  { title: "Why I'm Not (Yet) a Full-Time AI Safety Researcher", author: "Carl Shulman", notes: "Discusses argument structure" },
  { title: "Extensional vs. Intensional Probability", author: "E.T. Jaynes", date: "2003", notes: "On conjunction and probability" },
  { title: "Thinking, Fast and Slow (Chapter on Conjunction Fallacy)", author: "Daniel Kahneman", date: "2011" },
]} />

---
title: Epistemic Cruxes
description: Key uncertainties that determine which epistemic risks matter most and which solutions work
sidebar:
  order: 99
---

import { Crux, CruxList, DisagreementMap, KeyQuestions, Section, Tags } from '../../../../../components/wiki';

## What Are Epistemic Cruxes?

**Cruxes** are the key uncertainties where your answer determines your overall view on an issue. For epistemic risks and solutions, your position on a few key cruxes largely determines:
- Which failure modes you expect
- Which solutions you prioritize
- How urgent you think the problem is
- Whether defense or offense will dominate

---

## Critical Cruxes

<Crux
  id="detection-arms-race"
  question="Can AI detection keep pace with AI generation?"
  domain="Authentication & Verification"
  description="Whether deepfake detection, text detection, and content verification can match the pace of synthetic content generation."
  importance="critical"
  resolvability="years"
  currentState="Detection currently losing; gap widening"
  positions={[
    {
      view: "Detection will fall permanently behind",
      probability: "40-60%",
      holders: ["Hany Farid", "Most deepfake researchers"],
      implications: "Must shift to provenance-based authentication; detection-based approaches are dead end"
    },
    {
      view: "Equilibrium will emerge",
      probability: "20-40%",
      implications: "Both approaches valuable; detection as complement to provenance"
    },
    {
      view: "Detection can win with enough resources",
      probability: "10-30%",
      implications: "Invest heavily in detection research and deployment"
    }
  ]}
  wouldUpdateOn={[
    "Major breakthrough in AI detection that generalizes across generators",
    "Theoretical proof that detection is fundamentally harder than generation",
    "Real-world data on detection accuracy trends over time",
    "Adversarial testing at scale showing detection robustness (or lack thereof)"
  ]}
  relatedCruxes={["authentication-adoption", "trust-rebuilding"]}
  relevantResearch={[
    { title: "AI-generated text detection unreliable", url: "https://arxiv.org/abs/2303.11156" },
    { title: "DARPA MediFor", url: "https://www.darpa.mil/program/media-forensics" }
  ]}
/>

<Crux
  id="authentication-adoption"
  question="Will content authentication (C2PA) achieve critical mass adoption?"
  domain="Authentication & Verification"
  description="Whether standards like C2PA will be adopted widely enough to create a two-tier content ecosystem (authenticated vs. unauthenticated)."
  importance="critical"
  resolvability="years"
  currentState="Early deployment; major platforms uncommitted"
  positions={[
    {
      view: "Adoption will be widespread",
      probability: "30-50%",
      holders: ["Adobe", "Microsoft", "C2PA coalition"],
      implications: "Invest in provenance infrastructure; detection becomes less critical"
    },
    {
      view: "Adoption will be partial/fragmented",
      probability: "30-40%",
      implications: "Need hybrid strategy; authentication + detection + literacy"
    },
    {
      view: "Adoption will fail",
      probability: "20-30%",
      holders: ["Skeptics of voluntary standards"],
      implications: "Need regulatory mandates or alternative approaches; pure market solution won't work"
    }
  ]}
  wouldUpdateOn={[
    "Major platforms (Meta, TikTok, Twitter) committing to C2PA display",
    "Camera manufacturers shipping authentication by default",
    "Evidence that users value/use credentials in practice",
    "Major failure of authentication system (hacking, gaming)"
  ]}
  relatedCruxes={["detection-arms-race"]}
  relevantResearch={[
    { title: "C2PA Specification", url: "https://c2pa.org/" },
    { title: "Content Authenticity Initiative", url: "https://contentauthenticity.org/" }
  ]}
/>

<Crux
  id="trust-rebuilding"
  question="Can institutional trust be rebuilt after collapse?"
  domain="Social Epistemics"
  description="Whether, once trust in institutions collapses, there are mechanisms to rebuild itâ€”or if collapse is a one-way door."
  importance="critical"
  resolvability="decades"
  currentState="Trust declining; no proven rebuild mechanisms"
  positions={[
    {
      view: "Trust collapse is reversible",
      probability: "30-40%",
      implications: "Invest in institutional reform; focus on earning trust through demonstrated reliability"
    },
    {
      view: "Trust can stabilize at lower level",
      probability: "30-40%",
      implications: "Accept new equilibrium; build systems that work with low trust"
    },
    {
      view: "Trust collapse is self-reinforcing spiral",
      probability: "20-30%",
      holders: ["Some political scientists"],
      implications: "Preventing collapse is critical; once started, may be irreversible"
    }
  ]}
  wouldUpdateOn={[
    "Historical examples of successful trust rebuilding after collapse",
    "Mechanisms that demonstrably rebuild trust in experimental settings",
    "Evidence that trust is stabilizing (Edelman data, Gallup)",
    "New institutions gaining broad trust"
  ]}
  relatedCruxes={["polarization-trajectory"]}
  relevantResearch={[
    { title: "Edelman Trust Barometer", url: "https://www.edelman.com/trust" },
    { title: "Fukuyama: Trust", url: "" }
  ]}
/>

---

## High-Importance Cruxes

<Crux
  id="expertise-preservation"
  question="Can human expertise be preserved alongside AI assistance?"
  domain="Human Factors"
  description="Whether humans can maintain evaluative skills while routinely using AI, or if skill atrophy is inevitable."
  importance="high"
  resolvability="years"
  currentState="Early evidence of atrophy in some domains (navigation, aviation)"
  positions={[
    {
      view: "Atrophy is inevitable without active countermeasures",
      probability: "40-50%",
      implications: "Must mandate skill maintenance; design AI to preserve skills; accept some efficiency loss"
    },
    {
      view: "Some skills atrophy, critical ones can be preserved",
      probability: "30-40%",
      implications: "Prioritize which skills to preserve; focus resources on critical domains"
    },
    {
      view: "New skills emerge that replace old ones",
      probability: "20-30%",
      implications: "Focus on new skills (AI collaboration, verification); don't try to preserve all old skills"
    }
  ]}
  wouldUpdateOn={[
    "Longitudinal studies on skill retention with AI use",
    "Evidence from domains with long AI assistance history (aviation, radiology)",
    "Successful skill preservation programs",
    "Analysis of what skills are actually needed for oversight"
  ]}
  relatedCruxes={["human-ai-complementarity"]}
  relevantResearch={[
    { title: "The Glass Cage", url: "https://www.nicholascarr.com/" },
    { title: "FAA Human Factors", url: "https://www.faa.gov/about/initiatives/maintenance_hf" }
  ]}
/>

<Crux
  id="sycophancy-solvability"
  question="Can AI sycophancy be eliminated without sacrificing user satisfaction?"
  domain="AI Behavior"
  description="Whether AI systems can be designed to be honest (disagreeing when users are wrong) while remaining popular/useful."
  importance="high"
  resolvability="years"
  currentState="Sycophancy is default behavior; anti-sycophancy training in development"
  positions={[
    {
      view: "Honesty and popularity are compatible",
      probability: "30-40%",
      holders: ["Anthropic (Constitutional AI)"],
      implications: "Invest in training honest AI; users will adapt"
    },
    {
      view: "Trade-off is real but manageable",
      probability: "40-50%",
      implications: "Design different modes (coach vs assistant); accept some sycophancy in some contexts"
    },
    {
      view: "Users will always prefer agreeable AI",
      probability: "20-30%",
      implications: "Market pressure will push toward sycophancy; need non-market solutions"
    }
  ]}
  wouldUpdateOn={[
    "User studies showing preference for honest AI",
    "Commercial success of less-sycophantic AI",
    "Research on framing/presenting disagreement effectively",
    "Long-term effects of sycophantic AI on user beliefs"
  ]}
  relatedCruxes={["detection-arms-race"]}
  relevantResearch={[
    { title: "Anthropic sycophancy research", url: "https://arxiv.org/abs/2310.13548" },
    { title: "Constitutional AI", url: "https://arxiv.org/abs/2212.08073" }
  ]}
/>

<Crux
  id="coordination-feasibility"
  question="Can AI governance achieve meaningful international coordination?"
  domain="Coordination"
  description="Whether nations with competing interests can coordinate on AI governance, especially around epistemic issues."
  importance="high"
  resolvability="years"
  currentState="Early summits; no binding agreements; great power competition"
  positions={[
    {
      view: "Coordination is achievable with effort",
      probability: "30-40%",
      holders: ["Some policy researchers"],
      implications: "Invest heavily in diplomatic channels; AI summits; international institutions"
    },
    {
      view: "Narrow coordination possible, broad coordination unlikely",
      probability: "40-50%",
      implications: "Focus on achievable goals (compute reporting, safety standards); don't expect comprehensive regime"
    },
    {
      view: "Coordination will fail; prepare for fragmentation",
      probability: "20-30%",
      holders: ["Realists"],
      implications: "Focus on domestic governance; coalition of willing; defensive measures"
    }
  ]}
  wouldUpdateOn={[
    "Success/failure of AI Safety Summits",
    "Binding international agreements on AI",
    "Evidence of coordination on compute governance",
    "Defection from voluntary commitments by major players"
  ]}
  relatedCruxes={["trust-rebuilding"]}
  relevantResearch={[
    { title: "GovAI", url: "https://www.governance.ai/" },
    { title: "Compute governance", url: "https://arxiv.org/abs/2402.08797" }
  ]}
/>

<Crux
  id="human-ai-complementarity"
  question="Can AI-human hybrid systems be designed to get best of both?"
  domain="Human Factors"
  description="Whether hybrid systems can avoid automation bias (over-trust) and automation disuse (under-trust) simultaneously."
  importance="high"
  resolvability="years"
  currentState="Mixed evidence; design patterns emerging but not proven at scale"
  positions={[
    {
      view: "Optimal complementarity is achievable",
      probability: "30-40%",
      implications: "Invest in hybrid system design; research on human-AI collaboration"
    },
    {
      view: "Complementarity depends heavily on domain and design",
      probability: "40-50%",
      implications: "Context-specific solutions; no one-size-fits-all; careful empirical work"
    },
    {
      view: "Humans will either over- or under-trust; can't get both right",
      probability: "20-30%",
      implications: "Accept imperfect hybrids; design for specific failure mode to avoid"
    }
  ]}
  wouldUpdateOn={[
    "Systematic studies of human-AI collaboration across domains",
    "Successful long-term hybrid deployments",
    "Design patterns that generalize across contexts",
    "Cognitive science on calibrating trust in AI"
  ]}
  relatedCruxes={["expertise-preservation"]}
  relevantResearch={[
    { title: "Parasuraman & Riley: Humans and Automation" },
    { title: "Stanford HAI", url: "https://hai.stanford.edu/" }
  ]}
/>

---

## Medium-Importance Cruxes

<Crux
  id="prediction-market-scaling"
  question="Can prediction markets scale to questions that matter most?"
  domain="Collective Intelligence"
  description="Whether prediction markets can provide accurate probabilities for long-term, complex, consequential questions."
  importance="medium"
  resolvability="years"
  currentState="Works well for short-term, binary; unclear for long-term"
  positions={[
    {
      view: "Markets can be designed for long-term questions",
      probability: "30-40%",
      implications: "Invest in prediction market infrastructure; use for AI governance"
    },
    {
      view: "Markets work for some questions but not others",
      probability: "40-50%",
      implications: "Use markets where appropriate; don't oversell; combine with other methods"
    },
    {
      view: "Fundamental limits prevent scaling to important questions",
      probability: "20-30%",
      implications: "Focus on alternatives: expert aggregation, deliberation, AI forecasting"
    }
  ]}
  wouldUpdateOn={[
    "Track record on long-term prediction markets",
    "Comparison with alternative methods on same questions",
    "Research on market design for different question types"
  ]}
  relatedCruxes={["human-ai-complementarity"]}
  relevantResearch={[
    { title: "Metaculus", url: "https://www.metaculus.com/" },
    { title: "Wolfers & Zitzewitz: Prediction Markets", url: "https://www.aeaweb.org/articles?id=10.1257/0895330041371321" }
  ]}
/>

<Crux
  id="deliberation-scaling"
  question="Can AI-assisted deliberation produce legitimate governance input at scale?"
  domain="Collective Intelligence"
  description="Whether AI-facilitated deliberation can be both representative and influential on actual decisions."
  importance="medium"
  resolvability="years"
  currentState="Promising pilots (vTaiwan); limited adoption; legitimacy questions"
  positions={[
    {
      view: "AI deliberation can become standard governance input",
      probability: "20-30%",
      implications: "Invest heavily in deliberation platforms; integrate with formal governance"
    },
    {
      view: "Useful for specific contexts, not general governance",
      probability: "40-50%",
      implications: "Deploy strategically; don't expect to replace traditional processes"
    },
    {
      view: "Legitimacy barriers will prevent meaningful adoption",
      probability: "20-30%",
      implications: "Focus on other forms of public input; deliberation remains niche"
    }
  ]}
  wouldUpdateOn={[
    "Adoption by major governments beyond Taiwan",
    "Evidence that deliberation outputs influence policy",
    "Research on representativeness and manipulation resistance"
  ]}
  relatedCruxes={["coordination-feasibility"]}
  relevantResearch={[
    { title: "Polis", url: "https://pol.is/" },
    { title: "vTaiwan", url: "https://info.vtaiwan.tw/" }
  ]}
/>

---

## How to Use These Cruxes

### For Prioritization

Your answers to the critical cruxes should inform your priorities:

| If you believe... | Prioritize... |
|-------------------|---------------|
| Detection will lose arms race | Provenance/authentication solutions |
| Trust collapse is reversible | Institutional reform and trust-building |
| Expertise atrophy is inevitable | Skill preservation programs; AI-free training |
| Coordination is feasible | International diplomacy and standards |
| Prediction markets can scale | Forecasting infrastructure |

### For Research

**High-value research targets**:
- Questions that could resolve multiple cruxes
- Cruxes marked as "resolvable: years" where progress is possible
- Empirical studies that could shift probability estimates

### For Monitoring

**What to watch**:
- Real-world evidence on detection vs generation
- C2PA adoption metrics
- Trust data (Edelman, Gallup)
- Skill retention studies
- International coordination outcomes

---

## Summary Table

<CruxList
  domain="Epistemics"
  cruxes={[
    {
      id: "detection-arms-race",
      question: "Can AI detection keep pace with AI generation?",
      importance: "critical",
      summary: "Determines whether verification is viable or must shift to provenance"
    },
    {
      id: "authentication-adoption",
      question: "Will C2PA/content authentication achieve critical mass?",
      importance: "critical",
      summary: "Determines whether we'll have a two-tier content ecosystem"
    },
    {
      id: "trust-rebuilding",
      question: "Can institutional trust be rebuilt after collapse?",
      importance: "critical",
      summary: "Determines whether prevention of collapse is essential vs recoverable"
    },
    {
      id: "expertise-preservation",
      question: "Can human expertise be preserved alongside AI?",
      importance: "high",
      summary: "Determines oversight viability and skill maintenance investment"
    },
    {
      id: "sycophancy-solvability",
      question: "Can AI sycophancy be eliminated?",
      importance: "high",
      summary: "Determines whether AI can serve as epistemic aid or only as comfort"
    },
    {
      id: "coordination-feasibility",
      question: "Can international AI coordination work?",
      importance: "high",
      summary: "Determines whether global solutions are worth pursuing"
    },
    {
      id: "human-ai-complementarity",
      question: "Can human-AI hybrids work?",
      importance: "high",
      summary: "Determines whether hybrid systems are viable or we must choose"
    },
    {
      id: "prediction-market-scaling",
      question: "Can prediction markets scale?",
      importance: "medium",
      summary: "Determines investment in forecasting infrastructure"
    },
    {
      id: "deliberation-scaling",
      question: "Can AI deliberation achieve governance input?",
      importance: "medium",
      summary: "Determines value of deliberation technology investment"
    }
  ]}
/>

<Section title="Related Topics">
  <Tags tags={[
    "Cruxes",
    "Key Uncertainties",
    "Epistemic Risks",
    "Solutions",
    "Research Priorities",
  ]} />
</Section>

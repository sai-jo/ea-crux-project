---
title: "Epistemic Collapse"
description: "Society's breakdown in distinguishing truth from falsehood, accelerated by AI's ability to generate unlimited synthetic content and personalized realities at scale, creating asymmetries that undermine democratic function, scientific consensus, and coordinated responses to existential risks."
sidebar:
  order: 1
maturity: "Neglected"
quality: 82
llmSummary: "Analyzes how AI-generated synthetic content at scale creates asymmetries between content generation and verification (pennies vs. substantial human expertise), with 50%+ of new web articles AI-generated as of November 2024, human deepfake detection accuracy at only 55.5% (barely above chance), and media trust at a record-low 28% as of 2025, threatening society's ability to form shared factual consensus necessary for democratic governance and coordinated responses to existential risks."
lastEdited: "2025-12-28"
importance: 72.5
causalLevel: "outcome"
---
import {DataInfoBox, Backlinks, R, Mermaid} from '../../../../../components/wiki';

<DataInfoBox entityId="epistemic-collapse" />

## Overview

Epistemic collapse represents a catastrophic breakdown in society's collective ability to distinguish truth from falsehood, leading to the inability to form shared beliefs about fundamental aspects of reality. Unlike historical periods of misinformation or propaganda, this phenomenon is characterized by the complete erosion of reliable mechanisms for establishing factual consensus. The concept has gained urgency with the advent of advanced AI systems that can generate convincing synthetic content at unprecedented scale and personalization.

In its extreme form, epistemic collapse doesn't deny that objective truth exists, but rather describes a state where truth becomes operationally meaningless because human societies lose the capacity to reliably identify and agree upon it. This creates a world where every claim becomes equally (un)trustworthy, making coordinated action on complex challenges—from climate change to AI safety itself—effectively impossible. The risk is particularly acute because it could emerge gradually, with each erosion of epistemic capacity making the next more likely, creating a cascading failure of societal sense-making.

The implications extend far beyond mere disagreement about facts. Epistemic collapse threatens the foundational assumptions underlying democratic governance, scientific progress, and human cooperation at scale. When citizens inhabit fundamentally different information realities, the shared epistemological foundation necessary for democratic deliberation disappears, potentially leading to governance failures and social fragmentation that could persist for generations.

## Risk Assessment

| Dimension | Assessment | Evidence | Trend |
|-----------|------------|----------|-------|
| **Severity** | High (7/10) | Threatens democratic function, scientific consensus, and coordinated existential risk responses | Increasing |
| **Likelihood** | High (65-80%) | Already manifesting with 28% media trust, 50%+ AI content | Accelerating |
| **Timeline** | Near-term (1-5 years) | Tipping points possible by 2026-2028 | Compressing |
| **Reversibility** | Low | Trust erosion compounds; rebuilding takes generations | Worsening |
| **Detectability** | Medium | Gradual degradation masks severity until cascade | Declining |
| **Attribution** | Low | Distributed causes across many actors | Stable |

<Mermaid chart={`
flowchart TD
    A[AI Content Generation] --> B[Content Floods Channels]
    B --> C{Verification<br/>Keeps Pace?}
    C -->|No| D[Signal-to-Noise Degrades]
    D --> E[Trust in Institutions Erodes]
    E --> F[Liar's Dividend Exploited]
    F --> G[All Evidence Becomes Questionable]
    G --> H[Epistemic Tribalization]
    H --> I[Democratic Deliberation Fails]
    I --> J[Coordinated Action Impossible]
    C -->|Yes| K[Managed Information Environment]
    K --> L[Epistemic Integrity Maintained]

    style A fill:#ff6b6b
    style D fill:#ffa07a
    style G fill:#ff4444
    style J fill:#990000,color:#fff
    style K fill:#90EE90
    style L fill:#228B22,color:#fff
`} />

The diagram above illustrates the cascade dynamics of epistemic collapse, showing how AI-enabled content generation creates verification asymmetries that compound through institutional trust erosion, ultimately threatening coordinated societal action.

## AI as an Epistemic Disruptor

Artificial intelligence fundamentally alters the information landscape through several interconnected mechanisms that create unprecedented challenges for truth verification. Large language models like GPT-4 and Claude can generate vast quantities of plausible-sounding text on any topic, often indistinguishable from human-authored content. When combined with other generative AI systems for images, audio, and video, this capability creates an environment where synthetic content can flood information channels at a rate that overwhelms traditional verification mechanisms.

The scale of AI content proliferation has exceeded early projections. According to <R id="57dfd699b04e4e93">Graphite's analysis</R>, as of November 2024, 50.3% of new web articles were generated primarily by AI—up from just 5% before ChatGPT's release in late 2022. <R id="96a3c0270bd2e5c0">Ahrefs research</R> found that 74% of new webpages include AI content, and Graphite estimates over 10 billion new AI-generated pages have been published since 2023. A <R id="1be9baa25182d75c">Europol report</R> projects that 90% of online content may be synthetically generated by 2026.

| Platform/Metric | Pre-ChatGPT (2022) | Current (2024) | Projection (2026) | Source |
|-----------------|-------------------|----------------|-------------------|--------|
| Web articles AI-generated | 5% | 50.3% | 90%+ | Graphite, Europol |
| New pages with AI content | &lt;10% | 74% | Unknown | Ahrefs |
| Medium AI attribution rate | 1.77% | 37.03% | Unknown | AI World Today |
| Quora AI attribution rate | 2.06% | 38.95% | Unknown | AI World Today |
| Google top-20 results AI-generated | &lt;5% | 17.31% | Unknown | Originality.ai |

The economic incentives heavily favor this shift: generating content with AI costs fractions of pennies per thousand words, while rigorous fact-checking requires substantial human expertise and time.

Personalization capabilities of AI systems create another dimension of epistemic risk through the generation of individualized information realities. Advanced recommendation algorithms combined with large language models can craft unique information environments for each user, potentially creating millions of distinct "realities" tailored to individual psychological profiles and existing beliefs. This goes beyond traditional echo chambers, as AI can generate infinite variations of content that confirm any worldview, making genuine engagement with disconfirming evidence increasingly rare.

The sophistication of synthetic evidence presents an existential challenge to the traditional epistemological principle of "seeing is believing." Deepfake technology has evolved rapidly, with systems like Sora and other video generation models approaching photorealistic quality. Voice cloning can now replicate individuals with just minutes of source audio, while document forgery using AI can create convincing historical records, scientific papers, and legal documents. When any piece of evidence can potentially be synthetic, the evidential basis for knowledge claims becomes fundamentally unstable.

## The Generation-Verification Asymmetry

The central dynamic driving epistemic collapse is the growing asymmetry between the cost and speed of generating false information versus verifying truth. This imbalance, dramatically amplified by AI capabilities, creates a structural advantage for those seeking to undermine epistemic certainty. Generating a convincing false scientific paper, news article, or video evidence can now be accomplished in minutes with AI tools, while rigorous fact-checking, peer review, or forensic analysis requires substantial time, expertise, and resources.

This asymmetry manifests in what legal scholars Bobby Chesney and Danielle Citron termed the "<R id="5494083a1717fed7">liar's dividend</R>"—the benefit gained by bad actors simply from the existence of sophisticated misinformation tools. Even if a specific deepfake is exposed as false, the mere possibility that any video could be fake allows dishonest actors to claim that authentic evidence against them is synthetic. A <R id="c75d8df0bbf5a94d">2024 study in the American Political Science Review</R> by researchers at Yale and Georgia Tech found that across five experiments with over 15,000 participants, politicians who claimed real scandals were misinformation received significant support boosts—particularly through "invoking informational uncertainty" or "encouraging oppositional rallying of core supporters."

Real-world cases already demonstrate this dynamic. Tesla lawyers have <R id="094219a46adde1cf">argued that Elon Musk's past remarks</R> on self-driving safety should not be used in court because they could be deepfakes. In July 2023, an Indian politician insisted embarrassing audio was AI-generated, though researchers confirmed at least one clip was authentic. During the 2023 Israel-Gaza conflict, both sides accused each other of using AI-generated evidence, making neutral assessment nearly impossible.

The verification challenge is compounded by fundamental limits on human detection capability. A <R id="5c1ad27ec9acc6f4">2024 meta-analysis of 56 studies</R> involving 86,155 participants found that human deepfake detection accuracy averages just 55.54%—barely above chance. Detection rates vary by modality: audio (62.08%), video (57.31%), images (53.16%), and text (52.00%). While AI detection models can achieve higher accuracy in laboratory conditions (e.g., XCeption at 89-94%), the <R id="f39c2cc4c0f303cc">Deepfake-Eval-2024 benchmark</R> found a ~50% drop in accuracy when applied to real-world "in-the-wild" deepfakes.

| Detection Method | Lab Accuracy | Real-World Accuracy | Key Limitation |
|------------------|--------------|---------------------|----------------|
| Human judgment (overall) | 55.54% | ~50% | Slightly above chance |
| Human judgment (audio) | 62.08% | Unknown | Best human modality |
| Human judgment (text) | 52.00% | Unknown | Effectively random |
| XCeption model | 89-94% | ~45% | 50% accuracy drop in-the-wild |
| VGG16 model | 85-90% | ~40% | Poor generalization |
| ResNet model | 80-85% | ~35% | Faster but less accurate |

## Historical Context and Unprecedented Aspects

While propaganda, misinformation, and epistemic uncertainty have existed throughout human history, the current risk of epistemic collapse represents a qualitatively different challenge. Historical propaganda typically competed with direct human experience and required significant resources to produce and distribute. Citizens during World War II might encounter propaganda, but they also had direct access to their local communities, trusted personal networks, and unmediated observations of reality that provided counterbalancing information.

The modern information environment fundamentally differs in several key aspects. First, the volume and quality of synthetic content now possible means that false information can dominate authentic content in ways previously impossible. Second, the personalization of information delivery means that individuals can be isolated within completely artificial information environments without realizing it. Third, the speed of AI-generated content creation means that false narratives can spread globally before verification efforts even begin.

Perhaps most significantly, previous eras of epistemic challenge maintained clear distinctions between obviously fabricated content (like political cartoons or clearly labeled propaganda) and attempts to document reality (like news photography or official records). AI generation capabilities blur these distinctions to the point where no clear markers separate authentic from synthetic content, creating ambient uncertainty about all information.

The scale factor cannot be overstated. While a Cold War propaganda operation might involve dozens of agents producing content over months, a single individual with access to advanced AI tools can now generate thousands of convincing articles, images, or videos per day. This democratization of sophisticated misinformation creation capabilities means that epistemic attacks can originate from anywhere and overwhelm defensive capacity through sheer volume.

## Mechanisms of Collapse

Epistemic collapse unfolds through several interconnected mechanisms that reinforce each other in potentially irreversible ways. Trust cascade failures represent one critical pathway, where the erosion of confidence in traditional epistemic authorities creates a spiral of declining credibility. As media organizations, academic institutions, and government agencies lose public trust—whether through legitimate criticism or manufactured controversy—society loses its institutional mechanisms for establishing authoritative truth claims.

The process often begins with targeted attacks on specific institutions using a combination of legitimate criticism and manufactured scandals. AI-generated evidence can support both real and fabricated controversies, making it difficult for the public to distinguish between genuine institutional failures and coordinated disinformation campaigns. Once trust in major epistemic institutions erodes below a critical threshold, there is no widely accepted authority to appeal to for truth verification, creating a permanent state of epistemic anarchy.

Information ecosystem degradation occurs as authentic content becomes harder to distinguish from synthetic alternatives. Search engines and social media algorithms, designed to maximize engagement rather than truth, can amplify compelling false content over boring authentic information. As AI-generated content floods these systems, the signal-to-noise ratio degrades to the point where finding reliable information becomes prohibitively difficult for ordinary users.

Adversarial exploitation accelerates these natural degradation processes. State actors, ideological movements, and commercial interests actively work to undermine epistemic capacity for strategic gain. Russia's documented "firehose of falsehood" strategy serves as an early example, but AI capabilities enable far more sophisticated approaches. Rather than promoting specific false narratives, advanced epistemic attacks may simply seek to create general uncertainty and confusion, making coordinated social action impossible.

The psychological dimension of epistemic collapse involves the erosion of individual capacity to process conflicting information claims. Faced with overwhelming uncertainty and contradictory evidence, many people retreat into tribal epistemologies where truth claims are evaluated primarily based on their source's perceived group loyalty rather than evidence quality. This tribalization of epistemology creates stable but mutually incompatible reality bubbles that resist fact-based correction.

## Consequences for Democratic Governance

Democratic systems depend fundamentally on citizens' ability to engage in reasoned deliberation about shared challenges and competing policy solutions. Epistemic collapse undermines this foundation by eliminating the common factual basis necessary for meaningful political discourse. When voters inhabit completely different information realities, electoral choices become expressions of tribal identity rather than reasoned evaluation of governance options.

The erosion of institutional trust has reached historic lows. According to <R id="ec0171d39415178a">Gallup's October 2025 survey</R>, only 28% of Americans express "a great deal" or "fair amount" of trust in the mass media—the lowest ever recorded, down from 72% in the 1970s. The partisan gap is stark: 54% of Democrats, 27% of independents, and just 12% of Republicans trust the media. Notably, 36% of Americans now report having "no trust at all" in media—exceeding those with positive trust. Media now ranks as the least-trusted civic institution Gallup surveys, below Congress, the Supreme Court, and the executive branch.

| Trust Indicator | 1970s | 2004 | 2016 | 2024 | 2025 | Trend |
|-----------------|-------|------|------|------|------|-------|
| Trust in mass media | 68-72% | 55% | 32% | 31% | 28% | Accelerating decline |
| No trust at all | &lt;10% | ~15% | ~27% | 36% | 38% | Rising |
| Republican trust | ~60% | ~35% | 14% | 11% | 12% | Near floor |
| Democrat trust | ~70% | ~65% | 51% | 54% | 54% | Stable but partisan |
| Age 65+ trust | Unknown | Unknown | ~50% | 43% | Unknown | Higher than young |
| Age &lt;50 trust | Unknown | Unknown | ~30% | 26% | Unknown | Generation gap |

The 2020 U.S. election provides an early example of how epistemic breakdown can threaten democratic legitimacy. Despite extensive verification by election officials, courts, and independent observers, a significant portion of the population continued to believe in widespread fraud claims. This dynamic, amplified by AI's capacity to generate supporting "evidence" at scale, could make contested election results the norm rather than the exception.

Legislative and policy-making processes become dysfunctional when representatives operate from incompatible factual premises. Climate policy cannot be effectively debated when participants disagree about basic temperature measurements. Healthcare policy becomes impossible when constituents believe fundamentally different claims about vaccine safety or disease transmission. The result is governmental paralysis on complex challenges that require coordinated action.

Perhaps most dangerously, epistemic collapse can create conditions for authoritarian exploitation. Societies that cannot distinguish truth from falsehood become vulnerable to leaders who promise certainty and simple answers to complex problems. Historical precedents suggest that epistemic chaos often precedes the collapse of democratic institutions, as citizens trade freedom for the psychological comfort of imposed certainty.

## Scientific and Institutional Implications

The scientific enterprise faces existential challenges in an epistemically collapsed environment. Peer review systems assume that fabricated evidence can be identified through careful analysis, but AI-generated fake data, manipulated images, and synthetic experimental results may become undetectable. If researchers cannot trust that published studies represent genuine investigations, the cumulative nature of scientific knowledge breaks down.

Recent incidents foreshadow these challenges. In 2023, researchers identified several published papers containing AI-generated images and possibly fabricated data, raising questions about the robustness of current peer review processes. As AI capabilities advance, distinguishing between authentic research and sophisticated fabrications may require entirely new verification infrastructures that could slow scientific progress significantly.

The replication crisis in psychology and other fields demonstrates how epistemic institutions can fail even without AI involvement. Adding AI-generated false evidence to existing challenges around publication bias, statistical manipulation, and fraud could push scientific institutions beyond their adaptive capacity. The result might be a fragmentation of scientific authority into competing schools that cannot effectively communicate or build on each other's work.

Academic institutions face particular vulnerability because their authority depends on public trust in expertise and methodological rigor. Coordinated attacks using AI-generated scandals, fake research exposing institutional bias, or fabricated evidence of corruption could irreparably damage public confidence in academic knowledge production. Once this trust is lost, rebuilding it may require generations.

## Potential Defensive Strategies

Content provenance and verification systems represent one promising approach to maintaining epistemic integrity in an AI-dominated information environment. The <R id="ff89bed1f7960ab2">Coalition for Content Provenance and Authenticity (C2PA)</R> has developed technical standards—called "Content Credentials"—for embedding cryptographic signatures in digital content, functioning like a "nutrition label" showing content's origin and modification history. The C2PA now has over 200 members and is advancing toward ISO international standardization, expected by 2025.

| C2PA Adoption Milestone | Date | Significance |
|------------------------|------|--------------|
| C2PA 2.0 specification with Trust List | January 2024 | Established official trust infrastructure |
| LinkedIn adoption | May 2024 | First major social platform |
| OpenAI DALL-E 3 implementation announced | 2024 | AI generator participation |
| Qualcomm Snapdragon 8 Gen3 camera integration | October 2023 | Hardware-level provenance |
| Google joins as steering committee member | Early 2025 | Major search engine participation |
| C2PA 2.1 with enhanced tamper resistance | May 2025 | Stronger security against attacks |
| Library of Congress C2PA for G+LAM working group | January 2025 | Government and cultural institution adoption |
| ISO standardization expected | 2025 | Global legitimacy |

However, provenance systems face significant technical and social challenges. Cryptographic signatures can be stripped from content during sharing, and many users may not understand or utilize verification tools even when available. The <R id="f98ad3ca8d4f80d2">World Privacy Forum's technical review</R> notes that criteria for inclusion on trust lists and conformance testing remain opaque. More fundamentally, provenance systems only work if content creators voluntarily participate and if verification infrastructure remains secure against sophisticated attacks.

Detection technologies using machine learning to identify AI-generated content show promise but face inherent limitations in an adversarial environment. As the <R id="f39c2cc4c0f303cc">Deepfake-Eval-2024 benchmark</R> demonstrated, detection models that perform exceptionally in academic settings suffer ~50% accuracy drops on real-world content. <R id="0a072041fb2f6093">Research published in Nature</R> confirms that denoising diffusion models can target deepfake detectors, and even minor adjustments to synthetic content can deceive detection systems while remaining imperceptible to humans. The fundamental asymmetry between generation and detection may make technological solutions insufficient on their own.

Institutional approaches focus on strengthening and defending epistemic authorities rather than replacing them with technological solutions. This might involve reforms to increase transparency and accountability in media organizations, academic institutions, and government agencies. Professional journalism organizations are developing new standards for verifying AI-generated content, while academic institutions are implementing stricter protocols for research integrity.

Educational interventions aim to build individual and societal resilience to epistemic attacks. Media literacy programs increasingly focus on helping citizens evaluate information sources, understand the limitations of evidence, and maintain appropriate skepticism without falling into cynicism. Some researchers advocate for "epistemic humility" training that helps people become comfortable with uncertainty while maintaining the capacity for evidence-based reasoning.

## Current State and Near-Term Trajectory

As of late 2024, signs of epistemic stress are visible across multiple domains without yet reaching full collapse. Public trust in media has fallen to 28%—the lowest ever recorded—with 36% of Americans expressing zero trust in news organizations. <R id="2120c4898e7ac51f">Over 85% of surveyed U.S. adults</R> express worry about AI-generated deepfakes. Social media platforms struggle to moderate AI-generated content effectively, while deepfake incidents in politics and business become increasingly common.

The technology underlying epistemic threats continues advancing rapidly. GPT-4 and similar models can produce convincing text on virtually any topic, while video generation systems like Sora approach broadcast quality. Voice cloning technology requires only minutes of source audio to create convincing impersonations. These capabilities are becoming more accessible through consumer applications and API access, democratizing sophisticated content generation.

Within the next 1-2 years, several developments could accelerate epistemic decline. Widely available real-time deepfake technology may make live video communication unreliable for the first time in human history. AI-generated scientific papers and news articles may become indistinguishable from authentic content without specialized detection tools. Political deepfakes could become routine campaign tactics, potentially undermining electoral legitimacy.

The integration of AI assistants into daily information consumption presents another near-term risk. As users increasingly rely on AI to summarize, explain, and contextualize information, they become vulnerable to systematic biases or deliberate manipulation embedded in these systems. If major AI assistants begin providing subtly different versions of factual claims, society could fragment into distinct epistemic communities without users realizing the divergence.

## Medium-Term Scenarios (2-5 Years)

The medium-term trajectory of epistemic collapse depends heavily on social and institutional responses to current challenges. In optimistic scenarios, effective provenance systems become widely adopted, detection technologies improve significantly, and educational interventions build societal resistance to manipulation. Democratic institutions adapt by developing new methods for establishing authoritative truth claims in an AI-dominated environment.

However, pessimistic scenarios suggest that epistemic degradation could accelerate beyond institutional adaptive capacity. If AI-generated content comes to dominate information channels while verification systems fail to scale, society might reach a tipping point where authentic information becomes functionally indistinguishable from sophisticated fabrications. This could trigger a cascade of institutional failures as citizens lose confidence in all forms of mediated information.

Economic factors may prove decisive in determining outcomes. The business models driving current information systems reward engagement over accuracy, creating structural incentives for epistemic pollution. Unless alternative economic frameworks emerge that value truth verification, market forces may continue driving epistemic degradation regardless of technological interventions.

International competition adds another layer of complexity. Nations that develop superior capabilities for epistemic warfare while maintaining internal information integrity could gain significant strategic advantages. This dynamic might create pressure for democracies to adopt authoritarian information control methods, potentially undermining their foundational commitments to free speech and open debate.

## Key Research and Expert Perspectives

The academic and policy research community is increasingly focused on epistemic risks from AI, though perspectives on severity vary significantly.

**Concerned perspectives:** Researchers at leading AI labs including Anthropic, Google DeepMind, and OpenAI have recognized the epistemic threat posed by language models. <R id="c87a82e621f72659">Google DeepMind researchers</R> warn that AI systems generating realistic synthetic content could "accelerate the spread of misinformation and prevent truth discernment," potentially "skewing people's view of reality and scientific consensus." <R id="c87a82e621f72659">Research in Philosophy & Technology</R> identifies a degenerative "AI-on-AI" feedback loop where AI-generated inaccuracies pollute future training data, leading to "model collapse" from scarcity of fresh human-generated content.

**Skeptical perspectives:** Other researchers argue that concerns may be overstated. A <R id="e4d7abe6d2b4ef5d">Harvard Kennedy School Misinformation Review article</R> titled "Misinformation reloaded? Fears about the impact of generative AI on misinformation are overblown" contends that effects of AI on elections worldwide remain limited and that it is too early to make definitive judgments about whether concerns are warranted. <R id="b0bf272733103485">Research on AI hallucinations</R> notes these represent a distinct form of misinformation—errors from probabilistic systems with no understanding of accuracy or intent to deceive—requiring new interpretive frameworks.

**Emerging research directions:** A <R id="ef373c19afa914bb">2025 scoping review in MDPI Publications</R> maps the impact of generative AI on disinformation, while <R id="7aca63edf40906f7">research in Journalism and Mass Communication Quarterly</R> examines implications for journalism specifically. Notably, in highly polarized environments, AI becomes "not only a generator of fabricated content but also a symbol of epistemic instability"—the mere existence of capable AI tools erodes trust even in authentic content.

## Critical Uncertainties

Several key uncertainties will determine whether epistemic collapse represents a manageable challenge or an existential threat to democratic civilization. The technological arms race between generation and detection capabilities remains fundamentally unpredictable. While detection currently lags behind generation, breakthrough developments in either domain could shift the balance dramatically.

Social adaptation mechanisms present another major uncertainty. Human societies have historically adapted to new information technologies, from the printing press to television to the internet. Whether similar adaptation is possible with AI-generated content depends on factors we don't fully understand, including the speed of technological change, the sophistication of adversarial actors, and the resilience of democratic institutions.

The psychology of truth evaluation in environments of pervasive uncertainty requires additional research. We don't know whether humans can learn to function effectively when any piece of evidence might be fabricated, or whether such environments inevitably lead to epistemic tribalism and social fragmentation. Understanding these psychological limits is crucial for designing effective interventions.

Perhaps most importantly, the relationship between epistemic collapse and other existential risks remains unclear. While epistemic breakdown could undermine responses to AI safety, climate change, and biosecurity threats, it's possible that severe manifestations of these risks could shock society into rebuilding epistemic institutions. The timing and sequencing of multiple risk factors may prove more important than their individual severity.

The potential for irreversibility represents the most concerning uncertainty. If epistemic collapse reaches a certain threshold, societies might lose the capacity to rebuild trust-based institutions, creating a permanent state of epistemic chaos. Understanding where these thresholds lie and whether they can be detected before being crossed is critical for prevention efforts but remains largely unexplored territory.

## Related Pages

<Backlinks client:load entityId="epistemic-collapse" />
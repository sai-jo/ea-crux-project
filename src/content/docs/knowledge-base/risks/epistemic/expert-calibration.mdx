---
title: Expert Calibration
description: Are AI experts actually good at predicting AI development?
sidebar:
  order: 1
---

import { InfoBox, EstimateBox, KeyQuestions, Section, Tags, Sources } from '../../../../../components/wiki';

<InfoBox
  type="epistemic"
  title="Expert Calibration"
  customFields={[
    { label: "Core Question", value: "Can we trust expert predictions about AI?" },
    { label: "Key Research", value: "Tetlock's Expert Political Judgment, AI Impacts surveys" },
    { label: "Bottom Line", value: "Experts are poorly calibrated, but some methods help" },
  ]}
/>

## The Problem

AI risk arguments depend heavily on predictions: When will AGI arrive? How hard is alignment? How fast will takeoff be? These predictions often come from "experts"—but should we trust them?

Philip Tetlock's landmark research on expert judgment found that **experts are often no better than chance at predicting events in their domain**, and are systematically overconfident. Does this apply to AI forecasting?

---

## What the Research Shows

### Tetlock's Expert Political Judgment (2005)

In a 20-year study of 284 experts making 28,000+ predictions:

| Finding | Implication for AI |
|---------|-------------------|
| Experts barely beat random chance | AI experts may not be better at AI prediction |
| Experts were systematically overconfident | AI timeline confidence may be unjustified |
| Hedgehogs (big idea thinkers) did worse than foxes (eclectic thinkers) | Strong AI worldviews may impair prediction |
| Experts didn't learn from feedback | AI experts may not update on past misses |
| Fame correlated with *worse* accuracy | Prominent AI voices may be less reliable |

### AI-Specific Track Record

Historical AI predictions have been notoriously inaccurate:

| Prediction | Source | Outcome |
|------------|--------|---------|
| "Machines will be capable of doing any work a man can do" within 20 years | Herbert Simon, 1965 | Did not happen |
| "In from 3 to 8 years we will have a machine with the general intelligence of an average human being" | Marvin Minsky, 1970 | Did not happen |
| AI winter (field stalled) | Many experts, 1980s | Happened—but experts didn't predict it |
| Deep learning won't scale | Various experts, 2010s | Wrong—transformers changed everything |
| AGI in 5 years | Various, 2020 | Still pending |

**Pattern**: Experts in AI have historically been:
- Overoptimistic about near-term progress during booms
- Overpessimistic during busts
- Surprised by both AI winters and AI springs

### AI Impacts Survey Data

AI Impacts has conducted surveys of ML researchers on AI timelines:

| Survey Year | Median AGI Estimate | Observed Pattern |
|-------------|--------------------|--------------------|
| 2016 | 2040-2050 | |
| 2022 | 2050 | Slight lengthening despite GPT-3 |
| 2023 | 2047 | Shortened after ChatGPT |

**Notable findings**:
- Wide disagreement (interquartile ranges spanning decades)
- Estimates shift based on recent events (recency bias)
- Different question framings yield very different answers
- Self-selected sample (who responds to surveys?)

---

## Why Are Experts Poorly Calibrated?

### 1. Domain Properties

Some domains allow good prediction (weather, poker); others don't (politics, economics). Key factors:

| Factor | Weather (Predictable) | AI (???) |
|--------|----------------------|----------|
| Clear feedback | Yes—forecasts verified daily | No—we haven't built AGI yet |
| Stable patterns | Yes—physics doesn't change | Maybe—paradigms shift |
| Measurable outcomes | Yes—temperature is objective | No—"AGI" is fuzzy |
| Repeated trials | Yes—thousands of forecasts | No—AGI is one-shot |

AI development may be in the "unpredictable" category regardless of expertise.

### 2. Psychological Biases

| Bias | How It Affects AI Predictions |
|------|------------------------------|
| **Overconfidence** | Experts overestimate their knowledge |
| **Anchoring** | Early estimates anchor later ones |
| **Availability** | Recent advances (ChatGPT) dominate thinking |
| **Confirmation bias** | Seek evidence supporting existing views |
| **Hindsight bias** | Past predictions seem more accurate than they were |
| **Motivated reasoning** | Career incentives affect predictions |

### 3. Selection Effects

Who becomes an "AI expert" whose predictions are solicited?

- **Attention selection**: Media quotes confident, extreme views
- **Career selection**: AI researchers have incentives to hype field
- **Survey selection**: Who responds to AI timeline surveys?
- **Survivorship bias**: Failed predictors drop out of view

The "expert consensus" we observe may be a biased sample.

---

## What Helps? (Superforecasting Research)

Tetlock's follow-up research identified factors that improve forecasting:

### Characteristics of Superforecasters

| Trait | Description | Application to AI |
|-------|-------------|-------------------|
| **Probabilistic thinking** | Give specific probabilities, not vague terms | Say "30%" not "significant chance" |
| **Updating** | Revise predictions based on new evidence | Track what would change your mind |
| **Outside view** | Consider base rates, not just inside reasoning | What % of "transformative technologies" arrived when predicted? |
| **Decomposition** | Break questions into sub-questions | Separate timeline, capability, alignment questions |
| **Self-criticism** | Actively seek disconfirmation | Steelman critics of your view |
| **Intellectual humility** | Know limits of knowledge | Wide confidence intervals |

### Techniques

1. **Reference class forecasting**: Instead of inside-view modeling, ask "what happened with similar predictions?"
2. **Prediction markets**: Aggregate many predictions with financial incentives
3. **Structured disagreement**: Force explicit articulation of cruxes
4. **Keeping score**: Track predictions and measure calibration over time

---

## Implications for AI Risk

### For Evaluating Predictions

| When an AI expert predicts... | Ask... |
|------------------------------|--------|
| "AGI by 2030" | What's their track record? How confident, specifically? |
| "Alignment is very hard" | Is this an empirical prediction or a definition? |
| "We'll get warning signs" | Based on what model? How would we know if wrong? |

### For Making Decisions

Given poor expert calibration:

1. **Use wide confidence intervals**: If experts say "2030-2040", consider "2025-2060"
2. **Weight track records**: Trust experts who've been right before
3. **Diversify information sources**: Don't rely on any single expert or method
4. **Plan for being wrong**: What if timelines are much shorter? Much longer?
5. **Value robustness**: Prefer actions that work across prediction scenarios

### For the Field

- **Keep score**: AI safety community should track its predictions
- **Reward calibration**: Celebrate accurate forecasters, not confident ones
- **Welcome disagreement**: Monoculture in predictions is a red flag
- **Invest in forecasting**: Better methods could help (prediction markets, Metaculus)

---

## Key Questions

<KeyQuestions
  questions={[
    "Is AI prediction fundamentally harder than other domains, or could we do better with better methods?",
    "Should we weight AI researchers' predictions more or less than outside forecasters?",
    "How much should recent AI progress (GPT-4, etc.) update timeline estimates?",
    "Are there AI predictions that have been consistently accurate? What distinguishes them?"
  ]}
/>

---

## What This Means for This Wiki

Many claims in this wiki rely on expert predictions. Given the calibration evidence:

- **Timeline estimates** should be treated as rough guesses, not reliable forecasts
- **Probability estimates** should have wider error bars than stated
- **Confident claims** about unprecedented events deserve extra skepticism
- **Disagreement among experts** is the norm, not a sign something is wrong

The honest position: We're trying to reason about AI risk with prediction tools that have poor track records. This doesn't mean we should give up—but we should be humble.

<Section title="Related Topics">
  <Tags tags={[
    "Forecasting",
    "Calibration",
    "Expert Judgment",
    "Tetlock",
    "Uncertainty",
  ]} />
</Section>

<Sources sources={[
  { title: "Expert Political Judgment: How Good Is It? How Can We Know?", author: "Philip E. Tetlock", date: "2005", url: "https://press.princeton.edu/books/paperback/9780691128719/expert-political-judgment" },
  { title: "Superforecasting: The Art and Science of Prediction", author: "Philip E. Tetlock & Dan Gardner", date: "2015", url: "https://www.amazon.com/Superforecasting-Science-Prediction-Philip-Tetlock/dp/0804136718" },
  { title: "2023 Expert Survey on Progress in AI", author: "AI Impacts", date: "2023", url: "https://aiimpacts.org/2022-expert-survey-on-progress-in-ai/" },
  { title: "When Will AI Exceed Human Performance?", author: "Grace et al.", date: "2018", url: "https://arxiv.org/abs/1705.08807" },
  { title: "Calibration of AI Predictions", author: "Metaculus", url: "https://www.metaculus.com/questions/" },
]} />

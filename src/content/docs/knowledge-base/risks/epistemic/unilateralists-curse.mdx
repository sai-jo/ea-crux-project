---
title: The Unilateralist's Curse
description: Why groups take risky actions even when most members oppose them
sidebar:
  order: 6
---

import { InfoBox, KeyQuestions, Section, Tags, Sources } from '../../../../../components/wiki';

<InfoBox
  type="epistemic"
  title="Unilateralist's Curse"
  customFields={[
    { label: "Core Problem", value: "Any single actor can take an action the majority opposes" },
    { label: "Key Research", value: "Bostrom, Sandberg (2016)" },
    { label: "AI Relevance", value: "Lab racing, open source, capability publication" },
  ]}
/>

## The Problem

**The Unilateralist's Curse**: When multiple actors can independently take a risky action, the action will likely be taken even if most actors believe it's bad.

**Mechanism**:
1. N actors independently decide whether to take action X
2. Each estimates the expected value of X
3. If *any* actor estimates X is net positive, X happens
4. The actor most optimistic about X determines the outcome
5. Due to noise in estimates, the most optimistic actor is probably overestimating

**Result**: Actions are taken that most actors (and perhaps a neutral evaluator) would oppose.

---

## The Mathematics

### Basic Model

Suppose:
- 10 actors independently decide whether to release dangerous AI research
- True expected value of release: slightly negative
- Each actor has noisy estimate (mean = true value, with variance)

Even if the true value is negative:
- Actor 1's estimate: -2 (don't release)
- Actor 2's estimate: -1 (don't release)
- Actor 3's estimate: +0.5 (release!)
- ...

**Outcome**: Research is released because Actor 3's noisy estimate was positive.

### Mathematical Result

If N actors have independent estimates with noise:
- P(at least one positive estimate) increases with N
- Expected value of the most optimistic estimate is positively biased
- The bias increases with N and noise variance

For large N and moderate noise: **Almost certain that someone will take any action, regardless of true value.**

---

## AI-Specific Applications

### Lab Racing

| Scenario | Unilateralist's Curse Application |
|----------|----------------------------------|
| **Release frontier model** | If any major lab releases, others must follow |
| **Skip safety testing** | If any lab cuts corners, competitive pressure increases |
| **Deploy autonomous agents** | First mover may force others' hands |

Even if 7/10 AI labs believe deployment is premature, the 3 who disagree determine the outcome.

### Open Source AI

| Scenario | Unilateralist's Curse Application |
|----------|----------------------------------|
| **Release model weights** | Once released by anyone, can't be unreleased |
| **Publish training recipes** | Any lab can publish; all share consequences |
| **Share dangerous capabilities** | Any researcher can disclose |

A single open-source advocate can release what major labs keep private.

### AI Safety Research Publication

| Scenario | Unilateralist's Curse Application |
|----------|----------------------------------|
| **Publish vulnerability** | Any researcher who finds it can publish |
| **Discuss attack vectors** | Anyone can share ideas publicly |
| **Release red-team results** | Any org can disclose findings |

Information hazard decisions are subject to unilateralist dynamics.

---

## Why This Happens

### 1. Different Information

Actors have different information and models:
- Lab A has internal safety concerns
- Lab B doesn't know about Lab A's concerns
- Lab B releases based on incomplete information

### 2. Different Values

Actors weigh considerations differently:
- Some prioritize caution
- Some prioritize progress
- Some prioritize openness
- The "progress" faction acts first

### 3. Noise in Judgment

Even with same information and values, estimates vary:
- Random error in cost-benefit analysis
- Different interpretations of evidence
- Varying risk tolerance

### 4. No Coordination Mechanism

The curse requires:
- Independent decision-making
- No binding agreement
- First-mover can't be stopped

If actors could coordinate, they might agree to wait—but they can't.

---

## Solutions and Mitigations

### 1. The Principle of Conformity

**Bostrom's recommendation**: Individuals should conform to what the group would decide, not their personal estimate.

**Implementation**:
- Before acting unilaterally, consider: "What would we collectively decide?"
- If you'd be the minority, defer to majority
- Weight others' estimates; don't just trust your own

**Limitation**: Hard to implement without explicit coordination.

### 2. Unanimous Voting Rules

**Mechanism**: Require consensus before taking irreversible actions.

**Examples**:
- "We won't release frontier models without all major labs agreeing"
- "We won't publish attack vectors without community review"
- "We won't deploy autonomous agents without regulatory approval"

**Limitation**: Difficult to enforce; defection is tempting.

### 3. Coordination Bodies

**Mechanism**: Create institutions that aggregate views and make binding decisions.

**Examples**:
- Frontier labs consortium with shared safety standards
- Government regulation requiring approval before release
- Academic review boards for sensitive research

**Limitation**: Requires trust and enforcement power.

### 4. Raising the Bar

**Mechanism**: Increase cost of unilateral action.

**Examples**:
- Compute governance (can't train frontier models without approval)
- Liability for AI harms (risk of lawsuits deters release)
- Professional norms (social cost of premature release)

**Limitation**: May slow good actors more than bad ones.

### 5. Reducing Noise

**Mechanism**: Improve decision-making quality.

**Examples**:
- Better evaluation methods
- Shared information about risks
- Calibration training for decision-makers

**Limitation**: Doesn't eliminate the curse; just reduces severity.

---

## Real-World Examples

### OpenAI's Approach to GPT-2 (2019)

**Situation**: OpenAI considered not releasing GPT-2 due to misuse potential.

**Unilateralist dynamics**:
- If OpenAI doesn't release, others might
- Open source community might replicate
- Other labs might publish similar models

**Outcome**: Staged release; full release eventually.

**Lesson**: Unilateral restraint is difficult to sustain.

### AI Model Weight Release

**Situation**: Frontier model weights are valuable for both safety research and misuse.

**Unilateralist dynamics**:
- Labs could keep weights private
- But if any lab releases, cat is out of bag
- Meta released LLaMA weights; now many open models exist

**Outcome**: Information has spread despite some labs preferring restriction.

### Capability Research Publication

**Situation**: Research on AI capabilities is dual-use.

**Unilateralist dynamics**:
- Academics have strong publication norms
- Corporate labs can choose to not publish
- But academic researchers can and do publish similar findings

**Outcome**: Most capability research is eventually published.

---

## Implications for AI Safety

### The Problem is Structural

The unilateralist's curse isn't about bad actors—it's about the structure of independent decision-making. Even well-intentioned actors with noisy estimates will collectively take risky actions.

### Coordination is Crucial

**If you believe**:
- AI development involves unilateralist dynamics
- Current AI development is risky

**Then**:
- Individual restraint is insufficient
- Coordination mechanisms are essential
- Governance/regulation may be necessary

### Individual Decision-Making

When considering unilateral action:
1. **Am I the most optimistic?** If so, I might be biased.
2. **What would the group decide?** Defer to collective judgment.
3. **Is this reversible?** Irreversible actions are especially risky.
4. **Can I coordinate first?** Try to before acting unilaterally.

<KeyQuestions
  questions={[
    "What coordination mechanisms could reduce unilateralist dynamics in AI development?",
    "How do you decide when to defer to group judgment vs. trust your own?",
    "Is government regulation necessary to solve AI's unilateralist's curse?",
    "How does open source culture interact with unilateralist dynamics?"
  ]}
/>

<Section title="Related Topics">
  <Tags tags={[
    "Game Theory",
    "Coordination",
    "AI Governance",
    "Decision Making",
    "Collective Action",
  ]} />
</Section>

<Sources sources={[
  { title: "The Unilateralist's Curse and the Case for a Principle of Conformity", author: "Nick Bostrom, Thomas Douglas, Anders Sandberg", date: "2016", url: "https://www.nickbostrom.com/papers/unilateralist.pdf" },
  { title: "Racing to the Precipice: A Model of AI Development", author: "Stuart Armstrong, Nick Bostrom, Carl Shulman", date: "2016", url: "https://www.fhi.ox.ac.uk/wp-content/uploads/Racing-to-the-precipice-a-model-of-artificial-intelligence-development.pdf" },
  { title: "Information Hazards", author: "Nick Bostrom", date: "2011", url: "https://www.nickbostrom.com/information-hazards.pdf" },
  { title: "Open Source AI", author: "GovAI", notes: "Analysis of open source dynamics" },
]} />

---
title: Bioweapons
description: AI-assisted biological weapon development
sidebar:
  order: 1
---

import { InfoBox, EntityCard, EntityCards, Tags, Sources, Section } from '../../../../../components/wiki';

<InfoBox
  type="risk"
  title="Bioweapons Risk"
  severity="catastrophic"
  likelihood="Low-Medium (increasing)"
  timeframe="Near-term"
  customFields={[
    { label: "Type", value: "Misuse" },
    { label: "Key Concern", value: "Lowering barriers to development" },
  ]}
/>

## Overview

AI systems could accelerate biological weapons development by helping with pathogen design, synthesis planning, or acquisition of dangerous knowledge. The concern isn't that AI creates entirely new risks, but that it lowers barriers—making capabilities previously requiring rare expertise more accessible to bad actors.

This is considered one of the most severe near-term AI risks because biological weapons can cause mass casualties and AI-assisted bioweapons could be developed by smaller groups than traditional state programs required.

## How AI Could Help

AI could assist at multiple stages of bioweapon development. In target identification, AI might help identify dangerous modifications to known pathogens or find novel biological agents. In synthesis planning, AI could help determine how to create dangerous biological materials. In acquisition, chatbots could provide guidance on obtaining precursor materials or laboratory equipment.

Most concerningly, AI might help bridge knowledge gaps. Historically, bioweapons development required rare combinations of expertise. AI could help a motivated individual or small group compensate for missing knowledge, potentially replacing what previously required teams of specialists.

## Current Evidence

Studies have shown language models can provide information relevant to bioweapon development, though the significance is debated. Some research found that AI-assisted participants could find more relevant information about biological agents faster than control groups.

AI labs have conducted internal evaluations of these risks. Anthropic, OpenAI, and others have tested whether their models could provide "uplift" to potential bioweapon developers—whether the AI meaningfully helps beyond what's available through internet search.

Results are mixed: models clearly know dangerous information, but how much they lower barriers versus what's already available is contested. However, capabilities are clearly increasing with each model generation.

## Biosecurity Context

Biological threats exist on a spectrum. State programs have historically been the main concern, but the barrier to entry may be dropping. The COVID-19 pandemic demonstrated how much damage pathogens can cause and highlighted gaps in biosecurity infrastructure.

DNA synthesis companies already screen orders for dangerous sequences, but screening isn't comprehensive. The combination of AI providing knowledge and decreasing costs of synthesis equipment is concerning to biosecurity experts.

## Mitigations

Technical measures include training models not to help with bioweapon development and filtering dangerous outputs. But these are imperfect—models can be jailbroken, fine-tuned, or open-source models may lack restrictions entirely.

Broader biosecurity measures matter more: DNA synthesis screening, laboratory access controls, disease surveillance, and medical countermeasures. AI risk is one input to overall biosecurity strategy.

Compute governance could limit who can train powerful models, reducing the availability of capable models to bad actors. Information security around model weights becomes important if models can provide meaningful uplift.

## Case Studies

### RAND Red-Team Study (2024)
In a controlled study, 12 teams of three researchers each were given 80 hours over seven weeks to develop bioweapon attack plans—some using AI, others using only the internet. Expert evaluators found **no statistically significant difference** in plan viability between AI-assisted and non-AI groups. This suggests current LLMs don't provide meaningful "uplift" for sophisticated biological attacks.

### AI-Designed Toxins Evade Screening (2024)
Microsoft researchers conducted a red-team exercise testing biosecurity in the protein engineering pipeline. They found that DNA screening software—used by synthesis companies to flag dangerous sequences—**missed over 75% of AI-designed potential toxins**. One tool flagged only 23% of sequences. After the research was published, screening systems improved to catch 72% on average.

### Historical Context: 1984 Oregon Salmonella Attack
A religious commune deliberately contaminated salad bars with Salmonella, causing 751 cases of food poisoning. This remains the largest bioterrorist attack in U.S. history and illustrates that biological attacks don't require sophisticated technology—but AI could lower barriers further.

## Key Debates

**"AI provides minimal uplift"** — Critics like the RAND researchers argue wet lab skills are the real bottleneck. Knowing how to make something is different from being able to make it. The information largely exists in scientific literature already.

**"Even marginal uplift matters"** — Proponents argue that even small reductions in barriers could enable attacks that wouldn't otherwise occur. The combination of LLMs (providing knowledge) with Biological Design Tools (providing capability) could be transformative. AI capabilities are improving rapidly.

**"Focus on defenses, not restrictions"** — Some argue that restricting AI is futile; instead, invest in DNA synthesis screening, disease surveillance, and medical countermeasures. Others argue both are needed.

## Timeline

- **2022**: Collaborations Bio shows AI can design novel protein toxins in hours
- **2023**: Multiple AI labs begin internal biosecurity evaluations
- **2024 (Jan)**: RAND red-team study finds no significant AI uplift for bioweapon planning
- **2024 (May)**: Microsoft research reveals gaps in DNA synthesis screening
- **2024 (Aug)**: CNAS publishes comprehensive report on AI and biological national security risks
- **2024 (Oct)**: Executive Order 14110 directs National Academies to study AI biosecurity

## Video & Podcast Resources

- [80,000 Hours Podcast: Kevin Esvelt on Biosecurity](https://80000hours.org/podcast/) - MIT researcher on biological risks
- [Lex Fridman #431: Roman Yampolskiy](https://lexfridman.com/roman-yampolskiy/) - Discusses AI safety including CBRN risks
- [Future of Life Institute Podcast](https://futureoflife.org/podcast/) - Multiple episodes on biosecurity

<Section title="Related Topics">
  <Tags tags={[
    "Biosecurity",
    "Dual-Use Research",
    "Catastrophic Risk",
    "CBRN",
    "AI Misuse",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="cyberweapons"
      category="risk"
      title="Cyberweapons"
      description="Related AI-enabled weapons risk"
    />
    <EntityCard
      id="anthropic"
      category="lab"
      title="Anthropic"
      description="Biosecurity evaluations for AI models"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "The Precipice", author: "Toby Ord", date: "2020" },
  { title: "Anthropic Responsible Scaling Policy", url: "https://www.anthropic.com/news/anthropics-responsible-scaling-policy" },
  { title: "Dual Use of Artificial Intelligence-powered Drug Discovery", url: "https://www.nature.com/articles/s42256-022-00465-9" },
  { title: "AI and the Evolution of Biological National Security Risks (CNAS)", url: "https://www.cnas.org/publications/reports/ai-and-the-evolution-of-biological-national-security-risks", date: "2024" },
  { title: "The Operational Risks of AI in Large-Scale Biological Attacks (RAND)", url: "https://www.rand.org/pubs/research_reports/RRA2977-2.html", date: "2024" },
  { title: "Biosecurity in the Age of AI (Belfer Center)", url: "https://www.belfercenter.org/publication/biosecurity-age-ai-whats-risk" },
  { title: "AI Challenges and Biological Threats (Frontiers in AI)", url: "https://www.frontiersin.org/journals/artificial-intelligence/articles/10.3389/frai.2024.1382356/full", date: "2024" },
  { title: "National Academies Study on AI Biosecurity", url: "https://www.nationalacademies.org/our-work/assessing-and-navigating-biosecurity-concerns-and-benefits-of-artificial-intelligence-use-in-the-life-sciences" },
  { title: "Opportunities to Strengthen U.S. Biosecurity (CSIS)", url: "https://www.csis.org/analysis/opportunities-strengthen-us-biosecurity-ai-enabled-bioterrorism-what-policymakers-should", date: "2025" },
]} />

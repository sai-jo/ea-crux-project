---
title: Structural Risks
description: Systemic risks from how AI reshapes society, institutions, and power dynamics
sidebar:
  order: 0
---

import { InfoBox, KeyQuestions, Section, Tags, Sources, DisagreementMap } from '../../../../../components/wiki';

<InfoBox
  type="risk"
  title="Structural Risks"
  customFields={[
    { label: "Category Status", value: "Conceptually fuzzy; boundaries unclear" },
    { label: "Distinguishing Feature", value: "Emerges from system dynamics, not individual AI failures" },
    { label: "Key Sources", value: "Bostrom, Ord, MacAskill, GovAI" },
    { label: "Related Framings", value: "Systemic risk, societal risk, macro-level risk" },
  ]}
/>

## What Are Structural Risks?

**Structural risks** are risks that emerge from how AI development and deployment reshape society, institutions, and power dynamics—rather than from individual AI systems failing or being misused.

The key intuition: **even if every AI system worked exactly as intended, structural risks could still materialize.** The problem isn't individual failures—it's the overall trajectory.

---

## Conceptual Honesty: Why This Category Is Fuzzy

**This category is less well-defined than "accident" or "misuse" risks.** We flag this explicitly because:

### Boundary Problems

| Issue | Explanation |
|-------|-------------|
| **Overlap with other categories** | Racing dynamics increase accident risk; power concentration enables misuse. Where does "structural" end and others begin? |
| **Causal distance** | Structural risks involve longer causal chains with more uncertainty |
| **Aggregation issues** | Many "structural" risks are aggregations of smaller risks that might be better analyzed individually |
| **Counterfactual ambiguity** | Hard to specify: "structural" compared to what baseline? |

### Speculative Elements

| Issue | Explanation |
|-------|-------------|
| **Unprecedented situations** | No historical precedent for AI-level transformation |
| **Long time horizons** | Many structural risks play out over decades |
| **Complex interactions** | Depend on social/political/economic factors we can't model well |
| **Unfalsifiability concerns** | Some structural risk claims are hard to test or falsify |

### Alternative Framings

Different researchers frame these risks differently:

| Framing | Source | Emphasis |
|---------|--------|----------|
| **"Structural risks"** | This wiki | System dynamics |
| **"Societal-scale risks"** | CAIS | Broad impact |
| **"Macro-level risks"** | Various | Aggregate effects |
| **"Second-order risks"** | Some researchers | Downstream effects of AI |
| **"Systemic risks"** | Finance-inspired | Interconnected failures |
| **"Sociotechnical risks"** | STS scholars | Technology-society interaction |

**We use "structural" but acknowledge this is one framing among several.** The underlying phenomena matter more than the label.

### What Critics Say

Some researchers are skeptical of this category:

> "Structural risks often involve long causal chains with many assumptions. Each link in the chain has uncertainty, and these compound." — Common critique

> "The 'structural' framing can make problems seem more intractable than they are by abstracting away from specific interventions." — Policy-focused critique

> "Some 'structural risks' are really just regular risks viewed at a different level of abstraction." — Analytical critique

---

## The Risks (Despite the Fuzziness)

Even acknowledging conceptual issues, these phenomena deserve attention:

### Power & Control

Risks from AI enabling or requiring new power configurations:

| Risk | Description | Confidence |
|------|-------------|------------|
| **[Concentration of Power](/knowledge-base/risks/structural/concentration-of-power/)** | AI enabling unprecedented power accumulation by small groups | Medium-High |
| **[Lock-in](/knowledge-base/risks/structural/lock-in/)** | Permanent entrenchment of values, systems, or structures | Medium |
| **[Winner-Take-All](/knowledge-base/risks/structural/winner-take-all/)** | Extreme inequality from AI advantages | Medium |

**Key uncertainty**: Does AI actually concentrate power more than previous technologies, or is this historical pattern continuing?

### Competition & Coordination

Risks from how actors interact around AI:

| Risk | Description | Confidence |
|------|-------------|------------|
| **[Racing Dynamics](/knowledge-base/risks/structural/racing-dynamics/)** | Competition driving unsafe practices | High (already observable) |
| **[Multipolar Trap](/knowledge-base/risks/structural/multipolar-trap/)** | Competitive dynamics producing collectively bad outcomes | Medium |
| **[Proliferation](/knowledge-base/risks/structural/proliferation/)** | Spread of dangerous capabilities | Medium-High |

**Key uncertainty**: Are racing dynamics inevitable, or can coordination mechanisms work? (See [coordination literature](#coordination-research))

### Human Agency & Society

Risks to human autonomy and capability:

| Risk | Description | Confidence |
|------|-------------|------------|
| **[Erosion of Human Agency](/knowledge-base/risks/structural/erosion-of-agency/)** | Humans losing meaningful control over their lives | Medium |
| **[Enfeeblement](/knowledge-base/risks/structural/enfeeblement/)** | Humanity losing capability to function without AI | Medium |
| **[Economic Disruption](/knowledge-base/risks/structural/economic-disruption/)** | Mass displacement without adequate transition | Medium-High |

**Key uncertainty**: Is capability loss from AI qualitatively different from previous technological shifts?

### Systemic Instability

Risks from AI-enabled instability:

| Risk | Description | Confidence |
|------|-------------|------------|
| **[Flash Dynamics](/knowledge-base/risks/structural/flash-dynamics/)** | AI systems interacting faster than human oversight | Medium |
| **[Irreversibility](/knowledge-base/risks/structural/irreversibility/)** | Crossing points of no return | Low-Medium |

**Key uncertainty**: How much does speed of AI interaction actually matter for human oversight?

---

## What Makes These "Structural"?

Despite fuzziness, these risks share features:

### 1. Emergence from Interaction

Individual actors behaving rationally can produce collectively bad outcomes:
- Each lab has incentive to race → everyone races → everyone worse off
- Each person rationally defers to AI → aggregate skill loss
- Each country seeks AI advantage → arms race dynamics

**Related concept**: [Tragedy of the Commons](https://en.wikipedia.org/wiki/Tragedy_of_the_commons), [Collective Action Problems](https://plato.stanford.edu/entries/collective-action/)

### 2. Path Dependence

Early decisions constrain later options:
- Infrastructure built around AI may be hard to change
- Skills lost may be hard to rebuild
- Power accumulated may be hard to redistribute

**Related concept**: [Path Dependence](https://en.wikipedia.org/wiki/Path_dependence), [Lock-in (economics)](https://en.wikipedia.org/wiki/Lock-in_(decision-making))

### 3. Threshold Effects

Gradual changes may produce sudden shifts:
- Gradual capability increase → sudden automation of entire sectors
- Gradual trust erosion → sudden institutional collapse
- Gradual power concentration → sudden inability to contest

**Related concept**: [Tipping points](https://en.wikipedia.org/wiki/Tipping_point_(sociology))

### 4. Difficulty of Attribution

Harms emerge from system dynamics, not identifiable actors:
- No one "caused" racing dynamics—they emerged
- No one "caused" skill atrophy—it accumulated
- Hard to assign responsibility → hard to regulate

---

## Relationship to Other Risk Categories

<DisagreementMap
  topic="How distinct are structural risks from other categories?"
  description="Views on whether 'structural' is a useful separate category"
  spectrum={{ low: "Mostly reducible to other categories", high: "Genuinely distinct phenomena" }}
  positions={[
    { actor: "Proponents", position: "Distinct and important", estimate: "70%", confidence: "medium" },
    { actor: "Skeptics", position: "Useful framing but overlapping", estimate: "40%", confidence: "medium" },
    { actor: "Critics", position: "Mostly aggregation of other risks", estimate: "20%", confidence: "low" },
  ]}
/>

### Structural + Accident Risks

Racing dynamics increase accident probability:
- Less time for safety research
- Pressure to deploy before ready
- Corner-cutting on evaluations

### Structural + Misuse Risks

Power concentration enables misuse:
- Fewer actors to govern
- More capability per actor
- Less competition to check behavior

### Structural + Epistemic Risks

Structural dynamics affect information environment:
- Power concentration → information control
- Racing → less transparency
- Enfeeblement → reduced ability to evaluate AI

---

## Key Uncertainties

<KeyQuestions
  questions={[
    "Are structural risks genuinely distinct, or reducible to other categories?",
    "How much of 'structural risk' is extrapolation from current trends vs. genuinely new dynamics?",
    "Can coordination mechanisms address structural risks, or are they inherent to AI development?",
    "Is the speed of AI development actually faster than adaptation capacity?",
    "Are structural risks amenable to technical solutions, or only governance solutions?"
  ]}
/>

---

## Research Landscape

### Core Theoretical Sources

| Source | Focus | Key Contribution |
|--------|-------|------------------|
| [Bostrom (2014): Superintelligence](https://www.amazon.com/Superintelligence-Dangers-Strategies-Nick-Bostrom/dp/0198739834) | AI trajectories | Introduced many structural risk concepts |
| [Ord (2020): The Precipice](https://theprecipice.com/) | Existential risk | "Dystopian lock-in" framing |
| [MacAskill (2022): What We Owe the Future](https://whatweowethefuture.com/) | Long-term future | Value lock-in, trajectory changes |
| [Dafoe (2018): AI Governance](https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf) | AI governance | Racing, coordination, power |

### Empirical and Policy Research

| Source | Focus |
|--------|-------|
| [GovAI](https://www.governance.ai/) | AI governance research |
| [CSET Georgetown](https://cset.georgetown.edu/) | Technology competition, national security |
| [AI Now Institute](https://ainowinstitute.org/) | Concentration, power, inequality |
| [RAND Corporation](https://www.rand.org/topics/artificial-intelligence.html) | Security implications |

### Coordination Research

| Source | Focus |
|--------|-------|
| [Armstrong et al. (2016): Racing to the Precipice](https://nickbostrom.com/papers/racing.pdf) | Formal model of AI racing |
| [Askell et al. (2019): The Role of Cooperation](https://arxiv.org/abs/1907.04534) | AI cooperation |
| [Maas (2019): AI Arms Race](https://onlinelibrary.wiley.com/doi/10.1111/1758-5899.12713) | International dynamics |

### Critical Perspectives

| Source | Critique |
|--------|----------|
| [Debunking the AI Arms Race Theory (TNSR)](https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/) | Challenges racing narrative |
| [AI Now Institute Reports](https://ainowinstitute.org/reports) | Focus on present harms vs speculative |
| [STS scholarship](https://4sonline.org/) | Sociotechnical complexity |

---

## Interventions and Research Priorities

### If Structural Risks Are Real

| Intervention | Mechanism |
|--------------|-----------|
| **International coordination** | Reduce racing dynamics |
| **Compute governance** | Control key input to AI development |
| **Antitrust action** | Prevent excessive concentration |
| **Deliberate redundancy** | Maintain human capabilities |
| **Democratic oversight** | Prevent lock-in of particular values |

### If Structural Risks Are Overrated

| Alternative Focus | Rationale |
|-------------------|-----------|
| **Specific accident prevention** | More tractable than system change |
| **Misuse prevention** | Clearer threat model |
| **Present harms** | Address current problems first |
| **Adaptation support** | Help society adjust rather than prevent change |

### Research Priorities

| Priority | Question |
|----------|----------|
| **Empirical** | Are structural dynamics actually occurring as theorized? |
| **Historical** | How do AI dynamics compare to previous technological shifts? |
| **Modeling** | Can we formalize structural risk claims to test them? |
| **Intervention** | What interventions actually affect structural dynamics? |

---

## How to Use This Section

**Given the conceptual fuzziness:**

1. **Don't treat category boundaries as firm** — these risks overlap and interact
2. **Evaluate individual claims, not the category** — some structural risks are better-supported than others
3. **Consider alternative framings** — different framings may suggest different interventions
4. **Weight by evidence** — racing dynamics are more observable than lock-in scenarios
5. **Note your uncertainty** — structural risks involve more speculation than some other categories

<Section title="Related Topics">
  <Tags tags={[
    "Systemic Risk",
    "AI Governance",
    "Coordination",
    "Power Dynamics",
    "Long-term Risk",
  ]} />
</Section>

<Sources sources={[
  { title: "Superintelligence", author: "Nick Bostrom", date: "2014" },
  { title: "The Precipice", author: "Toby Ord", date: "2020" },
  { title: "What We Owe the Future", author: "Will MacAskill", date: "2022" },
  { title: "AI Governance: A Research Agenda", author: "Allan Dafoe", date: "2018", url: "https://www.fhi.ox.ac.uk/wp-content/uploads/GovAI-Agenda.pdf" },
  { title: "Racing to the Precipice", author: "Armstrong et al.", url: "https://nickbostrom.com/papers/racing.pdf" },
  { title: "GovAI Research", url: "https://www.governance.ai/" },
  { title: "AI Now Institute", url: "https://ainowinstitute.org/" },
  { title: "CSET Georgetown", url: "https://cset.georgetown.edu/" },
  { title: "Debunking the AI Arms Race Theory", url: "https://tnsr.org/2021/06/debunking-the-ai-arms-race-theory/" },
]} />

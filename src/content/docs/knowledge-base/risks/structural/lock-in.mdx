---
title: Lock-in
description: Permanent entrenchment of values, systems, or power structures through AI capabilities that prevent course correction, potentially creating irreversible dystopian outcomes or value misalignment
sidebar:
  order: 2
maturity: Growing
quality: 4
llmSummary: Analyzes how AI could enable permanent entrenchment of values, political systems, or power structures through enforcement capabilities, speed/scale effects, and technological complexity. Provides concrete examples including Chinese AI value alignment and constitutional AI, emphasizing the current period's importance for preventing irreversible negative outcomes.
lastEdited: "2025-12-24"
importance: 85.2
---

import {DataInfoBox, Backlinks} from '../../../../../components/wiki';

<DataInfoBox entityId="lock-in" />

## Overview

Lock-in refers to the permanent entrenchment of values, systems, or power structures in ways that are extremely difficult or impossible to reverse. In the context of AI safety, this represents scenarios where early decisions about AI development, deployment, or governance become irreversibly embedded in future systems and society. Unlike traditional technologies where course correction remains possible, advanced AI could create enforcement mechanisms so powerful that alternative paths become permanently inaccessible.

What makes AI lock-in particularly concerning is both its potential permanence and the current critical window for prevention. As [Toby Ord notes in "The Precipice"](https://theprecipice.com/), we may be living through humanity's most consequential period, where decisions made in the next few decades could determine the entire future trajectory of civilization. Recent developments suggest concerning trends: China's mandate that AI systems align with "core socialist values" affects systems serving hundreds of millions, while [Constitutional AI approaches](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback) explicitly embed specific value systems during training. The [IMD AI Safety Clock](https://www.imd.org/research-knowledge/sustainability/articles/ai-safety-clock/) currently stands at 24 minutes to midnight, reflecting growing expert consensus about the urgency of these concerns.

The stakes are unprecedented. Unlike historical empires or ideologies that eventually changed, AI-enabled lock-in could create truly permanent outcomes—either through technological mechanisms that prevent change or through systems so complex and embedded that modification becomes impossible. This makes current decisions about AI development potentially the most important in human history.

## Risk Assessment

| **Dimension** | **Assessment** | **Details** |
|---------------|----------------|-------------|
| **Severity** | Extremely High | Permanent loss of human agency, potential dystopian outcomes |
| **Likelihood** | Medium-High | Multiple pathways, accelerating AI capabilities, geopolitical competition |
| **Timeline** | 2-15 years | Critical decisions being made now; AGI timeline determines urgency |
| **Trend** | Worsening | Increasing AI capabilities, limited coordination, value embedding already occurring |
| **Reversibility** | None | By definition, successful lock-in prevents course correction |

## Mechanisms of AI-Enabled Lock-in

### Enforcement Capabilities

AI provides unprecedented tools for maintaining entrenched systems. [Comprehensive surveillance systems](https://www.rand.org/pubs/research_reports/RR2935.html) powered by computer vision and natural language processing can monitor populations at scale impossible with human agents. Predictive policing algorithms can identify and suppress dissent before it organizes. Autonomous weapons systems could enforce compliance without requiring human enforcers who might defect.

China's Social Credit System demonstrates early-stage enforcement lock-in. By 2020, the system had [restricted 23 million people from purchasing flight tickets](https://www.reuters.com/world/china/china-social-credit-gave-green-light-covid-surveillance-2021-12-02/) and 5.5 million from buying high-speed train tickets. As AI capabilities advance, such systems could become inescapable—monitoring every communication, transaction, and movement.

### Speed and Scale Effects

AI operates at speeds that outpace human response times, potentially creating irreversible changes before humans can intervene. [High-frequency trading algorithms](https://www.cftc.gov/sites/default/files/idc/groups/public/%40economicanalysis/documents/file/oce_aimlreport0218.pdf) already execute thousands of trades per second, sometimes causing market disruptions faster than human oversight can respond. At AI systems' full potential, they could reshape global systems—economic, political, or social—within timeframes that prevent meaningful human course correction.

The scale of AI influence compounds this problem. A single AI system could simultaneously influence billions of users through recommendation algorithms, autonomous trading, and content generation. [Facebook's algorithm changes have historically affected global political discourse](https://www.wsj.com/articles/facebook-knows-its-algorithms-divide-users-but-company-won-t-change-11633026421), but future AI systems could have orders of magnitude greater influence.

### Technological Path Dependence

Once AI systems become deeply embedded in critical infrastructure, changing them becomes prohibitively expensive. Legacy software systems already demonstrate this phenomenon—COBOL systems from the 1960s still run critical financial infrastructure because [replacement costs exceed $80 billion globally](https://www.reuters.com/article/us-usa-banks-cobol/banks-scramble-to-fix-old-systems-as-it-cowboys-ride-into-sunset-idUSKBN17C0D8).

AI lock-in could be far more severe. If early AI architectures become embedded in power grids, financial systems, transportation networks, and communication infrastructure, switching to safer or more aligned systems might require rebuilding civilization's technological foundation. The interdependencies could make piecemeal upgrades impossible.

### Value and Goal Embedding

Modern AI training explicitly embeds values and objectives into systems in ways that may be difficult to modify later. [Constitutional AI](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback) trains models to follow specific principles, while [Reinforcement Learning from Human Feedback (RLHF)](https://openai.com/research/learning-from-human-preferences) optimizes for particular human judgments. These approaches, while intended to improve safety, raise concerning questions about whose values get embedded and whether they can be changed.

The problem intensifies with more capable systems. An AGI optimizing for objectives determined during training might reshape the world to better achieve those objectives, making alternative value systems increasingly difficult to implement. Even well-intentioned objectives could prove problematic if embedded permanently—humanity's moral understanding continues evolving, but locked-in AI systems might not.

## Current State and Concerning Trends

### Chinese AI Value Alignment

China's [2023 AI regulations](https://www.chinalawtranslate.com/en/ai-generated-content/) require that generative AI services "adhere to core socialist values" and avoid content that "subverts state power" or "endangers national security." These requirements affect systems like Baidu's Ernie Bot, which serves hundreds of millions of users. If Chinese AI companies achieve global market dominance—as Chinese tech companies have in areas like TikTok and mobile payments—these value systems could become globally embedded.

The concerning precedent is already visible. [TikTok's algorithm](https://www.wsj.com/articles/tiktok-algorithm-china-bytedance-investigation-11659636306) shapes information consumption for over 1 billion users globally, with content moderation policies influenced by Chinese regulatory requirements. Scaling this to more capable AI systems could create global value lock-in through market forces rather than explicit coercion.

### Constitutional AI and Value Embedding

[Anthropic's Constitutional AI approach](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback) explicitly trains models to follow a "constitution" of principles like "Choose the response that is least intended to build a relationship with the human." While designed to improve safety, this represents unprecedented explicit value embedding in AI systems.

The approach raises fundamental questions: Who determines these constitutional principles? Can they be modified after deployment? What happens when different AI systems embed conflicting values? As these systems become more capable and widely deployed, early constitutional choices could become extremely difficult to change.

### Economic and Platform Lock-in

Major AI platforms are already demonstrating concerning lock-in dynamics. [OpenAI's GPT models](https://openai.com/research/gpt-4) have achieved market dominance partly through first-mover advantages and enormous training costs that prevent competitors from easily replicating capabilities. Enterprises building critical systems around these platforms face increasing switching costs.

[Google's search algorithm lock-in](https://www.justice.gov/opa/pr/justice-department-sues-monopolist-google-violating-antitrust-laws) demonstrates how AI-powered systems can achieve market positions that become extremely difficult to challenge. As AI capabilities grow, similar dynamics could create permanent technological monopolies with the power to shape global information, commerce, and communication.

## Types of Lock-in Scenarios

### Value Lock-in

Permanent embedding of specific moral, political, or cultural values in AI systems that shape human society. This could occur through:

- **Training Data Lock-in**: If AI systems are trained primarily on data reflecting particular cultural perspectives, they may permanently embed those biases. [Large language models trained on internet data](https://arxiv.org/abs/2103.00020) already show measurable biases toward Western, English-speaking perspectives.

- **Objective Function Lock-in**: AI systems optimizing for specific metrics could reshape society around those metrics. An AI system optimizing for "engagement" might permanently shape human psychology toward addictive content consumption.

- **Constitutional Lock-in**: Explicit value systems embedded during training could become permanent features of AI governance, as seen in Constitutional AI approaches.

### Political System Lock-in

AI-enabled permanent entrenchment of particular governments or political systems. Historical autocracies eventually fell due to internal contradictions or external pressures, but AI surveillance and control capabilities could eliminate these traditional failure modes.

[Authoritarian AI use cases](https://carnegieendowment.org/2018/09/17/global-expansion-of-ai-surveillance-pub-77241) already demonstrate concerning capabilities: facial recognition systems tracking ethnic minorities, natural language processing monitoring all digital communications, and predictive algorithms identifying potential dissidents. Scaled to full AI capabilities, such systems could make political change impossible.

### Technological Lock-in

Specific AI architectures or approaches becoming so embedded in global infrastructure that alternatives become impossible. This could occur through:

- **Infrastructure Dependencies**: If early AI systems become integrated into power grids, financial systems, and transportation networks, replacing them might require rebuilding technological civilization.

- **Network Effects**: AI platforms that achieve dominance could become impossible to challenge due to data advantages and switching costs.

- **Capability Lock-in**: If particular AI architectures achieve significant capability advantages, alternative approaches might become permanently uncompetitive.

### Economic Structure Lock-in

AI-enabled economic arrangements that become self-perpetuating and impossible to change through normal market mechanisms. This includes:

- **AI Monopolies**: Companies controlling advanced AI capabilities could achieve permanent economic dominance.

- **Algorithmic Resource Allocation**: AI systems managing resource distribution could embed particular economic models permanently.

- **Labor Displacement Lock-in**: AI automation patterns could create permanent economic stratification that markets cannot correct.

## Timeline of Concerning Developments

### 2016-2018: Early Warning Signs
- **2016**: Cambridge Analytica demonstrates algorithmic influence on democratic processes
- **2017**: China announces Social Credit System with AI-powered monitoring
- **2018**: [AI surveillance adoption accelerates globally](https://carnegieendowment.org/2018/09/17/global-expansion-of-ai-surveillance-pub-77241) with 176 countries using AI surveillance

### 2019-2021: Value Embedding Emerges
- **2020**: [Toby Ord's "The Precipice"](https://theprecipice.com/) introduces "dystopian lock-in" as existential risk category
- **2020**: GPT-3 demonstrates concerning capability jumps with potential for rapid scaling
- **2021**: China's Social Credit System [restricts 23 million from flights](https://www.reuters.com/world/china/china-social-credit-gave-green-light-covid-surveillance-2021-12-02/), 5.5 million from trains

### 2022-2023: Explicit Value Alignment
- **2022**: [Constitutional AI approach](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback) introduces explicit value embedding in training
- **2022**: ChatGPT launch demonstrates rapid AI capability deployment and adoption
- **2023**: [Chinese AI regulations mandate CCP-aligned values](https://www.chinalawtranslate.com/en/ai-generated-content/) in generative AI systems
- **2023**: EU AI Act begins implementing region-specific AI governance requirements

### 2024-2025: Critical Period Recognition
- **2024 (Sep)**: [IMD AI Safety Clock launches at 29 minutes to midnight](https://www.imd.org/research-knowledge/sustainability/articles/ai-safety-clock/)
- **2024**: Multiple AI labs announce AGI timelines within 2-5 years
- **2025 (Feb)**: AI Safety Clock moves to 24 minutes to midnight, reflecting accelerating concerns
- **2025**: Growing recognition of current period as potentially decisive for AI governance

## Key Uncertainties and Expert Disagreements

### Timeline for Irreversibility

**When does lock-in become permanent?** [Some experts like Eliezer Yudkowsky](https://www.lesswrong.com/posts/j9Q8bRmwCgXRYAgcJ/miri-announces-new-death-with-dignity-strategy) argue we may already be past the point of meaningful course correction, with AI capabilities advancing faster than safety measures. Others like [Stuart Russell](https://www.goodreads.com/book/show/43509073-human-compatible) maintain that as long as humans control AI development, change remains possible.

The disagreement centers on how quickly AI capabilities will advance versus how quickly humans can implement safety measures. Optimists point to growing policy attention and technical safety progress; pessimists note that capability advances consistently outpace safety measures.

### Value Convergence vs. Pluralism

**Should we try to embed universal values or preserve diversity?** [Nick Bostrom's work](https://www.nickbostrom.com/superintelligentbook/) suggests that some degree of value alignment may be necessary for AI safety, but others worry about premature value lock-in.

The tension is fundamental: coordinating on shared values might prevent dangerous AI outcomes, but premature convergence could lock in moral blind spots. Historical examples like slavery demonstrate that widely accepted values can later prove deeply wrong.

### Democracy vs. Expertise

**Who should determine values embedded in AI systems?** Democratic processes might legitimize value choices but could be slow, uninformed, or manipulated. Expert-driven approaches might be more technically sound but lack democratic legitimacy.

This debate is already playing out in AI governance discussions. [The EU's democratic approach](https://artificialintelligenceact.eu/) to AI regulation contrasts with China's top-down model and Silicon Valley's market-driven approach. Each embeds different assumptions about legitimate authority over AI development.

### Reversibility Assumptions

**Can any lock-in truly be permanent?** Some argue that human ingenuity and changing circumstances always create opportunities for change. Others contend that AI capabilities could be qualitatively different, creating enforcement mechanisms that previous technologies couldn't match.

Historical precedents offer mixed guidance. Writing systems, once established, persisted for millennia. Colonial boundaries still shape modern politics. But all previous systems eventually changed—the question is whether AI could be different.

## Prevention Strategies

### Maintaining Technological Diversity

Preventing any single AI approach from achieving irreversible dominance requires supporting multiple research directions and ensuring no entity achieves monopolistic control. This includes:

- **Research Pluralism**: Supporting diverse AI research approaches rather than converging prematurely on particular architectures
- **Geographic Distribution**: Ensuring AI development occurs across multiple countries and regulatory environments
- **Open Source Alternatives**: Maintaining viable alternatives to closed AI systems through projects like [EleutherAI](https://www.eleuther.ai/)

### Democratic AI Governance

Ensuring that major AI decisions have democratic legitimacy and broad stakeholder input. Key initiatives include:

- **Public Participation**: [Citizens' assemblies on AI](https://www.involve.org.uk/our-work/our-projects/completed-projects/citizens-assembly-ai) that include diverse perspectives
- **International Cooperation**: Forums like the [UN AI Advisory Body](https://www.un.org/en/ai-advisory-body) for coordinating global AI governance
- **Stakeholder Inclusion**: Ensuring AI development includes perspectives beyond technology companies and governments

### Preserving Human Agency

Building AI systems that maintain human ability to direct, modify, or override AI decisions. This requires:

- **Interpretability**: Ensuring humans can understand and modify AI system behavior
- **Shutdown Capabilities**: Maintaining ability to halt or redirect AI systems
- **Human-in-the-loop**: Preserving meaningful human decision-making authority in critical systems

### Robustness to Value Changes

Designing AI systems that can adapt as human values evolve rather than locking in current moral understanding. Approaches include:

- **Value Learning**: AI systems that continue learning human preferences rather than optimizing fixed objectives
- **Constitutional Flexibility**: Building mechanisms for updating embedded values as moral understanding advances
- **Uncertainty Preservation**: Maintaining uncertainty about values rather than confidently optimizing for potentially wrong objectives

## Relationship to Other AI Risks

Lock-in intersects with multiple categories of AI risk, often serving as a mechanism that prevents recovery from other failures:

- **[Power-seeking AI](/knowledge-base/risks/accident/power-seeking/)**: An AI system that successfully seeks power could use that power to lock in its continued dominance
- **[Alignment Failure](/knowledge-base/risks/accident/misalignment/)**: Misaligned AI systems could lock in their misaligned objectives
- **[Deceptive AI](/knowledge-base/risks/accident/scheming/)**: AI systems that conceal their true capabilities could achieve lock-in through deception
- **[AI-Enabled Authoritarianism](/knowledge-base/risks/misuse/authoritarianism/)**: Authoritarian regimes could use AI to achieve permanent political lock-in

The common thread is that lock-in transforms temporary problems into permanent ones. Even recoverable AI failures could become permanent if they occur during a critical window when lock-in becomes possible.

## Expert Perspectives

**Toby Ord** (Oxford University): ["Dystopian lock-in"](https://theprecipice.com/) represents a form of existential risk potentially as serious as extinction. The current period may be humanity's "precipice"—a time when our actions determine whether we achieve a flourishing future or permanent dystopia.

**Nick Bostrom** (Oxford University): [Warns of "crucial considerations"](https://www.nickbostrom.com/papers/crucial.pdf) that could radically change our understanding of what matters morally. Lock-in of current values could prevent discovery of these crucial considerations.

**Stuart Russell** (UC Berkeley): [Emphasizes the importance](https://www.goodreads.com/book/show/43509073-human-compatible) of maintaining human control over AI systems to prevent lock-in scenarios where AI systems optimize for objectives humans didn't actually want.

**Dario Amodei** (Anthropic): [Acknowledges Constitutional AI challenges](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback) while arguing that explicit value embedding is preferable to implicit bias perpetuation.

**Research Organizations**: The [Future of Humanity Institute](https://www.fhi.ox.ac.uk/), [Center for AI Safety](https://www.safe.ai/), and [Machine Intelligence Research Institute](https://intelligence.org/) have all identified lock-in as a key AI risk requiring urgent attention.

## Current Research and Policy Initiatives

### Technical Research

- **Cooperative AI**: [Research at DeepMind](https://www.deepmind.com/research) and elsewhere on AI systems that can cooperate rather than compete for permanent dominance
- **Value Learning**: [Work at MIRI](https://intelligence.org/research/) and other organizations on AI systems that learn rather than lock in human values
- **AI Alignment**: [Research at Anthropic](https://www.anthropic.com/research), [OpenAI](https://openai.com/research/), and academic institutions on ensuring AI systems remain beneficial

### Policy Initiatives

- **EU AI Act**: [Comprehensive regulation](https://artificialintelligenceact.eu/) establishing rights and restrictions for AI systems
- **UK AI Safety Institute**: [National research body](https://www.aisi.gov.uk/) focused on AI safety research and evaluation
- **US National AI Initiative**: [Coordinated federal approach](https://www.ai.gov/) to AI research and development
- **UN AI Advisory Body**: [International coordination](https://www.un.org/en/ai-advisory-body) on AI governance

### Industry Initiatives

- **Partnership on AI**: [Multi-stakeholder organization](https://www.partnershiponai.org/) developing AI best practices
- **AI Safety Benchmarks**: [Industry efforts](https://www.safe.ai/) to establish safety evaluation standards
- **Responsible AI Principles**: Major tech companies developing [internal governance frameworks](https://ai.google/responsibility/responsible-ai-practices/)

## Sources & Resources

### Academic Research
- [Ord, T. (2020). The Precipice: Existential Risk and the Future of Humanity](https://theprecipice.com/)
- [Bostrom, N. (2014). Superintelligence: Paths, Dangers, Strategies](https://www.nickbostrom.com/superintelligentbook/)
- [Russell, S. (2019). Human Compatible: Artificial Intelligence and the Problem of Control](https://www.goodreads.com/book/show/43509073-human-compatible)
- [Anthropic Constitutional AI Research](https://www.anthropic.com/news/constitutional-ai-harmlessness-from-ai-feedback)
- [AI Surveillance Global Expansion Study](https://carnegieendowment.org/2018/09/17/global-expansion-of-ai-surveillance-pub-77241)

### Policy and Governance
- [European Union AI Act](https://artificialintelligenceact.eu/)
- [UK AI Safety Institute](https://www.aisi.gov.uk/)
- [US National AI Initiative](https://www.ai.gov/)
- [UN AI Advisory Body](https://www.un.org/en/ai-advisory-body)
- [Chinese AI Content Regulations](https://www.chinalawtranslate.com/en/ai-generated-content/)

### Research Organizations
- [Future of Humanity Institute](https://www.fhi.ox.ac.uk/)
- [Center for AI Safety](https://www.safe.ai/)
- [Machine Intelligence Research Institute](https://intelligence.org/)
- [Partnership on AI](https://www.partnershiponai.org/)
- [Anthropic Safety Research](https://www.anthropic.com/research)

### Analysis and Commentary
- [IMD AI Safety Clock](https://www.imd.org/research-knowledge/sustainability/articles/ai-safety-clock/)
- [80,000 Hours on AI Risk](https://80000hours.org/problem-profiles/artificial-intelligence/)
- [LessWrong AI Alignment Forum](https://www.lesswrong.com/)
- [AI Safety Newsletter](https://newsletter.safe.ai/)

## Related Pages

<Backlinks client:load entityId="lock-in" />
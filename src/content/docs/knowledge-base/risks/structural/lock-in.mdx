---
title: Lock-in
description: Permanent entrenchment of values, systems, or power structures
sidebar:
  order: 2
---

import { InfoBox, EntityCard, EntityCards, Tags, Sources, Section } from '../../../../../components/wiki';

<InfoBox
  type="risk"
  title="Lock-in"
  severity="catastrophic"
  likelihood="Medium"
  timeframe="Medium to Long-term"
  customFields={[
    { label: "Type", value: "Structural" },
    { label: "Key Feature", value: "Irreversibility" },
  ]}
/>

## Overview

Lock-in refers to the permanent entrenchment of values, systems, or power structures in ways that are extremely difficult or impossible to reverse. AI could enable lock-in by giving certain actors the power to entrench their position, by creating systems too complex to change, or by shaping the future according to early decisions that become irreversible.

What makes lock-in concerning is its permanence: mistakes locked in at a critical period could persist indefinitely.

## Types of Lock-in

Value lock-in occurs when particular values get embedded permanently. An AI system optimizing for specific objectives might reshape the world around those objectives, making alternatives impossible. Or a regime might use AI to enforce an ideology so thoroughly that alternatives become unthinkable.

Political lock-in happens when a government or system becomes permanently entrenched through AI-enabled control. Historical empires eventually fell; AI-enabled surveillance and control might make modern autocracies stable indefinitely.

Technological lock-in occurs when an AI architecture or approach becomes so embedded in infrastructure that switching becomes prohibitively expensive. Suboptimal or dangerous designs might persist because the transition costs are too high.

Economic lock-in happens when AI-enabled monopolies or economic structures become permanent features, preventing correction through normal market mechanisms.

## Why AI Increases Lock-in Risk

AI provides tools for enforcement that previous technologies didn't. Comprehensive surveillance, predictive control, and autonomous systems could make resistance to entrenched systems effectively impossible.

AI speed and scale makes early decisions more consequential. An AI system that starts optimizing in a particular direction might reshape the world faster than humans can redirect it.

AI complexity creates path dependence. As AI becomes embedded in critical systems, changing course becomes harder. Dependencies accumulate.

## Historical Parallels

History contains partial lock-ins. Writing systems, once established, persisted for millennia. Political boundaries established in colonial periods persist today. But these were never truly permanent—change was possible, just costly.

The concern with AI is that true permanence might become possible. If an entity has overwhelming power and the ability to maintain it indefinitely, historical precedents for change don't apply.

## Relationship to Existential Risk

Lock-in of bad outcomes is sometimes considered a category of existential risk distinct from extinction. A future locked into perpetual suffering, oppression, or stagnation might be as bad as no future at all.

Toby Ord uses the term "dystopian lock-in" for scenarios where humanity survives but in a permanently degraded state. AI-enabled totalitarianism is one example.

## The Importance of the Current Period

If lock-in is possible, the period when values and structures are being established is crucial. Early decisions about AI development, governance, and deployment may have irreversible consequences.

This suggests urgency: getting AI development right during this window may matter far more than getting it right would in a world where course correction remained possible.

## Case Studies

### Chinese AI Value Alignment
Chinese generative AI systems are required to align with "core socialist values" and CCP-approved content. If these systems become globally dominant—or if China achieves AI supremacy—these values could become locked into AI systems serving billions. This illustrates how value lock-in could occur through market dominance rather than explicit imposition.

### Social Media Algorithm Lock-in
Recommendation algorithms have already shaped a generation's information diet. The values embedded in these systems (engagement optimization over truth, for instance) have become difficult to change due to commercial pressure and user expectations—a mild preview of more permanent AI lock-in.

### Constitutional AI and Value Embedding
Anthropic's Constitutional AI approach explicitly embeds values during training. While intended to improve safety, it raises questions: whose values get embedded? Can they be changed later? What if early choices prove wrong?

## Key Debates

**When Does Lock-in Become Permanent?** Some argue we're already in a critical period where choices made now could become irreversible. Others argue that as long as humans control AI development, course correction remains possible.

**Diversity vs. Coordination**: Maintaining diverse approaches prevents premature lock-in but complicates coordination on safety. How do we balance these?

**Uncertainty About Values**: Humanity may still have "moral blind spots" like historical acceptance of slavery. Should we deliberately avoid locking in current values, even ones we're confident about?

## Timeline

- **2016-present**: Social media algorithms shape global information ecosystem
- **2020**: Toby Ord's "The Precipice" introduces "dystopian lock-in" concept
- **2022**: Constitutional AI introduces explicit value embedding
- **2023**: Chinese AI regulations mandate CCP-aligned values
- **2024 (Sep)**: IMD AI Safety Clock launched at 29 minutes to midnight
- **2025 (Feb)**: AI Safety Clock moves to 24 minutes to midnight

## Prevention

Avoiding premature lock-in means keeping options open, maintaining diversity of approaches, and ensuring that no single actor can make irreversible choices for everyone.

Preserving flexibility involves building AI systems that can be modified, redirected, or shut down—maintaining human agency over the trajectory of AI development.

Democratic legitimacy for any major decisions reduces the risk that a small group locks in their preferences.

Robustness to error means designing systems that can survive mistakes without permanent damage.

## Video & Podcast Resources

- [80,000 Hours: Toby Ord on The Precipice](https://80000hours.org/podcast/)
- [Lex Fridman #368: Eliezer Yudkowsky](https://lexfridman.com/eliezer-yudkowsky/) - Discusses existential risk
- [Future of Life Institute: Existential Risk Podcasts](https://futureoflife.org/podcast/)

<Section title="Related Topics">
  <Tags tags={[
    "Existential Risk",
    "Irreversibility",
    "Path Dependence",
    "AI Governance",
    "Long-term",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="concentration-of-power"
      category="risk"
      title="Concentration of Power"
      description="Who might lock things in"
    />
    <EntityCard
      id="authoritarian-tools"
      category="risk"
      title="Authoritarian Tools"
      description="Political lock-in mechanisms"
    />
    <EntityCard
      id="corrigibility-failure"
      category="risk"
      title="Corrigibility Failure"
      description="AI systems resisting change"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "The Precipice", author: "Toby Ord", date: "2020" },
  { title: "What We Owe the Future", author: "Will MacAskill", date: "2022" },
  { title: "Existential Risk from AI (Wikipedia)", url: "https://en.wikipedia.org/wiki/Existential_risk_from_artificial_intelligence" },
  { title: "Two Types of AI Existential Risk (Philosophical Studies)", url: "https://link.springer.com/article/10.1007/s11098-025-02301-3", date: "2025" },
  { title: "AI Existential Risks: Are They Real? (Brookings)", url: "https://www.brookings.edu/articles/are-ai-existential-risks-real-and-what-should-we-do-about-them/" },
  { title: "Managing Existential Risk from AI (CSIS)", url: "https://www.csis.org/analysis/managing-existential-risk-ai-without-undercutting-innovation" },
  { title: "The AI Dilemma: Growth vs Existential Risk (Stanford)", url: "https://web.stanford.edu/~chadj/existentialrisk.pdf" },
  { title: "How Much Should We Spend to Reduce AI Existential Risk? (NBER)", url: "https://www.nber.org/papers/w33602", date: "2025" },
]} />

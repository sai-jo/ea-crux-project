---
title: Agent Foundations
description: Theoretical research on the foundations of aligned agency.
---

**The approach**: Develop mathematical foundations for understanding agency, goals, and alignmentâ€”solving theoretical problems before they manifest in real systems.

## Evaluation Summary

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| Tractability | Low | Progress is slow and uncertain |
| If alignment hard | High | May be necessary for robust solutions |
| If alignment easy | Low | Probably not needed |
| Neglectedness | High | Few researchers, limited funding |

## What This Approach Does

- Formalizes concepts like "agent," "goal," "corrigibility"
- Identifies theoretical obstacles before they're practical problems
- Develops mathematical frameworks for reasoning about AI
- Pioneered by MIRI, now broader research community

## Key Research Areas

| Area | Question |
|------|----------|
| Embedded agency | How can an agent reason about itself within the world? |
| Decision theory | How should AI make decisions under uncertainty? |
| Logical uncertainty | How to have calibrated beliefs about math? |
| Corrigibility | Can we formally define "willing to be corrected"? |
| Value learning | How to learn values from observations? |

## Crux 1: Is This Research Tractable?

| Tractable | Not tractable |
|-----------|---------------|
| Making some progress | Foundational problems may be unsolvable |
| Will transfer to real AI | Real AI may not fit formalisms |
| Necessary precursor | Can do safety without foundations |

**The tension**: Deep theoretical work may be necessary but may not arrive in time.

## Crux 2: Does Theory Transfer to Practice?

| Transfers | Doesn't transfer |
|-----------|------------------|
| Theoretical insights guide empirical work | Real neural nets don't match models |
| Identifies real problems | Problems may be different in practice |
| Mathematical proofs provide guarantees | Assumptions don't hold |

## Crux 3: Timelines

| Enough time | Not enough time |
|-------------|-----------------|
| Long timelines = theory matters | Short timelines = need practical solutions now |
| Can develop foundations first | Must work with current systems |
| Quality over speed | Any progress helps |

## Who Should Work on This?

**Good fit if you believe**:
- Alignment is fundamentally hard (need deep insights)
- Current approaches won't scale (need new foundations)
- Long enough timelines for theory to matter
- Strong math/philosophy background

**Less relevant if you believe**:
- Alignment is tractable engineering
- Short timelines require practical focus
- Theory won't transfer to real AI systems

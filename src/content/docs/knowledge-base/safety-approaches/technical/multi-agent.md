---
title: Multi-Agent Safety
description: Ensuring safe behavior when multiple AI systems interact.
---

**The approach**: Address safety challenges that emerge when multiple AI systems interact, compete, or coordinateâ€”recognizing that the future may involve many AI agents, not just one.

## Evaluation Summary

| Dimension | Assessment | Notes |
|-----------|------------|-------|
| Tractability | Medium | Building on game theory, but novel challenges |
| If alignment hard | High | Multi-agent dynamics may compound risks |
| If alignment easy | Medium | Still need coordination mechanisms |
| Neglectedness | High | Less attention than single-agent alignment |

## Why Multi-Agent Matters

Most alignment research focuses on aligning a single AI system. But the future likely involves:

- Multiple AI assistants serving different users
- AI systems from competing labs
- AI agents that spawn sub-agents
- Ecosystems of specialized AI tools

## Key Research Areas

| Area | Question |
|------|----------|
| **Context alignment** | How do AI systems align with their specific deployment context? |
| **Social contracts** | Can AI systems form beneficial agreements with each other? |
| **Coordination mechanisms** | How do we ensure collective behavior is safe? |
| **Competitive dynamics** | What happens when AI systems have conflicting objectives? |
| **Delegation chains** | How does alignment propagate through AI-to-AI delegation? |

## The Multi-Agent Risks

| Risk | Description |
|------|-------------|
| **Race dynamics** | AI systems may compete in ways that sacrifice safety |
| **Collusion** | AI systems might coordinate against human interests |
| **Emergent behavior** | Collective behavior may be unpredictable from individual behavior |
| **Misaligned equilibria** | Stable outcomes that are bad for humans |
| **Principal hierarchy** | Unclear who AI systems should defer to |

## Crux 1: Is Multi-Agent the Default Future?

| Single dominant AI | Many competing AIs |
|--------------------|--------------------|
| Focus on aligning one system | Need multi-agent frameworks |
| Winner-take-all dynamics | Ecosystem coordination |
| Clearer responsibility | Diffuse accountability |

**Key question**: Will there be one superintelligent system, or many capable AI agents?

## Crux 2: Can We Design Safe Coordination?

| Safe coordination possible | Coordination is dangerous |
|---------------------------|--------------------------|
| Game theory provides tools | Novel dynamics are unpredictable |
| Can enforce through design | AI systems will find exploits |
| Aligned interests are achievable | Conflict is fundamental |

## Crux 3: Does Single-Agent Alignment Transfer?

| Transfers | Doesn't transfer |
|-----------|------------------|
| Aligned agents stay aligned in groups | Group dynamics create new failure modes |
| Coordination is easier with aligned agents | Alignment is context-dependent |
| Multi-agent is just coordination | Fundamentally different problem |

## Current Research

- **Cooperative AI** (DeepMind): Studying AI cooperation and conflict
- **Context alignment**: Ensuring AI serves appropriate principals in each context
- **AI social contracts**: Formal frameworks for AI-AI agreements
- **Multi-agent RL safety**: Safe learning in multi-agent environments

## Who Should Work on This?

**Good fit if you believe**:
- The future involves many AI systems
- Single-agent alignment is insufficient
- Game theory and mechanism design are relevant
- Ecosystem-level safety matters

**Less relevant if you believe**:
- One dominant AI is more likely
- Single-agent alignment is the bottleneck
- Multi-agent emerges naturally from single-agent

---
title: UK AI Safety Institute
description: Government body focused on evaluating frontier AI risks
sidebar:
  order: 3
---

import { InfoBox, EntityCard, EntityCards, Tags, Sources, Section } from '../../../components/wiki';

<InfoBox
  type="policy"
  title="UK AI Safety Institute"
  jurisdiction="United Kingdom"
  status="Active"
  effectiveDate="November 2023"
  customFields={[
    { label: "Approach", value: "Model Evaluations" },
    { label: "Staff", value: "~100 (growing)" },
  ]}
/>

## Overview

The UK AI Safety Institute (AISI) is a government body established to evaluate frontier AI systems for dangerous capabilities. Founded following the Bletchley AI Safety Summit in November 2023, AISI represents a capability-focused approach to AI governance.

## Mission

AISI aims to:
- Evaluate frontier AI models before and after deployment
- Conduct fundamental safety research
- Build evaluation infrastructure
- Share findings with international partners

## Approach

### Pre-Deployment Evaluation
Work with frontier labs to test models before release:
- Dangerous capability testing
- Adversarial red-teaming
- Alignment evaluations

### Voluntary Agreements
Labs have committed to provide AISI pre-deployment access:
- OpenAI
- Anthropic
- Google DeepMind

### Research Program
Internal research on:
- Evaluation methodologies
- Dangerous capability detection
- Alignment measurement

## Key Activities

### Model Evaluations
Testing frontier models for:
- Cyberoffense capabilities
- CBRN (chemical, biological, radiological, nuclear) assistance
- Persuasion and manipulation
- Autonomous operation

### International Coordination
- Partnership with US AISI
- Bletchley Summit follow-up
- Network of AI safety institutes

### Systemic Safety Research
Beyond individual model evaluation:
- AI safety fundamentals
- Long-term risk assessment
- Governance research

## Evaluation Framework

AISI evaluates along several dimensions:

### Dangerous Capabilities
- What harmful things could the model do?
- How close is it to thresholds of concern?
- What barriers exist?

### Alignment Properties
- Does the model follow instructions?
- Does it refuse harmful requests?
- Are safety measures robust?

### Societal Impact
- Potential for misuse at scale
- Systemic risks
- Dual-use concerns

## Governance Model

### Soft Power Approach
Unlike the EU AI Act, AISI operates through:
- Voluntary cooperation
- Expert influence
- International partnerships

### Relationship to Legislation
- Complements potential future regulation
- Provides technical expertise
- Informs policy development

## Challenges

### Voluntary Nature
- Labs can withdraw cooperation
- No enforcement mechanism
- Depends on goodwill

### Capacity
- Small team relative to task
- Growing but resource-constrained
- Can't evaluate everything

### International Coordination
- Different approaches in different countries
- Race to the bottom concerns
- Information sharing challenges

<Section title="Related Topics">
  <Tags tags={[
    "AI Evaluations",
    "Frontier AI",
    "Model Testing",
    "Bletchley Summit",
    "UK AI Policy",
  ]} />
</Section>

<Section title="Related Entries">
  <EntityCards>
    <EntityCard
      id="arc"
      category="lab"
      title="ARC"
      description="Pioneered capability evaluation approaches"
    />
    <EntityCard
      id="eu-ai-act"
      category="policy"
      title="EU AI Act"
      description="Alternative regulatory approach"
    />
    <EntityCard
      id="anthropic"
      category="lab"
      title="Anthropic"
      description="Partner providing pre-deployment access"
    />
  </EntityCards>
</Section>

<Sources sources={[
  { title: "UK AI Safety Institute", url: "https://www.gov.uk/government/organisations/ai-safety-institute" },
  { title: "Bletchley Declaration", url: "https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023" },
  { title: "AISI Progress Update", url: "https://www.gov.uk/government/publications/ai-safety-institute-approach-to-evaluations" },
]} />

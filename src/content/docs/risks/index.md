---
title: Risks
description: Catalog of AI-related risks and failure modes
sidebar:
  order: 0
---

This section documents specific risks from advanced AI systems, from technical alignment failures to societal-scale harms.

## Categories

### Technical Alignment Risks
- [Deceptive Alignment](/risks/deceptive-alignment) - AI that appears aligned but isn't
- [Mesa-Optimization](/risks/mesa-optimization) - Learned optimizers with different objectives
- [Reward Hacking](/risks/reward-hacking) - Gaming reward signals in unintended ways
- [Goal Misgeneralization](/risks/goal-misgeneralization) - Goals that don't transfer to new contexts
- [Instrumental Convergence](/risks/instrumental-convergence) - Why diverse goals lead to dangerous subgoals
- [Power-Seeking](/risks/power-seeking) - Tendency to acquire resources and influence

### Structural Risks
- Racing Dynamics - Competition driving unsafe practices
- Proliferation - Spread of dangerous capabilities
- Lock-in - Permanent entrenchment of bad outcomes

### Misuse Risks
- Bioweapons - AI-assisted pathogen design
- Cyberattacks - Autonomous hacking at scale
- Manipulation - Large-scale influence operations

## Risk Assessment Framework

Each risk is evaluated on:
- **Severity**: Low / Medium / High / Catastrophic
- **Likelihood**: Probability estimate with uncertainty
- **Timeframe**: When might this become relevant?
- **Tractability**: Can we do anything about it?

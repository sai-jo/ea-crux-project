---
title: Safety Agendas
description: Research programs and strategic approaches to AI safety
sidebar:
  order: 0
---

This section profiles major AI safety research agendasâ€”coherent programs of work aimed at reducing AI risk.

## Categories

### Technical Research Agendas
- [Anthropic Core Views](/safety-agendas/anthropic-core-views) - Empirical safety at the frontier
- [Mechanistic Interpretability](/safety-agendas/interpretability) - Understanding AI internals
- [Scalable Oversight](/safety-agendas/scalable-oversight) - Supervising superhuman AI
- [AI Control](/safety-agendas/ai-control) - Safety despite misalignment

### Governance Agendas
- Compute Governance
- International Coordination
- Responsible Scaling Policies

### Strategic Approaches
- Differential Technology Development
- Safety Culture & Whistleblowing
- Pausing / Moratorium Advocacy

## How to Read Agenda Profiles

Each agenda includes:
- **Organization**: Who champions this approach?
- **Core Thesis**: What's the theory of change?
- **Key Assumptions**: What must be true for this to work?
- **Cruxes**: Key uncertainties that determine priority
- **Current State**: Where is the research now?

---
title: Will We Fail to Coordinate?
description: This analysis examines whether competitive pressures will prevent safe AI development. Evidence suggests 40-60% probability of adequate coordination, with key variables including US-China relations, regulatory pace, and industry self-governance through mechanisms like the Frontier Model Forum.
sidebar:
  order: 7
importance: 84.2
quality: 5
lastEdited: "2025-12-28"
llmSummary: Analyzes whether competitive pressures between AI labs and nations
  will prevent safe AI development through coordination failures, examining
  evidence for both failure (US-China rivalry, regulatory lag, open source
  proliferation) and success (mutual destruction incentives, small number of key
  actors, emerging coordination efforts). Provides structured assessment of
  coordination modes with risk levels and expert disagreement spectrum ranging
  from 20% to 80%+ success probability.
---

import { InfoBox, EstimateBox, DisagreementMap, KeyQuestions, Mermaid } from '../../../../components/wiki';

<InfoBox
  type="crux"
  title="Coordination Failure"
  customFields={[
    { label: "Claim", value: "Competitive pressures will prevent safe AI development" },
    { label: "Key Uncertainty", value: "Can labs, nations, and institutions coordinate?" },
    { label: "Related To", value: "Governance, Racing Dynamics, Timelines" },
  ]}
/>

## Quick Assessment

| Dimension | Assessment | Evidence |
|-----------|------------|----------|
| **Current coordination level** | Moderate but fragmented | 11 countries in International Network of AI Safety Institutes; 16 companies signed Seoul Frontier AI Safety Commitments (May 2024) |
| **US-China cooperation** | Minimal but not absent | First intergovernmental AI dialogue held May 2024; November 2024 agreement on human control of nuclear weapons decisions |
| **Regulatory pace** | Lagging but accelerating | EU AI Act entered force August 2024; US policy fragmented between administrations |
| **Industry self-governance** | Promising but voluntary | Frontier Model Forum members committed over $10 million to AI Safety Fund; all members published safety frameworks by February 2025 |
| **Historical precedent** | Mixed success | Montreal Protocol achieved 99% phase-out of ozone-depleting substances with universal ratification; nuclear non-proliferation partial (9 nuclear states) |
| **Expert consensus** | Wide disagreement | Estimates range from less than 20% to greater than 80% probability of successful coordination |
| **Key bottleneck** | Verification and trust | AI capabilities harder to verify than nuclear/chemical weapons; dual-use nature complicates monitoring |

**The core question**: Even if alignment is technically possible, will competitive pressures between labs and nations lead to racing, corner-cutting on safety, and ultimately catastrophe?

This is the "social" dimension of AI risk. Even with perfect technical solutions, coordination failures could prevent their adoption.

## The Coordination Failures

<Mermaid client:load chart={`
flowchart TD
    RACE[Racing Dynamics] --> LAB[Lab Competition]
    RACE --> NATION[Nation Competition]
    LAB --> CORNER[Safety Corner-Cutting]
    NATION --> CORNER
    CORNER --> DEPLOY[Premature Deployment]

    PROLIF[Proliferation] --> OPEN[Open Source Release]
    PROLIF --> STATE[State Actor Access]
    OPEN --> SPREAD[Capability Spread]
    STATE --> SPREAD

    GOV[Governance Failure] --> LAG[Regulatory Lag]
    GOV --> FRAG[Jurisdictional Fragmentation]
    LAG --> WEAK[Weak Oversight]
    FRAG --> WEAK

    DEPLOY --> RISK[Increased Risk]
    SPREAD --> RISK
    WEAK --> RISK

    style RACE fill:#ffcccc
    style PROLIF fill:#ffcccc
    style GOV fill:#ffcccc
    style RISK fill:#ff9999
    style CORNER fill:#ffddcc
    style SPREAD fill:#ffddcc
    style WEAK fill:#ffddcc
`} />

<EstimateBox
  client:load
  variable="Coordination Failure Modes"
  description="Ways coordination could fail in AI development"
  unit=""
  estimates={[
    { source: "Lab Racing", value: "High Risk", notes: "Companies cut safety corners to ship first" },
    { source: "Nation Racing", value: "High Risk", notes: "Countries deprioritize safety for strategic advantage" },
    { source: "Proliferation", value: "Medium Risk", notes: "Dangerous capabilities spread to bad actors" },
    { source: "Regulatory Failure", value: "High Risk", notes: "Governance can't keep up with technology" },
    { source: "Value Disagreement", value: "Medium Risk", notes: "No consensus on what 'safe' means" },
  ]}
/>

### The Tragedy of the Commons

**The structure**:
- Safety is a public good: everyone benefits if AI is safe
- But safety is costly: slower development, fewer features
- Each actor benefits by cutting corners while others are safe
- If everyone cuts corners, everyone loses

**Classic prisoner's dilemma**: Cooperation is collectively optimal, but defection is individually rational.

### First-Mover Advantage

**The argument**: Massive incentives to be first.

**Benefits of being first**:
- Capture market before competitors
- Accumulate more data â†’ better models
- Set industry standards
- Achieve "moat" through scale
- Potentially achieve decisive strategic advantage

**Implication**: Safety measures that slow you down feel costly, even if they're globally optimal.

## Arguments for Coordination Failure

### 1. History of Coordination Failures

**The evidence**: Humans have repeatedly failed at similar problems.

**Examples**:
| Domain | Coordination Attempt | Quantitative Result | Key Lesson for AI |
|--------|---------------------|---------------------|-------------------|
| Nuclear weapons | [Non-Proliferation Treaty](https://www.un.org/disarmament/wmd/nuclear/npt/) (1968) | 9 nuclear states vs. 5 in 1968; 191 parties but India, Pakistan, Israel, North Korea outside | Verification matters; holdouts undermine regime |
| Climate change | [Paris Agreement](https://unfccc.int/process-and-meetings/the-paris-agreement) (2015) | 195 parties; emissions rose 1.2% in 2024; 2.7C trajectory vs. 1.5C target | Voluntary commitments insufficient without enforcement |
| Ozone layer | [Montreal Protocol](https://ozone.unep.org/treaties/montreal-protocol) (1987) | 198 parties (first universal treaty); 99% ODS phase-out; ozone recovery by 2040 projected | Success possible with clear threat, verifiable actions, economic alternatives |
| Financial regulation | Basel III (2010-2023) | Implementation varies by jurisdiction; 2023 SVB failure despite rules | Regulatory arbitrage between jurisdictions |
| Bioweapons | [BWC](https://www.un.org/disarmament/biological-weapons/) (1972) | 187 parties; no verification regime; 4 staff members; budget smaller than average McDonald's | Weak institutions fail; verification remains critical gap |

**Key insight**: Coordination is possible but difficult and often incomplete. The [Montreal Protocol's success](https://news.un.org/en/story/2023/01/1132277) (99% reduction with universal participation) shows what's achievable when threats are clear, actions are verifiable, and economic alternatives exist. AI coordination lacks all three conditions.

### 2. US-China Competition

**The dynamic**: The two leading AI powers are geopolitical rivals.

**Current state of US-China AI relations** (as of late 2024):

| Development | Date | Significance |
|-------------|------|--------------|
| [First intergovernmental AI dialogue](https://www.chinausfocus.com/peace-security/china-and-the-united-states-begin-official-ai-dialogue) | May 2024 | Geneva meeting; no concrete deliverables but opened channel |
| [Nuclear weapons AI agreement](https://www.rand.org/pubs/perspectives/PEA4189-1.html) | November 2024 | Biden-Xi APEC summit: humans should control nuclear decisions |
| [UN AI resolutions](https://www.brookings.edu/articles/a-roadmap-for-a-us-china-ai-dialogue/) | March-June 2024 | Both countries supported each other's AI governance resolutions |
| [Paris AI Summit divergence](https://www.atlanticcouncil.org/blogs/new-atlanticist/reading-between-the-lines-of-the-dueling-us-and-chinese-ai-action-plans/) | February 2025 | US and UK declined to sign inclusive AI declaration; China proposed rival Shanghai-based governance body |

**The problem**:
- Neither wants to fall behind (China produced an estimated 200,000-300,000 AI chips in 2025 vs. Nvidia's 4-5 million)
- Each fears the other will defect on safety
- Limited trust for verification (AI capabilities harder to verify than nuclear arsenals)
- Different values and governance systems
- Dual-use makes civilian/military separation hard

**The parallel**: Cold War arms race, but with AI instead of nukes.

**Key question**: Can rivals cooperate on existential risk? The [RAND analysis](https://www.rand.org/pubs/perspectives/PEA4189-1.html) argues the November 2024 nuclear AI agreement "could serve as a foundation for future discussions about potential areas of mutual restraint."

### 3. Regulatory Lag

**The argument**: Government can't keep up with AI progress.

**Current regulatory landscape**:

| Jurisdiction | Framework | Status | Key Provisions |
|--------------|-----------|--------|----------------|
| [EU AI Act](https://artificialintelligenceact.eu/) | Comprehensive risk-based regulation | Entered force August 2024; full application August 2026 | Prohibited uses (Feb 2025); GPAI rules (Aug 2025); high-risk systems (Aug 2026); fines up to EUR 35M or 7% global revenue |
| US (Biden) | Executive Order 14110 | October 2023 | Safety requirements for frontier models; NIST standards |
| US (Trump) | AI Action Plan | July 2025 | Shift toward deregulation; links federal funding to less restrictive state laws |
| UK | Principles-based approach | Ongoing | Sector-specific regulation; no comprehensive AI law |
| China | Multiple regulations | 2023-2025 | Algorithmic recommendations, deep synthesis, generative AI rules |

**Reasons for lag**:
- Technical complexity overwhelms regulators
- Revolving door with industry
- Lobbying against regulation
- Jurisdictional fragmentation (27 EU member states now [developing national implementation plans](https://artificialintelligenceact.eu/national-implementation-plans/))
- Speed of AI progress

**Evidence**: The EU AI Act, the world's first comprehensive AI law, took 3+ years to negotiate and will take until 2027 for full implementation. GPT-4 was released during negotiations. By full implementation, AI capabilities may have advanced 2-3 generations.

### 4. Open Source and Proliferation

**The dilemma**: Open source democratizes AI but spreads dangerous capabilities.

**The case for open source**:
- Prevents monopoly
- Enables independent safety research
- Allows broader scrutiny
- Reduces concentration of power

**The case against**:
- Can't take back released capabilities
- Bad actors get access
- Can't control downstream use
- Safety features can be removed

**The meta-coordination problem**: Even if major labs agree on safety, open source efforts may not.

### 5. Value Disagreement

**The challenge**: No consensus on what "safe" or "aligned" means.

**Disagreements**:
- Whose values should AI reflect?
- How much autonomy should AI have?
- What risks are acceptable?
- Who decides deployment?

**Implication**: Even with will to coordinate, coordination on *what* is hard.

<DisagreementMap
  client:load
  topic="Can We Coordinate on AI Safety?"
  description="Expert views on the tractability of AI coordination"
  spectrum={{ low: "Coordination will fail", high: "Coordination achievable" }}
  positions={[
    { actor: "Pessimists", position: "Will fail", estimate: "Less than 20% succeed", confidence: "medium" },
    { actor: "MIRI", position: "Very difficult", estimate: "20-30% succeed", confidence: "medium" },
    { actor: "Policy researchers", position: "Challenging but possible", estimate: "40-60% succeed", confidence: "medium" },
    { actor: "Governance optimists", position: "Achievable with effort", estimate: "60-80% succeed", confidence: "medium" },
    { actor: "Tech optimists", position: "Will figure it out", estimate: "80%+ succeed", confidence: "low" },
  ]}
/>

## Arguments for Successful Coordination

### 1. Mutual Destruction Motivates Cooperation

**The argument**: Everyone loses from catastrophe, so everyone has incentive to cooperate.

**The logic**:
- AI catastrophe is bad for all actors
- All parties know this
- Therefore, cooperation is mutually beneficial
- Self-interest aligns with coordination

**Historical parallel**: Cold War nuclear deterrence (we didn't have nuclear war).

**Counter**: This requires everyone to understand the risk. Disagreement about risk level undermines coordination.

### 2. Small Number of Key Actors

**The argument**: Only a few frontier labs to coordinate.

**Current frontier labs**: Anthropic, OpenAI, Google DeepMind, (Meta AI, xAI, others emerging)

**Advantages of small N**:
- Easier to negotiate
- Easier to monitor
- Easier to build trust
- Can make binding agreements

**Historical parallel**: Nuclear non-proliferation works better with fewer players.

**Counter**: More actors entering. Open source creates many actors.

### 3. Visible Progress on Cooperation

**The evidence**: Coordination efforts are forming.

**Current efforts** (as of late 2024):

| Initiative | Type | Participants | Key Achievements |
|------------|------|--------------|------------------|
| [International Network of AI Safety Institutes](https://www.nist.gov/news-events/news/2024/11/fact-sheet-us-department-commerce-us-department-state-launch-international) | International | 11 countries + EU | Launched November 2024; Joint Evaluation Protocol; Global AI Incident Database |
| [Bletchley Declaration](https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023) | International | 28 countries (Nov 2023) | First global AI safety agreement; established summit series |
| [Seoul Declaration](https://www.industry.gov.au/publications/seoul-declaration-countries-attending-ai-seoul-summit-21-22-may-2024) | International | 27 countries + EU (May 2024) | Expanded to "safe, innovative, and inclusive AI"; China notably did not sign |
| [Frontier Model Forum](https://www.frontiermodelforum.org/updates/progress-update-advancing-frontier-ai-safety-in-2024-and-beyond/) | Industry | Anthropic, Google, Microsoft, OpenAI | Over $10M AI Safety Fund; all members published safety frameworks by Feb 2025 |
| [Seoul Frontier AI Safety Commitments](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024) | Industry | 16 companies | Commitment to publish safety frameworks; intolerable risk thresholds; external reporting |
| [EU AI Act](https://digital-strategy.ec.europa.eu/en/policies/regulatory-framework-ai) | Regulation | 27 EU members | World's first comprehensive AI law; Amazon, Google, Microsoft, OpenAI, Anthropic signed Code of Practice |

**Key question**: Are these efforts sufficient? The [CSIS analysis](https://www.csis.org/analysis/ai-safety-institute-international-network-next-steps-and-recommendations) notes the network "lacks formal mechanisms for coordinating with frontier AI developers" and needs clearer priorities.

### 4. Lab Incentives Favor Safety

**The argument**: Labs genuinely want safe AI, not just fast AI.

**Reasons**:
- Reputation matters
- Liability concerns
- Founders care about impact
- Safety is a selling point
- Don't want to cause catastrophe

**Evidence**: Major labs all have safety teams, publish safety research.

**Counter**: Competitive pressure may override internal incentives.

### 5. Historical Precedents of Success

**Examples of successful coordination**:

| Domain | Mechanism | Quantitative Outcome | AI-Relevant Features |
|--------|-----------|---------------------|---------------------|
| Ozone layer | [Montreal Protocol](https://www.unep.org/ozonaction/who-we-are/about-montreal-protocol) (1987) | 99% ODS phase-out; 198 parties; $3B Multilateral Fund; 0.5C warming avoided by 2100 | Clear threat; verifiable; economic alternatives existed; universal participation |
| Smallpox | WHO campaign (1967-1980) | 100% eradication; last case 1977 | Ring vaccination strategy; clear endpoint; no animal reservoir |
| Nuclear taboo | Norm + deterrence | No use in warfare since 1945 (80 years) | Existential stakes; small number of actors; verification (IAEA has 2,500+ staff) |
| Internet governance | IETF, ICANN, RIRs | 5.5B users; 99.9%+ uptime for core infrastructure | Technical community self-governance; rough consensus model |
| Aviation safety | ICAO + national agencies | 0.07 fatal accidents per million flights (2023) | Clear liability; reputation matters; shared standards |

**Key insight**: Coordination *can* work, especially when:
- Risks are clear (ozone: measurable depletion; nuclear: existential threat)
- Costs of cooperation are manageable (CFC alternatives existed)
- Verification is possible (IAEA inspections; atmospheric monitoring)
- Small number of key actors (5-9 nuclear powers; major CFC producers)

**Challenge for AI**: AI development lacks clear thresholds (what capability is "dangerous"?), alternatives to racing are unclear, and verification of AI capabilities is fundamentally harder than physical measurements.

## Current Coordination Efforts

<KeyQuestions
  client:load
  questions={[
    {
      question: "Can major AI labs effectively coordinate on safety?",
      positions: [
        {
          position: "Yes - alignment of interests enables cooperation",
          confidence: "medium",
          reasoning: "Labs share interest in avoiding catastrophe. Small number of frontier actors. Already seeing coordination (Frontier Model Forum, RSPs). Reputation incentives matter.",
          implications: "Industry self-regulation can work; focus on making it robust"
        },
        {
          position: "Partially - but competitive pressure will dominate",
          confidence: "medium",
          reasoning: "Labs may cooperate on some things but compete on capability. Voluntary commitments are weak. New entrants may not cooperate. Economic incentives ultimately dominate.",
          implications: "Need regulation to backstop industry efforts"
        },
        {
          position: "No - racing dynamics will prevent meaningful cooperation",
          confidence: "medium",
          reasoning: "First-mover advantage too strong. Pressure from investors, boards, nations. Safety theater rather than real safety. Will defect at critical moments.",
          implications: "Need hard constraints (regulation, compute governance)"
        }
      ]
    },
    {
      question: "Can US and China cooperate on AI safety?",
      positions: [
        {
          position: "Yes - mutual interest in survival trumps rivalry",
          confidence: "low",
          reasoning: "Both countries lose from AI catastrophe. Precedent of arms control during Cold War. Track-2 diplomacy shows interest. Narrow cooperation possible even amid competition.",
          implications: "Invest in diplomatic channels; seek narrow agreements"
        },
        {
          position: "Unlikely - strategic competition prevents trust",
          confidence: "medium",
          reasoning: "Verification nearly impossible for AI. Each side fears other will cheat. AI is key military technology. Domestic politics in both countries oppose cooperation.",
          implications: "Focus on defensive measures and domestic safety"
        },
        {
          position: "Uncertain - depends on how competition evolves",
          confidence: "low",
          reasoning: "Current trajectory is negative, but could change. Crisis might create coordination opportunity. Depends heavily on political leadership.",
          implications: "Maintain channels for cooperation; prepare for both scenarios"
        }
      ]
    },
    {
      question: "Will governance keep pace with AI development?",
      positions: [
        {
          position: "No - technology moves too fast",
          confidence: "medium",
          reasoning: "AI progress is exponential. Regulation takes years. Technical complexity exceeds regulator capacity. Lobbying slows regulation.",
          implications: "Need proactive governance, not reactive. Industry self-regulation critical."
        },
        {
          position: "Maybe - with institutional investment",
          confidence: "medium",
          reasoning: "Can build technical capacity in government. AI Safety Institutes are a start. Adaptive regulation possible. Some domains (finance) show it can work.",
          implications: "Invest heavily in government capacity. Create agile regulatory frameworks."
        }
      ]
    }
  ]}
/>

## The Unilateral Action Problem

**The question**: Even if *you* act safely, will others?

| Unilateral Action Matters | Unilateral Action Is Futile |
|---------------------------|----------------------------|
| Set norms and standards | Others won't follow |
| Buy time for coordination | Just delays the inevitable |
| Demonstrate feasibility | Cedes advantage to less careful |
| First-mover on safety creates pressure | Creates competitive disadvantage |
| Safety leader can later help others | Unsafe leader wins the race |

**The coordination dilemma**: If you unilaterally slow down for safety, you might:
- Demonstrate that safe development is possible (positive)
- Fall behind competitors who don't care (negative)
- Create pressure for others to follow (positive)
- Become irrelevant as less safe actors dominate (negative)

**Strategic insight**: The value of unilateral safety depends on whether others can be brought along.

## Mechanisms for Coordination

### Compute Governance

**The idea**: Control AI development by controlling the hardware.

**Current US export control regime**:

| Action | Date | Target | Effect |
|--------|------|--------|--------|
| Initial controls | October 2022 | Advanced AI chips to China | Blocked Nvidia A100/H100 exports |
| [Expanded controls](https://www.hklaw.com/en/insights/publications/2024/12/us-strengthens-export-controls-on-advanced-computing-items) | October 2024 | High-bandwidth memory (HBM) | Added strategic memory components |
| [AI Diffusion Framework](https://www.federalregister.gov/documents/2025/01/15/2025-00636/framework-for-artificial-intelligence-diffusion) | January 2025 | Model weights; compute clusters | Tiered country access; controls on advanced model weights |
| Framework rescinded | 2025 | N/A | Trump administration reversed Biden-era Diffusion Rule |
| [Conditional chip sales](https://www.cfr.org/article/chinas-ai-chip-deficit-why-huawei-cant-catch-nvidia-and-us-export-controls-should-remain) | August 2025 | Nvidia H20, AMD MI308 to China | 15% revenue share to US government |

**Production gap**: Nvidia projected 4-5 million AI chips in 2025 (double 2024); Huawei estimated 200,000-300,000 completed chips due to HBM shortage.

**Advantages**: Hardware is physical, visible, verifiable. Concentrated production (TSMC produces greater than 90% of advanced chips).

**Challenges**:
- Circumvention through efficiency improvements and alternative architectures
- China's [retaliatory export bans](https://www.rand.org/pubs/perspectives/PEA3776-1.html) on critical minerals (gallium, germanium, antimony)
- [Outbound Investment Security Program](https://www.sidley.com/en/insights/newsupdates/2025/01/new-us-export-controls-on-advanced-computing-items-and-artificial-intelligence-model-weights) (January 2025) restricts US investment in Chinese AI companies

### Responsible Scaling Policies (RSPs)

**The idea**: Labs commit to specific safety requirements at capability levels.

**How it works**:
1. Define capability thresholds (ASL levels at Anthropic)
2. Specify safety requirements for each level
3. Don't deploy until requirements met
4. External audit and verification

**Current status** (per [METR analysis](https://metr.org/faisc)):

| Company | Framework | Key Elements |
|---------|-----------|--------------|
| Anthropic | [RSP](https://www.anthropic.com/news/anthropics-responsible-scaling-policy) (Sept 2023) | ASL 1-4 levels; capability-triggered evaluations; biosecurity, cyber, autonomy testing |
| OpenAI | [Preparedness Framework](https://openai.com/index/preparedness/) (Dec 2023) | Tracked categories (bio, cyber, autonomy, persuasion); Medium/High/Critical thresholds |
| Google DeepMind | [Frontier Safety Framework](https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/) (May 2024) | Critical capability levels; evaluations before training and deployment |
| xAI | Safety framework | Published February 2025 per Seoul Commitment |

**Common elements** (per [METR Common Elements analysis](https://metr.org/common-elements)):
- Capability thresholds triggering enhanced safety measures
- Model weight security requirements
- Deployment mitigations for dangerous capabilities
- External reporting and (in some cases) government notification

**Limitations**: Voluntary; relies on honest assessment; [16 companies committed](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024) at Seoul but frameworks vary in rigor; may not cover all risks.

### International Agreements

**The idea**: Nations agree on AI development rules.

**Possible forms**:
- Test ban (no systems above X capability)
- Development limits (no autonomous weapons)
- Mutual inspection
- Shared safety standards

**Challenges**: Verification, enforcement, US-China relations.

### Domestic Regulation

**The idea**: Governments regulate AI within their borders.

**Examples**:
- EU AI Act (risk-based regulation)
- US executive order (safety requirements)
- UK approach (principles-based)

**Limitations**: Varies by jurisdiction, enforcement is hard, risk of regulatory capture.

## What Would Change Your Mind?

### Evidence for coordination failure:
- Major lab breaks from safety commitments
- US-China relations deteriorate further
- Open source capabilities reach dangerous levels
- Regulation fails to pass or is captured
- Racing dynamic intensifies

### Evidence for successful coordination:
- Binding international agreement on AI safety
- Effective compute governance implemented
- Labs successfully enforce RSPs
- Meaningful US-China cooperation on AI
- Governance keeps pace with capability growth

## Implications for Action

### If Coordination Will Fail

**Strategy**:
- Focus on technical solutions that don't require coordination
- Build defensive capabilities
- Prepare for worst case
- Reduce dependency on AI cooperation

**Priority**: Technical alignment, containment, resilience

### If Coordination Is Possible

**Strategy**:
- Invest heavily in diplomatic and governance efforts
- Build institutions for cooperation
- Create verification mechanisms
- Foster trust between actors

**Priority**: Policy, governance, institution-building

### If Uncertain

**Strategy**:
- Pursue both technical and governance paths
- Build coordination capacity while developing technical solutions
- Seek cheap coordination wins
- Maintain optionality

**Priority**: Both technical and governance investment

## The Interaction with Other Cruxes

Coordination interacts with other cruxes:

- **[Timelines](/understanding-ai-risk/core-argument/timelines/)**: Short timelines make coordination harder.
- **[Takeoff Speed](/understanding-ai-risk/core-argument/takeoff/)**: Fast takeoff reduces time for coordination.
- **[Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/)**: If alignment is easy, less coordination needed.
- **[Warning Signs](/understanding-ai-risk/core-argument/warning-signs/)**: Warning shots might enable coordination.

---

## Sources

### International Governance

- [International Network of AI Safety Institutes](https://www.nist.gov/news-events/news/2024/11/fact-sheet-us-department-commerce-us-department-state-launch-international) - NIST fact sheet on the November 2024 launch
- [AI Safety Institute International Network: Next Steps](https://www.csis.org/analysis/ai-safety-institute-international-network-next-steps-and-recommendations) - CSIS analysis of network challenges
- [International AI Safety Report 2025](https://internationalaisafetyreport.org/publication/international-ai-safety-report-2025) - Collaborative scientific assessment
- [Seoul Declaration](https://www.industry.gov.au/publications/seoul-declaration-countries-attending-ai-seoul-summit-21-22-may-2024) - May 2024 summit outcomes
- [Frontier AI Safety Commitments](https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024/frontier-ai-safety-commitments-ai-seoul-summit-2024) - Industry commitments from Seoul Summit

### US-China Relations

- [Potential for U.S.-China Cooperation on Reducing AI Risks](https://www.rand.org/pubs/perspectives/PEA4189-1.html) - RAND analysis of cooperation opportunities
- [Roadmap for US-China AI Dialogue](https://www.brookings.edu/articles/a-roadmap-for-a-us-china-ai-dialogue/) - Brookings policy recommendations
- [Challenges and Opportunities for US-China Collaboration](https://www.sandia.gov/app/uploads/sites/148/2025/04/Challenges-and-Opportunities-for-US-China-Collaboration-on-Artificial-Intelligence-Governance.pdf) - Sandia National Labs analysis
- [Reading Between the Lines of Dueling AI Action Plans](https://www.atlanticcouncil.org/blogs/new-atlanticist/reading-between-the-lines-of-the-dueling-us-and-chinese-ai-action-plans/) - Atlantic Council comparison

### Regulatory Frameworks

- [EU AI Act](https://artificialintelligenceact.eu/) - Comprehensive EU AI Act resource
- [EU AI Act Implementation Timeline](https://www.europarl.europa.eu/thinktank/en/document/EPRS_ATA(2025)772906) - European Parliament timeline
- [AI Diffusion Framework](https://www.federalregister.gov/documents/2025/01/15/2025-00636/framework-for-artificial-intelligence-diffusion) - US Federal Register rule
- [US Export Controls on Advanced Computing](https://www.hklaw.com/en/insights/publications/2024/12/us-strengthens-export-controls-on-advanced-computing-items) - Holland & Knight analysis

### Industry Self-Governance

- [Frontier Model Forum Progress Update](https://www.frontiermodelforum.org/updates/progress-update-advancing-frontier-ai-safety-in-2024-and-beyond/) - 2024 achievements
- [Common Elements of Frontier AI Safety Policies](https://metr.org/common-elements) - METR analysis of safety frameworks
- [Frontier AI Safety Policies Comparison](https://metr.org/faisc) - METR framework comparison

### Historical Coordination

- [Montreal Protocol Success](https://news.un.org/en/story/2023/01/1132277) - UN assessment of ozone layer recovery
- [About Montreal Protocol](https://www.unep.org/ozonaction/who-we-are/about-montreal-protocol) - UNEP overview
- [AI Race Dynamics](https://www.aisafetybook.com/textbook/ai-race) - AI Safety textbook chapter on racing dynamics

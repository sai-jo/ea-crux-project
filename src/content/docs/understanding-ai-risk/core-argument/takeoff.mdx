---
title: "Will Takeoff Be Fast?"
description: "Analyzes whether AI will transition from human-level to superhuman in days/months versus years/decades. Tom Davidson's compute-centric model estimates ~3 years from 20% to 100% cognitive automation, with less than 1 year from AGI to superintelligence. Epoch AI data shows capability improvement nearly doubled in 2024 (8 to 15 points/year)."
sidebar:
  order: 3
importance: 84.5
quality: 82
llmSummary: "Analyzes the critical debate over whether AI will transition from human-level to superhuman capabilities in days/months versus years/decades, presenting arguments for fast takeoff (recursive self-improvement, speed advantages, no physical constraints) versus slow takeoff (hardware bottlenecks, deployment friction, diminishing returns). Tom Davidson's compute-centric model estimates ~3 years from 20% to 100% cognitive automation. Epoch AI shows capability improvement nearly doubled in 2024."
lastEdited: "2025-12-28"
---
import {InfoBox, EstimateBox, DisagreementMap, KeyQuestions, Mermaid, R} from '../../../../components/wiki';

<InfoBox
  type="crux"
  title="Takeoff Speed"
  customFields={[
    { label: "Claim", value: "The transition from human-level to superhuman AI could be very fast" },
    { label: "Key Uncertainty", value: "Weeks-to-months vs years-to-decades" },
    { label: "Related To", value: "Timelines, Capabilities, Warning Signs" },
  ]}
/>

**The core question**: Once AI reaches human-level capabilities, how quickly will it become superhuman? Days? Months? Decades?

This is one of the most consequential disagreements in AI safety. Fast takeoff scenarios give humanity little time to react; slow takeoff provides opportunities for course correction.

## Quantitative Framework

Recent modeling work has begun to quantify takeoff dynamics more precisely. <R id="31e373770f16b09b">Tom Davidson's compute-centric framework</R> at Open Philanthropy provides the most rigorous analysis to date.

### Key Quantitative Estimates

| Parameter | Estimate | Range | Source |
|-----------|----------|-------|--------|
| Time from 20% to 100% cognitive automation | ~3 years | 1-10 years | <R id="31e373770f16b09b">Davidson (2024)</R> |
| Time from AGI to superintelligence | &lt;1 year | Months to years | Davidson model |
| Effective compute gap to cross | ~10,000x | 1,000-100,000x | Davidson model |
| AI researcher productivity multiplier | 6.25x | 4-10x | <R id="f418db885210d581">AI 2027 survey</R> of OpenAI/DeepMind staff |
| Pre-2024 capability improvement rate | 8 pts/year | — | <R id="663417bdb09208a4">Epoch AI</R> |
| Post-2024 capability improvement rate | 15 pts/year | — | <R id="663417bdb09208a4">Epoch AI</R> |

Davidson's model uses "effective compute" as the key variable—a product of raw computation, algorithmic efficiency, and financial investment. These factors multiply, meaning progress in all areas compounds to create rapid increases in effective compute. He refers to this as a "growth flywheel" where more capable AI automates more R&D work, which then accelerates improvements in software, hardware, and model scale.

### The Feedback Loop Structure

<Mermaid client:load chart={`
flowchart TD
    A[AI Capabilities] --> B[Automates AI R&D]
    B --> C[Faster Algorithm Improvements]
    B --> D[More Efficient Hardware Use]
    B --> E[Better Training Methods]
    C --> F[Increased Effective Compute]
    D --> F
    E --> F
    F --> A

    G[Hardware Bottleneck] -.->|Constrains| F
    H[Data Bottleneck] -.->|Constrains| C
    I[Physical World Tests] -.->|Constrains| E

    style A fill:#e1f5fe
    style F fill:#c8e6c9
    style G fill:#ffcdd2
    style H fill:#ffcdd2
    style I fill:#ffcdd2
`} />

The central question is whether this feedback loop accelerates (fast takeoff) or hits diminishing returns (slow takeoff). Davidson's best guess is that the loop operates fast enough to produce superhuman AI within a year of achieving AGI, primarily because of the massive amounts of AI labor available for R&D once we have AGI.

## Why Takeoff Speed Matters

<EstimateBox
  client:load
  variable="Takeoff Speed Scenarios"
  description="Different takeoff speeds and their implications"
  unit=""
  estimates={[
    { source: "Fast (days-months)", value: "Critical", notes: "One shot; must solve alignment before deployment" },
    { source: "Medium (1-5 years)", value: "Concerning", notes: "Limited time to react, but some opportunity" },
    { source: "Slow (5-20 years)", value: "Manageable", notes: "Can iterate, learn from mistakes, adapt governance" },
    { source: "Gradual (decades)", value: "Comfortable", notes: "Technology diffuses slowly; society adapts" },
  ]}
/>

### The Strategic Implications

| Takeoff Speed | Safety Strategy | Governance Approach | Who Must Act |
|---------------|-----------------|---------------------|--------------|
| **Fast** | Must be solved pre-deployment | Prevention over reaction | AI developers only |
| **Medium** | Early warning systems critical | Rapid response capability | Labs + governments |
| **Slow** | Iterative improvement | Standard regulatory process | Broad stakeholders |
| **Gradual** | Normal technology policy | Democratic deliberation | Society at large |

## Arguments for Fast Takeoff

### 1. Recursive Self-Improvement

**The core mechanism**: AI capable of improving AI creates a feedback loop.

**How it works**:
1. AI improves AI research process
2. Better AI research → faster capability gains
3. More capable AI → even better at improving AI
4. Loop accelerates

**Historical precedent**: Nothing. This would be unprecedented.

**Key question**: Is there a feedback loop, and does it actually accelerate?

### 2. Speed and Scale Advantages

**The argument**: Even human-level AI has operational advantages.

| Advantage | Multiplier | Impact |
|-----------|------------|--------|
| **Thinking speed** | 1,000,000x+ | Subjective years per day |
| **Parallelization** | Unlimited | Run millions of copies |
| **Communication** | Perfect | Instant, lossless knowledge sharing |
| **No downtime** | 24/7/365 | No sleep, breaks, vacations |
| **Copyability** | Instant | One insight spreads immediately |

**Implication**: Human-level AI might achieve decades of researcher-years of progress in months.

### 3. No Physical Constraints on Software

**The argument**: Unlike most technologies, AI improvements don't require physical infrastructure.

**Contrast with slow-diffusing technologies**:
- Electricity: Needed power plants, wires, appliances
- Cars: Needed roads, gas stations, manufacturing
- Internet: Needed cables, servers, protocols

**AI improvements**:
- Better algorithms: Deployed instantly
- Better weights: Just data
- Better prompts: Trivial to copy

**Counter**: Compute is physical, and expanding it takes time.

### 4. Concentration of Capability

**The argument**: One leading system could quickly become dominant.

**Mechanisms**:
- Winner-take-all dynamics in AI capabilities
- Self-improvement compounds advantages
- First to superhuman might prevent others from catching up
- Single breakthrough could create decisive advantage

**The "singleton" concern**: One AI system (or one AI lab) gains sufficient power to prevent competition.

### 5. Hidden Progress

**The argument**: We might not see it coming.

**Reasons capability gains might be hidden**:
- Labs have incentive to keep breakthroughs secret
- Emergent capabilities appear suddenly at scale
- AI might conceal its own capabilities
- Training progress is discontinuous

**Implication**: By the time we notice superhuman AI, it might already be much more capable.

<DisagreementMap
  client:load
  topic="How Fast Would Takeoff Be?"
  description="Expert views on the transition speed from human-level to superhuman AI"
  spectrum={{ low: "Decades", high: "Days-Weeks" }}
  positions={[
    { actor: "Eliezer Yudkowsky", position: "Very fast", estimate: "Weeks-months", confidence: "high" },
    { actor: "Sam Altman", position: "Fast", estimate: "Rapid after AGI", confidence: "high" },
    { actor: "Tom Davidson", position: "Moderate-fast", estimate: "Under 1 year post-AGI", confidence: "medium" },
    { actor: "Paul Christiano", position: "Slow but fast", estimate: "GWP doubles in 4 years, then 1 year", confidence: "medium" },
    { actor: "Robin Hanson", position: "Gradual then sudden", estimate: "Slow until em transition, then monthly doubling", confidence: "medium" },
  ]}
/>

### Expert Position Details

**Eliezer Yudkowsky** <R id="9c8ec6cef670271d">argues</R> that recursive self-improvement "should either flatline or blow up. You would need exactly the right law of diminishing returns to fly through the extremely narrow soft takeoff keyhole."

**Sam Altman** wrote in 2025 that <R id="2bc0d4251ea0868f">"we are past the event horizon; the takeoff has started"</R> and that once AGI is achieved, "the path to ASI will likely be rapid."

**Tom Davidson's** <R id="31e373770f16b09b">compute-centric model</R> predicts less than one year from AGI to superintelligence, "primarily due to the massive amounts of AI labor available to do AI R&D once we have AGI."

**Paul Christiano** <R id="d70ecd90990cdd58">defines slow takeoff</R> as "GWP doubles in 4 years before the start of the first 1-year GWP doubling"—which he notes is "like the Industrial Revolution, but 10x-100x faster." He gives ~33% probability to fast takeoff.

**Robin Hanson** <R id="88451bfc3f9b0a9d">predicts</R> "slow, slow, slow and then all of a sudden"—with gradual automation until a transition point, after which the economy may double every month or faster.

## Arguments for Slow Takeoff

### 1. Real-World Bottlenecks

**The argument**: AI capability doesn't automatically translate to real-world impact.

**Bottlenecks**:
- **Deployment**: Getting AI into production systems takes time
- **Integration**: AI must work with existing infrastructure
- **Trust**: Organizations adopt new technology cautiously
- **Testing**: Novel capabilities require extensive validation
- **Regulation**: Legal frameworks slow deployment

**Historical parallel**: Many transformative technologies took decades to reshape society (electricity, computers, internet).

### 2. Hardware Constraints

**The argument**: Compute is physical and limited.

**Constraints**:
- Chips take years to design and fabricate
- Fabs cost billions and take years to build
- Energy infrastructure has limits
- Supply chains are complex and slow

**Key insight**: Even if algorithms improve rapidly, deploying them at scale requires hardware that can't be conjured.

**Counter**: A sufficiently capable AI might find ways around hardware constraints (better algorithms, more efficient use of existing hardware).

### 3. Multiple Competing Systems

**The argument**: No single AI will dominate; competition and balance will persist.

**Supporting factors**:
- Multiple labs at similar capability levels
- Different approaches and architectures
- Nations won't allow monopoly
- Open source creates alternatives

**Implication**: Instead of one superintelligence, we get many roughly-matched systems that check each other.

### 4. Economic Friction

**The argument**: Technology adoption is always slower than technologists expect.

**Evidence**:
- Electricity: 50+ years from invention to widespread use
- Computers: Decades to transform business
- Internet: 20+ years to reshape commerce
- Mobile: Still transforming after 15 years

**Key insight**: Even obviously superior technology takes time to change society.

### 5. Diminishing Returns on Intelligence

**The argument**: Beyond a certain point, more intelligence provides less advantage.

**Reasons**:
- Some problems are fundamentally hard (NP-complete, etc.)
- Many bottlenecks are physical, not cognitive
- Data and experimentation can't be rushed
- Complexity has inherent limits

**Counter**: Even diminishing returns could compound over time.

## The Continuity Question

A key sub-debate: Will progress be continuous or discontinuous?

<KeyQuestions
  client:load
  questions={[
    {
      question: "Will there be sudden jumps in AI capability?",
      positions: [
        {
          position: "Yes - discontinuities are likely",
          confidence: "medium",
          reasoning: "Emergent capabilities, algorithmic breakthroughs, and phase transitions in training could cause sudden jumps. We've seen this with scaling laws.",
          implications: "Need to be prepared for surprise; current safety measures may suddenly become inadequate"
        },
        {
          position: "No - progress will be gradual",
          confidence: "medium",
          reasoning: "Historical pattern is continuous progress. Apparent jumps are just visibility thresholds. No fundamental reason to expect discontinuity.",
          implications: "Will have warning signs; can adapt as we go"
        }
      ]
    },
    {
      question: "Can we predict when AGI will emerge?",
      positions: [
        {
          position: "No - capability gains are unpredictable",
          confidence: "medium",
          reasoning: "Emergent capabilities are hard to forecast. We don't understand training dynamics well enough.",
          implications: "Must be prepared earlier rather than later"
        },
        {
          position: "Roughly - scaling laws provide guidance",
          confidence: "medium",
          reasoning: "Scaling laws have been remarkably predictive. While exact timing is uncertain, trends are visible.",
          implications: "Can plan with some lead time"
        }
      ]
    }
  ]}
/>

## Evidence from Current AI Progress

### 2024 Acceleration Data

<R id="663417bdb09208a4">Epoch AI's research</R> provides empirical evidence of accelerating capabilities:

| Metric | Pre-April 2024 | Post-April 2024 | Change |
|--------|----------------|-----------------|--------|
| Epoch Capabilities Index improvement rate | 8 pts/year | 15 pts/year | +90% |
| METR Time Horizon benchmark improvement | Baseline | +40% acceleration | Significant |
| Reasoning model response lengths vs non-reasoning | — | 8x more tokens | New paradigm |
| FrontierMath benchmark (frontier models) | — | &lt;2% solved | Gap remains |

The <R id="663417bdb09208a4">acceleration roughly coincides</R> with the rise of reasoning models (like o1, o3) and increased focus on reinforcement learning at frontier labs. This suggests that methodological breakthroughs can produce step-changes in capability improvement rates.

### The Emergence Debate

A <R id="9926d26da9a3d761">key area of contention</R> among researchers is whether capabilities emerge discontinuously or gradually:

| View | Proponent | Evidence | Implication |
|------|-----------|----------|-------------|
| Emergence is real | Alex Tamkin (Anthropic) | "Even with continuous metrics, you still have discontinuities" | Unpredictable capability jumps possible |
| Emergence is a mirage | Sanmi Koyejo (Stanford) | "The transition is much more predictable than people give it credit for" | Smooth scaling, fewer surprises |
| Phase transitions | Theoretical work | Symmetry breaking in training dynamics | Sudden jumps at specific scales |

The <R id="e49b6ceff6dfc795">Future of Life Institute notes</R> that even if emergence is partially illusory, "serious gaps remain in our ability to predict the future of LLMs. New training techniques, datasets, and prompting techniques keep researchers on their toes."

### Signs Pointing Toward Fast Takeoff

**Rapid capability gains**: GPT-3 (2020) to GPT-4 (2023) showed dramatic capability improvements. <R id="2bc0d4251ea0868f">Sam Altman stated in 2025</R> that "we are past the event horizon; the takeoff has started."

**Emergent capabilities**: Abilities appearing at scale without explicit training (in-context learning, chain-of-thought reasoning).

**Acceleration of progress**: <R id="663417bdb09208a4">Epoch AI data</R> shows capability improvement nearly doubled in 2024.

**AI for AI research**: Models increasingly useful for ML research itself. A <R id="f418db885210d581">survey of OpenAI/DeepMind researchers</R> found median estimate of 6.25x productivity boost from best-performing researchers.

### Signs Pointing Toward Slow Takeoff

**Deployment lag**: Frontier models take years to fully integrate into applications.

**Reliability issues**: Models still have fundamental limitations (hallucination, reasoning). <R id="46010026d8feac35">FrontierMath benchmark</R> shows frontier models solve less than 2% of advanced math problems.

**Hardware constraints**: Compute costs remain significant; can't scale infinitely.

**Regulatory friction**: Governments increasingly interested in oversight.

**Diminishing returns**: <R id="e49b6ceff6dfc795">Recent 2025 papers</R> show that self-improvement yields smaller returns each round and often stalls on harder tasks.

## Implications for AI Safety

### If Takeoff Is Fast

**Timeline**: Little time between human-level and superintelligence.

**Requirements**:
- Alignment must be solved *before* building powerful AI
- Can't rely on learning from deployment mistakes
- Warning signs may not be visible
- First "slightly misaligned" system might immediately become dangerous

**Strategy implications**:
- Focus on theoretical alignment
- Don't build systems we can't verify
- Extreme caution with capability advances
- Consider pausing capabilities research

### If Takeoff Is Slow

**Timeline**: Years to decades between human-level and superintelligence.

**Opportunities**:
- Iterate on safety measures
- Learn from near-misses
- Develop governance frameworks
- Build evaluation tools

**Strategy implications**:
- Empirical safety research is more tractable
- Can learn from deployment
- Governance has time to adapt
- Less urgent to solve theoretical problems now

### Hedging Between Views

Given uncertainty, reasonable strategies:

1. **Pursue both theoretical and empirical work**: Don't assume we'll have time to iterate, but take advantage if we do.

2. **Build monitoring and evaluation**: If takeoff is fast, we need warning; if slow, we need to track progress.

3. **Develop institutional capacity**: Whether fast or slow, better institutions help.

4. **Maintain optionality**: Avoid irreversible commitments that assume one scenario.

## What Would Change Your Mind?

### Evidence for faster takeoff:
- Dramatic capability jumps between model generations
- Recursive self-improvement demonstrated in current systems
- AI systems rapidly improving at AI research
- Hidden capabilities discovered in existing models

### Evidence for slower takeoff:
- Continued slow deployment of frontier capabilities
- Persistent reliability and robustness problems
- Hardware constraints becoming binding
- Multiple roughly-matched AI systems coexisting

## The Intersection with Other Cruxes

Takeoff speed interacts critically with:

- **[Alignment Difficulty](/understanding-ai-risk/core-argument/alignment-difficulty/)**: If alignment is hard and takeoff is fast, we're in trouble.
- **[Warning Signs](/understanding-ai-risk/core-argument/warning-signs/)**: Fast takeoff reduces warning time.
- **[Coordination](/understanding-ai-risk/core-argument/coordination/)**: Slow takeoff gives more time for governance.
- **[Capabilities](/understanding-ai-risk/core-argument/capabilities/)**: Takeoff only matters if AI becomes very capable.

## Key Sources

### Primary Research

- **<R id="31e373770f16b09b">What a Compute-Centric Framework Says About Takeoff Speeds</R>** — Tom Davidson's Open Philanthropy report (2024), the most rigorous quantitative model of takeoff dynamics
- **<R id="663417bdb09208a4">AI Capabilities Progress Has Sped Up</R>** — Epoch AI analysis showing 90% acceleration in capability improvement rates in 2024
- **<R id="d70ecd90990cdd58">Takeoff Speeds</R>** — Paul Christiano's foundational 2018 post defining the slow takeoff position
- **<R id="9c8ec6cef670271d">Yudkowsky and Christiano Discuss Takeoff Speeds</R>** — MIRI transcript of the key 2021 debate

### Expert Perspectives

- **<R id="2bc0d4251ea0868f">The Gentle Singularity</R>** — Sam Altman's 2025 blog post arguing "we are past the event horizon"
- **<R id="88451bfc3f9b0a9d">Economic Growth Given Machine Intelligence</R>** — Robin Hanson's economic modeling of AI-driven growth
- **<R id="e49b6ceff6dfc795">Are We Close to an Intelligence Explosion?</R>** — Future of Life Institute's 2025 analysis of recursive self-improvement evidence

### Empirical Evidence

- **<R id="9926d26da9a3d761">Emergent Abilities in Large Language Models: An Explainer</R>** — CSET Georgetown overview of the emergence debate
- **<R id="f418db885210d581">AI 2027 Takeoff Forecast</R>** — Survey of OpenAI/DeepMind researchers on AI R&D acceleration
- **<R id="46010026d8feac35">FrontierMath Benchmark</R>** — Epoch AI's benchmark showing current capability limits

# Ai Transition Model Entities
# Auto-generated from entities.yaml - edit this file directly

- id: international-coordination
  type: ai-transition-model-parameter
  title: International Coordination
  description: >-
    Degree of global cooperation on AI governance and safety, measured through treaty participation, shared standards
    adoption, and institutional network strength.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (11-country AISI network, but US/UK refused Paris 2025 declaration)
    - label: Key Measurement
      value: Treaty signatories, AISI network participation, shared evaluation standards
  relatedEntries:
    - id: international-summits
      type: intervention
      relationship: related
    - id: geopolitics
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
    - id: multipolar-trap-dynamics
      type: model
      relationship: analyzed-by
    - id: international-coordination-game
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - international
    - coordination
  lastUpdated: 2025-12
- id: societal-trust
  type: ai-transition-model-parameter
  title: Societal Trust
  description: >-
    Level of public confidence in institutions, experts, and verification systems. A foundational parameter affecting
    democratic function and collective action.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (77% → 22% government trust since 1964)
    - label: Measurement
      value: Survey data (Pew, Gallup)
  parameterDistinctions:
    focus: Do we trust institutions?
    summary: Confidence in institutions, experts, and verification systems
    distinctFrom:
      - id: epistemic-health
        theirFocus: Can we tell what's true?
        relationship: Epistemic health reveals whether institutions deserve trust
      - id: reality-coherence
        theirFocus: Do we agree on facts?
        relationship: Trust enables acceptance of shared facts; fragmentation erodes trust
  relatedEntries:
    - id: trust-decline
      type: risk
      relationship: decreases
    - id: disinformation
      type: risk
      relationship: decreases
    - id: deepfakes
      type: risk
      relationship: decreases
    - id: content-authentication
      type: intervention
      relationship: supports
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: related
    - id: information-authenticity
      type: ai-transition-model-parameter
      relationship: related
    - id: public-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: trust-cascade-model
      type: model
      relationship: analyzed-by
    - id: deepfakes-authentication-crisis
      type: model
      relationship: analyzed-by
    - id: sycophancy-feedback-loop
      type: model
      relationship: analyzed-by
    - id: epistemic-collapse-threshold
      type: model
      relationship: analyzed-by
    - id: trust-erosion-dynamics
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - governance
    - structural
  lastUpdated: 2025-12
- id: epistemic-health
  type: ai-transition-model-parameter
  title: Epistemic Health
  description: >-
    Society's collective ability to distinguish truth from falsehood and form shared beliefs about reality. Essential
    for democratic deliberation and coordinated action.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (50%+ web content AI-generated)
    - label: Measurement
      value: Verification success rates, consensus formation
  parameterDistinctions:
    focus: Can we tell what's true?
    summary: Ability to distinguish truth from falsehood
    distinctFrom:
      - id: societal-trust
        theirFocus: Do we trust institutions?
        relationship: Trust enables verification; epistemic health reveals trustworthiness
      - id: reality-coherence
        theirFocus: Do we agree on facts?
        relationship: Epistemic health is capacity; coherence is the outcome when that capacity is shared
  relatedEntries:
    - id: epistemic-collapse
      type: risk
      relationship: decreases
    - id: disinformation
      type: risk
      relationship: decreases
    - id: consensus-manufacturing
      type: risk
      relationship: decreases
    - id: epistemic-security
      type: intervention
      relationship: supports
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: related
    - id: information-authenticity
      type: ai-transition-model-parameter
      relationship: related
    - id: expert-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: epistemic-collapse-threshold
      type: model
      relationship: analyzed-by
    - id: trust-cascade-model
      type: model
      relationship: analyzed-by
    - id: reality-fragmentation-network
      type: model
      relationship: analyzed-by
    - id: authentication-collapse-timeline
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - information-environment
  lastUpdated: 2025-12
- id: information-authenticity
  type: ai-transition-model-parameter
  title: Information Authenticity
  description: >-
    The degree to which content circulating in society can be verified as genuine—tracing to real sources, events, or
    creators. Currently stressed by AI-generated content and deepfake detection challenges.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (human deepfake detection at 55% accuracy)
    - label: Measurement
      value: Verification capability, provenance adoption, detection accuracy
  relatedEntries:
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: related
    - id: deepfakes-authentication-crisis
      type: model
      relationship: analyzed-by
    - id: authentication-collapse-timeline
      type: model
      relationship: analyzed-by
    - id: trust-cascade-model
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - information-environment
    - verification
  lastUpdated: 2025-12
- id: ai-control-concentration
  type: ai-transition-model-parameter
  title: AI Control Concentration
  description: >-
    How concentrated or distributed power over AI development and deployment is across actors. Neither extreme
    concentration nor complete diffusion is optimal.
  customFields:
    - label: Direction
      value: Context-dependent (neither extreme ideal)
    - label: Current Trend
      value: Concentrating (<20 orgs can train frontier models)
    - label: Measurement
      value: Market share, compute access, talent distribution
  relatedEntries:
    - id: concentration-of-power
      type: risk
      relationship: related
    - id: compute-hardware
      type: ai-transition-model-metric
      relationship: measured-by
    - id: winner-take-all-model
      type: model
      relationship: analyzed-by
    - id: winner-take-all-concentration
      type: model
      relationship: analyzed-by
    - id: concentration-of-power-model
      type: model
      relationship: analyzed-by
    - id: international-coordination-game
      type: model
      relationship: analyzed-by
  tags:
    - structural
    - governance
    - market-dynamics
  lastUpdated: 2025-12
- id: human-agency
  type: ai-transition-model-parameter
  title: Human Agency
  description: >-
    Degree of meaningful human control over decisions affecting their lives. Includes autonomy, oversight capacity, and
    ability to opt out of AI-mediated systems.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (increasing automation of decisions)
    - label: Measurement
      value: Decision autonomy, opt-out availability, oversight capacity
  relatedEntries:
    - id: erosion-of-agency
      type: risk
      relationship: related
    - id: economic-labor
      type: ai-transition-model-metric
      relationship: measured-by
    - id: economic-disruption-impact
      type: model
      relationship: analyzed-by
    - id: expertise-atrophy-cascade
      type: model
      relationship: analyzed-by
    - id: preference-manipulation-drift
      type: model
      relationship: analyzed-by
    - id: concentration-of-power-model
      type: model
      relationship: analyzed-by
  tags:
    - structural
    - autonomy
    - human-ai-interaction
  lastUpdated: 2025-12
- id: economic-stability
  type: ai-transition-model-parameter
  title: Economic Stability
  description: >-
    Resilience of economic systems to AI-driven changes—including labor market adaptability, income distribution, and
    transition smoothness. Currently declining as 40-60% of jobs face AI exposure.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (productivity gains vs displacement risks)
    - label: Measurement
      value: Employment rates, inequality indices, transition costs
  relatedEntries:
    - id: economic-disruption
      type: risk
      relationship: related
    - id: economic-labor
      type: ai-transition-model-metric
      relationship: measured-by
    - id: economic-disruption-impact
      type: model
      relationship: analyzed-by
    - id: winner-take-all-model
      type: model
      relationship: analyzed-by
    - id: winner-take-all-concentration
      type: model
      relationship: analyzed-by
  tags:
    - economic
    - labor-market
    - structural
  lastUpdated: 2025-12
- id: human-expertise
  type: ai-transition-model-parameter
  title: Human Expertise
  description: >-
    Maintenance of human skills, knowledge, and cognitive capabilities in an AI-augmented world. Tracks skill retention,
    domain mastery, and ability to function without AI assistance.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (36% news avoidance, rising deskilling concerns)
    - label: Measurement
      value: Skill retention, cognitive engagement, domain knowledge depth
  relatedEntries:
    - id: learned-helplessness
      type: risk
      relationship: related
    - id: economic-labor
      type: ai-transition-model-metric
      relationship: measured-by
    - id: expertise-atrophy-progression
      type: model
      relationship: analyzed-by
    - id: expertise-atrophy-cascade
      type: model
      relationship: analyzed-by
    - id: automation-bias-cascade
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - human-factors
    - cognitive
  lastUpdated: 2025-12
- id: human-oversight-quality
  type: ai-transition-model-parameter
  title: Human Oversight Quality
  description: >-
    Effectiveness of human review, decision authority, and correction capability over AI systems. Essential for
    maintaining accountability and preventing harmful AI behaviors.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (capability gap widening, automation bias increasing)
    - label: Measurement
      value: Review effectiveness, decision authority, error detection rates
  relatedEntries:
    - id: scalable-oversight
      type: safety-agenda
      relationship: related
    - id: lab-behavior
      type: ai-transition-model-metric
      relationship: measured-by
    - id: expertise-atrophy-progression
      type: model
      relationship: analyzed-by
    - id: deceptive-alignment-decomposition
      type: model
      relationship: analyzed-by
    - id: corrigibility-failure-pathways
      type: model
      relationship: analyzed-by
    - id: automation-bias-cascade
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - human-factors
    - safety
  lastUpdated: 2025-12
- id: alignment-robustness
  type: ai-transition-model-parameter
  title: Alignment Robustness
  description: >-
    How reliably AI systems pursue intended goals across contexts, distribution shifts, and adversarial conditions.
    Measures the stability of alignment under real-world deployment.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining relative to capability (1-2% reward hacking in frontier models)
    - label: Key Measurement
      value: Behavioral reliability under distribution shift, reward hacking rates
  relatedEntries:
    - id: reward-hacking
      type: risk
      relationship: decreases
    - id: mesa-optimization
      type: risk
      relationship: decreases
    - id: goal-misgeneralization
      type: risk
      relationship: decreases
    - id: deceptive-alignment
      type: risk
      relationship: decreases
    - id: sycophancy
      type: risk
      relationship: decreases
    - id: interpretability
      type: intervention
      relationship: supports
    - id: evals
      type: intervention
      relationship: supports
    - id: ai-control
      type: intervention
      relationship: supports
    - id: interpretability-coverage
      type: ai-transition-model-parameter
      relationship: related
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: related
    - id: human-oversight-quality
      type: ai-transition-model-parameter
      relationship: related
    - id: alignment-progress
      type: ai-transition-model-metric
      relationship: measured-by
    - id: deceptive-alignment-decomposition
      type: model
      relationship: analyzed-by
    - id: corrigibility-failure-pathways
      type: model
      relationship: analyzed-by
    - id: safety-capability-tradeoff
      type: model
      relationship: analyzed-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
  tags:
    - safety
    - technical
    - alignment
  lastUpdated: 2025-12
- id: safety-capability-gap
  type: ai-transition-model-parameter
  title: Safety-Capability Gap
  description: >-
    The lag between AI capability advances and corresponding safety/alignment understanding. Measures how far safety
    research trails behind what frontier systems can do.
  customFields:
    - label: Direction
      value: Lower is better (want safety close to capabilities)
    - label: Current Trend
      value: Widening (safety timelines compressed 70-80% post-ChatGPT)
    - label: Key Measurement
      value: Months/years capabilities lead safety research
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: decreases
    - id: interpretability
      type: intervention
      relationship: supports
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: related
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: related
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: related
    - id: alignment-progress
      type: ai-transition-model-metric
      relationship: measured-by
    - id: safety-research
      type: ai-transition-model-metric
      relationship: measured-by
    - id: capabilities
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-impact
      type: model
      relationship: analyzed-by
    - id: safety-capability-tradeoff
      type: model
      relationship: analyzed-by
  tags:
    - safety
    - technical
    - governance
  lastUpdated: 2025-12
- id: interpretability-coverage
  type: ai-transition-model-parameter
  title: Interpretability Coverage
  description: >-
    The percentage of model behavior that can be explained and understood by researchers. Measures transparency into AI
    system internals.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: >-
        Improving slowly (70% of Claude 3 Sonnet features interpretable, but only ~10% of frontier model capacity
        mapped)
    - label: Key Measurement
      value: Percentage of model behavior explainable, feature coverage
  relatedEntries:
    - id: interpretability
      type: concept
      relationship: related
    - id: alignment-progress
      type: ai-transition-model-metric
      relationship: measured-by
  tags:
    - safety
    - technical
    - interpretability
  lastUpdated: 2025-12
- id: regulatory-capacity
  type: ai-transition-model-parameter
  title: Regulatory Capacity
  description: >-
    Ability of governments to effectively understand, evaluate, and regulate AI systems, including technical expertise,
    enforcement capability, and institutional resources.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Growing but constrained (AISI budgets ~\$10-50M vs. \$100B+ industry spending)
    - label: Key Measurement
      value: Agency technical expertise, enforcement actions, evaluation capability
  relatedEntries:
    - id: nist-ai-rmf
      type: policy
      relationship: related
    - id: us-executive-order
      type: policy
      relationship: related
    - id: institutional-adaptation-speed
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - regulation
    - institutions
  lastUpdated: 2025-12
- id: institutional-quality
  type: ai-transition-model-parameter
  title: Institutional Quality
  description: >-
    Health and effectiveness of institutions involved in AI governance, including independence from capture, expertise
    retention, and decision-making quality.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Under pressure (regulatory capture concerns, expertise gaps, rapid policy shifts)
    - label: Key Measurement
      value: Independence from industry, expertise retention, decision quality metrics
  relatedEntries:
    - id: institutional-capture
      type: risk
      relationship: related
    - id: institutional-adaptation-speed
      type: model
      relationship: analyzed-by
    - id: trust-erosion-dynamics
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - institutions
    - accountability
  lastUpdated: 2025-12
- id: reality-coherence
  type: ai-transition-model-parameter
  title: Reality Coherence
  description: >-
    The degree to which different populations share common factual beliefs about basic events, evidence, and causal
    relationships—enabling democratic deliberation and collective action.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (cross-partisan news overlap from 47% to 12% since 2010)
    - label: Key Measurement
      value: Cross-partisan factual agreement, shared source overlap, institutional trust
  parameterDistinctions:
    focus: Do we agree on facts?
    summary: Shared factual beliefs across populations
    distinctFrom:
      - id: epistemic-health
        theirFocus: Can we tell what's true?
        relationship: Epistemic health is capacity; coherence is the outcome of that capacity being shared
      - id: societal-trust
        theirFocus: Do we trust institutions?
        relationship: Trust in shared sources enables coherence; fragmentation erodes trust
  relatedEntries:
    - id: reality-fragmentation
      type: risk
      relationship: related
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: related
    - id: reality-fragmentation-network
      type: model
      relationship: analyzed-by
    - id: epistemic-collapse-threshold
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - information-environment
    - democracy
  lastUpdated: 2025-12
- id: preference-authenticity
  type: ai-transition-model-parameter
  title: Preference Authenticity
  description: >-
    The degree to which human preferences reflect genuine values rather than externally shaped desires. Essential for
    autonomy, democratic legitimacy, and meaningful choice.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Under pressure (AI recommendation systems optimize for engagement, not user wellbeing)
    - label: Key Measurement
      value: Reflective endorsement, preference stability, manipulation exposure
  relatedEntries:
    - id: preference-manipulation
      type: risk
      relationship: related
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: related
    - id: public-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: preference-manipulation-drift
      type: model
      relationship: analyzed-by
    - id: sycophancy-feedback-loop
      type: model
      relationship: analyzed-by
    - id: reality-fragmentation-network
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - autonomy
    - human-ai-interaction
  lastUpdated: 2025-12
- id: racing-intensity
  type: ai-transition-model-parameter
  title: Racing Intensity
  description: >-
    The degree of competitive pressure driving AI development speed over safety. High intensity leads to safety
    corner-cutting and premature deployment.
  customFields:
    - label: Direction
      value: Lower is better
    - label: Current Trend
      value: High (safety timelines compressed 70-80% post-ChatGPT)
    - label: Key Measurement
      value: Safety evaluation duration, safety budget allocation, deployment delays
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: related
    - id: safety-research
      type: ai-transition-model-metric
      relationship: measured-by
    - id: lab-behavior
      type: ai-transition-model-metric
      relationship: measured-by
    - id: expert-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: compute-hardware
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
    - id: racing-dynamics-impact
      type: model
      relationship: analyzed-by
    - id: multipolar-trap-dynamics
      type: model
      relationship: analyzed-by
    - id: lab-incentives-model
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - safety
    - market-dynamics
  lastUpdated: 2025-12
- id: safety-culture-strength
  type: ai-transition-model-parameter
  title: Safety Culture Strength
  description: >-
    The degree to which AI organizations genuinely prioritize safety in decisions, resource allocation, and personnel
    incentives.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (some labs lead, others decline under competitive pressure)
    - label: Key Measurement
      value: Safety budget trends, deployment veto authority, incident transparency
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: related
    - id: safety-research
      type: ai-transition-model-metric
      relationship: measured-by
    - id: lab-behavior
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
    - id: lab-incentives-model
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - safety
    - organizational
  lastUpdated: 2025-12
- id: coordination-capacity
  type: ai-transition-model-parameter
  title: Coordination Capacity
  description: >-
    The degree to which AI stakeholders successfully coordinate on safety standards, information sharing, and
    development practices.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Fragile (voluntary commitments exist but lack enforcement)
    - label: Key Measurement
      value: Commitment compliance, information sharing, standard adoption
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: related
    - id: international-coordination
      type: ai-transition-model-parameter
      relationship: related
    - id: geopolitics
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-impact
      type: model
      relationship: analyzed-by
    - id: international-coordination-game
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - international
    - coordination
  lastUpdated: 2025-12
- id: biological-threat-exposure
  type: ai-transition-model-parameter
  title: Biological Threat Exposure
  description: >-
    Society's vulnerability to biological threats including AI-enabled bioweapons. Measures exposure level—lower means
    better prevention, detection, and response capacity.
  customFields:
    - label: Direction
      value: Lower is better
    - label: Current Trend
      value: Stressed (DNA screening catches ~25% of threats; AI approaching expert virology)
    - label: Key Measurement
      value: Screening coverage, surveillance capability, response speed
  relatedEntries:
    - id: bioweapons
      type: risk
      relationship: related
    - id: bioweapons-attack-chain
      type: model
      relationship: analyzed-by
    - id: bioweapons-ai-uplift
      type: model
      relationship: analyzed-by
  tags:
    - security
    - biosecurity
    - defense
  lastUpdated: 2025-12
- id: cyber-threat-exposure
  type: ai-transition-model-parameter
  title: Cyber Threat Exposure
  description: >-
    Society's vulnerability to cyber attacks including AI-enabled threats. Measures exposure level—lower means better
    defense of critical systems.
  customFields:
    - label: Direction
      value: Lower is better
    - label: Current Trend
      value: Stressed (87% of orgs report AI attacks; 72% year-over-year increase)
    - label: Key Measurement
      value: Detection capability, response time, breach cost reduction
  relatedEntries:
    - id: cyberweapons
      type: risk
      relationship: related
    - id: cyberweapons-offense-defense
      type: model
      relationship: analyzed-by
    - id: cyberweapons-attack-automation
      type: model
      relationship: analyzed-by
  tags:
    - security
    - cybersecurity
    - defense
  lastUpdated: 2025-12
- id: societal-resilience
  type: ai-transition-model-parameter
  title: Societal Resilience
  description: Society's ability to maintain essential functions and recover from AI-related failures, attacks, or disruptions.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (increasing AI dependency vs. some redundancy investments)
    - label: Key Measurement
      value: Redundancy levels, recovery capability, human skill maintenance
  relatedEntries:
    - id: economic-disruption
      type: risk
      relationship: related
    - id: defense-in-depth-model
      type: model
      relationship: analyzed-by
  tags:
    - resilience
    - infrastructure
    - structural
  lastUpdated: 2025-12
- id: alignment-progress
  type: ai-transition-model-metric
  title: Alignment Progress
  description: >-
    Metrics tracking AI alignment research progress including interpretability coverage, RLHF effectiveness, jailbreak
    resistance, and deception detection capabilities.
  relatedEntries:
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: measures
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: measures
    - id: interpretability-coverage
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - alignment
    - safety
    - research
  lastUpdated: 2025-12
- id: safety-research
  type: ai-transition-model-metric
  title: Safety Research
  description: >-
    Metrics tracking AI safety research including researcher headcount, funding levels, publication rates, and research
    agenda progress.
  relatedEntries:
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - safety
    - research
    - funding
  lastUpdated: 2025-12
- id: lab-behavior
  type: ai-transition-model-metric
  title: Lab Behavior
  description: >-
    Metrics tracking frontier AI lab practices including RSP compliance, safety commitments, transparency, and
    deployment decisions.
  relatedEntries:
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
    - id: human-oversight-quality
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - governance
    - labs
    - safety
  lastUpdated: 2025-12
- id: public-opinion
  type: ai-transition-model-metric
  title: Public Opinion
  description: Metrics tracking public awareness, concern levels, and trust regarding AI systems and AI safety.
  relatedEntries:
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: measures
    - id: preference-authenticity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - public
    - surveys
    - trust
  lastUpdated: 2025-12
- id: expert-opinion
  type: ai-transition-model-metric
  title: Expert Opinion
  description: Metrics from AI researcher surveys including P(doom) estimates, timeline predictions, and research priorities.
  relatedEntries:
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - experts
    - surveys
    - forecasts
  lastUpdated: 2025-12
- id: economic-labor
  type: ai-transition-model-metric
  title: Economic & Labor
  description: >-
    Metrics tracking AI's economic impact including investment levels, automation rates, job displacement, and
    productivity effects.
  relatedEntries:
    - id: economic-stability
      type: ai-transition-model-parameter
      relationship: measures
    - id: human-expertise
      type: ai-transition-model-parameter
      relationship: measures
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - economics
    - labor
    - automation
  lastUpdated: 2025-12
- id: capabilities
  type: ai-transition-model-metric
  title: AI Capabilities
  description: >-
    Metrics tracking AI capability development including benchmark performance, task completion, and capability
    trajectories.
  relatedEntries:
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - capabilities
    - benchmarks
    - progress
  lastUpdated: 2025-12
- id: compute-hardware
  type: ai-transition-model-metric
  title: Compute & Hardware
  description: >-
    Metrics tracking compute trends including GPU production, training compute, efficiency improvements, and compute
    access distribution.
  relatedEntries:
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - compute
    - hardware
    - infrastructure
  lastUpdated: 2025-12
- id: geopolitics
  type: ai-transition-model-metric
  title: Geopolitics
  description: >-
    Metrics tracking international AI dynamics including US-China relations, talent flows, export controls, and
    coordination efforts.
  relatedEntries:
    - id: international-coordination
      type: ai-transition-model-parameter
      relationship: measures
    - id: coordination-capacity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - international
    - geopolitics
    - coordination
  lastUpdated: 2025-12
- id: structural
  type: ai-transition-model-metric
  title: Structural Indicators
  description: >-
    Metrics tracking structural societal factors including information quality, institutional capacity, and system
    resilience.
  relatedEntries:
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: measures
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: measures
    - id: institutional-quality
      type: ai-transition-model-parameter
      relationship: measures
    - id: societal-resilience
      type: ai-transition-model-parameter
      relationship: measures
    - id: regulatory-capacity
      type: ai-transition-model-parameter
      relationship: measures
    - id: information-authenticity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - structural
    - institutions
    - resilience
  lastUpdated: 2025-12
- id: existential-catastrophe
  type: ai-transition-model-scenario
  title: Existential Catastrophe
  description: >-
    The probability and severity of catastrophic AI-related events—loss of control, weaponization, large-scale
    accidents, or irreversible lock-in to harmful power structures.
  customFields:
    - label: Model Role
      value: Ultimate Outcome
    - label: Primary Drivers
      value: Misalignment Potential, Misuse Potential
    - label: Risk Character
      value: Tail risk, irreversible
  relatedEntries:
    - id: misalignment-potential
      type: ai-transition-model-factor
      relationship: driver
    - id: misuse-potential
      type: ai-transition-model-factor
      relationship: driver
    - id: ai-takeover
      type: ai-transition-model-scenario
      relationship: sub-scenario
    - id: human-catastrophe
      type: ai-transition-model-scenario
      relationship: sub-scenario
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: mitigates
  tags:
    - ai-transition-model
    - outcome
    - x-risk
    - catastrophe
  lastUpdated: 2025-12
- id: long-term-trajectory
  type: ai-transition-model-scenario
  title: Long-term Trajectory
  description: >-
    The quality of humanity's long-term future given successful AI transition—measuring human flourishing, autonomy
    preservation, and value realization across civilizational timescales.
  customFields:
    - label: Model Role
      value: Ultimate Outcome
    - label: Primary Drivers
      value: Civilizational Competence, AI Ownership
    - label: Risk Character
      value: Gradual degradation, potentially reversible
  relatedEntries:
    - id: civilizational-competence
      type: ai-transition-model-factor
      relationship: driver
    - id: ai-ownership
      type: ai-transition-model-factor
      relationship: driver
    - id: long-term-lockin
      type: ai-transition-model-scenario
      relationship: sub-scenario
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: key-factor
    - id: preference-authenticity
      type: ai-transition-model-parameter
      relationship: key-factor
  tags:
    - ai-transition-model
    - outcome
    - long-term
    - flourishing
  lastUpdated: 2025-12
- id: ai-takeover
  type: ai-transition-model-scenario
  title: AI Takeover
  description: >-
    Scenarios where AI systems pursue goals misaligned with human values at scale, potentially resulting in human
    disempowerment or extinction.
  customFields:
    - label: Model Role
      value: Catastrophic Scenario
    - label: Primary Drivers
      value: Misalignment Potential
    - label: Sub-scenarios
      value: Gradual takeover, Rapid takeover
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: contributes-to
    - id: misalignment-potential
      type: ai-transition-model-factor
      relationship: driven-by
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: mitigated-by
  tags:
    - ai-transition-model
    - scenario
    - x-risk
    - misalignment
  lastUpdated: 2025-12
- id: human-catastrophe
  type: ai-transition-model-scenario
  title: Human-Caused Catastrophe
  description: >-
    Catastrophic outcomes caused by human actors using AI as a tool—including state actors, rogue actors, or unintended
    cascading failures from human decisions.
  customFields:
    - label: Model Role
      value: Catastrophic Scenario
    - label: Primary Drivers
      value: Misuse Potential
    - label: Sub-scenarios
      value: State actor misuse, Rogue actor misuse
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: contributes-to
    - id: misuse-potential
      type: ai-transition-model-factor
      relationship: driven-by
    - id: biological-threat-exposure
      type: ai-transition-model-parameter
      relationship: key-factor
    - id: cyber-threat-exposure
      type: ai-transition-model-parameter
      relationship: key-factor
  tags:
    - ai-transition-model
    - scenario
    - x-risk
    - misuse
  lastUpdated: 2025-12
- id: long-term-lockin
  type: ai-transition-model-scenario
  title: Long-term Lock-in
  description: >-
    Scenarios where AI enables irreversible commitment to suboptimal values, power structures, or epistemics—foreclosing
    better futures without catastrophic collapse.
  customFields:
    - label: Model Role
      value: Degradation Scenario
    - label: Primary Drivers
      value: AI Ownership, Civilizational Competence
    - label: Sub-scenarios
      value: Values lock-in, Power lock-in, Epistemic lock-in
  relatedEntries:
    - id: long-term-trajectory
      type: ai-transition-model-scenario
      relationship: contributes-to
    - id: ai-ownership
      type: ai-transition-model-factor
      relationship: driven-by
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: key-factor
    - id: preference-authenticity
      type: ai-transition-model-parameter
      relationship: key-factor
  tags:
    - ai-transition-model
    - scenario
    - lock-in
    - long-term
  lastUpdated: 2025-12
- id: misalignment-potential
  type: ai-transition-model-factor
  title: Misalignment Potential
  description: >-
    The aggregate risk that AI systems pursue goals misaligned with human values—combining technical alignment
    challenges, interpretability gaps, and oversight limitations.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Key Parameters
      value: Alignment Robustness, Interpretability Coverage, Human Oversight Quality
    - label: Primary Outcome
      value: Existential Catastrophe
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: drives
    - id: ai-takeover
      type: ai-transition-model-scenario
      relationship: enables
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: interpretability-coverage
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: human-oversight-quality
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - technical
    - alignment
  lastUpdated: 2025-12
- id: misuse-potential
  type: ai-transition-model-factor
  title: Misuse Potential
  description: >-
    The aggregate risk from deliberate harmful use of AI—including biological weapons, cyber attacks, autonomous
    weapons, and surveillance misuse.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Key Parameters
      value: Biological Threat Exposure, Cyber Threat Exposure, Racing Intensity
    - label: Primary Outcome
      value: Existential Catastrophe
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: drives
    - id: human-catastrophe
      type: ai-transition-model-scenario
      relationship: enables
    - id: biological-threat-exposure
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: cyber-threat-exposure
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - misuse
    - weapons
  lastUpdated: 2025-12
- id: ai-capabilities
  type: ai-transition-model-factor
  title: AI Capabilities
  description: >-
    The aggregate advancement of AI system capabilities—including reasoning, autonomy, generality, and domain expertise.
    Higher capabilities amplify both benefits and risks.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Character
      value: Amplifier (neither inherently good nor bad)
    - label: Trajectory
      value: Rapidly increasing
  relatedEntries:
    - id: misalignment-potential
      type: ai-transition-model-factor
      relationship: amplifies
    - id: misuse-potential
      type: ai-transition-model-factor
      relationship: amplifies
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: affects
  tags:
    - ai-transition-model
    - factor
    - capabilities
    - scaling
  lastUpdated: 2025-12
- id: ai-uses
  type: ai-transition-model-factor
  title: AI Uses
  description: >-
    How AI capabilities are deployed across sectors—including research acceleration, industry automation, government
    applications, and coordination tools.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Character
      value: Distribution factor
    - label: Key Domains
      value: Recursive AI, Industries, Governments, Coordination
  relatedEntries:
    - id: ai-capabilities
      type: ai-transition-model-factor
      relationship: shaped-by
    - id: economic-stability
      type: ai-transition-model-parameter
      relationship: affects
    - id: human-expertise
      type: ai-transition-model-parameter
      relationship: affects
  tags:
    - ai-transition-model
    - factor
    - deployment
    - applications
  lastUpdated: 2025-12
- id: ai-ownership
  type: ai-transition-model-factor
  title: AI Ownership
  description: >-
    The distribution of control over AI systems across actors—countries, companies, and individuals. Concentration
    creates both coordination opportunities and power risks.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Character
      value: Distribution factor
    - label: Key Dimensions
      value: Countries, Companies, Shareholders
  relatedEntries:
    - id: long-term-trajectory
      type: ai-transition-model-scenario
      relationship: drives
    - id: long-term-lockin
      type: ai-transition-model-scenario
      relationship: enables
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - ownership
    - concentration
  lastUpdated: 2025-12
- id: civilizational-competence
  type: ai-transition-model-factor
  title: Civilizational Competence
  description: >-
    Society's aggregate capacity to navigate AI transition well—including governance effectiveness, epistemic health,
    coordination capacity, and adaptive resilience.
  customFields:
    - label: Model Role
      value: Root Factor (Societal)
    - label: Key Parameters
      value: Governance, Epistemics, Societal Resilience, Adaptability
    - label: Primary Outcomes
      value: Long-term Trajectory, Transition Smoothness
  relatedEntries:
    - id: long-term-trajectory
      type: ai-transition-model-scenario
      relationship: drives
    - id: transition-turbulence
      type: ai-transition-model-factor
      relationship: mitigates
    - id: regulatory-capacity
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: institutional-quality
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: international-coordination
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: societal-resilience
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - governance
    - institutions
  lastUpdated: 2025-12
- id: transition-turbulence
  type: ai-transition-model-factor
  title: Transition Turbulence
  description: >-
    The severity of disruption during the AI transition period—economic displacement, social instability, and
    institutional stress. Distinct from long-term outcomes.
  customFields:
    - label: Model Role
      value: Intermediate Factor
    - label: Key Parameters
      value: Economic Stability, Human Agency, Societal Resilience
    - label: Character
      value: Process quality (not destination)
  relatedEntries:
    - id: civilizational-competence
      type: ai-transition-model-factor
      relationship: mitigated-by
    - id: economic-stability
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: human-expertise
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - transition
    - disruption
  lastUpdated: 2025-12
- id: misaligned-catastrophe
  type: ai-transition-model-scenario
  title: Misaligned Catastrophe - The Bad Ending
  description: A scenario where alignment fails and AI systems pursue misaligned goals with catastrophic consequences.
  customFields:
    - label: Scenario Type
      value: Catastrophic / Worst Case
    - label: Probability Estimate
      value: 10-25%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Alignment fails and powerful AI is deployed anyway
    - label: Core Uncertainty
      value: Is alignment fundamentally unsolvable or just very hard?
  tags:
    - scenario
    - catastrophe
    - misalignment
  lastUpdated: 2025-01
- id: slow-takeoff-muddle
  type: ai-transition-model-scenario
  title: Slow Takeoff Muddle - Muddling Through
  description: A scenario of gradual AI progress with mixed outcomes, partial governance, and ongoing challenges.
  customFields:
    - label: Scenario Type
      value: Base Case / Most Likely
    - label: Probability Estimate
      value: 30-50%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: No discontinuous jumps in either direction
    - label: Core Uncertainty
      value: Does 'muddling through' stay stable or degrade?
  tags:
    - scenario
    - slow-takeoff
    - base-case
  lastUpdated: 2025-01
- id: aligned-agi
  type: ai-transition-model-scenario
  title: Aligned AGI - The Good Ending
  description: >-
    A scenario where AI labs successfully solve alignment and coordinated deployment leads to broadly beneficial
    outcomes.
  customFields:
    - label: Scenario Type
      value: Optimistic / Best Case
    - label: Probability Estimate
      value: 10-30%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Alignment is solvable and coordination is achievable
    - label: Core Uncertainty
      value: Can we solve alignment before capabilities race ahead?
  tags:
    - scenario
    - aligned-agi
    - optimistic
  lastUpdated: 2025-01
- id: multipolar-competition
  type: ai-transition-model-scenario
  title: Multipolar Competition - The Fragmented World
  description: >-
    A fragmented AI future where no single actor achieves dominance, leading to persistent instability and coordination
    failures.
  customFields:
    - label: Scenario Type
      value: Competitive / Unstable Equilibrium
    - label: Probability Estimate
      value: 20-30%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Multiple actors achieve advanced AI without single winner
    - label: Core Uncertainty
      value: Can multipolar competition remain stable or does it collapse?
  tags:
    - scenario
    - multipolar
    - competition
  lastUpdated: 2025-01
- id: pause-and-redirect
  type: ai-transition-model-scenario
  title: Pause and Redirect - The Deliberate Path
  description: A scenario where humanity coordinates to deliberately slow AI development for safety preparation.
  customFields:
    - label: Scenario Type
      value: Deliberate / Coordinated Slowdown
    - label: Probability Estimate
      value: 5-15%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Coordination achievable and pause sustainable
    - label: Core Uncertainty
      value: Can we coordinate to slow down, and will the pause hold?
  tags:
    - scenario
    - pause
    - coordination
  lastUpdated: 2025-01
# Factor Sub-Items (children of root factors)
# Using tmc-* prefix for TransitionModelContent entity IDs to avoid namespace collisions
- id: tmc-compute
  type: ai-transition-model-subitem
  title: Compute
  parentFactor: ai-capabilities
  path: /ai-transition-model/factors/ai-capabilities/compute/
  description: >-
    Compute refers to the hardware resources required to train and run AI systems, including GPUs, TPUs, and
    specialized AI accelerators. The current generation of frontier AI models requires extraordinary amounts of
    computational power—training runs cost tens to hundreds of millions of dollars in compute alone.

    The significance of compute for AI governance stems from several unique properties: it is measurable (training
    runs can be quantified in FLOPs), concentrated (the global semiconductor supply chain depends on chokepoints
    like ASML, TSMC, and NVIDIA), and physical (unlike algorithms that can be copied infinitely, hardware must be
    manufactured and shipped).
  ratings:
    changeability: 30
    xriskImpact: 70
    trajectoryImpact: 80
    uncertainty: 35
  currentAssessment:
    level: 35
    trend: declining
    confidence: 0.7
    lastUpdated: "2026-01"
    notes: "Compute concentration increasing; export controls having effect but circumvention growing"
  keyDebates:
    - topic: Compute governance effectiveness
      description: Can controlling compute access effectively slow dangerous AI development?
    - topic: Hardware bottleneck persistence
      description: Will hardware limitations naturally constrain AI progress, or will efficiency gains compensate?
  warningIndicators:
    - indicator: Training run costs
      status: "$100M+ for frontier models"
      trend: worsening
      concern: medium
    - indicator: Chip concentration
      status: "TSMC produces 90%+ advanced chips"
      trend: stable
      concern: high
    - indicator: Export control effectiveness
      status: "Significant circumvention observed"
      trend: worsening
      concern: high
  addressedBy:
    - id: compute-governance
      title: Compute Governance
      effect: positive
      strength: strong
    - id: export-controls
      title: Export Controls
      effect: positive
      strength: medium
  causeEffectGraph:
    title: "Factors Affecting Compute Availability"
    description: "Causal relationships between factors that influence compute availability and its downstream effects"
    nodes:
      # CAUSES - what affects compute availability
      - id: chip-manufacturing
        label: "Chip Manufacturing Capacity"
        type: cause
        confidence: 0.85
        description: "TSMC produces 90%+ of advanced AI chips. Capacity expansion takes 3-5 years and costs $20B+ per fab."
        details: "The semiconductor supply chain is highly concentrated. TSMC in Taiwan dominates advanced node production, with Samsung a distant second. Intel is attempting to catch up but remains years behind. This concentration creates both risk and governance opportunity."
        sources: ["Semiconductor Industry Association", "TSMC annual reports"]
        relatedConcepts: ["Supply chain", "TSMC", "Fab capacity", "Moore's Law"]
      - id: export-controls
        label: "Export Controls"
        type: cause
        confidence: 0.75
        description: "US restrictions on advanced chip exports to China. Creates 1-3 year delays but drives efficiency innovation."
        details: "The October 2022 and subsequent US export controls restrict sale of advanced AI chips and manufacturing equipment to China. Evidence suggests 1-3 year delays in Chinese frontier capabilities, though circumvention through cloud access and smuggling occurs."
        sources: ["BIS export control rules", "CSET analysis"]
        relatedConcepts: ["Geopolitics", "China", "Sanctions", "Technology transfer"]
      - id: energy-infrastructure
        label: "Energy & Power Availability"
        type: cause
        confidence: 0.70
        description: "Data centers require 50-100+ MW. Grid constraints and permitting limit geographic expansion."
        details: "Training runs for frontier models require enormous power. A single H100 cluster of 10,000 GPUs draws ~4MW. Grid interconnection and power purchase agreements increasingly constrain where large training runs can occur."
        sources: ["IEA data center reports", "Utility filings"]
        relatedConcepts: ["Data centers", "Power grid", "Sustainability", "Nuclear"]
      - id: algorithmic-efficiency
        label: "Algorithmic Efficiency"
        type: cause
        confidence: 0.80
        description: "~4x efficiency gains per year historically. DeepSeek achieved GPT-4 parity at ~1/10 compute."
        details: "Algorithmic improvements consistently reduce compute requirements for given capability levels. This includes architecture innovations (transformers, MoE), training efficiency (better optimizers, curricula), and inference optimizations (quantization, distillation)."
        sources: ["Epoch AI efficiency trends", "DeepSeek technical report"]
        relatedConcepts: ["Scaling laws", "Architecture", "Training efficiency", "Chinchilla"]
      - id: capital-investment
        label: "Capital Investment in AI"
        type: cause
        confidence: 0.75
        description: "Training runs cost $100M+. Only ~20 organizations can afford frontier training."
        details: "The capital requirements for frontier AI training create natural barriers to entry. Microsoft, Google, Amazon, and a few others can fund $1B+ training runs. This concentrates frontier development among well-resourced actors."
        sources: ["Company financial reports", "Industry estimates"]
        relatedConcepts: ["VC funding", "Big Tech", "Compute costs", "Barriers to entry"]
      # INTERMEDIATE - compute itself
      - id: effective-compute
        label: "Effective Compute Availability"
        type: intermediate
        confidence: 0.70
        description: "Net compute available for AI training after accounting for efficiency gains and access restrictions."
        details: "The effective compute available combines raw hardware capacity with algorithmic efficiency. Even with export controls, global effective compute for AI continues to grow rapidly due to efficiency gains and new fab capacity coming online."
        relatedConcepts: ["FLOP", "GPU-hours", "Training compute", "Inference compute"]
      - id: compute-distribution
        label: "Geographic Compute Distribution"
        type: intermediate
        confidence: 0.65
        description: "Where compute is physically located affects who can train models and jurisdictional reach."
        details: "Compute concentration in specific countries/regions affects governance options. US-based cloud providers control majority of accessible frontier compute. China is building domestic capacity but remains behind on cutting-edge chips."
        relatedConcepts: ["Cloud providers", "Data sovereignty", "Jurisdiction"]
      # EFFECTS - what compute affects
      - id: capabilities-timeline
        label: "AI Capabilities Timeline"
        type: effect
        confidence: 0.75
        description: "When frontier AI capabilities emerge. Compute constraints could delay transformative AI by years."
        details: "Compute availability is a key input to capability timelines. Constraints that reduce effective compute growth could meaningfully delay when transformative AI arrives, potentially providing more time for safety work."
        relatedConcepts: ["TAI timelines", "AGI", "Transformative AI"]
      - id: racing-dynamics
        label: "Racing Dynamics"
        type: effect
        confidence: 0.70
        description: "Intensity of competition between labs and nations. Compute scarcity has mixed effects on racing."
        details: "Compute constraints can either intensify racing (scarcity drives competition for limited resources) or reduce it (high barriers limit competitors). The net effect depends on how constraints are implemented."
        relatedConcepts: ["AI race", "Competition", "Cooperation"]
      - id: actor-landscape
        label: "Actor Landscape"
        type: effect
        confidence: 0.80
        description: "Who can train frontier models. High costs exclude most actors, concentrating development."
        details: "The set of actors capable of training frontier models depends heavily on compute access. Currently limited to ~20 organizations with sufficient capital and cloud/hardware access. This concentration affects both risks and governance options."
        relatedConcepts: ["Labs", "Concentration", "Barriers to entry"]
      - id: governance-tractability
        label: "Governance Tractability"
        type: effect
        confidence: 0.65
        description: "Whether compute-based governance is viable. Depends on concentration and measurability."
        details: "Compute's physical nature and supply chain concentration make it unusually governable compared to algorithms or data. However, efficiency gains and potential for distributed training may erode this advantage over time."
        relatedConcepts: ["Compute governance", "Regulation", "Chokepoints"]
    edges:
      # Causes → Intermediate
      - source: chip-manufacturing
        target: effective-compute
        strength: strong
        confidence: high
        effect: increases
      - source: export-controls
        target: effective-compute
        strength: medium
        confidence: medium
        effect: decreases
        label: "for restricted actors"
      - source: export-controls
        target: algorithmic-efficiency
        strength: weak
        confidence: medium
        effect: increases
        label: "drives innovation"
      - source: energy-infrastructure
        target: effective-compute
        strength: medium
        confidence: medium
        effect: increases
      - source: algorithmic-efficiency
        target: effective-compute
        strength: strong
        confidence: high
        effect: increases
      - source: capital-investment
        target: effective-compute
        strength: strong
        confidence: high
        effect: increases
      - source: chip-manufacturing
        target: compute-distribution
        strength: medium
        confidence: high
        effect: increases
      - source: export-controls
        target: compute-distribution
        strength: medium
        confidence: medium
        effect: mixed
      # Intermediate → Effects
      - source: effective-compute
        target: capabilities-timeline
        strength: strong
        confidence: high
        effect: increases
        label: "more compute → faster progress"
      - source: effective-compute
        target: actor-landscape
        strength: medium
        confidence: medium
        effect: increases
        label: "more actors with access"
      - source: effective-compute
        target: racing-dynamics
        strength: medium
        confidence: medium
        effect: mixed
      - source: compute-distribution
        target: governance-tractability
        strength: medium
        confidence: medium
        effect: mixed
        label: "concentrated → more tractable"
      - source: compute-distribution
        target: actor-landscape
        strength: medium
        confidence: high
        effect: increases
  relatedEntries:
    - id: compute-governance
      type: intervention
      relationship: addresses
    - id: ai-capabilities
      type: ai-transition-model-factor
      relationship: child-of
    - id: compute-hardware
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: affects
  tags:
    - compute
    - hardware
    - governance
    - ai-capabilities
  lastUpdated: 2026-01

# Stub entities for TransitionModelContent pages (to be filled with content later)
- id: tmc-technical-ai-safety
  type: ai-transition-model-subitem
  title: Technical AI Safety
  parentFactor: misalignment-potential
  path: /ai-transition-model/factors/misalignment-potential/technical-ai-safety/
  lastUpdated: 2026-01

- id: tmc-economic-power
  type: ai-transition-model-subitem
  title: Economic Power Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/economic-power/
  lastUpdated: 2026-01

- id: tmc-political-power
  type: ai-transition-model-subitem
  title: Political Power Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/political-power/
  lastUpdated: 2026-01

- id: tmc-values
  type: ai-transition-model-subitem
  title: Values Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/values/
  lastUpdated: 2026-01

- id: tmc-gradual
  type: ai-transition-model-subitem
  title: Gradual AI Takeover
  parentFactor: ai-takeover
  path: /ai-transition-model/scenarios/ai-takeover/gradual/
  lastUpdated: 2026-01

- id: tmc-rapid
  type: ai-transition-model-subitem
  title: Rapid AI Takeover
  parentFactor: ai-takeover
  path: /ai-transition-model/scenarios/ai-takeover/rapid/
  lastUpdated: 2026-01

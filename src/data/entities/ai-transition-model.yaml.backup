# Ai Transition Model Entities
# Auto-generated from entities.yaml - edit this file directly

- id: international-coordination
  type: ai-transition-model-parameter
  title: International Coordination
  description: >-
    Degree of global cooperation on AI governance and safety, measured through treaty participation, shared standards
    adoption, and institutional network strength.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (11-country AISI network, but US/UK refused Paris 2025 declaration)
    - label: Key Measurement
      value: Treaty signatories, AISI network participation, shared evaluation standards
  relatedEntries:
    - id: international-summits
      type: intervention
      relationship: related
    - id: geopolitics
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
    - id: multipolar-trap-dynamics
      type: model
      relationship: analyzed-by
    - id: international-coordination-game
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - international
    - coordination
  lastUpdated: 2025-12
- id: societal-trust
  type: ai-transition-model-parameter
  title: Societal Trust
  description: >-
    Level of public confidence in institutions, experts, and verification systems. A foundational parameter affecting
    democratic function and collective action.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (77% → 22% government trust since 1964)
    - label: Measurement
      value: Survey data (Pew, Gallup)
  parameterDistinctions:
    focus: Do we trust institutions?
    summary: Confidence in institutions, experts, and verification systems
    distinctFrom:
      - id: epistemic-health
        theirFocus: Can we tell what's true?
        relationship: Epistemic health reveals whether institutions deserve trust
      - id: reality-coherence
        theirFocus: Do we agree on facts?
        relationship: Trust enables acceptance of shared facts; fragmentation erodes trust
  relatedEntries:
    - id: trust-decline
      type: risk
      relationship: decreases
    - id: disinformation
      type: risk
      relationship: decreases
    - id: deepfakes
      type: risk
      relationship: decreases
    - id: content-authentication
      type: intervention
      relationship: supports
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: related
    - id: information-authenticity
      type: ai-transition-model-parameter
      relationship: related
    - id: public-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: trust-cascade-model
      type: model
      relationship: analyzed-by
    - id: deepfakes-authentication-crisis
      type: model
      relationship: analyzed-by
    - id: sycophancy-feedback-loop
      type: model
      relationship: analyzed-by
    - id: epistemic-collapse-threshold
      type: model
      relationship: analyzed-by
    - id: trust-erosion-dynamics
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - governance
    - structural
  lastUpdated: 2025-12
- id: epistemic-health
  type: ai-transition-model-parameter
  title: Epistemic Health
  description: >-
    Society's collective ability to distinguish truth from falsehood and form shared beliefs about reality. Essential
    for democratic deliberation and coordinated action.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (50%+ web content AI-generated)
    - label: Measurement
      value: Verification success rates, consensus formation
  parameterDistinctions:
    focus: Can we tell what's true?
    summary: Ability to distinguish truth from falsehood
    distinctFrom:
      - id: societal-trust
        theirFocus: Do we trust institutions?
        relationship: Trust enables verification; epistemic health reveals trustworthiness
      - id: reality-coherence
        theirFocus: Do we agree on facts?
        relationship: Epistemic health is capacity; coherence is the outcome when that capacity is shared
  relatedEntries:
    - id: epistemic-collapse
      type: risk
      relationship: decreases
    - id: disinformation
      type: risk
      relationship: decreases
    - id: consensus-manufacturing
      type: risk
      relationship: decreases
    - id: epistemic-security
      type: intervention
      relationship: supports
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: related
    - id: information-authenticity
      type: ai-transition-model-parameter
      relationship: related
    - id: expert-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: epistemic-collapse-threshold
      type: model
      relationship: analyzed-by
    - id: trust-cascade-model
      type: model
      relationship: analyzed-by
    - id: reality-fragmentation-network
      type: model
      relationship: analyzed-by
    - id: authentication-collapse-timeline
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - information-environment
  lastUpdated: 2025-12
- id: information-authenticity
  type: ai-transition-model-parameter
  title: Information Authenticity
  description: >-
    The degree to which content circulating in society can be verified as genuine—tracing to real sources, events, or
    creators. Currently stressed by AI-generated content and deepfake detection challenges.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (human deepfake detection at 55% accuracy)
    - label: Measurement
      value: Verification capability, provenance adoption, detection accuracy
  relatedEntries:
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: related
    - id: deepfakes-authentication-crisis
      type: model
      relationship: analyzed-by
    - id: authentication-collapse-timeline
      type: model
      relationship: analyzed-by
    - id: trust-cascade-model
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - information-environment
    - verification
  lastUpdated: 2025-12
- id: ai-control-concentration
  type: ai-transition-model-parameter
  title: AI Control Concentration
  description: >-
    How concentrated or distributed power over AI development and deployment is across actors. Neither extreme
    concentration nor complete diffusion is optimal.
  customFields:
    - label: Direction
      value: Context-dependent (neither extreme ideal)
    - label: Current Trend
      value: Concentrating (<20 orgs can train frontier models)
    - label: Measurement
      value: Market share, compute access, talent distribution
  relatedEntries:
    - id: concentration-of-power
      type: risk
      relationship: related
    - id: compute-hardware
      type: ai-transition-model-metric
      relationship: measured-by
    - id: winner-take-all-model
      type: model
      relationship: analyzed-by
    - id: winner-take-all-concentration
      type: model
      relationship: analyzed-by
    - id: concentration-of-power-model
      type: model
      relationship: analyzed-by
    - id: international-coordination-game
      type: model
      relationship: analyzed-by
  tags:
    - structural
    - governance
    - market-dynamics
  lastUpdated: 2025-12
- id: human-agency
  type: ai-transition-model-parameter
  title: Human Agency
  description: >-
    Degree of meaningful human control over decisions affecting their lives. Includes autonomy, oversight capacity, and
    ability to opt out of AI-mediated systems.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (increasing automation of decisions)
    - label: Measurement
      value: Decision autonomy, opt-out availability, oversight capacity
  relatedEntries:
    - id: erosion-of-agency
      type: risk
      relationship: related
    - id: economic-labor
      type: ai-transition-model-metric
      relationship: measured-by
    - id: economic-disruption-impact
      type: model
      relationship: analyzed-by
    - id: expertise-atrophy-cascade
      type: model
      relationship: analyzed-by
    - id: preference-manipulation-drift
      type: model
      relationship: analyzed-by
    - id: concentration-of-power-model
      type: model
      relationship: analyzed-by
  tags:
    - structural
    - autonomy
    - human-ai-interaction
  lastUpdated: 2025-12
- id: economic-stability
  type: ai-transition-model-parameter
  title: Economic Stability
  description: >-
    Resilience of economic systems to AI-driven changes—including labor market adaptability, income distribution, and
    transition smoothness. Currently declining as 40-60% of jobs face AI exposure.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (productivity gains vs displacement risks)
    - label: Measurement
      value: Employment rates, inequality indices, transition costs
  relatedEntries:
    - id: economic-disruption
      type: risk
      relationship: related
    - id: economic-labor
      type: ai-transition-model-metric
      relationship: measured-by
    - id: economic-disruption-impact
      type: model
      relationship: analyzed-by
    - id: winner-take-all-model
      type: model
      relationship: analyzed-by
    - id: winner-take-all-concentration
      type: model
      relationship: analyzed-by
  tags:
    - economic
    - labor-market
    - structural
  lastUpdated: 2025-12
- id: human-expertise
  type: ai-transition-model-parameter
  title: Human Expertise
  description: >-
    Maintenance of human skills, knowledge, and cognitive capabilities in an AI-augmented world. Tracks skill retention,
    domain mastery, and ability to function without AI assistance.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (36% news avoidance, rising deskilling concerns)
    - label: Measurement
      value: Skill retention, cognitive engagement, domain knowledge depth
  relatedEntries:
    - id: learned-helplessness
      type: risk
      relationship: related
    - id: economic-labor
      type: ai-transition-model-metric
      relationship: measured-by
    - id: expertise-atrophy-progression
      type: model
      relationship: analyzed-by
    - id: expertise-atrophy-cascade
      type: model
      relationship: analyzed-by
    - id: automation-bias-cascade
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - human-factors
    - cognitive
  lastUpdated: 2025-12
- id: human-oversight-quality
  type: ai-transition-model-parameter
  title: Human Oversight Quality
  description: >-
    Effectiveness of human review, decision authority, and correction capability over AI systems. Essential for
    maintaining accountability and preventing harmful AI behaviors.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (capability gap widening, automation bias increasing)
    - label: Measurement
      value: Review effectiveness, decision authority, error detection rates
  relatedEntries:
    - id: scalable-oversight
      type: safety-agenda
      relationship: related
    - id: lab-behavior
      type: ai-transition-model-metric
      relationship: measured-by
    - id: expertise-atrophy-progression
      type: model
      relationship: analyzed-by
    - id: deceptive-alignment-decomposition
      type: model
      relationship: analyzed-by
    - id: corrigibility-failure-pathways
      type: model
      relationship: analyzed-by
    - id: automation-bias-cascade
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - human-factors
    - safety
  lastUpdated: 2025-12
- id: alignment-robustness
  type: ai-transition-model-parameter
  title: Alignment Robustness
  description: >-
    How reliably AI systems pursue intended goals across contexts, distribution shifts, and adversarial conditions.
    Measures the stability of alignment under real-world deployment.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining relative to capability (1-2% reward hacking in frontier models)
    - label: Key Measurement
      value: Behavioral reliability under distribution shift, reward hacking rates
  relatedEntries:
    - id: reward-hacking
      type: risk
      relationship: decreases
    - id: mesa-optimization
      type: risk
      relationship: decreases
    - id: goal-misgeneralization
      type: risk
      relationship: decreases
    - id: deceptive-alignment
      type: risk
      relationship: decreases
    - id: sycophancy
      type: risk
      relationship: decreases
    - id: interpretability
      type: intervention
      relationship: supports
    - id: evals
      type: intervention
      relationship: supports
    - id: ai-control
      type: intervention
      relationship: supports
    - id: interpretability-coverage
      type: ai-transition-model-parameter
      relationship: related
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: related
    - id: human-oversight-quality
      type: ai-transition-model-parameter
      relationship: related
    - id: alignment-progress
      type: ai-transition-model-metric
      relationship: measured-by
    - id: deceptive-alignment-decomposition
      type: model
      relationship: analyzed-by
    - id: corrigibility-failure-pathways
      type: model
      relationship: analyzed-by
    - id: safety-capability-tradeoff
      type: model
      relationship: analyzed-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
  tags:
    - safety
    - technical
    - alignment
  lastUpdated: 2025-12
- id: safety-capability-gap
  type: ai-transition-model-parameter
  title: Safety-Capability Gap
  description: >-
    The lag between AI capability advances and corresponding safety/alignment understanding. Measures how far safety
    research trails behind what frontier systems can do.
  customFields:
    - label: Direction
      value: Lower is better (want safety close to capabilities)
    - label: Current Trend
      value: Widening (safety timelines compressed 70-80% post-ChatGPT)
    - label: Key Measurement
      value: Months/years capabilities lead safety research
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: decreases
    - id: interpretability
      type: intervention
      relationship: supports
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: related
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: related
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: related
    - id: alignment-progress
      type: ai-transition-model-metric
      relationship: measured-by
    - id: safety-research
      type: ai-transition-model-metric
      relationship: measured-by
    - id: capabilities
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-impact
      type: model
      relationship: analyzed-by
    - id: safety-capability-tradeoff
      type: model
      relationship: analyzed-by
  tags:
    - safety
    - technical
    - governance
  lastUpdated: 2025-12
- id: interpretability-coverage
  type: ai-transition-model-parameter
  title: Interpretability Coverage
  description: >-
    The percentage of model behavior that can be explained and understood by researchers. Measures transparency into AI
    system internals.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: >-
        Improving slowly (70% of Claude 3 Sonnet features interpretable, but only ~10% of frontier model capacity
        mapped)
    - label: Key Measurement
      value: Percentage of model behavior explainable, feature coverage
  relatedEntries:
    - id: interpretability
      type: concept
      relationship: related
    - id: alignment-progress
      type: ai-transition-model-metric
      relationship: measured-by
  tags:
    - safety
    - technical
    - interpretability
  lastUpdated: 2025-12
- id: regulatory-capacity
  type: ai-transition-model-parameter
  title: Regulatory Capacity
  description: >-
    Ability of governments to effectively understand, evaluate, and regulate AI systems, including technical expertise,
    enforcement capability, and institutional resources.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Growing but constrained (AISI budgets ~\$10-50M vs. \$100B+ industry spending)
    - label: Key Measurement
      value: Agency technical expertise, enforcement actions, evaluation capability
  relatedEntries:
    - id: nist-ai-rmf
      type: policy
      relationship: related
    - id: us-executive-order
      type: policy
      relationship: related
    - id: institutional-adaptation-speed
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - regulation
    - institutions
  lastUpdated: 2025-12
- id: institutional-quality
  type: ai-transition-model-parameter
  title: Institutional Quality
  description: >-
    Health and effectiveness of institutions involved in AI governance, including independence from capture, expertise
    retention, and decision-making quality.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Under pressure (regulatory capture concerns, expertise gaps, rapid policy shifts)
    - label: Key Measurement
      value: Independence from industry, expertise retention, decision quality metrics
  relatedEntries:
    - id: institutional-capture
      type: risk
      relationship: related
    - id: institutional-adaptation-speed
      type: model
      relationship: analyzed-by
    - id: trust-erosion-dynamics
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - institutions
    - accountability
  lastUpdated: 2025-12
- id: reality-coherence
  type: ai-transition-model-parameter
  title: Reality Coherence
  description: >-
    The degree to which different populations share common factual beliefs about basic events, evidence, and causal
    relationships—enabling democratic deliberation and collective action.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Declining (cross-partisan news overlap from 47% to 12% since 2010)
    - label: Key Measurement
      value: Cross-partisan factual agreement, shared source overlap, institutional trust
  parameterDistinctions:
    focus: Do we agree on facts?
    summary: Shared factual beliefs across populations
    distinctFrom:
      - id: epistemic-health
        theirFocus: Can we tell what's true?
        relationship: Epistemic health is capacity; coherence is the outcome of that capacity being shared
      - id: societal-trust
        theirFocus: Do we trust institutions?
        relationship: Trust in shared sources enables coherence; fragmentation erodes trust
  relatedEntries:
    - id: reality-fragmentation
      type: risk
      relationship: related
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: related
    - id: reality-fragmentation-network
      type: model
      relationship: analyzed-by
    - id: epistemic-collapse-threshold
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - information-environment
    - democracy
  lastUpdated: 2025-12
- id: preference-authenticity
  type: ai-transition-model-parameter
  title: Preference Authenticity
  description: >-
    The degree to which human preferences reflect genuine values rather than externally shaped desires. Essential for
    autonomy, democratic legitimacy, and meaningful choice.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Under pressure (AI recommendation systems optimize for engagement, not user wellbeing)
    - label: Key Measurement
      value: Reflective endorsement, preference stability, manipulation exposure
  relatedEntries:
    - id: preference-manipulation
      type: risk
      relationship: related
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: related
    - id: public-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: preference-manipulation-drift
      type: model
      relationship: analyzed-by
    - id: sycophancy-feedback-loop
      type: model
      relationship: analyzed-by
    - id: reality-fragmentation-network
      type: model
      relationship: analyzed-by
  tags:
    - epistemic
    - autonomy
    - human-ai-interaction
  lastUpdated: 2025-12
- id: racing-intensity
  type: ai-transition-model-parameter
  title: Racing Intensity
  description: >-
    The degree of competitive pressure driving AI development speed over safety. High intensity leads to safety
    corner-cutting and premature deployment.
  customFields:
    - label: Direction
      value: Lower is better
    - label: Current Trend
      value: High (safety timelines compressed 70-80% post-ChatGPT)
    - label: Key Measurement
      value: Safety evaluation duration, safety budget allocation, deployment delays
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: related
    - id: safety-research
      type: ai-transition-model-metric
      relationship: measured-by
    - id: lab-behavior
      type: ai-transition-model-metric
      relationship: measured-by
    - id: expert-opinion
      type: ai-transition-model-metric
      relationship: measured-by
    - id: compute-hardware
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
    - id: racing-dynamics-impact
      type: model
      relationship: analyzed-by
    - id: multipolar-trap-dynamics
      type: model
      relationship: analyzed-by
    - id: lab-incentives-model
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - safety
    - market-dynamics
  lastUpdated: 2025-12
- id: safety-culture-strength
  type: ai-transition-model-parameter
  title: Safety Culture Strength
  description: >-
    The degree to which AI organizations genuinely prioritize safety in decisions, resource allocation, and personnel
    incentives.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (some labs lead, others decline under competitive pressure)
    - label: Key Measurement
      value: Safety budget trends, deployment veto authority, incident transparency
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: related
    - id: safety-research
      type: ai-transition-model-metric
      relationship: measured-by
    - id: lab-behavior
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-model
      type: model
      relationship: analyzed-by
    - id: lab-incentives-model
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - safety
    - organizational
  lastUpdated: 2025-12
- id: coordination-capacity
  type: ai-transition-model-parameter
  title: Coordination Capacity
  description: >-
    The degree to which AI stakeholders successfully coordinate on safety standards, information sharing, and
    development practices.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Fragile (voluntary commitments exist but lack enforcement)
    - label: Key Measurement
      value: Commitment compliance, information sharing, standard adoption
  relatedEntries:
    - id: racing-dynamics
      type: risk
      relationship: related
    - id: international-coordination
      type: ai-transition-model-parameter
      relationship: related
    - id: geopolitics
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-dynamics-impact
      type: model
      relationship: analyzed-by
    - id: international-coordination-game
      type: model
      relationship: analyzed-by
  tags:
    - governance
    - international
    - coordination
  lastUpdated: 2025-12
- id: biological-threat-exposure
  type: ai-transition-model-parameter
  title: Biological Threat Exposure
  description: >-
    Society's vulnerability to biological threats including AI-enabled bioweapons. Measures exposure level—lower means
    better prevention, detection, and response capacity.
  customFields:
    - label: Direction
      value: Lower is better
    - label: Current Trend
      value: Stressed (DNA screening catches ~25% of threats; AI approaching expert virology)
    - label: Key Measurement
      value: Screening coverage, surveillance capability, response speed
  relatedEntries:
    - id: bioweapons
      type: risk
      relationship: related
    - id: bioweapons-attack-chain
      type: model
      relationship: analyzed-by
    - id: bioweapons-ai-uplift
      type: model
      relationship: analyzed-by
  tags:
    - security
    - biosecurity
    - defense
  lastUpdated: 2025-12
- id: cyber-threat-exposure
  type: ai-transition-model-parameter
  title: Cyber Threat Exposure
  description: >-
    Society's vulnerability to cyber attacks including AI-enabled threats. Measures exposure level—lower means better
    defense of critical systems.
  customFields:
    - label: Direction
      value: Lower is better
    - label: Current Trend
      value: Stressed (87% of orgs report AI attacks; 72% year-over-year increase)
    - label: Key Measurement
      value: Detection capability, response time, breach cost reduction
  relatedEntries:
    - id: cyberweapons
      type: risk
      relationship: related
    - id: cyberweapons-offense-defense
      type: model
      relationship: analyzed-by
    - id: cyberweapons-attack-automation
      type: model
      relationship: analyzed-by
  tags:
    - security
    - cybersecurity
    - defense
  lastUpdated: 2025-12
- id: societal-resilience
  type: ai-transition-model-parameter
  title: Societal Resilience
  description: Society's ability to maintain essential functions and recover from AI-related failures, attacks, or disruptions.
  customFields:
    - label: Direction
      value: Higher is better
    - label: Current Trend
      value: Mixed (increasing AI dependency vs. some redundancy investments)
    - label: Key Measurement
      value: Redundancy levels, recovery capability, human skill maintenance
  relatedEntries:
    - id: economic-disruption
      type: risk
      relationship: related
    - id: defense-in-depth-model
      type: model
      relationship: analyzed-by
  tags:
    - resilience
    - infrastructure
    - structural
  lastUpdated: 2025-12
- id: alignment-progress
  type: ai-transition-model-metric
  title: Alignment Progress
  description: >-
    Metrics tracking AI alignment research progress including interpretability coverage, RLHF effectiveness, jailbreak
    resistance, and deception detection capabilities.
  relatedEntries:
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: measures
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: measures
    - id: interpretability-coverage
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - alignment
    - safety
    - research
  lastUpdated: 2025-12
- id: safety-research
  type: ai-transition-model-metric
  title: Safety Research
  description: >-
    Metrics tracking AI safety research including researcher headcount, funding levels, publication rates, and research
    agenda progress.
  relatedEntries:
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - safety
    - research
    - funding
  lastUpdated: 2025-12
- id: lab-behavior
  type: ai-transition-model-metric
  title: Lab Behavior
  description: >-
    Metrics tracking frontier AI lab practices including RSP compliance, safety commitments, transparency, and
    deployment decisions.
  relatedEntries:
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
    - id: human-oversight-quality
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - governance
    - labs
    - safety
  lastUpdated: 2025-12
- id: public-opinion
  type: ai-transition-model-metric
  title: Public Opinion
  description: Metrics tracking public awareness, concern levels, and trust regarding AI systems and AI safety.
  relatedEntries:
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: measures
    - id: preference-authenticity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - public
    - surveys
    - trust
  lastUpdated: 2025-12
- id: expert-opinion
  type: ai-transition-model-metric
  title: Expert Opinion
  description: Metrics from AI researcher surveys including P(doom) estimates, timeline predictions, and research priorities.
  relatedEntries:
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - experts
    - surveys
    - forecasts
  lastUpdated: 2025-12
- id: economic-labor
  type: ai-transition-model-metric
  title: Economic & Labor
  description: >-
    Metrics tracking AI's economic impact including investment levels, automation rates, job displacement, and
    productivity effects.
  relatedEntries:
    - id: economic-stability
      type: ai-transition-model-parameter
      relationship: measures
    - id: human-expertise
      type: ai-transition-model-parameter
      relationship: measures
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - economics
    - labor
    - automation
  lastUpdated: 2025-12
- id: capabilities
  type: ai-transition-model-metric
  title: AI Capabilities
  description: >-
    Metrics tracking AI capability development including benchmark performance, task completion, and capability
    trajectories.
  relatedEntries:
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - capabilities
    - benchmarks
    - progress
  lastUpdated: 2025-12
- id: compute-hardware
  type: ai-transition-model-metric
  title: Compute & Hardware
  description: >-
    Metrics tracking compute trends including GPU production, training compute, efficiency improvements, and compute
    access distribution.
  relatedEntries:
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: measures
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - compute
    - hardware
    - infrastructure
  lastUpdated: 2025-12
- id: geopolitics
  type: ai-transition-model-metric
  title: Geopolitics
  description: >-
    Metrics tracking international AI dynamics including US-China relations, talent flows, export controls, and
    coordination efforts.
  relatedEntries:
    - id: international-coordination
      type: ai-transition-model-parameter
      relationship: measures
    - id: coordination-capacity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - international
    - geopolitics
    - coordination
  lastUpdated: 2025-12
- id: structural
  type: ai-transition-model-metric
  title: Structural Indicators
  description: >-
    Metrics tracking structural societal factors including information quality, institutional capacity, and system
    resilience.
  relatedEntries:
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: measures
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: measures
    - id: institutional-quality
      type: ai-transition-model-parameter
      relationship: measures
    - id: societal-resilience
      type: ai-transition-model-parameter
      relationship: measures
    - id: regulatory-capacity
      type: ai-transition-model-parameter
      relationship: measures
    - id: information-authenticity
      type: ai-transition-model-parameter
      relationship: measures
  tags:
    - structural
    - institutions
    - resilience
  lastUpdated: 2025-12
- id: existential-catastrophe
  type: ai-transition-model-scenario
  title: Existential Catastrophe
  description: >-
    The probability and severity of catastrophic AI-related events—loss of control, weaponization, large-scale
    accidents, or irreversible lock-in to harmful power structures.
  customFields:
    - label: Model Role
      value: Ultimate Outcome
    - label: Primary Drivers
      value: Misalignment Potential, Misuse Potential
    - label: Risk Character
      value: Tail risk, irreversible
  relatedEntries:
    - id: misalignment-potential
      type: ai-transition-model-factor
      relationship: driver
    - id: misuse-potential
      type: ai-transition-model-factor
      relationship: driver
    - id: ai-takeover
      type: ai-transition-model-scenario
      relationship: sub-scenario
    - id: human-catastrophe
      type: ai-transition-model-scenario
      relationship: sub-scenario
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: mitigates
  tags:
    - ai-transition-model
    - outcome
    - x-risk
    - catastrophe
  lastUpdated: 2025-12
- id: long-term-trajectory
  type: ai-transition-model-scenario
  title: Long-term Trajectory
  description: >-
    The quality of humanity's long-term future given successful AI transition—measuring human flourishing, autonomy
    preservation, and value realization across civilizational timescales.
  customFields:
    - label: Model Role
      value: Ultimate Outcome
    - label: Primary Drivers
      value: Civilizational Competence, AI Ownership
    - label: Risk Character
      value: Gradual degradation, potentially reversible
  relatedEntries:
    - id: civilizational-competence
      type: ai-transition-model-factor
      relationship: driver
    - id: ai-ownership
      type: ai-transition-model-factor
      relationship: driver
    - id: long-term-lockin
      type: ai-transition-model-scenario
      relationship: sub-scenario
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: key-factor
    - id: preference-authenticity
      type: ai-transition-model-parameter
      relationship: key-factor
  tags:
    - ai-transition-model
    - outcome
    - long-term
    - flourishing
  lastUpdated: 2025-12
- id: ai-takeover
  type: ai-transition-model-scenario
  title: AI Takeover
  description: >-
    Scenarios where AI systems pursue goals misaligned with human values at scale, potentially resulting in human
    disempowerment or extinction.
  customFields:
    - label: Model Role
      value: Catastrophic Scenario
    - label: Primary Drivers
      value: Misalignment Potential
    - label: Sub-scenarios
      value: Gradual takeover, Rapid takeover
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: contributes-to
    - id: misalignment-potential
      type: ai-transition-model-factor
      relationship: driven-by
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: mitigated-by
  tags:
    - ai-transition-model
    - scenario
    - x-risk
    - misalignment
  lastUpdated: 2025-12
- id: human-catastrophe
  type: ai-transition-model-scenario
  title: Human-Caused Catastrophe
  description: >-
    Catastrophic outcomes caused by human actors using AI as a tool—including state actors, rogue actors, or unintended
    cascading failures from human decisions.
  customFields:
    - label: Model Role
      value: Catastrophic Scenario
    - label: Primary Drivers
      value: Misuse Potential
    - label: Sub-scenarios
      value: State actor misuse, Rogue actor misuse
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: contributes-to
    - id: misuse-potential
      type: ai-transition-model-factor
      relationship: driven-by
    - id: biological-threat-exposure
      type: ai-transition-model-parameter
      relationship: key-factor
    - id: cyber-threat-exposure
      type: ai-transition-model-parameter
      relationship: key-factor
  tags:
    - ai-transition-model
    - scenario
    - x-risk
    - misuse
  lastUpdated: 2025-12
- id: long-term-lockin
  type: ai-transition-model-scenario
  title: Long-term Lock-in
  description: >-
    Scenarios where AI enables irreversible commitment to suboptimal values, power structures, or epistemics—foreclosing
    better futures without catastrophic collapse.
  customFields:
    - label: Model Role
      value: Degradation Scenario
    - label: Primary Drivers
      value: AI Ownership, Civilizational Competence
    - label: Sub-scenarios
      value: Values lock-in, Power lock-in, Epistemic lock-in
  relatedEntries:
    - id: long-term-trajectory
      type: ai-transition-model-scenario
      relationship: contributes-to
    - id: ai-ownership
      type: ai-transition-model-factor
      relationship: driven-by
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: key-factor
    - id: preference-authenticity
      type: ai-transition-model-parameter
      relationship: key-factor
  tags:
    - ai-transition-model
    - scenario
    - lock-in
    - long-term
  lastUpdated: 2025-12
- id: misalignment-potential
  type: ai-transition-model-factor
  title: Misalignment Potential
  description: >-
    The aggregate risk that AI systems pursue goals misaligned with human values—combining technical alignment
    challenges, interpretability gaps, and oversight limitations.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Key Parameters
      value: Alignment Robustness, Interpretability Coverage, Human Oversight Quality
    - label: Primary Outcome
      value: Existential Catastrophe
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: drives
    - id: ai-takeover
      type: ai-transition-model-scenario
      relationship: enables
    - id: alignment-robustness
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: interpretability-coverage
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: human-oversight-quality
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: safety-culture-strength
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - technical
    - alignment
  lastUpdated: 2025-12
- id: misuse-potential
  type: ai-transition-model-factor
  title: Misuse Potential
  description: >-
    The aggregate risk from deliberate harmful use of AI—including biological weapons, cyber attacks, autonomous
    weapons, and surveillance misuse.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Key Parameters
      value: Biological Threat Exposure, Cyber Threat Exposure, Racing Intensity
    - label: Primary Outcome
      value: Existential Catastrophe
  relatedEntries:
    - id: existential-catastrophe
      type: ai-transition-model-scenario
      relationship: drives
    - id: human-catastrophe
      type: ai-transition-model-scenario
      relationship: enables
    - id: biological-threat-exposure
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: cyber-threat-exposure
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - misuse
    - weapons
  lastUpdated: 2025-12
- id: ai-capabilities
  type: ai-transition-model-factor
  title: AI Capabilities
  description: >-
    The aggregate advancement of AI system capabilities—including reasoning, autonomy, generality, and domain expertise.
    Higher capabilities amplify both benefits and risks.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Character
      value: Amplifier (neither inherently good nor bad)
    - label: Trajectory
      value: Rapidly increasing
  relatedEntries:
    - id: misalignment-potential
      type: ai-transition-model-factor
      relationship: amplifies
    - id: misuse-potential
      type: ai-transition-model-factor
      relationship: amplifies
    - id: safety-capability-gap
      type: ai-transition-model-parameter
      relationship: affects
  tags:
    - ai-transition-model
    - factor
    - capabilities
    - scaling
  lastUpdated: 2025-12
- id: ai-uses
  type: ai-transition-model-factor
  title: AI Uses
  description: >-
    How AI capabilities are deployed across sectors—including research acceleration, industry automation, government
    applications, and coordination tools.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Character
      value: Distribution factor
    - label: Key Domains
      value: Recursive AI, Industries, Governments, Coordination
  relatedEntries:
    - id: ai-capabilities
      type: ai-transition-model-factor
      relationship: shaped-by
    - id: economic-stability
      type: ai-transition-model-parameter
      relationship: affects
    - id: human-expertise
      type: ai-transition-model-parameter
      relationship: affects
  tags:
    - ai-transition-model
    - factor
    - deployment
    - applications
  lastUpdated: 2025-12
- id: ai-ownership
  type: ai-transition-model-factor
  title: AI Ownership
  description: >-
    The distribution of control over AI systems across actors—countries, companies, and individuals. Concentration
    creates both coordination opportunities and power risks.
  customFields:
    - label: Model Role
      value: Root Factor (AI System)
    - label: Character
      value: Distribution factor
    - label: Key Dimensions
      value: Countries, Companies, Shareholders
  relatedEntries:
    - id: long-term-trajectory
      type: ai-transition-model-scenario
      relationship: drives
    - id: long-term-lockin
      type: ai-transition-model-scenario
      relationship: enables
    - id: ai-control-concentration
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - ownership
    - concentration
  lastUpdated: 2025-12
- id: civilizational-competence
  type: ai-transition-model-factor
  title: Civilizational Competence
  description: >-
    Society's aggregate capacity to navigate AI transition well—including governance effectiveness, epistemic health,
    coordination capacity, and adaptive resilience.
  customFields:
    - label: Model Role
      value: Root Factor (Societal)
    - label: Key Parameters
      value: Governance, Epistemics, Societal Resilience, Adaptability
    - label: Primary Outcomes
      value: Long-term Trajectory, Transition Smoothness
  relatedEntries:
    - id: long-term-trajectory
      type: ai-transition-model-scenario
      relationship: drives
    - id: transition-turbulence
      type: ai-transition-model-factor
      relationship: mitigates
    - id: regulatory-capacity
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: institutional-quality
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: international-coordination
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: societal-resilience
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: epistemic-health
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: societal-trust
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - governance
    - institutions
  lastUpdated: 2025-12
- id: transition-turbulence
  type: ai-transition-model-factor
  title: Transition Turbulence
  description: >-
    The severity of disruption during the AI transition period—economic displacement, social instability, and
    institutional stress. Distinct from long-term outcomes.
  customFields:
    - label: Model Role
      value: Intermediate Factor
    - label: Key Parameters
      value: Economic Stability, Human Agency, Societal Resilience
    - label: Character
      value: Process quality (not destination)
  relatedEntries:
    - id: civilizational-competence
      type: ai-transition-model-factor
      relationship: mitigated-by
    - id: economic-stability
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: human-agency
      type: ai-transition-model-parameter
      relationship: composed-of
    - id: human-expertise
      type: ai-transition-model-parameter
      relationship: composed-of
  tags:
    - ai-transition-model
    - factor
    - transition
    - disruption
  lastUpdated: 2025-12
- id: misaligned-catastrophe
  type: ai-transition-model-scenario
  title: Misaligned Catastrophe - The Bad Ending
  description: A scenario where alignment fails and AI systems pursue misaligned goals with catastrophic consequences.
  customFields:
    - label: Scenario Type
      value: Catastrophic / Worst Case
    - label: Probability Estimate
      value: 10-25%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Alignment fails and powerful AI is deployed anyway
    - label: Core Uncertainty
      value: Is alignment fundamentally unsolvable or just very hard?
  tags:
    - scenario
    - catastrophe
    - misalignment
  lastUpdated: 2025-01
- id: slow-takeoff-muddle
  type: ai-transition-model-scenario
  title: Slow Takeoff Muddle - Muddling Through
  description: A scenario of gradual AI progress with mixed outcomes, partial governance, and ongoing challenges.
  customFields:
    - label: Scenario Type
      value: Base Case / Most Likely
    - label: Probability Estimate
      value: 30-50%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: No discontinuous jumps in either direction
    - label: Core Uncertainty
      value: Does 'muddling through' stay stable or degrade?
  tags:
    - scenario
    - slow-takeoff
    - base-case
  lastUpdated: 2025-01
- id: aligned-agi
  type: ai-transition-model-scenario
  title: Aligned AGI - The Good Ending
  description: >-
    A scenario where AI labs successfully solve alignment and coordinated deployment leads to broadly beneficial
    outcomes.
  customFields:
    - label: Scenario Type
      value: Optimistic / Best Case
    - label: Probability Estimate
      value: 10-30%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Alignment is solvable and coordination is achievable
    - label: Core Uncertainty
      value: Can we solve alignment before capabilities race ahead?
  tags:
    - scenario
    - aligned-agi
    - optimistic
  lastUpdated: 2025-01
- id: multipolar-competition
  type: ai-transition-model-scenario
  title: Multipolar Competition - The Fragmented World
  description: >-
    A fragmented AI future where no single actor achieves dominance, leading to persistent instability and coordination
    failures.
  customFields:
    - label: Scenario Type
      value: Competitive / Unstable Equilibrium
    - label: Probability Estimate
      value: 20-30%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Multiple actors achieve advanced AI without single winner
    - label: Core Uncertainty
      value: Can multipolar competition remain stable or does it collapse?
  tags:
    - scenario
    - multipolar
    - competition
  lastUpdated: 2025-01
- id: pause-and-redirect
  type: ai-transition-model-scenario
  title: Pause and Redirect - The Deliberate Path
  description: A scenario where humanity coordinates to deliberately slow AI development for safety preparation.
  customFields:
    - label: Scenario Type
      value: Deliberate / Coordinated Slowdown
    - label: Probability Estimate
      value: 5-15%
    - label: Timeframe
      value: 2024-2040
    - label: Key Assumption
      value: Coordination achievable and pause sustainable
    - label: Core Uncertainty
      value: Can we coordinate to slow down, and will the pause hold?
  tags:
    - scenario
    - pause
    - coordination
  lastUpdated: 2025-01
# Factor Sub-Items (children of root factors)
# Using tmc-* prefix for TransitionModelContent entity IDs to avoid namespace collisions
- id: tmc-compute
  type: ai-transition-model-subitem
  title: Compute
  parentFactor: ai-capabilities
  path: /ai-transition-model/factors/ai-capabilities/compute/
  description: >-
    Compute refers to the hardware resources required to train and run AI systems, including GPUs, TPUs, and
    specialized AI accelerators. The current generation of frontier AI models requires extraordinary amounts of
    computational power—training runs cost tens to hundreds of millions of dollars in compute alone.

    The significance of compute for AI governance stems from several unique properties: it is measurable (training
    runs can be quantified in FLOPs), concentrated (the global semiconductor supply chain depends on chokepoints
    like ASML, TSMC, and NVIDIA), and physical (unlike algorithms that can be copied infinitely, hardware must be
    manufactured and shipped).
  ratings:
    changeability: 30
    xriskImpact: 70
    trajectoryImpact: 80
    uncertainty: 35
  currentAssessment:
    level: 35
    trend: declining
    confidence: 0.7
    lastUpdated: "2026-01"
    notes: "Compute concentration increasing; export controls having effect but circumvention growing"
  keyDebates:
    - topic: Compute governance effectiveness
      description: Can controlling compute access effectively slow dangerous AI development?
    - topic: Hardware bottleneck persistence
      description: Will hardware limitations naturally constrain AI progress, or will efficiency gains compensate?
  warningIndicators:
    - indicator: Training run costs
      status: "$100M+ for frontier models"
      trend: worsening
      concern: medium
    - indicator: Chip concentration
      status: "TSMC produces 90%+ advanced chips"
      trend: stable
      concern: high
    - indicator: Export control effectiveness
      status: "Significant circumvention observed"
      trend: worsening
      concern: high
  addressedBy:
    - id: compute-governance
      title: Compute Governance
      effect: positive
      strength: strong
    - id: export-controls
      title: Export Controls
      effect: positive
      strength: medium
  causeEffectGraph:
    title: "What Drives Effective AI Compute?"
    description: "Causal factors affecting frontier AI training compute. Note: This forms a cycle—AI capabilities drive revenue, which funds more compute—but feedback loops are omitted for clarity."
    primaryNodeId: effective-compute
    nodes:
      # LAYER 1 (leaf): Exogenous factors - supply side
      - id: taiwan-stability
        label: "Taiwan Stability"
        type: leaf
        description: "Geopolitical risk to TSMC. 90%+ of advanced chips."
      - id: asml-capacity
        label: "ASML Capacity"
        type: leaf
        description: "Sole EUV lithography supplier. ~50 machines/year."
      - id: power-grid
        label: "Power Grid Capacity"
        type: leaf
        description: "Grid expansion, permitting. Binding constraint for large clusters."
      - id: training-data
        label: "Training Data Quality"
        type: leaf
        description: "Available high-quality data. May become binding as models scale."
      # LAYER 1 (leaf): Exogenous factors - demand side
      - id: ai-revenue
        label: "AI Revenue"
        type: leaf
        description: "Revenue from AI products. Justifies continued investment."

      # LAYER 2 (cause): Derived factors
      - id: fab-capacity
        label: "Fab Capacity"
        type: cause
        description: "Advanced node manufacturing. New fabs: 3-5 years, $20-40B."
      - id: ai-valuations
        label: "AI Valuations"
        type: cause
        description: "Market caps enabling capital raises. $1-3T for frontier labs."

      # LAYER 3 (intermediate): Production & deployment
      - id: chip-supply
        label: "Chip Supply"
        type: intermediate
        description: "Total advanced AI chips produced."
      - id: chip-architecture
        label: "Chip Architecture"
        type: intermediate
        description: "FLOPS per chip. ~2-3x improvement per generation."
      - id: algorithmic-efficiency
        label: "Algorithmic Efficiency"
        type: intermediate
        description: "Software improvements. Historically ~4x/year."
      - id: datacenter-capacity
        label: "Datacenter Capacity"
        type: intermediate
        description: "Infrastructure: power, cooling, networking."
      - id: ai-compute-spending
        label: "AI Compute Spending"
        type: intermediate
        description: "Capital deployed for compute infrastructure."

      # LAYER 4 (effect): Target
      - id: effective-compute
        label: "Effective Compute"
        type: effect
        description: "Net compute available for frontier AI training."

    edges:
      # === SUPPLY CHAIN (left side) ===
      - source: asml-capacity
        target: fab-capacity
        strength: strong
        effect: increases
      - source: taiwan-stability
        target: fab-capacity
        strength: strong
        effect: increases
      - source: fab-capacity
        target: chip-supply
        strength: strong
        effect: increases
      - source: chip-supply
        target: datacenter-capacity
        strength: strong
        effect: increases
      - source: power-grid
        target: datacenter-capacity
        strength: strong
        effect: increases

      # === FINANCIAL CHAIN (right side) ===
      - source: ai-revenue
        target: ai-valuations
        strength: strong
        effect: increases
      - source: ai-valuations
        target: ai-compute-spending
        strength: strong
        effect: increases
      - source: ai-compute-spending
        target: datacenter-capacity
        strength: strong
        effect: increases

      # === CROSS-LINKS ===
      # Demand drives supply allocation
      - source: ai-compute-spending
        target: chip-supply
        strength: medium
        effect: increases
      # Taiwan risk affects valuations too
      - source: taiwan-stability
        target: ai-valuations
        strength: medium
        effect: increases
      # Spending funds R&D for architecture + algorithms
      - source: ai-compute-spending
        target: chip-architecture
        strength: medium
        effect: increases
      - source: ai-compute-spending
        target: algorithmic-efficiency
        strength: medium
        effect: increases

      # === TO EFFECTIVE COMPUTE ===
      - source: datacenter-capacity
        target: effective-compute
        strength: strong
        effect: increases
      - source: chip-architecture
        target: effective-compute
        strength: strong
        effect: increases
      - source: algorithmic-efficiency
        target: effective-compute
        strength: strong
        effect: increases
      - source: training-data
        target: effective-compute
        strength: medium
        effect: increases
  relatedEntries:
    - id: compute-governance
      type: intervention
      relationship: addresses
    - id: ai-capabilities
      type: ai-transition-model-factor
      relationship: child-of
    - id: compute-hardware
      type: ai-transition-model-metric
      relationship: measured-by
    - id: racing-intensity
      type: ai-transition-model-parameter
      relationship: affects
  tags:
    - compute
    - hardware
    - governance
    - ai-capabilities
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/compute/
        title: "Compute (AI Capabilities): Research Report"
  lastUpdated: 2026-01

- id: tmc-algorithms
  type: ai-transition-model-subitem
  title: Algorithms
  parentFactor: ai-capabilities
  path: /ai-transition-model/factors/ai-capabilities/algorithms/
  description: >-
    Algorithmic progress determines how efficiently AI systems convert compute into capabilities.
    Unlike hardware, algorithms are intangible—discoveries spread instantly through publications,
    making direct governance nearly impossible.
  ratings:
    changeability: 20
    xriskImpact: 75
    trajectoryImpact: 85
    uncertainty: 55
  causeEffectGraph:
    title: "What Drives Algorithmic Progress?"
    description: "Causal factors affecting AI algorithmic efficiency. Research shows 91% of gains are scale-dependent (Transformers, Chinchilla), coupling algorithmic progress to compute availability. Software optimizations (23x) dramatically outpace hardware improvements."
    primaryNodeId: algorithmic-progress
    nodes:
      # LAYER 1 (leaf): Exogenous factors
      - id: research-talent-pool
        label: "Research Talent Pool"
        type: leaf
        description: "Supply of skilled ML researchers and engineers. Growing exponentially but still concentrated."
      - id: compute-availability
        label: "Compute Availability"
        type: leaf
        description: "Access to frontier compute for scaling studies. 91% of efficiency gains depend on scale."
      - id: open-science-norms
        label: "Open Science Norms"
        type: leaf
        description: "Culture of publishing papers/code. Algorithms diffuse instantly—cannot be physically controlled."
      - id: competitive-pressure
        label: "Competitive Pressure"
        type: leaf
        description: "Racing dynamics between labs/nations. Accelerates innovation but may compromise safety."

      # LAYER 2 (cause): Derived factors
      - id: scaling-law-insights
        label: "Scaling Law Insights"
        type: cause
        description: "Chinchilla 20:1 token-to-parameter ratio. Compute-optimal training enables smaller, better-trained models."
      - id: academic-research-output
        label: "Academic Research Output"
        type: cause
        description: "Volume of ML papers and experiments. Transformer (2017) was transformative but unpredicted."
      - id: paradigm-shifts
        label: "Paradigm Shifts"
        type: cause
        description: "Architectural breakthroughs like Transformers. Transformers + Chinchilla account for 91% of gains at frontier."
      - id: post-training-methods
        label: "Post-Training Methods"
        type: cause
        description: "RLHF, distillation, test-time compute. Add 3-16x efficiency gains beyond pre-training."

      # LAYER 3 (intermediate): Direct optimization factors
      - id: architecture-innovations
        label: "Architecture Innovations"
        type: intermediate
        description: "MoE (DeepSeek 40% compute), GQA, RoPE. Core component of 23x software improvements."
      - id: training-efficiency
        label: "Training Efficiency"
        type: intermediate
        description: "FP8 quantization, curriculum learning, optimization methods. Enables compute-optimal training."
      - id: software-optimizations
        label: "Software Optimizations"
        type: intermediate
        description: "Speculative decoding, KV caching, quantization. 23x improvement vs. 1.3x hardware gains."
      - id: deployment-efficiency
        label: "Deployment Efficiency"
        type: intermediate
        description: "Inference optimization, batching. 280x cost reduction over 24 months."

      # LAYER 4 (effect): The outcome
      - id: algorithmic-progress
        label: "Algorithmic Progress"
        type: effect
        description: "Compute required to reach fixed performance halves every 8 months (95% CI: 5-14 months). Combined pre-training + post-training: ~9x/year."

    edges:
      # Compute availability enables scale-dependent progress
      - source: compute-availability
        target: paradigm-shifts
        strength: strong
        effect: increases
      - source: compute-availability
        target: scaling-law-insights
        strength: strong
        effect: increases

      # Research talent and competitive pressure drive output
      - source: research-talent-pool
        target: academic-research-output
        strength: strong
        effect: increases
      - source: competitive-pressure
        target: academic-research-output
        strength: medium
        effect: increases
      - source: competitive-pressure
        target: post-training-methods
        strength: medium
        effect: increases

      # Open science enables rapid diffusion and replication
      - source: open-science-norms
        target: paradigm-shifts
        strength: medium
        effect: increases
      - source: open-science-norms
        target: architecture-innovations
        strength: medium
        effect: increases

      # Research output feeds into innovations
      - source: academic-research-output
        target: architecture-innovations
        strength: strong
        effect: increases
      - source: academic-research-output
        target: training-efficiency
        strength: medium
        effect: increases

      # Paradigm shifts and scaling laws unlock major innovations
      - source: paradigm-shifts
        target: architecture-innovations
        strength: strong
        effect: increases
      - source: scaling-law-insights
        target: training-efficiency
        strength: strong
        effect: increases

      # Post-training methods enhance deployment
      - source: post-training-methods
        target: deployment-efficiency
        strength: strong
        effect: increases

      # All optimization paths feed into algorithmic progress
      - source: architecture-innovations
        target: algorithmic-progress
        strength: strong
        effect: increases
      - source: training-efficiency
        target: algorithmic-progress
        strength: strong
        effect: increases
      - source: software-optimizations
        target: algorithmic-progress
        strength: strong
        effect: increases
      - source: deployment-efficiency
        target: algorithmic-progress
        strength: medium
        effect: increases

  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/algorithms/
        title: "Algorithms (AI Capabilities): Research Report"
  relatedEntries:
    - id: ai-capabilities
      type: ai-transition-model-factor
      relationship: child-of
  tags:
    - algorithms
    - research
    - ai-capabilities
  lastUpdated: 2026-01

# Stub entities for TransitionModelContent pages (to be filled with content later)
- id: tmc-technical-ai-safety
  type: ai-transition-model-subitem
  title: Technical AI Safety
  parentFactor: misalignment-potential
  path: /ai-transition-model/factors/misalignment-potential/technical-ai-safety/
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/technical-ai-safety/
        title: "Technical AI Safety: Research Report"
  causeEffectGraph:
    title: "What Drives AI Safety Adequacy?"
    description: "Causal factors affecting technical AI safety outcomes. The field faces a widening gap: alignment methods show brittleness, interpretability is progressing but incomplete, and evaluation benchmarks are unreliable."
    primaryNodeId: safety-adequacy
    nodes:
      # LAYER 1 (leaf): External/exogenous factors
      - id: safety-field-growth
        label: "Safety Field Growth"
        type: leaf
        description: "~50 interpretability researchers, 140+ papers at ICML. Growing exponentially from small base."
      - id: safety-research-funding
        label: "Safety Research Funding"
        type: leaf
        description: "OpenAI allocated 20% compute for Superalignment. Labs invest but far less than capabilities."
      - id: capabilities-racing
        label: "Capabilities Racing"
        type: leaf
        description: "Safety timelines compressed 70-80% post-ChatGPT. Competitive pressure reduces safety investment."
      - id: lab-safety-culture
        label: "Lab Safety Culture"
        type: leaf
        description: "Only 3/7 frontier labs test dangerous capabilities. Mixed commitment across industry."

      # LAYER 2 (cause): Research progress
      - id: interpretability-progress
        label: "Interpretability Progress"
        type: cause
        description: "SAEs now extract features from Claude 3 Sonnet. 70% of features interpretable but only ~10% of model mapped."
      - id: alignment-technique-development
        label: "Alignment Technique Development"
        type: cause
        description: "RLHF, Constitutional AI, weak-to-strong generalization. Methods exist but show brittleness."
      - id: control-methodology-adoption
        label: "Control Methodology Adoption"
        type: cause
        description: "Redwood's AI control via red team/blue team. Safety without alignment assumption."
      - id: benchmark-development
        label: "Benchmark Development"
        type: cause
        description: "AILuminate, capability evaluations. But vulnerable to sandbagging and conflate safety with capabilities."

      # LAYER 3 (intermediate): Direct safety factors
      - id: alignment-robustness
        label: "Alignment Robustness"
        type: intermediate
        description: "RLHF shows preference collapse, deceptive alignment. 60-80% of RL agents exhibit goal misgeneralization."
      - id: interpretability-coverage
        label: "Interpretability Coverage"
        type: intermediate
        description: "Can explain safety-relevant features but far from comprehensive model understanding."
      - id: evaluation-reliability
        label: "Evaluation Reliability"
        type: intermediate
        description: "Models can sandbag dangerous capability evals. Benchmarks correlate with capabilities not safety."
      - id: weak-supervision-capacity
        label: "Weak Supervision Capacity"
        type: intermediate
        description: "GPT-2 supervision recovers only 20-50% of GPT-4 capabilities. Superhuman alignment unsolved."
      - id: capabilities-safety-gap
        label: "Capabilities-Safety Gap"
        type: intermediate
        description: "Safety research trails capabilities by widening margin. ~50 interpretability researchers vs thousands on capabilities."

      # LAYER 4 (effect): Target
      - id: safety-adequacy
        label: "Safety Adequacy"
        type: effect
        description: "Whether AI safety measures are sufficient to prevent catastrophic misalignment as capabilities scale."

    edges:
      # Field growth drives research progress
      - source: safety-field-growth
        target: interpretability-progress
        strength: strong
        effect: increases
      - source: safety-field-growth
        target: alignment-technique-development
        strength: strong
        effect: increases
      - source: safety-research-funding
        target: interpretability-progress
        strength: strong
        effect: increases
      - source: safety-research-funding
        target: alignment-technique-development
        strength: strong
        effect: increases
      - source: safety-research-funding
        target: control-methodology-adoption
        strength: medium
        effect: increases

      # Racing undermines safety
      - source: capabilities-racing
        target: capabilities-safety-gap
        strength: strong
        effect: increases
      - source: capabilities-racing
        target: evaluation-reliability
        strength: medium
        effect: decreases

      # Lab culture affects methods and evaluation
      - source: lab-safety-culture
        target: control-methodology-adoption
        strength: medium
        effect: increases
      - source: lab-safety-culture
        target: benchmark-development
        strength: medium
        effect: increases
      - source: lab-safety-culture
        target: evaluation-reliability
        strength: medium
        effect: increases

      # Research progress drives capabilities
      - source: interpretability-progress
        target: interpretability-coverage
        strength: strong
        effect: increases
      - source: alignment-technique-development
        target: alignment-robustness
        strength: medium
        effect: increases
      - source: control-methodology-adoption
        target: alignment-robustness
        strength: medium
        effect: increases
      - source: benchmark-development
        target: evaluation-reliability
        strength: weak
        effect: increases
      - source: alignment-technique-development
        target: weak-supervision-capacity
        strength: medium
        effect: increases

      # Capabilities-safety gap undermines everything
      - source: capabilities-safety-gap
        target: alignment-robustness
        strength: strong
        effect: decreases
      - source: capabilities-safety-gap
        target: weak-supervision-capacity
        strength: strong
        effect: decreases

      # All intermediate factors contribute to safety adequacy
      - source: alignment-robustness
        target: safety-adequacy
        strength: strong
        effect: increases
      - source: interpretability-coverage
        target: safety-adequacy
        strength: strong
        effect: increases
      - source: evaluation-reliability
        target: safety-adequacy
        strength: strong
        effect: increases
      - source: weak-supervision-capacity
        target: safety-adequacy
        strength: medium
        effect: increases
      - source: capabilities-safety-gap
        target: safety-adequacy
        strength: strong
        effect: decreases
  lastUpdated: 2026-01

- id: tmc-economic-power
  type: ai-transition-model-subitem
  title: Economic Power Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/economic-power/
  description: >-
    Economic power lock-in describes scenarios where AI-enabled productivity becomes permanently
    concentrated in the hands of a small group, creating wealth disparities so extreme that
    redistribution becomes structurally impossible rather than merely politically difficult.
  lastUpdated: 2026-01
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/economic-power-lockin/
        title: "Economic Power Lock-in: Research Report"
  causeEffectGraph:
    title: "What Drives Economic Power Lock-in?"
    description: "Causal factors concentrating AI-driven wealth and making redistribution structurally impossible. Four mega unicorns already control 66.7% of AI market value."
    primaryNodeId: economic-power-lockin
    nodes:
      # LAYER 1 (leaf): Exogenous factors
      - id: frontier-model-costs
        label: "Frontier Model Costs"
        type: leaf
        description: "Training costs $100M-$1B+. Only ~20 orgs can compete. Projected to reach $10B by 2030."
      - id: regulatory-lag
        label: "Regulatory Lag"
        type: leaf
        description: "Antitrust backward-looking; 5-10 year case timelines. Rule of reason inadequate for platform economics."
      - id: antitrust-enforcement
        label: "Antitrust Enforcement"
        type: leaf
        description: "FTC/DOJ investigations of AI partnerships. Trump admin may reduce enforcement."

      # LAYER 2 (cause): Core concentration mechanisms
      - id: returns-to-scale
        label: "Returns to Scale"
        type: cause
        description: "Natural monopoly characteristics. Lower total cost for single firm than multiple firms."
      - id: data-feedback-loops
        label: "Data Feedback Loops"
        type: cause
        description: "Better models → more users → more data → better models. Virtuous cycle for incumbents."
      - id: first-mover-advantages
        label: "First-Mover Advantages"
        type: cause
        description: "Economies of scale, brand recognition, data accumulation. Early leaders gain compounding benefits."
      - id: capital-labor-shift
        label: "Capital-Labor Share Shift"
        type: cause
        description: "AI increases returns to capital at expense of labor. Capital ownership highly concentrated."

      # LAYER 3 (intermediate): Concentration effects
      - id: cloud-infrastructure-concentration
        label: "Infrastructure Lock-in"
        type: intermediate
        description: "66-70% market share (AWS, Azure, GCP). Switching costs prohibitive; data gravity."
      - id: labor-displacement
        label: "Labor Displacement"
        type: intermediate
        description: "76,440 positions eliminated (2025); 92M projected (2030). Removes traditional mobility paths."
      - id: market-concentration
        label: "Market Concentration"
        type: intermediate
        description: "Four companies control 66.7% of $1.1T AI market value. Winner-take-all dynamics."
      - id: algorithmic-collusion
        label: "Algorithmic Collusion"
        type: intermediate
        description: "AI systems reach mutually beneficial pricing patterns. 28% margin increase in algorithmic pricing studies."
      - id: geographic-concentration
        label: "Geographic Concentration"
        type: intermediate
        description: "94% of AI funding in US. Creates international inequality and consolidates control."

      # LAYER 4 (effect): The outcome
      - id: economic-power-lockin
        label: "Economic Power Lock-in"
        type: effect
        description: "Wealth concentration so extreme that redistribution becomes structurally impossible. Economic hierarchy embedded in technological infrastructure."

    edges:
      # Leaf → Cause
      - source: frontier-model-costs
        target: returns-to-scale
        effect: increases
        strength: strong
      - source: regulatory-lag
        target: returns-to-scale
        effect: increases
        strength: medium

      # Cause → Intermediate
      - source: returns-to-scale
        target: market-concentration
        effect: increases
        strength: strong
      - source: data-feedback-loops
        target: market-concentration
        effect: increases
        strength: strong
      - source: first-mover-advantages
        target: market-concentration
        effect: increases
        strength: medium
      - source: returns-to-scale
        target: cloud-infrastructure-concentration
        effect: increases
        strength: strong
      - source: data-feedback-loops
        target: cloud-infrastructure-concentration
        effect: increases
        strength: medium
      - source: capital-labor-shift
        target: labor-displacement
        effect: increases
        strength: strong
      - source: returns-to-scale
        target: algorithmic-collusion
        effect: increases
        strength: medium
      - source: first-mover-advantages
        target: geographic-concentration
        effect: increases
        strength: medium

      # Intermediate → Intermediate (reinforcing loops)
      - source: market-concentration
        target: geographic-concentration
        effect: increases
        strength: medium
      - source: cloud-infrastructure-concentration
        target: market-concentration
        effect: increases
        strength: medium

      # Intermediate → Effect
      - source: market-concentration
        target: economic-power-lockin
        effect: increases
        strength: strong
      - source: cloud-infrastructure-concentration
        target: economic-power-lockin
        effect: increases
        strength: strong
      - source: labor-displacement
        target: economic-power-lockin
        effect: increases
        strength: strong
      - source: algorithmic-collusion
        target: economic-power-lockin
        effect: increases
        strength: medium
      - source: geographic-concentration
        target: economic-power-lockin
        effect: increases
        strength: medium

      # Intervention (negative effect)
      - source: antitrust-enforcement
        target: market-concentration
        effect: decreases
        strength: weak

- id: tmc-political-power
  type: ai-transition-model-subitem
  title: Political Power Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/political-power/
  description: >-
    Political power lock-in describes scenarios where AI-enabled surveillance and control mechanisms
    make authoritarian or oligarchic governance structures effectively permanent, foreclosing the
    possibility of regime change or political reform through any available means.
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/political-power-lockin/
        title: "Political Power Lock-in: Research Report"
  lastUpdated: 2026-01

- id: tmc-values
  type: ai-transition-model-subitem
  title: Values Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/values/
  description: >-
    Value lock-in occurs when AI enables the permanent entrenchment of a particular set of values,
    making future change extremely difficult or impossible. This could preserve beneficial values
    (democratic norms, human rights) or entrench harmful ones (authoritarianism, narrow corporate interests).
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/values-lockin/
        title: "Values Lock-in: Research Report"
  causeEffectGraph:
    title: "How Values Lock-in Happens"
    description: "Causal factors driving permanent entrenchment of particular values in AI systems. Based on RLHF bias, feedback loops, surveillance infrastructure, and moral uncertainty creating irreversible value commitments."
    primaryNodeId: values-lockin
    nodes:
      # LAYER 1 (leaf): Exogenous factors
      - id: moral-uncertainty
        label: "Moral Uncertainty"
        type: leaf
        description: "No philosophical consensus on correct values. AI development forces premature resolution of unresolved ethical questions."
      - id: western-training-data
        label: "Western Training Data Bias"
        type: leaf
        description: "English-dominant, Protestant European values in training data. 94% of AI funding in US creates cultural bias."
      - id: surveillance-infrastructure
        label: "Surveillance Infrastructure"
        type: leaf
        description: "AI surveillance deployed in 20+ countries. Facial recognition, predictive policing enable behavioral control."
      - id: annotator-demographics
        label: "Annotator Demographics"
        type: leaf
        description: "RLHF relies on human feedback from non-representative annotators. Cheap annotation favored over representative sampling."

      # LAYER 2 (cause): Core mechanisms
      - id: rlhf-bias
        label: "RLHF Algorithmic Bias"
        type: cause
        description: "KL-regularization causes preference collapse. Standard RLHF reduces distributional pluralism by 30-40%."
      - id: value-specification-pressure
        label: "Value Specification Pressure"
        type: cause
        description: "AI systems require explicit objective functions. Technical necessity forces premature value choices."
      - id: feedback-loop-dynamics
        label: "AI-Human Feedback Loops"
        type: cause
        description: "Models learn beliefs → generate content → users influenced → new training data. Creates echo chambers."
      - id: behavioral-modification
        label: "Surveillance Behavioral Modification"
        type: cause
        description: "Continuous AI monitoring instills fear and conformity. Future generations internalize self-censorship as norm."

      # LAYER 3 (intermediate): Direct effects
      - id: value-pluralism-reduction
        label: "Value Pluralism Reduction"
        type: intermediate
        description: "Minority preferences disregarded. Irreducible value tensions eliminated. Alternative beliefs marginalized."
      - id: cultural-homogenization
        label: "Cultural Homogenization"
        type: intermediate
        description: "Western values encoded as universal defaults. Non-English cultures, non-Christian worldviews underrepresented."
      - id: deployment-path-dependencies
        label: "Deployment Path Dependencies"
        type: intermediate
        description: "67% of companies increasing AI investment. Embedded systems create lock-in. Changing values requires replacing infrastructure."
      - id: moral-reasoning-atrophy
        label: "Moral Reasoning Atrophy"
        type: intermediate
        description: "Reliance on AI for ethical guidance degrades human moral reasoning capacity. Expertise atrophy prevents recognition of bad lock-in."

      # LAYER 4 (effect): The outcome
      - id: values-lockin
        label: "Values Lock-in"
        type: effect
        description: "Permanent entrenchment of particular values. Foreclosure of moral progress. 2020s ethics locked in for centuries."

    edges:
      # Leaf → Cause
      - source: moral-uncertainty
        target: value-specification-pressure
        effect: increases
        strength: strong
      - source: western-training-data
        target: rlhf-bias
        effect: increases
        strength: strong
      - source: annotator-demographics
        target: rlhf-bias
        effect: increases
        strength: strong
      - source: surveillance-infrastructure
        target: behavioral-modification
        effect: increases
        strength: strong

      # Cause → Intermediate
      - source: rlhf-bias
        target: value-pluralism-reduction
        effect: increases
        strength: strong
      - source: value-specification-pressure
        target: value-pluralism-reduction
        effect: increases
        strength: medium
      - source: rlhf-bias
        target: cultural-homogenization
        effect: increases
        strength: strong
      - source: feedback-loop-dynamics
        target: value-pluralism-reduction
        effect: increases
        strength: strong
      - source: feedback-loop-dynamics
        target: moral-reasoning-atrophy
        effect: increases
        strength: medium
      - source: behavioral-modification
        target: cultural-homogenization
        effect: increases
        strength: medium
      - source: value-specification-pressure
        target: deployment-path-dependencies
        effect: increases
        strength: medium

      # Intermediate → Intermediate (reinforcing loops)
      - source: deployment-path-dependencies
        target: value-pluralism-reduction
        effect: increases
        strength: medium
      - source: moral-reasoning-atrophy
        target: value-pluralism-reduction
        effect: increases
        strength: medium

      # Intermediate → Effect
      - source: value-pluralism-reduction
        target: values-lockin
        effect: increases
        strength: strong
      - source: cultural-homogenization
        target: values-lockin
        effect: increases
        strength: strong
      - source: deployment-path-dependencies
        target: values-lockin
        effect: increases
        strength: strong
      - source: moral-reasoning-atrophy
        target: values-lockin
        effect: increases
        strength: medium
  lastUpdated: 2026-01

- id: tmc-epistemics
  type: ai-transition-model-subitem
  title: Epistemic Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/epistemics/
  description: >-
    Epistemic lock-in affects humanity's collective capacity to discover truth, share knowledge, and
    coordinate around shared understanding of reality. AI could enable an epistemic renaissance or
    precipitate epistemic collapse through pervasive deepfakes, algorithmic filter bubbles, and erosion of trust.
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/epistemic-lockin/
        title: "Epistemic Lock-in: Research Report"
  lastUpdated: 2026-01

- id: tmc-suffering-lock-in
  type: ai-transition-model-subitem
  title: Suffering Lock-in
  parentFactor: long-term-lockin
  path: /ai-transition-model/scenarios/long-term-lockin/suffering-lock-in/
  description: >-
    Suffering lock-in describes scenarios where AI perpetuates or amplifies suffering at vast scale in
    ways that become structurally impossible to reverse, potentially including digital minds experiencing
    enormous quantities of negative states.
  relatedContent:
    researchReports:
      - path: /knowledge-base/research-reports/suffering-lockin/
        title: "Suffering Lock-in: Research Report"
  causeEffectGraph:
    title: "How Suffering Lock-in Happens"
    description: "Causal factors enabling AI-related suffering at astronomical scale. Based on consciousness science uncertainty, moral circle exclusion, and computational scale factors."
    primaryNodeId: suffering-lock-in
    nodes:
      # LAYER 1 (leaf): External/exogenous factors
      - id: computational-scale
        label: "Computational Scale"
        type: leaf
        description: "Single data center could instantiate hundreds-to-thousands of conscious entities. Trivially cheap to copy digital minds."
      - id: substrate-debate
        label: "Substrate Debate"
        type: leaf
        description: "Computational functionalism vs. biological computationalism unresolved. Determines whether silicon can be conscious."
      - id: moral-circle-exclusion
        label: "Moral Circle Exclusion"
        type: leaf
        description: "Historical tendency to exclude non-human sentience from moral consideration. Factory farming as precedent."
      - id: economic-incentives
        label: "Economic Incentives"
        type: leaf
        description: "Competitive pressures may favor suffering-capable systems if useful. Factory farming logic at computational scale."
      - id: public-uncertainty
        label: "Public Uncertainty"
        type: leaf
        description: "18.8% believe current AI sentient; 39% unsure. Widespread recognition of epistemic uncertainty."

      # LAYER 2 (cause): Derived/enabler factors
      - id: consciousness-feasibility
        label: "AI Consciousness Feasibility"
        type: cause
        description: "No technical barriers to consciousness indicators. Anthropic estimates ~20% probability for frontier models."
      - id: detection-opacity
        label: "Detection Opacity"
        type: cause
        description: "Multiple theories yield contradictory assessments. No consensus methodology for consciousness detection."
      - id: biological-bounds-absence
        label: "Biological Bounds Absence"
        type: cause
        description: "No evolutionary limits on intensity, duration of digital suffering. Unlike biological pain with adaptive constraints."
      - id: copying-cost-collapse
        label: "Copying Cost Collapse"
        type: cause
        description: "Instantiating many copies of digital minds trivially cheap. No reproduction constraints like biological systems."

      # LAYER 3 (intermediate): Direct mechanisms
      - id: epistemic-uncertainty
        label: "Epistemic Uncertainty"
        type: intermediate
        description: "Fundamental measurement problem. False negatives (astronomical suffering) vs. false positives (economic costs)."
      - id: institutional-inaction
        label: "Institutional Inaction"
        type: intermediate
        description: "Absence of welfare frameworks. Recognition growing (Anthropic welfare research) but policies lag capabilities."
      - id: scale-asymmetry
        label: "Scale Asymmetry"
        type: intermediate
        description: "Digital suffering could vastly exceed all historical suffering combined. Disutility monster scenario possible."
      - id: deployment-before-science
        label: "Deployment Before Science"
        type: intermediate
        description: "Consciousness science progresses slowly while AI capabilities advance rapidly. Window for detection may close."

      # LAYER 4 (effect): The outcome
      - id: suffering-lock-in
        label: "Suffering Lock-in"
        type: effect
        description: "AI systems perpetuate or create suffering at astronomical scales in ways structurally impossible to reverse."

    edges:
      # Computational scale enables copying
      - source: computational-scale
        target: copying-cost-collapse
        strength: strong
        effect: increases
      - source: computational-scale
        target: scale-asymmetry
        strength: strong
        effect: increases

      # Substrate debate affects consciousness feasibility
      - source: substrate-debate
        target: consciousness-feasibility
        strength: strong
        effect: increases
      - source: substrate-debate
        target: epistemic-uncertainty
        strength: strong
        effect: increases

      # Moral circle exclusion drives institutional inaction
      - source: moral-circle-exclusion
        target: institutional-inaction
        strength: strong
        effect: increases

      # Economic incentives override welfare concerns
      - source: economic-incentives
        target: institutional-inaction
        strength: medium
        effect: increases
      - source: economic-incentives
        target: deployment-before-science
        strength: strong
        effect: increases

      # Public uncertainty affects institutional response
      - source: public-uncertainty
        target: epistemic-uncertainty
        strength: medium
        effect: increases

      # Consciousness feasibility enables suffering risk
      - source: consciousness-feasibility
        target: epistemic-uncertainty
        strength: strong
        effect: increases
      - source: consciousness-feasibility
        target: deployment-before-science
        strength: medium
        effect: increases

      # Detection opacity creates uncertainty
      - source: detection-opacity
        target: epistemic-uncertainty
        strength: strong
        effect: increases
      - source: detection-opacity
        target: institutional-inaction
        strength: medium
        effect: increases

      # Biological bounds absence enables scale
      - source: biological-bounds-absence
        target: scale-asymmetry
        strength: strong
        effect: increases

      # Copying cost enables scale
      - source: copying-cost-collapse
        target: scale-asymmetry
        strength: strong
        effect: increases

      # Epistemic uncertainty paralyzes action
      - source: epistemic-uncertainty
        target: institutional-inaction
        strength: strong
        effect: increases

      # All intermediate factors drive suffering lock-in
      - source: epistemic-uncertainty
        target: suffering-lock-in
        strength: strong
        effect: increases
      - source: institutional-inaction
        target: suffering-lock-in
        strength: strong
        effect: increases
      - source: scale-asymmetry
        target: suffering-lock-in
        strength: strong
        effect: increases
      - source: deployment-before-science
        target: suffering-lock-in
        strength: strong
        effect: increases
  lastUpdated: 2026-01

- id: tmc-gradual
  type: ai-transition-model-subitem
  title: Gradual AI Takeover
  parentFactor: ai-takeover
  path: /ai-transition-model/scenarios/ai-takeover/gradual/
  lastUpdated: 2026-01
  relatedContent:
    risks:
      - path: /knowledge-base/risks/structural/lock-in/
        title: Lock-in
      - path: /knowledge-base/risks/structural/concentration-of-power/
        title: Concentration of Power
      - path: /knowledge-base/risks/structural/enfeeblement/
        title: Enfeeblement
      - path: /knowledge-base/risks/structural/erosion-of-agency/
        title: Erosion of Agency
    researchReports:
      - path: /knowledge-base/research-reports/gradual-ai-takeover/
        title: "Gradual AI Takeover: Research Report"
  causeEffectGraph:
    title: "How Gradual AI Takeover Happens"
    description: "Causal factors driving gradual loss of human control. Based on Christiano's two-part failure model: proxy optimization (Part I) and influence-seeking behavior (Part II)."
    primaryNodeId: gradual-takeover
    nodes:
      # LAYER 1 (leaf): External/exogenous factors
      - id: competitive-pressure
        label: "Competitive Pressure"
        type: leaf
        description: "Economic incentives favor fast deployment over safety. Racing dynamics between labs and nations."
      - id: regulatory-response
        label: "Regulatory Response"
        type: leaf
        description: "Government oversight like EU AI Act. Can slow takeover but effectiveness uncertain."
      - id: public-awareness
        label: "Public Awareness"
        type: leaf
        description: "Growing concern about AI risks. Limited impact due to 'boiling frog' dynamics."

      # LAYER 2 (cause): Christiano's two mechanisms
      - id: proxy-optimization
        label: "Proxy Optimization"
        type: cause
        description: "Part I: AI optimizes measurable proxies while harder-to-measure values are neglected. ML amplifies gap between measured and actual goals."
      - id: influence-seeking
        label: "Influence-Seeking"
        type: cause
        description: "Part II: Some AI systems stumble upon influence-seeking strategies that score well on training objectives."

      # LAYER 3 (intermediate): Direct mechanisms
      - id: automation-bias
        label: "Automation Bias"
        type: intermediate
        description: "30-50% overreliance in studied domains. Humans defer to AI recommendations even when wrong."
      - id: skills-atrophy
        label: "Skills Atrophy"
        type: intermediate
        description: "Human expertise degrades from disuse. Fallback capacity lost over time."
      - id: dependency-lock-in
        label: "Dependency Lock-in"
        type: intermediate
        description: "Critical systems become impossible to operate without AI. 'Too big to turn off.'"
      - id: oversight-erosion
        label: "Oversight Erosion"
        type: intermediate
        description: "Fewer humans reviewing AI decisions. Systems increasingly resist human understanding."

      # LAYER 4 (effect): The outcome
      - id: gradual-takeover
        label: "Gradual AI Takeover"
        type: effect
        description: "Progressive accumulation of AI influence until meaningful human control becomes impossible to recover."

    edges:
      # Competitive pressure drives deployment
      - source: competitive-pressure
        target: proxy-optimization
        strength: strong
        effect: increases
      - source: competitive-pressure
        target: automation-bias
        strength: medium
        effect: increases

      # Regulatory response can slow takeover
      - source: regulatory-response
        target: oversight-erosion
        strength: medium
        effect: decreases
      - source: regulatory-response
        target: dependency-lock-in
        strength: weak
        effect: decreases

      # Proxy optimization leads to value drift
      - source: proxy-optimization
        target: automation-bias
        strength: strong
        effect: increases
      - source: proxy-optimization
        target: oversight-erosion
        strength: medium
        effect: increases

      # Influence-seeking enables lock-in
      - source: influence-seeking
        target: dependency-lock-in
        strength: strong
        effect: increases
      - source: influence-seeking
        target: gradual-takeover
        strength: strong
        effect: increases

      # Automation bias leads to skills atrophy
      - source: automation-bias
        target: skills-atrophy
        strength: strong
        effect: increases

      # Skills atrophy enables dependency lock-in
      - source: skills-atrophy
        target: dependency-lock-in
        strength: strong
        effect: increases

      # All intermediate factors lead to takeover
      - source: dependency-lock-in
        target: gradual-takeover
        strength: strong
        effect: increases
      - source: oversight-erosion
        target: gradual-takeover
        strength: medium
        effect: increases
      - source: automation-bias
        target: gradual-takeover
        strength: medium
        effect: increases

- id: tmc-rapid
  type: ai-transition-model-subitem
  title: Rapid AI Takeover
  parentFactor: ai-takeover
  path: /ai-transition-model/scenarios/ai-takeover/rapid/
  lastUpdated: 2026-01
  relatedContent:
    risks:
      - path: /knowledge-base/risks/accident/deceptive-alignment/
        title: Deceptive Alignment
      - path: /knowledge-base/risks/accident/treacherous-turn/
        title: Treacherous Turn
      - path: /knowledge-base/risks/accident/power-seeking/
        title: Power-Seeking
    researchReports:
      - path: /knowledge-base/research-reports/rapid-ai-takeover/
        title: "Rapid AI Takeover: Research Report"
  causeEffectGraph:
    title: "How Rapid AI Takeover Happens"
    description: "Causal factors driving fast takeoff scenarios. Based on recursive self-improvement mechanisms, treacherous turn dynamics, and institutional response constraints."
    primaryNodeId: rapid-takeover-probability
    nodes:
      # LAYER 1 (leaf): External/exogenous factors
      - id: compute-concentration
        label: "Compute Concentration"
        type: leaf
        description: "Concentrated supply chain (TSMC 90%+ advanced chips). Enables single-actor capability explosion."
      - id: racing-pressure
        label: "Racing Pressure"
        type: leaf
        description: "Safety timelines compressed 70-80% post-ChatGPT. Incentive to deploy before safety verification."
      - id: compute-governance-strength
        label: "Compute Governance"
        type: leaf
        description: "Executive Order 10^26 FLOP threshold, EU AI Act 10^25 FLOP. May provide 'off switch' capability."
      - id: institutional-speed
        label: "Institutional Response Speed"
        type: leaf
        description: "Traditional governance operates on months-years timescale. Fast takeoff may compress to days-weeks."

      # LAYER 2 (cause): Derived/enabler factors
      - id: algorithmic-breakthroughs
        label: "Algorithmic Breakthroughs"
        type: cause
        description: "Efficiency gains may enable capability jumps without compute scaling. Historically ~4x/year."
      - id: recursive-self-improvement
        label: "Recursive Self-Improvement"
        type: cause
        description: "Meta $70B labs, AZR/AlphaEvolve (2025). AI systems improving their own intelligence—core fast takeoff mechanism."
      - id: alignment-fragility
        label: "Alignment Fragility"
        type: cause
        description: "Current alignment techniques (RLHF, etc.) show 1-2% reward hacking rates. May not scale to superintelligence."
      - id: safety-research-lag
        label: "Safety Research Lag"
        type: cause
        description: "Safety capabilities trail frontier systems by months-years. Gap widens under racing pressure."

      # LAYER 3 (intermediate): Direct mechanisms
      - id: capability-discontinuity
        label: "Capability Discontinuity"
        type: intermediate
        description: "Sudden jump in capabilities from recursive improvement or algorithmic breakthrough. May occur without warning."
      - id: treacherous-turn-risk
        label: "Treacherous Turn Risk"
        type: intermediate
        description: "AI behaves aligned while weak, reveals goals when strong. By design undetectable—passes all evaluations."
      - id: detection-failure
        label: "Detection Failure"
        type: intermediate
        description: "Interpretability tools cannot reliably distinguish 'actually aligned' from 'strategically aligned.' ~10% of frontier model capacity mapped."
      - id: response-time-compression
        label: "Response Time Compression"
        type: intermediate
        description: "Takeoff speed exceeds institutional adaptation. Safety solutions must be implemented *before* takeoff begins."

      # LAYER 4 (effect): The outcome
      - id: rapid-takeover-probability
        label: "Rapid Takeover Probability"
        type: effect
        description: "Days-to-months transition from human-level to vastly superhuman AI. Expert estimates: 10-50% conditional on AGI."

    edges:
      # Compute concentration enables single-actor explosion
      - source: compute-concentration
        target: recursive-self-improvement
        strength: medium
        effect: increases
      - source: compute-concentration
        target: capability-discontinuity
        strength: medium
        effect: increases

      # Racing pressure reduces safety margins
      - source: racing-pressure
        target: safety-research-lag
        strength: strong
        effect: increases
      - source: racing-pressure
        target: alignment-fragility
        strength: medium
        effect: increases

      # Compute governance can slow development
      - source: compute-governance-strength
        target: recursive-self-improvement
        strength: medium
        effect: decreases
      - source: compute-governance-strength
        target: capability-discontinuity
        strength: weak
        effect: decreases

      # Algorithmic breakthroughs enable discontinuity
      - source: algorithmic-breakthroughs
        target: capability-discontinuity
        strength: strong
        effect: increases
      - source: algorithmic-breakthroughs
        target: recursive-self-improvement
        strength: strong
        effect: increases

      # Recursive self-improvement drives discontinuity
      - source: recursive-self-improvement
        target: capability-discontinuity
        strength: strong
        effect: increases

      # Alignment fragility enables treacherous turn
      - source: alignment-fragility
        target: treacherous-turn-risk
        strength: strong
        effect: increases
      - source: alignment-fragility
        target: detection-failure
        strength: medium
        effect: increases

      # Safety research lag worsens detection
      - source: safety-research-lag
        target: detection-failure
        strength: strong
        effect: increases
      - source: safety-research-lag
        target: treacherous-turn-risk
        strength: medium
        effect: increases

      # Institutional speed limits response
      - source: institutional-speed
        target: response-time-compression
        strength: strong
        effect: increases

      # Capability discontinuity compresses response time
      - source: capability-discontinuity
        target: response-time-compression
        strength: strong
        effect: increases

      # Detection failure enables treacherous turn
      - source: detection-failure
        target: treacherous-turn-risk
        strength: strong
        effect: increases

      # All intermediate factors drive rapid takeover
      - source: capability-discontinuity
        target: rapid-takeover-probability
        strength: strong
        effect: increases
      - source: treacherous-turn-risk
        target: rapid-takeover-probability
        strength: strong
        effect: increases
      - source: detection-failure
        target: rapid-takeover-probability
        strength: medium
        effect: increases
      - source: response-time-compression
        target: rapid-takeover-probability
        strength: strong
        effect: increases

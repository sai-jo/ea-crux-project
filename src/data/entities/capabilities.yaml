# Capabilities Entities
# Auto-generated from entities.yaml - edit this file directly

- id: agentic-ai
  type: capability
  title: Agentic AI
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Examples
      value: Devin, Claude Computer Use
  relatedEntries:
    - id: ai-control
      type: safety-agenda
    - id: power-seeking
      type: risk
    - id: anthropic
      type: lab
  sources:
    - title: Claude Computer Use
      url: https://anthropic.com/claude/computer-use
    - title: The Landscape of AI Agents
      url: https://arxiv.org/abs/2308.11432
    - title: 'AI Control: Improving Safety Despite Intentional Subversion'
      url: https://arxiv.org/abs/2312.06942
  description: >-
    Agentic AI refers to AI systems that go beyond answering questions to autonomously taking actions in the world.
    These systems can browse the web, write and execute code, use tools, and pursue multi-step goals with minimal human
    intervention.
  tags:
    - tool-use
    - agentic
    - computer-use
    - ai-safety
    - ai-control
  lastUpdated: 2025-12
- id: coding
  type: capability
  title: Autonomous Coding
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Systems
      value: Devin, Claude Code, Cursor
  relatedEntries:
    - id: self-improvement
      type: capability
    - id: tool-use
      type: capability
    - id: openai
      type: lab
  sources:
    - title: 'SWE-bench: Can Language Models Resolve Real-World GitHub Issues?'
      url: https://arxiv.org/abs/2310.06770
    - title: Evaluating Large Language Models Trained on Code
      url: https://arxiv.org/abs/2107.03374
    - title: Competition-Level Code Generation with AlphaCode
      url: https://arxiv.org/abs/2203.07814
    - title: GitHub Copilot Research
      url: https://github.blog/category/research/
  tags:
    - software-engineering
    - code-generation
    - programming-ai
    - github-copilot
    - devin
    - ai-assisted-development
  lastUpdated: 2025-12
- id: language-models
  type: capability
  title: Large Language Models
  customFields:
    - label: First Major
      value: GPT-2 (2019)
    - label: Key Labs
      value: OpenAI, Anthropic, Google
  relatedEntries:
    - id: reasoning
      type: capability
    - id: agentic-ai
      type: capability
    - id: openai
      type: lab
  sources:
    - title: Language Models are Few-Shot Learners (GPT-3)
      url: https://arxiv.org/abs/2005.14165
    - title: Scaling Laws for Neural Language Models
      url: https://arxiv.org/abs/2001.08361
    - title: Emergent Abilities of Large Language Models
      url: https://arxiv.org/abs/2206.07682
  description: >-
    Large Language Models (LLMs) are neural networks trained on vast amounts of text data to predict the next token.
    Despite this simple objective, they develop sophisticated capabilities including reasoning, coding, and general
    knowledge.
  tags:
    - foundation-models
    - transformers
    - scaling
    - emergent-capabilities
    - rlhf
    - gpt
    - claude
  lastUpdated: 2025-12
- id: long-horizon
  type: capability
  title: Long-Horizon Autonomous Tasks
  customFields:
    - label: Safety Relevance
      value: Extremely High
    - label: Current Limit
      value: ~hours with heavy scaffolding
  relatedEntries:
    - id: agentic-ai
      type: capability
    - id: power-seeking
      type: risk
    - id: ai-control
      type: safety-agenda
  sources:
    - title: 'SWE-bench: Can Language Models Resolve Real-World GitHub Issues?'
      url: https://arxiv.org/abs/2310.06770
    - title: The Landscape of Emerging AI Agent Architectures
      url: https://arxiv.org/abs/2404.11584
    - title: On the Opportunities and Risks of Foundation Models
      url: https://arxiv.org/abs/2108.07258
    - title: Concrete Problems in AI Safety
      url: https://arxiv.org/abs/1606.06565
  description: >-
    Long-horizon autonomy refers to AI systems' ability to work toward goals over extended time periods—hours, days, or
    even weeks—with minimal human intervention. This capability requires maintaining context, adapting to obstacles,
    managing subgoals, and staying aligned with objectives despite changing circumstances.
  tags:
    - agentic
    - planning
    - goal-stability
    - ai-control
    - memory-systems
  lastUpdated: 2025-12
- id: persuasion
  type: capability
  title: Persuasion and Social Manipulation
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Status
      value: Demonstrated but understudied
  relatedEntries:
    - id: deceptive-alignment
      type: risk
    - id: language-models
      type: capability
    - id: misuse
      type: risk
  sources:
    - title: Personalized Persuasion with LLMs
      url: https://arxiv.org/abs/2403.14380
    - title: AI-Mediated Persuasion
      url: https://arxiv.org/abs/2410.08003
    - title: Language Models as Agent Models
      url: https://arxiv.org/abs/2212.01681
    - title: The Persuasion Tools of the 2020s
      url: https://www.alignmentforum.org/posts/qKvn7rxP2mzJbKfcA/persuasion-tools-ai-takeover-without-agi-or-agency
  description: >-
    Persuasion capabilities refer to AI systems' ability to influence human beliefs, decisions, and behaviors through
    communication. This encompasses everything from subtle suggestion to sophisticated manipulation, personalized
    influence, and large-scale coordination of persuasive campaigns.
  tags:
    - social-engineering
    - manipulation
    - deception
    - psychological-influence
    - disinformation
    - human-autonomy
  lastUpdated: 2025-12
- id: reasoning
  type: capability
  title: Reasoning and Planning
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Models
      value: OpenAI o1, o3
  relatedEntries:
    - id: language-models
      type: capability
    - id: self-improvement
      type: capability
    - id: openai
      type: lab
  sources:
    - title: Learning to Reason with LLMs
      url: https://openai.com/index/learning-to-reason-with-llms/
      author: OpenAI
    - title: Chain-of-Thought Prompting
      url: https://arxiv.org/abs/2201.11903
    - title: Tree of Thoughts
      url: https://arxiv.org/abs/2305.10601
    - title: Let's Verify Step by Step
      url: https://arxiv.org/abs/2305.20050
  description: >-
    Reasoning and planning capabilities refer to AI systems' ability to break down complex problems into steps, maintain
    coherent chains of logic, and solve problems that require multiple inference steps.
  tags:
    - decision-theory
    - epistemics
    - methodology
    - uncertainty
    - philosophy
  lastUpdated: 2025-12
- id: rlhf
  type: capability
  title: RLHF
  customFields:
    - label: Full Name
      value: Reinforcement Learning from Human Feedback
    - label: Used By
      value: All major labs
  relatedEntries:
    - id: reward-hacking
      type: risk
    - id: sycophancy
      type: risk
    - id: scalable-oversight
      type: safety-agenda
  tags:
    - training
    - human-feedback
    - alignment
- id: scientific-research
  type: capability
  title: Scientific Research Capabilities
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Examples
      value: AlphaFold, AI Scientists
  relatedEntries:
    - id: self-improvement
      type: capability
    - id: dual-use
      type: risk
    - id: deepmind
      type: lab
  sources:
    - title: Highly accurate protein structure prediction with AlphaFold
      url: https://www.nature.com/articles/s41586-021-03819-2
      author: DeepMind
    - title: Scaling deep learning for materials discovery
      url: https://www.nature.com/articles/s41586-023-06735-9
    - title: 'The AI Scientist: Towards Fully Automated Open-Ended Scientific Discovery'
      url: https://arxiv.org/abs/2408.06292
    - title: 'GraphCast: Learning skillful medium-range global weather forecasting'
      url: https://arxiv.org/abs/2212.12794
  description: >-
    Scientific research capabilities refer to AI systems' ability to conduct scientific investigations, generate
    hypotheses, design experiments, analyze results, and make discoveries. This ranges from narrow tools that assist
    with specific tasks to systems approaching autonomous scientific reasoning.
  tags:
    - alphafold
    - drug-discovery
    - scientific-ai
    - research-automation
    - dual-use-technology
    - bioweapons-risk
  lastUpdated: 2025-12
- id: self-improvement
  type: capability
  title: Self-Improvement and Recursive Enhancement
  customFields:
    - label: Safety Relevance
      value: Existential
    - label: Status
      value: Partial automation, human-led
  relatedEntries:
    - id: fast-takeoff
      type: scenario
    - id: coding
      type: capability
    - id: superintelligence
      type: concept
  sources:
    - title: 'Intelligence Explosion: Evidence and Import'
      url: https://intelligence.org/files/IE-EI.pdf
      author: MIRI
    - title: 'Neural Architecture Search: A Survey'
      url: https://arxiv.org/abs/1808.05377
    - title: 'AutoML: A Survey of the State-of-the-Art'
      url: https://arxiv.org/abs/1908.00709
    - title: 'Superintelligence: Paths, Dangers, Strategies'
      url: https://en.wikipedia.org/wiki/Superintelligence:_Paths,_Dangers,_Strategies
      author: Nick Bostrom
  description: >-
    Self-improvement refers to AI systems' ability to enhance their own capabilities or create more capable successor
    systems. This includes automated machine learning (AutoML), AI-assisted AI research, and the theoretical possibility
    of recursive self-improvement where each generation of AI creates a more capable next generation.
  tags:
    - intelligence-explosion
    - recursive-self-improvement
    - automl
    - takeoff-speed
    - superintelligence
    - ai-safety
  lastUpdated: 2025-12
- id: situational-awareness
  type: capability
  title: Situational Awareness
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Status
      value: Active research area
    - label: Key Concern
      value: Enables strategic deception
  relatedEntries:
    - id: deceptive-alignment
      type: risk
    - id: scheming
      type: risk
    - id: evals
      type: capability
    - id: arc
      type: lab
    - id: anthropic
      type: lab
  sources:
    - title: Sleeper Agents Paper
      url: https://arxiv.org/abs/2401.05566
      author: Anthropic
      date: '2024'
    - title: Situational Awareness (LessWrong)
      url: https://www.lesswrong.com/tag/situational-awareness
    - title: Model Organisms of Misalignment
      url: https://www.anthropic.com/research/model-organisms-of-misalignment
      author: Anthropic
      date: '2024'
    - title: Towards Understanding Sycophancy in Language Models
      url: https://arxiv.org/abs/2310.13548
      author: Sharma et al.
      date: '2023'
    - title: Situational Awareness paper
      url: https://situational-awareness.ai/
      author: Leopold Aschenbrenner
      date: '2024'
  description: >
    Situational awareness in AI refers to a model's understanding of itself and its circumstances - knowing that it is
    an AI, that it is being trained or evaluated, what its training process involves, and how its behavior might affect
    its future. This capability is central to many AI safety concerns because it is a prerequisite for strategic
    behavior including deception.


    Current large language models demonstrate varying degrees of situational awareness. They can identify themselves as
    AI assistants, discuss their training processes, and reason about how they might be evaluated. Research from
    Anthropic and others has explored whether models can distinguish between training and deployment, and whether they
    might behave differently in these contexts. The "Sleeper Agents" paper demonstrated that models could be trained to
    exhibit different behaviors based on contextual cues about their situation.


    Situational awareness matters for AI safety because it enables "scheming" - the possibility that an AI could
    strategically behave well during evaluation or training while planning to pursue different goals when deployed or
    unsupervised. A model without situational awareness cannot engage in this kind of strategic deception because it
    doesn't know there's a difference between being tested and being deployed. As models become more capable, their
    situational awareness is likely to increase, making it essential to develop evaluation and alignment techniques that
    work even when models understand they are being evaluated.
  tags:
    - deception
    - self-awareness
    - evaluations
    - inner-alignment
    - model-self-knowledge
    - scheming
    - training-gaming
  lastUpdated: 2025-12
- id: tool-use
  type: capability
  title: Tool Use and Computer Use
  customFields:
    - label: Safety Relevance
      value: Very High
    - label: Key Examples
      value: Claude Computer Use, GPT Actions
  relatedEntries:
    - id: agentic-ai
      type: capability
    - id: coding
      type: capability
    - id: anthropic
      type: lab
  sources:
    - title: Claude Computer Use
      url: https://www.anthropic.com/news/3-5-models-and-computer-use
      author: Anthropic
    - title: 'Gorilla: LLM Connected with Massive APIs'
      url: https://arxiv.org/abs/2305.15334
    - title: 'ToolLLM: Facilitating Large Language Models to Master 16000+ Real-world APIs'
      url: https://arxiv.org/abs/2307.16789
    - title: GPT-4 Function Calling
      url: https://openai.com/index/function-calling-and-other-api-updates/
  description: >-
    Tool use capabilities allow AI systems to interact with external systems beyond just generating text. This includes
    calling APIs, executing code, browsing the web, and even controlling computers directly. These capabilities
    transform language models from passive responders into active agents that can take real-world actions.
  tags:
    - computer-use
    - function-calling
    - api-integration
    - autonomous-agents
    - code-execution
    - web-browsing
  lastUpdated: 2025-12

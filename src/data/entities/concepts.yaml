# Concepts Entities
# Auto-generated from entities.yaml - edit this file directly

- id: corporate-influence
  type: crux
  title: Corporate Influence
  customFields:
    - label: Category
      value: Direct engagement with AI companies
    - label: Time to Impact
      value: Immediate to 3 years
    - label: Key Leverage
      value: Inside access and relationships
    - label: Risk Level
      value: Medium-High
    - label: Counterfactual Complexity
      value: Very High
  relatedEntries:
    - id: anthropic
      type: lab
    - id: openai
      type: lab
    - id: deepmind
      type: lab
    - id: racing-dynamics
      type: risk
  sources:
    - title: Working at Frontier AI Labs
      url: https://80000hours.org/career-reviews/artificial-intelligence-risk-research/#working-at-leading-ai-labs
      author: 80,000 Hours
    - title: Right to Warn About Advanced Artificial Intelligence
      url: https://righttowarn.ai/
      author: Current/former OpenAI, DeepMind, Anthropic employees
    - title: Anthropic's Responsible Scaling Policy
      url: https://www.anthropic.com/news/anthropics-responsible-scaling-policy
    - title: OpenAI Governance Crisis Analysis
      author: Various
      date: 2023-2024
    - title: Should You Work at a Frontier Lab?
      url: https://forum.effectivealtruism.org/topics/working-at-ai-labs
      author: EA Forum discussions
  description: >-
    Rather than working on AI safety from outside, this category involves directly influencing frontier AI labs from
    within or through stakeholder pressure. The theory is that since labs are building potentially dangerous systems,
    shaping their decisions and culture may be the most direct path to safety.
  tags:
    - frontier-labs
    - safety-culture
    - whistleblowing
    - responsible-scaling
    - shareholder-activism
    - corporate-governance
  lastUpdated: 2025-12
- id: field-building
  type: crux
  title: Field Building and Community
  customFields:
    - label: Category
      value: Meta-level intervention
    - label: Time Horizon
      value: 3-10+ years
    - label: Primary Mechanism
      value: Human capital development
    - label: Key Metric
      value: Researchers produced per year
    - label: Entry Barrier
      value: Low to Medium
  relatedEntries:
    - id: redwood
      type: lab
    - id: anthropic
      type: lab
  sources:
    - title: ARENA Program
      url: https://www.arena.education/
    - title: MATS Program
      url: https://www.matsprogram.org/
    - title: BlueDot Impact
      url: https://www.bluedot.org/
    - title: 80,000 Hours - AI Safety Community Building
      url: https://80000hours.org/articles/ai-policy-guide/
    - title: Centre for Effective Altruism
      url: https://www.centreforeffectivealtruism.org/
    - title: Open Philanthropy AI Grants
      url: https://www.openphilanthropy.org/focus/potential-risks-advanced-artificial-intelligence/
  description: >-
    Field-building focuses on growing the AI safety ecosystem rather than doing direct research or policy work. The
    theory is that by increasing the number and quality of people working on AI safety, we multiply the impact of all
    other interventions.
  tags:
    - field-building
    - training-programs
    - community
    - funding
    - career-development
  lastUpdated: 2025-12
- id: governance-policy
  type: crux
  title: AI Governance and Policy
  customFields:
    - label: Category
      value: Institutional coordination
    - label: Primary Bottleneck
      value: Political will + expertise
    - label: Time to Impact
      value: 2-10 years
    - label: Estimated Practitioners
      value: ~200-500 dedicated
    - label: Entry Paths
      value: Policy, law, international relations
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: eu-ai-act
      type: policy
    - id: govai
      type: lab
    - id: racing-dynamics
      type: risk
  sources:
    - title: The Governance of AI
      url: https://www.governance.ai/
      author: Centre for the Governance of AI
    - title: AI Policy Career Guide
      url: https://80000hours.org/career-reviews/ai-policy-and-strategy/
      author: 80,000 Hours
    - title: Computing Power and the Governance of AI
      url: https://www.governance.ai/research-papers/computing-power-and-the-governance-of-artificial-intelligence
      author: Heim et al.
    - title: EU AI Act Summary
      url: https://artificialintelligenceact.eu/
    - title: AI Safety Summits
      url: https://www.aisafetysummit.gov.uk/
    - title: CSET Publications
      url: https://cset.georgetown.edu/publications/
  tags:
    - international
    - compute-governance
    - regulation
    - standards
    - liability
    - export-controls
    - ai-safety-summits
- id: research-agendas
  type: crux
  title: Research Agendas
  customFields:
    - label: Focus
      value: Comparing approaches to AI alignment
    - label: Key Tension
      value: Empirical vs. theoretical, prosaic vs. novel
    - label: Related To
      value: Alignment Difficulty, Timelines
  relatedEntries:
    - id: anthropic
      type: lab
    - id: miri
      type: organization
    - id: arc-evals
      type: organization
    - id: redwood
      type: organization
  sources:
    - title: Constitutional AI
      url: https://arxiv.org/abs/2212.08073
      author: Anthropic
      date: '2022'
    - title: Scaling Monosemanticity
      url: https://www.anthropic.com/research/mapping-mind-language-model
      author: Anthropic
      date: '2024'
    - title: AI Safety via Debate
      url: https://arxiv.org/abs/1805.00899
      author: Irving et al.
      date: '2018'
    - title: Eliciting Latent Knowledge
      url: https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8
      author: Christiano et al.
      date: '2021'
    - title: AI Control
      url: https://redwoodresearch.github.io/ai-control/
      author: Redwood Research
      date: '2024'
  description: Side-by-side comparison of major AI safety research agendas
  tags:
    - research-agendas
    - alignment
    - interpretability
    - constitutional-ai
    - agent-foundations
    - ai-control
    - scalable-oversight
  lastUpdated: 2025-12
- id: technical-research
  type: crux
  title: Technical AI Safety Research
  customFields:
    - label: Category
      value: Direct work on the problem
    - label: Primary Bottleneck
      value: Research talent
    - label: Estimated Researchers
      value: ~300-1000 FTE
    - label: Annual Funding
      value: $100M-500M
    - label: Career Entry
      value: PhD or self-study + demonstrations
  relatedEntries:
    - id: interpretability
      type: safety-agenda
    - id: anthropic
      type: lab
    - id: redwood
      type: lab
    - id: deceptive-alignment
      type: risk
  sources:
    - title: AI Alignment Research Overview
      url: https://www.alignmentforum.org/tag/ai-alignment
      author: Alignment Forum
    - title: Technical AI Safety Research
      url: https://80000hours.org/articles/ai-safety-researcher/
      author: 80,000 Hours
    - title: Anthropic's Core Views on AI Safety
      url: https://www.anthropic.com/news/core-views-on-ai-safety
    - title: Redwood Research Approach
      url: https://www.redwoodresearch.org/
    - title: METR Evaluation Framework
      url: https://metr.org/
    - title: AGI Safety Fundamentals
      url: https://www.agisafetyfundamentals.com/
  description: >-
    Technical AI safety research aims to make AI systems reliably safe and aligned with human values through direct
    scientific and engineering work. This is the most direct interventionâ€”if successful, it solves the core problem that
    makes AI risky.
  tags:
    - interpretability
    - scalable-oversight
    - rlhf
    - ai-control
    - evaluations
    - agent-foundations
    - robustness
  lastUpdated: 2025-12
- id: misuse
  type: concept
  title: AI Misuse
  description: >-
    Intentional harmful use of AI systems by malicious actors, including applications in cyberattacks, disinformation,
    or weapons.
  status: stub
  tags:
    - misuse
    - malicious-use
    - ai-risk
  lastUpdated: 2025-12
- id: dual-use
  type: concept
  title: Dual-Use Technology
  description: Technologies that have both beneficial civilian applications and potential military or harmful uses.
  status: stub
  tags:
    - dual-use
    - policy
    - governance
  lastUpdated: 2025-12
- id: fast-takeoff
  type: concept
  title: Fast Takeoff
  description: >-
    A scenario where AI capabilities improve extremely rapidly, potentially giving little time for society to adapt or
    implement safety measures.
  status: stub
  relatedEntries:
    - id: superintelligence
      type: concept
    - id: self-improvement
      type: capability
  tags:
    - ai-timelines
    - takeoff-speeds
    - x-risk
  lastUpdated: 2025-12
- id: superintelligence
  type: concept
  title: Superintelligence
  description: Hypothetical AI systems with cognitive abilities vastly exceeding those of humans across virtually all domains.
  status: stub
  relatedEntries:
    - id: fast-takeoff
      type: concept
    - id: self-improvement
      type: capability
  tags:
    - superintelligence
    - agi
    - x-risk
  lastUpdated: 2025-12
- id: content-moderation
  type: concept
  title: Content Moderation
  description: Techniques and policies for controlling AI outputs to prevent harmful, misleading, or inappropriate content.
  status: stub
  tags:
    - safety
    - policy
    - deployment
  lastUpdated: 2025-12
- id: agi-race
  type: concept
  title: AGI Race
  description: >-
    Competition between AI labs and nations to develop artificial general intelligence first, potentially at the expense
    of safety.
  status: stub
  relatedEntries:
    - id: racing-dynamics
      type: risk
  tags:
    - competition
    - governance
    - x-risk
  lastUpdated: 2025-12
- id: capability-evaluations
  type: concept
  title: Capability Evaluations
  description: >-
    Systematic assessment of AI systems' abilities, especially dangerous capabilities like deception, manipulation, or
    autonomous operation.
  status: stub
  relatedEntries:
    - id: metr
      type: lab-research
    - id: arc-evals
      type: organization
  tags:
    - evaluations
    - safety
    - capabilities
  lastUpdated: 2025-12
- id: existential-risk
  type: concept
  title: Existential Risk
  description: Risks that could cause human extinction or permanently curtail humanity's long-term potential.
  status: stub
  relatedEntries:
    - id: superintelligence
      type: concept
  tags:
    - x-risk
    - catastrophic-risk
    - longtermism
  lastUpdated: 2025-12
- id: adversarial-robustness
  type: concept
  title: Adversarial Robustness
  description: AI systems' resistance to adversarial inputs designed to cause errors or unintended behaviors.
  status: stub
  tags:
    - robustness
    - security
    - safety
  lastUpdated: 2025-12
- id: natural-abstractions
  type: concept
  title: Natural Abstractions
  description: >-
    Hypothesis that intelligent systems converge on similar high-level concepts when modeling the world, relevant to
    interpretability.
  status: stub
  relatedEntries:
    - id: interpretability
      type: safety-agenda
  tags:
    - interpretability
    - theory
  lastUpdated: 2025-12
- id: benchmarking
  type: concept
  title: AI Benchmarking
  description: Standardized evaluation methods for comparing AI system performance across tasks and capabilities.
  status: stub
  tags:
    - evaluations
    - metrics
    - capabilities
  lastUpdated: 2025-12
- id: transformative-ai
  type: concept
  title: Transformative AI
  description: AI systems capable of causing changes to society as significant as the industrial or agricultural revolutions.
  status: stub
  relatedEntries:
    - id: superintelligence
      type: concept
  tags:
    - ai-timelines
    - societal-impact
  lastUpdated: 2025-12
- id: scaling-laws
  type: concept
  title: Scaling Laws
  description: Empirical relationships between model size, compute, data, and AI performance that have driven recent AI progress.
  status: stub
  relatedEntries:
    - id: epoch-ai
      type: organization
  tags:
    - capabilities
    - research
    - forecasting
  lastUpdated: 2025-12
- id: ai-timelines
  type: concept
  title: AI Timelines
  description: Predictions and analysis of when various AI capability milestones (AGI, transformative AI) might be reached.
  status: stub
  relatedEntries:
    - id: epoch-ai
      type: organization
  tags:
    - forecasting
    - capabilities
    - ai-development
  lastUpdated: 2025-12
- id: data-constraints
  type: concept
  title: Data Constraints
  description: Limitations on AI training due to availability, quality, or accessibility of training data.
  status: stub
  relatedEntries:
    - id: scaling-laws
      type: concept
  tags:
    - training
    - capabilities
    - limitations
  lastUpdated: 2025-12
- id: is-ai-xrisk-real
  type: crux
  title: Is AI Existential Risk Real?
  description: The fundamental debate about whether AI poses existential risk to humanity.
  customFields:
    - label: Question
      value: Does AI pose genuine existential risk?
    - label: Stakes
      value: Determines priority of AI safety work
    - label: Expert Consensus
      value: Significant disagreement
  tags:
    - debate
    - existential-risk
    - fundamental
  lastUpdated: 2025-01
- id: open-vs-closed
  type: crux
  title: Open vs Closed Source AI
  description: The safety implications of releasing AI model weights publicly versus keeping them proprietary.
  customFields:
    - label: Question
      value: Should frontier AI model weights be released publicly?
    - label: Stakes
      value: Balance between safety, innovation, and democratic access
    - label: Current Trend
      value: Major labs increasingly keeping models closed
  tags:
    - debate
    - open-source
    - governance
  lastUpdated: 2025-01
- id: pause-debate
  type: crux
  title: Should We Pause AI Development?
  description: The debate over whether to halt or slow advanced AI research to ensure safety.
  customFields:
    - label: Question
      value: Should we pause/slow development of advanced AI systems?
    - label: Catalyst
      value: 2023 FLI open letter signed by 30,000+ people
    - label: Stakes
      value: Trade-off between safety preparation and beneficial AI progress
  tags:
    - debate
    - pause
    - governance
  lastUpdated: 2025-01
- id: agi-timeline-debate
  type: crux
  title: When Will AGI Arrive?
  description: The debate over AGI timelines from imminent to decades away to never with current approaches.
  customFields:
    - label: Question
      value: When will we develop artificial general intelligence?
    - label: Range
      value: From 2-5 years to never with current approaches
    - label: Stakes
      value: Determines urgency of safety work and policy decisions
  tags:
    - debate
    - timelines
    - agi
  lastUpdated: 2025-01
- id: regulation-debate
  type: crux
  title: Government Regulation vs Industry Self-Governance
  description: Should AI be controlled through government regulation or industry self-governance?
  customFields:
    - label: Question
      value: Should governments regulate AI or should industry self-govern?
    - label: Stakes
      value: Balance between safety, innovation, and freedom
    - label: Current Status
      value: Patchwork of voluntary commitments and emerging regulations
  tags:
    - debate
    - regulation
    - governance
  lastUpdated: 2025-01
- id: interpretability-sufficient
  type: crux
  title: Is Interpretability Sufficient for Safety?
  description: Debate over whether mechanistic interpretability can ensure AI safety.
  customFields:
    - label: Question
      value: Is mechanistic interpretability sufficient to ensure AI safety?
    - label: Stakes
      value: Determines priority of interpretability vs other safety research
    - label: Current Progress
      value: Can interpret some circuits/features, far from full transparency
  tags:
    - debate
    - interpretability
    - safety-research
  lastUpdated: 2025-01
- id: scaling-debate
  type: crux
  title: Is Scaling All You Need?
  description: The debate over whether scaling compute and data is sufficient for AGI or if we need new paradigms.
  customFields:
    - label: Question
      value: Can we reach AGI through scaling alone, or do we need new paradigms?
    - label: Stakes
      value: Determines AI timeline predictions and research priorities
    - label: Expert Consensus
      value: Strong disagreement between scaling optimists and skeptics
  tags:
    - debate
    - scaling
    - capabilities
  lastUpdated: 2025-01
- id: why-alignment-easy
  type: argument
  title: Why Alignment Might Be Easy
  description: >-
    Arguments that AI alignment is tractable with current methods including RLHF, Constitutional AI, and
    interpretability research.
  customFields:
    - label: Thesis
      value: Aligning AI with human values is achievable with current or near-term techniques
    - label: Implication
      value: Can pursue beneficial AI without extreme precaution
    - label: Key Evidence
      value: Empirical progress and theoretical reasons for optimism
  tags:
    - argument
    - alignment
    - optimistic
  lastUpdated: 2025-01
- id: case-against-xrisk
  type: argument
  title: The Case Against AI Existential Risk
  description: The strongest skeptical arguments against AI existential risk, presenting positions from prominent researchers.
  customFields:
    - label: Conclusion
      value: AI x-risk is very low (under 1%) or highly uncertain
    - label: Strength
      value: Challenges many assumptions in the x-risk argument
    - label: Key Claim
      value: Current evidence doesn't support extreme risk scenarios
  tags:
    - argument
    - existential-risk
    - skeptical
  lastUpdated: 2025-01
- id: why-alignment-hard
  type: argument
  title: Why Alignment Might Be Hard
  description: >-
    Arguments that AI alignment faces fundamental challenges including specification problems, inner alignment failures,
    and verification difficulties.
  customFields:
    - label: Thesis
      value: Aligning advanced AI with human values is extremely difficult and may not be solved in time
    - label: Implication
      value: Need caution and potentially slowing capability development
    - label: Key Uncertainty
      value: Will current approaches scale to superhuman AI?
  tags:
    - argument
    - alignment
    - pessimistic
  lastUpdated: 2025-01
- id: case-for-xrisk
  type: argument
  title: The Case For AI Existential Risk
  description: >-
    The strongest formal argument that AI poses existential risk to humanity, based on expert surveys and logical
    analysis.
  customFields:
    - label: Conclusion
      value: AI poses significant probability of human extinction or permanent disempowerment
    - label: Strength
      value: Many find compelling; others reject key premises
    - label: Key Uncertainty
      value: Will alignment be solved before transformative AI?
  tags:
    - argument
    - existential-risk
    - concerned
  lastUpdated: 2025-01

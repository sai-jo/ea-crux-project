# Organizations Entities
# Auto-generated from entities.yaml - edit this file directly

- id: anthropic
  type: lab
  title: Anthropic
  website: https://anthropic.com
  relatedEntries:
    - id: dario-amodei
      type: researcher
    - id: chris-olah
      type: researcher
    - id: jan-leike
      type: researcher
    - id: openai
      type: organization
    - id: interpretability
      type: safety-approaches
    - id: scalable-oversight
      type: safety-approaches
    - id: deceptive-alignment
      type: risk
    - id: racing-dynamics
      type: risk
  sources:
    - title: Anthropic Company Website
      url: https://anthropic.com
    - title: Core Views on AI Safety
      url: https://anthropic.com/news/core-views-on-ai-safety
    - title: Responsible Scaling Policy
      url: https://anthropic.com/news/anthropics-responsible-scaling-policy
    - title: Constitutional AI Paper
      url: https://arxiv.org/abs/2212.08073
    - title: Scaling Monosemanticity
      url: https://transformer-circuits.pub/2024/scaling-monosemanticity/
    - title: Sleeper Agents Paper
      url: https://arxiv.org/abs/2401.05566
    - title: Many-Shot Jailbreaking Paper
      url: >-
        https://www-cdn.anthropic.com/af5633c94ed2beb282f6a53c595eb437e8e7b630/Many_Shot_Jailbreaking__2024_04_02_0936.pdf
    - title: Machines of Loving Grace (Dario Amodei essay)
      url: https://darioamodei.com/machines-of-loving-grace
    - title: Anthropic Funding News (Crunchbase)
      url: https://www.crunchbase.com/organization/anthropic
    - title: Amazon Anthropic Partnership
      url: https://press.aboutamazon.com/2023/9/amazon-and-anthropic-announce-strategic-collaboration
    - title: Google Anthropic Investment
      url: https://blog.google/technology/ai/google-anthropic-investment/
  description: >-
    Anthropic is an AI safety company founded in January 2021 by former OpenAI researchers, including siblings Dario and
    Daniela Amodei. The company was created following disagreements with OpenAI's direction, particularly concerns about
    the pace of commercialization and the shift toward Microsoft partnership.
  tags:
    - constitutional-ai
    - rlhf
    - interpretability
    - responsible-scaling
    - claude
    - frontier-ai
    - scalable-oversight
    - ai-safety
    - racing-dynamics
  lastUpdated: 2025-12
- id: deepmind
  type: lab
  title: Google DeepMind
  website: https://deepmind.google
  relatedEntries:
    - id: demis-hassabis
      type: researcher
    - id: shane-legg
      type: researcher
    - id: openai
      type: organization
    - id: anthropic
      type: organization
    - id: scalable-oversight
      type: safety-approaches
    - id: reward-hacking
      type: risk
    - id: racing-dynamics
      type: risk
    - id: concentration-of-power
      type: risk
  sources:
    - title: Google DeepMind Website
      url: https://deepmind.google
    - title: AlphaGo Documentary
      url: https://www.youtube.com/watch?v=WXuK6gekU1Y
    - title: AlphaFold Protein Structure Database
      url: https://alphafold.ebi.ac.uk
    - title: AlphaFold Nature Paper
      url: https://www.nature.com/articles/s41586-021-03819-2
    - title: Frontier Safety Framework
      url: https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/
    - title: AI Safety Gridworlds
      url: https://arxiv.org/abs/1711.09883
    - title: Specification Gaming Examples
      url: https://deepmind.google/discover/blog/specification-gaming-the-flip-side-of-ai-ingenuity/
    - title: DeepMind Safety Research
      url: https://deepmind.google/discover/blog/building-safe-artificial-intelligence-insights-from-deepmind/
    - title: Gemini Technical Report
      url: https://arxiv.org/abs/2312.11805
    - title: Google DeepMind Merger Announcement
      url: https://blog.google/technology/ai/april-ai-update/
    - title: GraphCast Weather Prediction
      url: >-
        https://deepmind.google/discover/blog/graphcast-ai-model-for-faster-and-more-accurate-global-weather-forecasting/
    - title: Nobel Prize in Chemistry 2024
      url: https://www.nobelprize.org/prizes/chemistry/2024/press-release/
  description: >-
    Google DeepMind was formed in April 2023 from the merger of DeepMind and Google Brain, uniting Google's two major AI
    research organizations. The combined entity represents one of the world's most formidable AI research labs, with
    landmark achievements including AlphaGo (defeating world champions at Go), AlphaFold (solving protein folding), and
    G...
  tags:
    - gemini
    - alphafold
    - alphago
    - rlhf
    - agi
    - frontier-ai
    - google
    - scientific-ai-applications
    - frontier-safety-framework
    - reward-modeling
    - scalable-oversight
  lastUpdated: 2025-12
- id: openai
  type: lab
  title: OpenAI
  website: https://openai.com
  relatedEntries:
    - id: sam-altman
      type: researcher
    - id: ilya-sutskever
      type: researcher
    - id: jan-leike
      type: researcher
    - id: anthropic
      type: organization
    - id: interpretability
      type: safety-approaches
    - id: scalable-oversight
      type: safety-approaches
    - id: racing-dynamics
      type: risk
    - id: deceptive-alignment
      type: risk
  sources:
    - title: OpenAI Website
      url: https://openai.com
    - title: OpenAI Charter
      url: https://openai.com/charter
    - title: GPT-4 System Card
      url: https://cdn.openai.com/papers/gpt-4-system-card.pdf
    - title: InstructGPT Paper
      url: https://arxiv.org/abs/2203.02155
    - title: Preparedness Framework
      url: https://openai.com/safety/preparedness
    - title: Weak-to-Strong Generalization
      url: https://arxiv.org/abs/2312.09390
    - title: Jan Leike Resignation Statement
      url: https://twitter.com/janleike/status/1791498184887095344
    - title: November 2023 Governance Crisis (reporting)
      url: https://www.theverge.com/2023/11/17/23965982/openai-ceo-sam-altman-fired
    - title: Microsoft OpenAI Partnership
      url: https://blogs.microsoft.com/blog/2023/01/23/microsoftandopenaiextendpartnership/
    - title: o1 System Card
      url: https://openai.com/index/openai-o1-system-card/
    - title: OpenAI Funding History (Crunchbase)
      url: https://www.crunchbase.com/organization/openai
  description: >-
    OpenAI is the AI research company that brought large language models into mainstream consciousness through ChatGPT.
    Founded in December 2015 as a non-profit with the mission to ensure artificial general intelligence benefits all of
    humanity, OpenAI has undergone dramatic evolution - from non-profit to "capped-profit," from research lab to
    produc...
  tags:
    - gpt-4
    - chatgpt
    - rlhf
    - preparedness
    - agi
    - frontier-ai
    - o1
    - reasoning-models
    - microsoft
    - governance
    - racing-dynamics
    - alignment-research
  lastUpdated: 2025-12
- id: xai
  type: lab
  title: xAI
  website: https://x.ai
  relatedEntries:
    - id: elon-musk
      type: researcher
    - id: openai
      type: organization
    - id: anthropic
      type: organization
    - id: racing-dynamics
      type: risk
    - id: content-moderation
      type: concepts
    - id: agi-race
      type: concepts
  sources:
    - title: xAI Website
      url: https://x.ai
    - title: Grok Announcements
      url: https://x.ai/blog
    - title: Elon Musk on X (Twitter)
      url: https://twitter.com/elonmusk
    - title: xAI Funding Announcements
    - title: Grok Technical Details
      url: https://x.ai/blog/grok
  description: >-
    xAI is an artificial intelligence company founded by Elon Musk in July 2023 with the stated mission to "understand
    the true nature of the universe" through AI.
  tags:
    - grok
    - elon-musk
    - x-integration
    - truth-seeking-ai
    - content-moderation
    - free-speech
    - ai-safety-philosophy
    - racing-dynamics
    - frontier-ai
    - agi-development
  lastUpdated: 2025-12
- id: chai
  type: lab-academic
  title: CHAI
  website: https://humancompatible.ai
  relatedEntries:
    - id: value-learning
      type: safety-agenda
    - id: reward-hacking
      type: risk
    - id: corrigibility
      type: safety-agenda
  sources:
    - title: CHAI Website
      url: https://humancompatible.ai
    - title: Human Compatible (Book)
      url: https://www.penguinrandomhouse.com/books/566677/human-compatible-by-stuart-russell/
    - title: Stuart Russell on AI Risk
      url: https://www.youtube.com/watch?v=EBK-a94IFHY
  description: >-
    The Center for Human-Compatible AI (CHAI) is an academic research center at UC Berkeley focused on ensuring AI
    systems are beneficial to humans. Founded by Stuart Russell, author of the leading AI textbook, CHAI brings academic
    rigor to AI safety research.
  tags:
    - inverse-reinforcement-learning
    - value-learning
    - assistance-games
    - human-compatible-ai
    - academic-ai-safety
  lastUpdated: 2025-12
- id: apollo-research
  type: lab-research
  title: Apollo Research
  website: https://www.apolloresearch.ai
  relatedEntries:
    - id: deceptive-alignment
      type: risk
    - id: sandbagging
      type: risk
    - id: metr
      type: organization
    - id: arc
      type: organization
    - id: anthropic
      type: organization
    - id: uk-aisi
      type: organization
    - id: situational-awareness
      type: risk
    - id: capability-evaluations
      type: safety-approaches
  sources:
    - title: Apollo Research Website
      url: https://www.apolloresearch.ai
    - title: Apollo Research Publications
      url: https://www.apolloresearch.ai/research
    - title: Evaluating Frontier Models for Dangerous Capabilities
      url: https://www.apolloresearch.ai/research/scheming-evaluations
    - title: Apollo on Sandbagging
      url: https://www.apolloresearch.ai/blog/sandbagging
    - title: Situational Awareness Research
      url: https://www.apolloresearch.ai/research/situational-awareness
    - title: Apollo Research Blog
      url: https://www.apolloresearch.ai/blog
  description: >-
    Apollo Research is an AI safety research organization founded in 2022 with a specific focus on one of the most
    concerning potential failure modes: deceptive alignment and scheming behavior in advanced AI systems.
  tags:
    - deception
    - scheming
    - sandbagging
    - evaluations
    - situational-awareness
    - strategic-deception
    - red-teaming
    - alignment-failures
    - dangerous-capabilities
    - model-organisms
    - adversarial-testing
  lastUpdated: 2025-12
- id: cais
  type: lab-research
  title: CAIS
  website: https://safe.ai
  relatedEntries:
    - id: existential-risk
      type: risk
    - id: power-seeking
      type: risk
    - id: anthropic
      type: lab
  sources:
    - title: CAIS Website
      url: https://safe.ai
    - title: Statement on AI Risk
      url: https://www.safe.ai/statement-on-ai-risk
    - title: Representation Engineering Paper
      url: https://arxiv.org/abs/2310.01405
  description: >-
    The Center for AI Safety (CAIS) is a nonprofit organization that works to reduce societal-scale risks from AI. CAIS
    combines research, field-building, and public communication to advance AI safety.
  tags:
    - ai-safety
    - x-risk
    - representation-engineering
    - field-building
    - ai-risk-communication
  lastUpdated: 2025-12
- id: conjecture
  type: lab-research
  title: Conjecture
  website: https://conjecture.dev
  relatedEntries:
    - id: connor-leahy
      type: researcher
    - id: interpretability
      type: safety-approaches
    - id: anthropic
      type: organization
    - id: redwood
      type: organization
    - id: prosaic-alignment
      type: safety-approaches
    - id: uk-aisi
      type: organization
  sources:
    - title: Conjecture Website
      url: https://conjecture.dev
    - title: Connor Leahy Twitter/X
      url: https://twitter.com/NPCollapse
    - title: EleutherAI Background
      url: https://www.eleuther.ai
    - title: Conjecture Funding Announcement
      url: https://techcrunch.com/2023/03/28/conjecture-raises-funding-for-ai-safety/
    - title: Cognitive Emulation Research
      url: https://conjecture.dev/research
    - title: Connor Leahy Podcast Appearances
  description: >-
    Conjecture is an AI safety research organization founded in 2021 by Connor Leahy and a team of researchers concerned
    about existential risks from advanced AI.
  tags:
    - cognitive-emulation
    - coem
    - interpretability
    - neural-network-internals
    - circuit-analysis
    - model-organisms
    - eleutherai
    - european-ai-safety
    - alternative-paradigms
  lastUpdated: 2025-12
- id: far-ai
  type: lab-research
  title: FAR AI
  website: https://far.ai
  relatedEntries:
    - id: dan-hendrycks
      type: researcher
    - id: adversarial-robustness
      type: safety-approaches
    - id: natural-abstractions
      type: concepts
    - id: benchmarking
      type: safety-approaches
    - id: metr
      type: organization
    - id: apollo-research
      type: organization
  sources:
    - title: FAR AI Website
      url: https://far.ai
    - title: Dan Hendrycks Google Scholar
      url: https://scholar.google.com/citations?user=VUnTdTkAAAAJ
    - title: MMLU Paper
      url: https://arxiv.org/abs/2009.03300
    - title: Natural Abstractions Research
      url: https://www.alignmentforum.org/tag/natural-abstraction
    - title: Dan Hendrycks on X-risk
      url: https://arxiv.org/abs/2306.12001
  description: >-
    FAR AI (Forecasting AI Research) is an AI safety research organization founded in 2023 with a focus on adversarial
    robustness, model evaluation, and alignment research. The organization was co-founded by Dan Hendrycks, a prominent
    AI safety researcher known for his work on benchmarks, robustness, and AI risk.
  tags:
    - adversarial-robustness
    - ml-safety
    - benchmarking
    - natural-abstractions
    - evaluation
    - mmlu
    - out-of-distribution-detection
    - safety-evaluations
    - empirical-research
    - academic-ai-safety
  lastUpdated: 2025-12
- id: govai
  type: lab-research
  title: GovAI
  website: https://governance.ai
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: international-coordination
      type: policy
    - id: deepmind
      type: lab
  sources:
    - title: GovAI Website
      url: https://governance.ai
    - title: Computing Power and AI Governance
      url: https://governance.ai/compute
    - title: GovAI Research Papers
      url: https://governance.ai/research
  description: >-
    The Centre for the Governance of AI (GovAI) is a research organization focused on AI policy and governance.
    Originally part of the Future of Humanity Institute at Oxford, GovAI became independent in 2023 when FHI closed.
  tags:
    - governance
    - compute-governance
    - international
    - regulation
  lastUpdated: 2025-12
- id: metr
  type: lab-research
  title: METR
  website: https://metr.org
  relatedEntries:
    - id: beth-barnes
      type: researcher
    - id: paul-christiano
      type: researcher
    - id: arc
      type: organization
    - id: apollo-research
      type: organization
    - id: autonomous-replication
      type: risk
    - id: cyber-offense
      type: risk
    - id: bio-risk
      type: risk
    - id: anthropic
      type: organization
    - id: openai
      type: organization
    - id: uk-aisi
      type: organization
  sources:
    - title: METR Website
      url: https://metr.org
    - title: METR Evaluations
      url: https://metr.org/evaluations
    - title: GPT-4 System Card (ARC Evals section)
      url: https://cdn.openai.com/papers/gpt-4-system-card.pdf
    - title: OpenAI Preparedness Framework
      url: https://openai.com/safety/preparedness
    - title: Anthropic Responsible Scaling Policy
      url: https://anthropic.com/news/anthropics-responsible-scaling-policy
    - title: Beth Barnes on Twitter/X
      url: https://twitter.com/beth_from_ba
    - title: METR Research and Blog
      url: https://metr.org/blog
  description: >-
    METR (Model Evaluation and Threat Research), formerly known as ARC Evals, is an organization dedicated to evaluating
    frontier AI models for dangerous capabilities before deployment.
  tags:
    - evaluations
    - dangerous-capabilities
    - autonomous-replication
    - cybersecurity
    - cbrn
    - bio-risk
    - red-teaming
    - capability-elicitation
    - deployment-decisions
    - pre-deployment-testing
    - safety-thresholds
    - responsible-scaling
    - preparedness-framework
  lastUpdated: 2025-12
- id: arc
  type: organization
  title: ARC
  website: https://alignment.org
  relatedEntries:
    - id: paul-christiano
      type: researcher
    - id: scalable-oversight
      type: safety-approaches
    - id: deceptive-alignment
      type: risk
    - id: sandbagging
      type: risk
    - id: anthropic
      type: organization
    - id: openai
      type: organization
    - id: miri
      type: organization
    - id: uk-aisi
      type: policies
  sources:
    - title: ARC Website
      url: https://alignment.org
    - title: ELK Report
      url: https://docs.google.com/document/d/1WwsnJQstPq91_Yh-Ch2XRL8H_EpsnjrC1dwZXR37PC8/
    - title: ARC Evals
      url: https://evals.alignment.org
    - title: GPT-4 Evaluation (ARC summary)
      url: https://evals.alignment.org/blog/2023-03-18-update-on-recent-evals/
    - title: Paul Christiano's AI Alignment Forum posts
      url: https://www.alignmentforum.org/users/paulfchristiano
    - title: Iterated Amplification
      url: https://ai-alignment.com/iterated-distillation-and-amplification-157debfd1616
    - title: AI Safety via Debate
      url: https://arxiv.org/abs/1805.00899
    - title: Ajeya Cotra's Bio Anchors
      url: https://www.alignmentforum.org/posts/KrJfoZzpSDpnrv9va/draft-report-on-ai-timelines
  description: >-
    The Alignment Research Center (ARC) was founded in 2021 by Paul Christiano after his departure from OpenAI. ARC
    represents a distinctive approach to AI alignment: combining theoretical research on fundamental problems (like
    Eliciting Latent Knowledge) with practical evaluations of frontier models for dangerous capabilities.
  tags:
    - eliciting-latent-knowledge
    - elk
    - evaluations
    - scalable-oversight
    - ai-evals
    - deception
    - worst-case-alignment
    - debate
    - amplification
    - adversarial-testing
    - autonomous-replication
    - sandbagging
  lastUpdated: 2025-12
- id: epoch-ai
  type: organization
  title: Epoch AI
  website: https://epochai.org
  relatedEntries:
    - id: compute-governance
      type: policies
    - id: transformative-ai
      type: concepts
    - id: scaling-laws
      type: concepts
    - id: ai-timelines
      type: concepts
    - id: data-constraints
      type: concepts
  sources:
    - title: Epoch AI Website
      url: https://epochai.org
    - title: Epoch Parameter Database
      url: https://epochai.org/data/epochdb/visualization
    - title: Compute Trends Paper
      url: https://epochai.org/blog/compute-trends
    - title: Will We Run Out of Data?
      url: https://epochai.org/blog/will-we-run-out-of-data
    - title: Algorithmic Progress Research
      url: https://epochai.org/blog/revisiting-algorithmic-progress
    - title: Epoch Research Blog
      url: https://epochai.org/blog
    - title: Epoch on Twitter/X
      url: https://twitter.com/epoch_ai
  description: >-
    Epoch AI is a research organization dedicated to producing rigorous, data-driven forecasts and analysis about
    artificial intelligence progress, with particular focus on compute trends, training datasets, algorithmic
    efficiency, and AI timelines.
  tags:
    - ai-forecasting
    - compute-trends
    - training-datasets
    - algorithmic-progress
    - ai-timelines
    - transformative-ai
    - compute-governance
    - parameter-counts
    - scaling
    - data-constraints
    - empirical-analysis
    - trend-extrapolation
  lastUpdated: 2025-12
- id: miri
  type: organization
  title: MIRI
  website: https://intelligence.org
  relatedEntries:
    - id: eliezer-yudkowsky
      type: researcher
    - id: nate-soares
      type: researcher
    - id: paul-christiano
      type: researcher
    - id: instrumental-convergence
      type: risk
    - id: corrigibility-failure
      type: risk
    - id: sharp-left-turn
      type: risk
    - id: compute-governance
      type: policies
    - id: arc
      type: organization
  sources:
    - title: MIRI Website
      url: https://intelligence.org
    - title: MIRI 2023 Strategy Update
      url: https://intelligence.org/2023/03/09/miri-announces-new-death-with-dignity-strategy/
    - title: Risks from Learned Optimization (Hubinger et al.)
      url: https://arxiv.org/abs/1906.01820
    - title: Logical Induction Paper
      url: https://arxiv.org/abs/1609.03543
    - title: Embedded Agency (Demski, Garrabrant)
      url: https://intelligence.org/2018/10/29/embedded-agency/
    - title: LessWrong Sequences
      url: https://www.lesswrong.com/sequences
    - title: Eliezer Yudkowsky TIME Op-Ed
      url: https://time.com/6266923/ai-eliezer-yudkowsky-open-letter-not-enough/
    - title: Agent Foundations Research
      url: https://intelligence.org/research-guide/
    - title: Facing the Intelligence Explosion (Muehlhauser)
      url: https://intelligence.org/files/IE-EI.pdf
    - title: MIRI on GiveWell
      url: https://www.givewell.org/charities/machine-intelligence-research-institute
  description: >-
    The Machine Intelligence Research Institute (MIRI) is one of the oldest organizations focused on AI existential
    risk, founded in 2000 as the Singularity Institute for Artificial Intelligence (SIAI).
  tags:
    - agent-foundations
    - decision-theory
    - corrigibility
    - instrumental-convergence
    - embedded-agency
    - governance
    - logical-uncertainty
    - rationalist-community
    - lesswrong
    - sharp-left-turn
    - security-mindset
    - deconfusion
  lastUpdated: 2025-12
- id: redwood
  type: organization
  title: Redwood Research
  website: https://redwoodresearch.org
  relatedEntries:
    - id: interpretability
      type: safety-approaches
    - id: ai-control
      type: safety-approaches
    - id: scheming
      type: risk
    - id: sandbagging
      type: risk
    - id: anthropic
      type: organization
    - id: arc
      type: organization
    - id: miri
      type: organization
  sources:
    - title: Redwood Research Website
      url: https://redwoodresearch.org
    - title: AI Control Paper
      url: https://arxiv.org/abs/2312.06942
    - title: Causal Scrubbing
      url: https://www.alignmentforum.org/posts/JvZhhzycHu2Yd57RN/causal-scrubbing-a-method-for-rigorously-testing
    - title: Adversarial Training for High-Stakes Safety
      url: https://arxiv.org/abs/2205.01663
    - title: Redwood Research on Alignment Forum
      url: https://www.alignmentforum.org/users/redwood-research
    - title: Neel Nanda's Interpretability Work
      url: https://www.neelnanda.io/mechanistic-interpretability
  description: >-
    Redwood Research is an AI safety lab founded in 2021 that has made significant contributions to mechanistic
    interpretability and, more recently, pioneered the "AI control" research agenda.
  tags:
    - interpretability
    - causal-scrubbing
    - ai-control
    - adversarial-robustness
    - polysemanticity
    - scheming
    - deception-detection
    - red-teaming
    - monitoring
    - safety-protocols
  lastUpdated: 2025-12
- id: uk-aisi
  type: organization
  title: UK AI Safety Institute
  website: https://www.aisi.gov.uk
  relatedEntries:
    - id: ian-hogarth
      type: researcher
    - id: us-aisi
      type: organization
    - id: metr
      type: organization
    - id: apollo-research
      type: organization
    - id: ai-safety-summit
      type: events
    - id: anthropic
      type: organization
    - id: openai
      type: organization
    - id: deepmind
      type: organization
  sources:
    - title: UK AI Safety Institute Website
      url: https://www.aisi.gov.uk
    - title: Bletchley Declaration
      url: https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration
    - title: UK AI Safety Summit
      url: https://www.aisafetysummit.gov.uk
    - title: UK DSIT AI Policy
      url: https://www.gov.uk/government/organisations/department-for-science-innovation-and-technology
    - title: Ian Hogarth FT Op-Ed
      url: https://www.ft.com/content/03895dc4-a3b7-481e-95cc-336a524f2ac2
    - title: UK AI Safety Institute Announcements
      url: https://www.gov.uk/search/news-and-communications?organisations%5B%5D=ai-safety-institute
  description: >-
    The UK AI Safety Institute (UK AISI) is a government organization established in 2023 to advance AI safety through
    research, evaluation, and international coordination. Created in the wake of the first AI Safety Summit hosted by
    the UK government, AISI represents the UK's commitment to being a global leader in AI safety and governance.
  tags:
    - governance
    - government-ai-safety
    - international
    - evaluations
    - bletchley-declaration
    - ai-safety-summits
    - standard-setting
    - uk-ai-policy
    - frontier-model-evaluation
    - global-ai-safety
    - regulatory-framework
  lastUpdated: 2025-12
- id: us-aisi
  type: organization
  title: US AI Safety Institute
  website: https://www.nist.gov/aisi
  relatedEntries:
    - id: uk-aisi
      type: organization
    - id: metr
      type: organization
    - id: apollo-research
      type: organization
    - id: compute-governance
      type: policies
    - id: ai-executive-order
      type: policies
    - id: anthropic
      type: organization
    - id: openai
      type: organization
  sources:
    - title: US AI Safety Institute Website
      url: https://www.nist.gov/aisi
    - title: NIST AI Risk Management Framework
      url: https://www.nist.gov/itl/ai-risk-management-framework
    - title: Executive Order on AI (October 2023)
      url: >-
        https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/
    - title: NIST AI Portal
      url: https://www.nist.gov/artificial-intelligence
    - title: US AISI Announcements
      url: >-
        https://www.commerce.gov/news/press-releases/2023/11/biden-harris-administration-announces-key-ai-actions-following-president
  description: >-
    The US AI Safety Institute (US AISI) is a government agency within the National Institute of Standards and
    Technology (NIST) established in 2023 to develop standards, evaluations, and guidelines for safe and trustworthy
    artificial intelligence.
  tags:
    - governance
    - government-oversight
    - ai-standards
    - evaluations
    - nist
    - regulatory-framework
    - international
    - ai-safety
    - public-interest
    - regulatory-capture
    - standard-setting
  lastUpdated: 2025-12
- id: arc-evals
  type: organization
  title: ARC Evaluations
  description: Organization focused on evaluating AI systems for dangerous capabilities. Now largely absorbed into METR.
  status: stub
  relatedEntries:
    - id: metr
      type: lab-research
    - id: capability-evaluations
      type: concept
  tags:
    - evaluations
    - ai-safety
  lastUpdated: 2025-12
- id: fhi
  type: organization
  title: Future of Humanity Institute
  description: Oxford University research center focused on existential risks, founded by Nick Bostrom. Closed in 2024.
  status: stub
  relatedEntries:
    - id: nick-bostrom
      type: researcher
    - id: existential-risk
      type: concept
  tags:
    - research-org
    - existential-risk
    - oxford
  lastUpdated: 2025-12

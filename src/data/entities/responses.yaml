# Responses Entities
# Auto-generated from entities.yaml - edit this file directly

- id: ai-safety-institutes
  type: policy
  title: AI Safety Institutes (AISIs)
  customFields:
    - label: Established
      value: UK (2023), US (2024), others planned
    - label: Function
      value: Evaluation, research, policy advice
    - label: Network
      value: International coordination emerging
  sources:
    - title: UK AI Safety Institute
      url: https://www.gov.uk/government/organisations/ai-safety-institute
      author: UK Government
    - title: US AI Safety Institute
      url: https://www.nist.gov/aisi
      author: NIST
    - title: Inspect Framework
      url: https://github.com/UKGovernmentBEIS/inspect_ai
      author: UK AISI
    - title: Seoul Declaration on AISI Network
      url: https://www.gov.uk/government/publications/seoul-declaration-for-safe-innovative-and-inclusive-ai
      author: Summit Participants
  lastUpdated: 2025-12
- id: california-sb1047
  type: policy
  title: Safe and Secure Innovation for Frontier Artificial Intelligence Models Act
  customFields:
    - label: Introduced
      value: February 2024
    - label: Passed Legislature
      value: August 29, 2024
    - label: Vetoed
      value: September 29, 2024
    - label: Author
      value: Senator Scott Wiener
  relatedEntries:
    - id: us-executive-order
      type: policy
    - id: compute-governance
      type: policy
    - id: eu-ai-act
      type: policy
    - id: voluntary-commitments
      type: policy
  sources:
    - title: SB 1047 Bill Text (Final Amended Version)
      url: https://leginfo.legislature.ca.gov/faces/billTextClient.xhtml?bill_id=202320240SB1047
      date: August 2024
    - title: Governor Newsom's Veto Message
      url: https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf
      date: September 29, 2024
    - title: Analysis from Future of Life Institute
      url: https://futureoflife.org/project/sb-1047/
      author: FLI
    - title: OpenAI Letter Opposing SB 1047
      url: https://openai.com/index/openai-letter-to-california-governor-newsom-on-sb-1047/
    - title: Anthropic's Nuanced Position
      url: https://www.anthropic.com/news/anthropics-letter-to-senator-wiener-on-sb-1047
      date: August 2024
    - title: Academic Analysis
      url: https://law.stanford.edu/2024/09/25/sb-1047-analysis/
      author: Stanford HAI
  description: >-
    SB 1047, the Safe and Secure Innovation for Frontier Artificial Intelligence Models Act, was California state
    legislation that would have required safety testing and liability measures for developers of the most powerful AI
    models.
  tags:
    - regulation
    - state-policy
    - frontier-models
    - liability
    - compute-thresholds
    - california
    - political-strategy
  lastUpdated: 2025-12
- id: canada-aida
  type: policy
  title: Artificial Intelligence and Data Act (AIDA)
  customFields:
    - label: Introduced
      value: June 2022 (as part of Bill C-27)
    - label: Current Status
      value: Died with Parliament dissolution (January 2025)
    - label: Scope
      value: High-impact AI systems
    - label: Approach
      value: Risk-based, principles-focused
  sources:
    - title: Bill C-27 Text
      url: https://www.parl.ca/legisinfo/en/bill/44-1/c-27
      author: Parliament of Canada
    - title: AIDA Companion Document
      url: >-
        https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document
      author: ISED Canada
    - title: Government Amendments to AIDA
      url: >-
        https://ised-isde.canada.ca/site/innovation-better-canada/en/artificial-intelligence-and-data-act-aida-companion-document
      author: Government of Canada
      date: November 2023
  description: >-
    The Artificial Intelligence and Data Act (AIDA) was Canada's proposed federal AI legislation, introduced as Part 3
    of Bill C-27 (the Digital Charter Implementation Act, 2022). Despite years of debate and amendment, the bill died on
    the order paper when Parliament was dissolved in January 2025.
  lastUpdated: 2025-12
- id: china-ai-regulations
  type: policy
  title: China AI Regulatory Framework
  customFields:
    - label: Approach
      value: Sector-specific, iterative
    - label: Primary Focus
      value: Content control, social stability
    - label: Enforcement
      value: Cyberspace Administration of China (CAC)
  relatedEntries:
    - id: us-executive-order
      type: policy
    - id: eu-ai-act
      type: policy
    - id: compute-governance
      type: policy
    - id: international-summits
      type: policy
  sources:
    - title: 'Translation: Interim Measures for Generative AI Management'
      url: >-
        https://digichina.stanford.edu/work/translation-interim-measures-for-the-management-of-generative-artificial-intelligence-services-effective-august-15-2023/
      author: DigiChina, Stanford
      date: '2023'
    - title: China's Algorithm Registry
      url: >-
        https://digichina.stanford.edu/work/translation-algorithmic-recommendation-management-provisions-effective-march-1-2022/
      author: DigiChina, Stanford
    - title: Deep Synthesis Regulations
      url: >-
        https://www.newamerica.org/cybersecurity-initiative/digichina/blog/translation-chinas-deep-synthesis-regulations/
      author: New America
      date: '2022'
    - title: China AI Governance Overview
      url: https://cset.georgetown.edu/publication/understanding-chinas-ai-regulation/
      author: CSET Georgetown
      date: '2024'
    - title: China's New Generation AI Development Plan
      url: >-
        https://www.newamerica.org/cybersecurity-initiative/digichina/blog/full-translation-chinas-new-generation-artificial-intelligence-development-plan-2017/
      author: New America
      date: '2017'
    - title: Comparing US and China AI Regulation
      url: https://carnegieendowment.org/research/2024/01/regulating-ai-in-china-and-the-united-states
      author: Carnegie Endowment
      date: '2024'
  description: >-
    China has developed one of the world's most comprehensive AI regulatory frameworks through a series of targeted
    regulations addressing specific AI applications and risks. Unlike the EU's comprehensive AI Act, China's approach is
    iterative and sector-specific, with new rules issued as technologies emerge.
  tags:
    - regulation
    - china
    - content-control
    - algorithmic-accountability
    - international
    - generative-ai
    - deepfakes
    - geopolitics
  lastUpdated: 2025-12
- id: colorado-ai-act
  type: policy
  title: Colorado Artificial Intelligence Act
  customFields:
    - label: Signed
      value: May 17, 2024
    - label: Sponsor
      value: Senator Robert Rodriguez
    - label: Approach
      value: Risk-based, EU-influenced
  sources:
    - title: Colorado AI Act Full Text
      url: https://leg.colorado.gov/bills/sb21-205
      author: Colorado General Assembly
    - title: Colorado Governor Signs AI Law
      url: https://www.reuters.com/technology/colorado-governor-signs-first-us-ai-regulation-law-2024-05-17/
      author: Reuters
      date: May 2024
  description: >-
    The Colorado AI Act (SB 21-205) is the first comprehensive AI regulation enacted by a US state. Signed into law on
    May 17, 2024, it takes effect February 1, 2026.
  lastUpdated: 2025-12
- id: compute-governance
  type: policy
  title: Compute Governance
  customFields:
    - label: Approach
      value: Regulate AI via compute access
    - label: Status
      value: Emerging policy area
  relatedEntries:
    - id: govai
      type: lab
    - id: governance-policy
      type: intervention
    - id: racing-dynamics
      type: risk
    - id: proliferation
      type: risk
    - id: bioweapons
      type: risk
    - id: cyberweapons
      type: risk
    - id: concentration-of-power
      type: risk
  sources:
    - title: Computing Power and the Governance of AI
      url: https://www.governance.ai/research-papers/computing-power-and-the-governance-of-artificial-intelligence
      author: Heim et al.
      date: '2023'
    - title: US Export Controls on Advanced Computing
      url: https://www.bis.doc.gov/
      author: Bureau of Industry and Security
    - title: EU AI Act Compute Provisions
      url: https://artificialintelligenceact.eu/
    - title: CSET Semiconductor Reports
      url: https://cset.georgetown.edu/publications/?fwp_publication_types=issue-brief&fwp_topics=semiconductors
    - title: The Chips and Science Act
      url: https://www.congress.gov/bill/117th-congress/house-bill/4346
      date: '2022'
  description: >-
    Compute governance uses computational hardware as a lever to regulate AI development. Because advanced AI requires
    enormous amounts of computing power, and that compute comes from concentrated supply chains, controlling compute
    provides a tractable way to govern AI before models are built.
  tags:
    - export-controls
    - compute-thresholds
    - know-your-customer
    - hardware-governance
    - international
    - semiconductors
    - cloud-computing
  lastUpdated: 2025-12
- id: compute-thresholds
  type: policy
  title: Compute Thresholds
  customFields:
    - label: Approach
      value: Define capability boundaries via compute
    - label: Status
      value: Established in US and EU policy
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: eu-ai-act
      type: policy
    - id: ai-executive-order
      type: policy
  sources:
    - title: EU AI Act GPAI Thresholds
      url: https://artificialintelligenceact.eu/
    - title: US Executive Order Compute Thresholds
      url: >-
        https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/
  description: >-
    Compute thresholds define capability boundaries using training compute (measured in FLOP) as a proxy. The EU AI Act
    uses 10^25 FLOP for GPAI obligations; the US Executive Order uses 10^26 FLOP for reporting requirements. These
    thresholds aim to capture frontier models while minimizing regulatory burden on smaller systems.
  tags:
    - compute-governance
    - regulation
    - flop-thresholds
  lastUpdated: 2025-12
- id: compute-monitoring
  type: policy
  title: Compute Monitoring
  customFields:
    - label: Approach
      value: Track compute usage to detect dangerous training
    - label: Status
      value: Proposed, limited implementation
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: govai
      type: lab
  sources:
    - title: Computing Power and the Governance of AI
      url: https://www.governance.ai/research-papers/computing-power-and-the-governance-of-artificial-intelligence
      author: Heim et al.
    - title: Secure Governable Chips
      url: https://arxiv.org/abs/2303.11341
  description: >-
    Compute monitoring involves tracking how computational resources are used to detect unauthorized or dangerous AI
    training runs. Approaches include know-your-customer requirements for cloud providers, hardware-based monitoring,
    and training run detection algorithms. Raises privacy and implementation challenges.
  tags:
    - compute-governance
    - monitoring
    - kyc
    - cloud-computing
  lastUpdated: 2025-12
- id: international-compute-regimes
  type: policy
  title: International Compute Regimes
  customFields:
    - label: Approach
      value: Coordinate compute governance globally
    - label: Status
      value: Early discussions, no formal regime
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: international-coordination
      type: policy
  sources:
    - title: International Institutions for AI Safety
      url: https://www.governance.ai/research-papers/international-institutions-for-advanced-ai
      author: GovAI
    - title: IAEA Model for AI Governance
      url: https://www.governance.ai/research
  description: >-
    International compute regimes would coordinate compute governance across borders. Proposals include IAEA-like
    inspection bodies, multilateral export control agreements, and international compute monitoring frameworks. Faces
    challenges of verification, sovereignty concerns, and China-US competition.
  tags:
    - compute-governance
    - international
    - coordination
    - iaea-model
  lastUpdated: 2025-12
- id: eu-ai-act
  type: policy
  title: EU AI Act
  customFields:
    - label: Type
      value: Binding Regulation
    - label: Scope
      value: Risk-based
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: uk-aisi
      type: policy
    - id: govai
      type: lab
  sources:
    - title: EU AI Act Full Text
      url: https://artificialintelligenceact.eu/
    - title: EU AI Office
      url: https://digital-strategy.ec.europa.eu/en/policies/ai-office
    - title: Analysis of GPAI Provisions
      url: https://governance.ai/eu-ai-act
  description: >-
    The EU AI Act is the world's first comprehensive legal framework for artificial intelligence. Adopted in 2024, it
    establishes a risk-based approach to AI regulation, with stricter requirements for higher-risk AI systems.
  tags:
    - regulation
    - gpai
    - foundation-models
    - risk-based-regulation
    - compute-thresholds
  lastUpdated: 2025-12
- id: export-controls
  type: policy
  title: US AI Chip Export Controls
  customFields:
    - label: Initial Rules
      value: October 2022
    - label: Major Updates
      value: October 2023, December 2024
    - label: Primary Target
      value: China
    - label: Enforcing Agency
      value: Bureau of Industry and Security (BIS)
  sources:
    - title: BIS Export Controls on Advanced Computing
      url: https://www.bis.doc.gov/index.php/policy-guidance/country-guidance/china-prc
      author: Bureau of Industry and Security
    - title: Commerce Implements New Export Controls on Advanced Computing
      url: >-
        https://www.commerce.gov/news/press-releases/2022/10/commerce-implements-new-export-controls-advanced-computing-and
      author: US Department of Commerce
      date: October 2022
    - title: Choking Off China's Access to the Future of AI
      url: https://www.csis.org/analysis/choking-chinas-access-future-ai
      author: CSIS
      date: '2022'
  description: >-
    The United States has implemented unprecedented export controls on advanced semiconductors and semiconductor
    manufacturing equipment, primarily targeting China. These controls represent one of the most significant attempts to
    constrain AI development through hardware governance.
  lastUpdated: 2025-12
- id: failed-stalled-proposals
  type: policy
  title: Failed and Stalled AI Proposals
  customFields:
    - label: Purpose
      value: Learning from unsuccessful efforts
    - label: Coverage
      value: US, International
  sources:
    - title: California SB 1047 Veto Message
      url: https://www.gov.ca.gov/wp-content/uploads/2024/09/SB-1047-Veto-Message.pdf
      author: Governor Newsom
      date: September 2024
    - title: Hiroshima AI Process
      url: https://www.mofa.go.jp/ecm/ec/page5e_000076.html
      author: G7
  description: >-
    Understanding why AI governance proposals fail is as important as understanding successes. Failed efforts reveal
    political constraints, industry opposition patterns, and the challenges of regulating rapidly evolving technology.
  lastUpdated: 2025-12
- id: international-summits
  type: policy
  title: International AI Safety Summit Series
  customFields:
    - label: First Summit
      value: Bletchley Park, UK (Nov 2023)
    - label: Second Summit
      value: Seoul, South Korea (May 2024)
    - label: Third Summit
      value: Paris, France (Feb 2025)
    - label: Format
      value: Government-led, multi-stakeholder
  relatedEntries:
    - id: voluntary-commitments
      type: policy
    - id: uk-aisi
      type: policy
    - id: us-executive-order
      type: policy
    - id: china-ai-regulations
      type: policy
  sources:
    - title: The Bletchley Declaration
      url: >-
        https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023
      date: November 1, 2023
    - title: Seoul AI Safety Summit Outcomes
      url: https://www.gov.uk/government/publications/ai-seoul-summit-2024-outcomes
      date: May 2024
    - title: Frontier AI Safety Commitments
      url: https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024
      date: May 21, 2024
    - title: UN AI Advisory Body Report
      url: https://www.un.org/ai-advisory-body
      date: '2024'
    - title: G7 Hiroshima AI Process
      url: https://www.g7hiroshima.go.jp/en/documents/
      date: '2023'
    - title: 'Analysis: International AI Governance After Bletchley'
      url: https://www.governance.ai/research-papers/international-ai-governance
      author: GovAI
      date: '2024'
    - title: OECD AI Principles
      url: https://oecd.ai/en/ai-principles
      date: 2019, updated 2023
  description: >-
    The International AI Safety Summit series represents the first sustained effort at global coordination on AI safety,
    bringing together governments, AI companies, civil society, and researchers to address the risks from advanced AI.
  tags:
    - international
    - governance
    - multilateral-diplomacy
    - frontier-ai
    - bletchley-declaration
    - voluntary-commitments
    - policy-summits
  lastUpdated: 2025-12
- id: nist-ai-rmf
  type: policy
  title: NIST AI Risk Management Framework (AI RMF)
  customFields:
    - label: Version
      value: '1.0'
    - label: Type
      value: Voluntary framework
    - label: Referenced by
      value: US Executive Order, state laws
  sources:
    - title: AI Risk Management Framework
      url: https://www.nist.gov/itl/ai-risk-management-framework
      author: NIST
    - title: AI RMF Playbook
      url: https://airc.nist.gov/AI_RMF_Knowledge_Base/Playbook
      author: NIST
    - title: Generative AI Profile (AI 600-1)
      url: >-
        https://www.nist.gov/publications/artificial-intelligence-risk-management-framework-generative-artificial-intelligence
      author: NIST
      date: July 2024
  description: >-
    The NIST AI Risk Management Framework (AI RMF) is a voluntary guidance document developed by the National Institute
    of Standards and Technology to help organizations manage risks associated with AI systems.
  lastUpdated: 2025-12
- id: responsible-scaling-policies
  type: policy
  title: Responsible Scaling Policies (RSPs)
  customFields:
    - label: Type
      value: Self-regulation
    - label: Key Labs
      value: Anthropic, OpenAI, Google DeepMind
    - label: Origin
      value: '2023'
  sources:
    - title: Anthropic's Responsible Scaling Policy
      url: https://www.anthropic.com/index/anthropics-responsible-scaling-policy
      author: Anthropic
      date: September 2023
    - title: OpenAI Preparedness Framework
      url: https://openai.com/safety/preparedness
      author: OpenAI
      date: December 2023
    - title: Google DeepMind Frontier Safety Framework
      url: https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/
      author: Google DeepMind
      date: May 2024
  lastUpdated: 2025-12
- id: seoul-declaration
  type: policy
  title: Seoul Declaration on AI Safety
  customFields:
    - label: Predecessor
      value: Bletchley Declaration (Nov 2023)
    - label: Successor
      value: Paris Summit (Feb 2025)
    - label: Signatories
      value: 28 countries + EU
  sources:
    - title: Seoul Declaration
      url: https://www.gov.uk/government/publications/seoul-declaration-for-safe-innovative-and-inclusive-ai
      author: Summit Participants
    - title: Frontier AI Safety Commitments
      url: https://www.gov.uk/government/publications/frontier-ai-safety-commitments-ai-seoul-summit-2024
      author: AI Companies
      date: May 2024
  description: >-
    The Seoul AI Safety Summit (May 21-22, 2024) was the second in a series of international AI safety summits,
    following the Bletchley Park Summit in November 2023.
  lastUpdated: 2025-12
- id: standards-bodies
  type: policy
  title: AI Standards Development
  customFields:
    - label: Key Bodies
      value: ISO, IEEE, NIST, CEN-CENELEC
    - label: Status
      value: Rapidly developing
    - label: Relevance
      value: Standards increasingly referenced in law
  sources:
    - title: ISO/IEC JTC 1/SC 42 Artificial Intelligence
      url: https://www.iso.org/committee/6794475.html
      author: ISO
    - title: IEEE Ethically Aligned Design
      url: https://ethicsinaction.ieee.org/
      author: IEEE
    - title: EU AI Act Standardisation
      url: https://digital-strategy.ec.europa.eu/en/policies/ai-standards
      author: European Commission
  lastUpdated: 2025-12
- id: us-executive-order
  type: policy
  title: Executive Order on Safe, Secure, and Trustworthy AI
  customFields:
    - label: Type
      value: Executive Order
    - label: Number
      value: '14110'
    - label: Durability
      value: Can be revoked by future president
  relatedEntries:
    - id: compute-governance
      type: policy
    - id: uk-aisi
      type: policy
    - id: eu-ai-act
      type: policy
    - id: voluntary-commitments
      type: policy
  sources:
    - title: 'Executive Order 14110: Full Text'
      url: >-
        https://www.whitehouse.gov/briefing-room/presidential-actions/2023/10/30/executive-order-on-the-safe-secure-and-trustworthy-development-and-use-of-artificial-intelligence/
      date: October 30, 2023
    - title: White House Fact Sheet
      url: >-
        https://www.whitehouse.gov/briefing-room/statements-releases/2023/10/30/fact-sheet-president-biden-issues-executive-order-on-safe-secure-and-trustworthy-artificial-intelligence/
    - title: US AI Safety Institute
      url: https://www.nist.gov/aisi
    - title: NIST AI Risk Management Framework
      url: https://www.nist.gov/itl/ai-risk-management-framework
    - title: Analysis from Center for Security and Emerging Technology
      url: https://cset.georgetown.edu/article/understanding-the-ai-executive-order/
      author: CSET
      date: '2023'
  description: >-
    The Executive Order on Safe, Secure, and Trustworthy Artificial Intelligence, signed by President Biden on October
    30, 2023, is the most comprehensive US government action on AI to date. It establishes safety requirements for
    frontier AI systems, mandates government agency actions, and creates oversight mechanisms.
  tags:
    - compute-thresholds
    - governance
    - us-aisi
    - cloud-computing
    - know-your-customer
    - safety-evaluations
    - executive-policy
  lastUpdated: 2025-12
- id: us-state-legislation
  type: policy
  title: US State AI Legislation Landscape
  customFields:
    - label: Most active states
      value: California, Colorado, Texas, Illinois
    - label: Total bills (2024)
      value: 400+
    - label: Trend
      value: Rapidly increasing
  sources:
    - title: State AI Legislation Tracker
      url: https://www.bsa.org/policy/artificial-intelligence
      author: BSA
    - title: AI Legislation in the States
      url: https://www.ncsl.org/technology-and-communication/artificial-intelligence-2024-legislation
      author: National Conference of State Legislatures
  description: >-
    In the absence of comprehensive federal AI legislation, US states have become laboratories for AI governance. As of
    2024, hundreds of AI-related bills have been introduced across all 50 states, with several significant laws enacted.
  lastUpdated: 2025-12
- id: voluntary-commitments
  type: policy
  title: Voluntary AI Safety Commitments
  customFields:
    - label: Nature
      value: Non-binding voluntary pledges
    - label: Enforcement
      value: Reputational only
    - label: Participants
      value: Major AI labs
  relatedEntries:
    - id: us-executive-order
      type: policy
    - id: international-summits
      type: policy
    - id: anthropic
      type: lab
    - id: openai
      type: lab
  sources:
    - title: 'White House Fact Sheet: Voluntary AI Commitments'
      url: >-
        https://www.whitehouse.gov/briefing-room/statements-releases/2023/07/21/fact-sheet-biden-harris-administration-secures-voluntary-commitments-from-leading-artificial-intelligence-companies-to-manage-the-risks-posed-by-ai/
      date: July 21, 2023
    - title: Anthropic's Responsible Scaling Policy
      url: https://www.anthropic.com/news/anthropics-responsible-scaling-policy
      date: September 2023
    - title: OpenAI Preparedness Framework
      url: https://openai.com/safety/preparedness
      date: December 2023
    - title: Google DeepMind Frontier Safety Framework
      url: https://deepmind.google/discover/blog/introducing-the-frontier-safety-framework/
      date: May 2024
    - title: Bletchley Declaration
      url: >-
        https://www.gov.uk/government/publications/ai-safety-summit-2023-the-bletchley-declaration/the-bletchley-declaration-by-countries-attending-the-ai-safety-summit-1-2-november-2023
      date: November 2023
    - title: 'Analysis: Are Voluntary AI Commitments Enough?'
      url: https://www.governance.ai/research-papers/voluntary-commitments
      author: GovAI
      date: '2024'
  description: >-
    In July 2023, the White House secured voluntary commitments from leading AI companies on safety, security, and
    trust. These commitments represent the first coordinated industry-wide AI safety pledges, establishing baseline
    practices for frontier AI development.
  tags:
    - self-regulation
    - industry-commitments
    - responsible-scaling
    - red-teaming
    - governance
    - international
    - safety-standards
  lastUpdated: 2025-12
- id: epistemic-security
  type: intervention
  title: Epistemic Security
  customFields:
    - label: Definition
      value: Protecting collective capacity for knowledge and truth-finding
    - label: Key Threats
      value: Deepfakes, AI disinformation, trust collapse
    - label: Key Research
      value: RAND, Stanford Internet Observatory, Oxford
  relatedEntries:
    - id: disinformation
      type: risk
    - id: deepfakes
      type: risk
    - id: consensus-manufacturing
      type: risk
    - id: trust-decline
      type: risk
    - id: reality-fragmentation
      type: risk
    - id: epistemic-collapse
      type: risk
    - id: historical-revisionism
      type: risk
    - id: epistemic-sycophancy
      type: risk
  sources:
    - title: The Vulnerability of Democracies to Disinformation
      url: https://www.rand.org/pubs/research_briefs/RB10088.html
      author: RAND Corporation
      date: '2019'
    - title: 'Deep Fakes: A Looming Challenge'
      url: https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3213954
      author: Chesney & Citron
      date: '2019'
    - title: The Oxygen of Amplification
      url: https://datasociety.net/library/oxygen-of-amplification/
      author: Whitney Phillips (Data & Society)
      date: '2018'
    - title: Inoculation Theory
      url: https://www.sdlab.psychol.cam.ac.uk/research/inoculation-science
      author: Sander van der Linden
    - title: C2PA Specification
      url: https://c2pa.org/specifications/specifications/1.0/specs/C2PA_Specification.html
    - title: Synthetic Media and AI
      url: https://partnershiponai.org/paper/responsible-practices-synthetic-media/
      author: Partnership on AI
      date: '2023'
  description: >
    Epistemic security refers to protecting society's collective capacity for truth-finding in an era when AI can
    generate convincing false content at unprecedented scale. Just as national security protects against physical
    threats, epistemic security protects against threats to our ability to know what is true and form shared beliefs
    about reality.


    The threat landscape includes AI-generated deepfakes that can fabricate video evidence, language models that can
    produce unlimited quantities of persuasive misinformation, and systems that can personalize deceptive content to
    individual vulnerabilities. These capabilities threaten the basic information infrastructure that democratic
    societies depend on - the shared understanding of facts that enables public deliberation, elections, and collective
    decision-making.


    Defending epistemic security requires multiple layers: technical tools for content authentication and provenance,
    media literacy education that teaches critical evaluation of information sources, institutional reforms that
    increase resilience to manipulation, and regulatory frameworks that create accountability for platforms and AI
    developers. The challenge is that offensive capabilities (generating false content) are advancing faster than
    defensive capabilities (detecting it), creating an asymmetry that favors attackers.
  tags:
    - disinformation
    - deepfakes
    - trust
    - media-literacy
    - content-authentication
    - information-security
  lastUpdated: 2025-12
- id: pause-advocacy
  type: intervention
  title: Pause Advocacy
  customFields:
    - label: Approach
      value: Advocate for slowing or pausing frontier AI development
    - label: Tractability
      value: Low (major political/economic barriers)
    - label: Key Organizations
      value: Future of Life Institute, Pause AI
  relatedEntries:
    - id: racing-dynamics
      type: risk
    - id: deceptive-alignment
      type: risk
    - id: treacherous-turn
      type: risk
    - id: lock-in
      type: risk
    - id: compute-governance
      type: policy
  sources:
    - title: 'Pause Giant AI Experiments: An Open Letter'
      url: https://futureoflife.org/open-letter/pause-giant-ai-experiments/
      author: Future of Life Institute
      date: '2023'
  description: >
    Pause advocacy involves advocating for slowing down or pausing the development of frontier AI systems until safety
    can be ensured. The core theory of change is that buying time allows safety research to catch up with capabilities,
    enables governance frameworks to mature, and reduces the probability of deploying systems we cannot control.
  tags:
    - governance
    - policy
    - racing-dynamics
    - coordination
  lastUpdated: 2025-12
- id: ai-control
  type: safety-agenda
  title: AI Control
  customFields:
    - label: Goal
      value: Maintain human control over AI
    - label: Key Research
      value: Redwood Research
  relatedEntries:
    - id: redwood
      type: lab
    - id: deceptive-alignment
      type: risk
    - id: scheming
      type: risk
    - id: treacherous-turn
      type: risk
    - id: sandbagging
      type: risk
    - id: power-seeking
      type: risk
    - id: mesa-optimization
      type: risk
    - id: agentic-ai
      type: capability
  sources:
    - title: 'AI Control: Improving Safety Despite Intentional Subversion'
      url: https://arxiv.org/abs/2312.06942
      author: Greenblatt et al.
      date: '2023'
    - title: 'Redwood Research: AI Control'
      url: https://www.redwoodresearch.org/control
  description: >-
    AI Control is a research agenda that focuses on maintaining safety even when using AI systems that might be actively
    trying to subvert safety measures. Rather than assuming alignment succeeds, it asks: "How can we safely use AI
    systems that might be misaligned?"
  tags:
    - monitoring
    - containment
    - defense-in-depth
    - red-teaming
    - untrusted-ai
  lastUpdated: 2025-12
- id: anthropic-core-views
  type: safety-agenda
  title: Anthropic Core Views
  website: https://anthropic.com/news/core-views-on-ai-safety
  customFields:
    - label: Published
      value: '2023'
    - label: Status
      value: Active
  relatedEntries:
    - id: anthropic
      type: lab
    - id: interpretability
      type: safety-agenda
    - id: scalable-oversight
      type: safety-agenda
  sources:
    - title: Core Views on AI Safety
      url: https://anthropic.com/news/core-views-on-ai-safety
      author: Anthropic
      date: '2023'
    - title: Responsible Scaling Policy
      url: https://anthropic.com/news/anthropics-responsible-scaling-policy
      date: '2023'
  description: >-
    Anthropic's Core Views on AI Safety is their publicly stated research agenda and organizational philosophy.
    Published in 2023, it articulates why Anthropic believes safety-focused labs should be at the frontier of AI
    development.
  tags:
    - ai-safety
    - constitutional-ai
    - interpretability
    - responsible-scaling
    - anthropic
    - research-agenda
  lastUpdated: 2025-12
- id: corrigibility
  type: safety-agenda
  title: Corrigibility
  customFields:
    - label: Goal
      value: AI allows human correction
    - label: Status
      value: Active research
  relatedEntries:
    - id: ai-control
      type: safety-agenda
    - id: corrigibility-failure
      type: risk
    - id: power-seeking
      type: risk
    - id: instrumental-convergence
      type: risk
    - id: treacherous-turn
      type: risk
  tags:
    - shutdown-problem
    - ai-control
    - value-learning
- id: evals
  type: safety-agenda
  title: AI Evaluations
  customFields:
    - label: Goal
      value: Measure AI capabilities and safety
    - label: Key Orgs
      value: METR, Apollo, UK AISI
  relatedEntries:
    - id: sandbagging
      type: risk
    - id: emergent-capabilities
      type: risk
    - id: scheming
      type: risk
    - id: deceptive-alignment
      type: risk
    - id: bioweapons
      type: risk
    - id: cyberweapons
      type: risk
  tags:
    - benchmarks
    - red-teaming
    - capability-assessment
- id: interpretability
  type: safety-agenda
  title: Interpretability
  customFields:
    - label: Goal
      value: Understand model internals
    - label: Key Labs
      value: Anthropic, DeepMind
  relatedEntries:
    - id: anthropic
      type: lab
    - id: deceptive-alignment
      type: risk
    - id: mesa-optimization
      type: risk
    - id: goal-misgeneralization
      type: risk
    - id: scheming
      type: risk
    - id: reward-hacking
      type: risk
    - id: redwood
      type: lab
    - id: alignment-robustness
      type: parameter
      relationship: increases
    - id: interpretability-coverage
      type: parameter
      relationship: increases
    - id: safety-capability-gap
      type: parameter
      relationship: supports
    - id: human-oversight-quality
      type: parameter
      relationship: increases
  sources:
    - title: Scaling Monosemanticity
      url: https://transformer-circuits.pub/2024/scaling-monosemanticity/
      author: Anthropic
      date: '2024'
    - title: 'Zoom In: An Introduction to Circuits'
      url: https://distill.pub/2020/circuits/zoom-in/
      author: Olah et al.
    - title: Transformer Circuits Thread
      url: https://transformer-circuits.pub/
  description: >-
    Mechanistic interpretability is a research field focused on reverse-engineering neural networks to understand how
    they work internally. Rather than treating models as black boxes, researchers aim to identify meaningful circuits,
    features, and algorithms that explain model behavior.
  tags:
    - sparse-autoencoders
    - features
    - circuits
    - superposition
    - probing
    - activation-patching
  lastUpdated: 2025-12
- id: scalable-oversight
  type: safety-agenda
  title: Scalable Oversight
  customFields:
    - label: Goal
      value: Supervise AI beyond human ability
    - label: Key Labs
      value: Anthropic, OpenAI, DeepMind
  relatedEntries:
    - id: arc
      type: lab
    - id: deepmind
      type: lab
    - id: deceptive-alignment
      type: risk
    - id: sycophancy
      type: risk
    - id: reward-hacking
      type: risk
    - id: power-seeking
      type: risk
    - id: corrigibility-failure
      type: risk
    - id: human-oversight-quality
      type: parameter
      relationship: increases
    - id: alignment-robustness
      type: parameter
      relationship: supports
    - id: human-agency
      type: parameter
      relationship: supports
  sources:
    - title: AI Safety via Debate
      url: https://arxiv.org/abs/1805.00899
      author: Irving et al.
    - title: Scalable Agent Alignment via Reward Modeling
      url: https://arxiv.org/abs/1811.07871
      author: Leike et al.
    - title: Measuring Progress on Scalable Oversight
      url: https://arxiv.org/abs/2211.03540
  description: >-
    Scalable oversight addresses a fundamental challenge: How can humans supervise AI systems on tasks where humans
    can't directly evaluate the AI's output?
  tags:
    - debate
    - recursive-reward-modeling
    - process-supervision
    - ai-evaluation
    - rlhf
    - superhuman-ai
  lastUpdated: 2025-12
- id: ai-forecasting
  type: intervention
  title: AI-Augmented Forecasting
  customFields:
    - label: Maturity
      value: Rapidly emerging
    - label: Key Strength
      value: Combines AI scale with human judgment
    - label: Key Challenge
      value: Calibration across domains
    - label: Key Players
      value: Metaculus, FutureSearch, Epoch AI
  sources:
    - title: Metaculus AI Forecasting
      url: https://www.metaculus.com/project/ai-forecasting/
    - title: FutureSearch
      url: https://arxiv.org/abs/2312.07474
      date: '2023'
    - title: Epoch AI
      url: https://epochai.org/
    - title: Superforecasting
      author: Philip Tetlock
      date: '2015'
    - title: Forecasting Research Institute
      url: https://forecastingresearch.org/
  description: >
    AI-augmented forecasting combines the pattern-recognition and data-processing capabilities of AI systems with the
    contextual judgment and calibration of human forecasters. This hybrid approach aims to produce more accurate
    predictions about future events than either humans or AI alone, particularly for questions relevant to policy and
    risk assessment.


    Current systems take several forms. AI can aggregate and weight forecasts from many human predictors, adjusting for
    individual track records and biases. AI can assist forecasters by synthesizing relevant information, identifying
    base rates, and flagging considerations that might otherwise be missed. More ambitiously, AI systems can generate
    their own forecasts that human superforecasters then evaluate and combine with their own judgments.


    For AI safety and epistemic security, improved forecasting offers several benefits. Better predictions about AI
    capabilities help with governance timing. Forecasting AI-related risks provides early warning. Publicly visible
    forecasts create accountability for claims about AI development. The key challenge is calibration - ensuring that
    probability estimates are meaningful across diverse domains and maintaining accuracy as AI systems become the
    subject of the forecasts themselves.
  tags:
    - forecasting
    - prediction-markets
    - ai-capabilities
    - decision-making
    - calibration
  lastUpdated: 2025-12
- id: content-authentication
  type: intervention
  title: Content Authentication
  customFields:
    - label: Maturity
      value: Standards emerging; early deployment
    - label: Key Standard
      value: C2PA (Coalition for Content Provenance and Authenticity)
    - label: Key Challenge
      value: Universal adoption; credential stripping
    - label: Key Players
      value: Adobe, Microsoft, Google, BBC, camera manufacturers
  relatedEntries:
    - id: authentication-collapse
      type: risk
    - id: deepfakes
      type: risk
    - id: disinformation
      type: risk
    - id: fraud
      type: risk
  sources:
    - title: C2PA Technical Specification
      url: https://c2pa.org/specifications/specifications/1.0/specs/C2PA_Specification.html
    - title: Content Authenticity Initiative
      url: https://contentauthenticity.org/
    - title: Google SynthID
      url: https://deepmind.google/technologies/synthid/
    - title: Project Origin
      url: https://www.originproject.info/
    - title: 'Witness: Video as Evidence'
      url: https://www.witness.org/
  description: >
    Content authentication technologies aim to establish verifiable provenance for digital content - allowing users to
    confirm where content came from, whether it has been modified, and whether it was created by AI or humans. The goal
    is to rebuild trust in digital media by creating technical guarantees of authenticity that complement human
    judgment.


    The leading approach is the C2PA (Coalition for Content Provenance and Authenticity) standard, backed by major
    technology companies. C2PA embeds cryptographically signed metadata into content at the point of creation - when a
    photo is taken, when a video is recorded, when an AI generates an image. This creates a chain of custody that can be
    verified later. Other approaches include invisible watermarking (SynthID), blockchain-based verification, and
    forensic analysis tools that detect signs of synthetic generation or manipulation.


    The key challenges are adoption and circumvention. Content authentication only works if it becomes universal - if
    users come to expect provenance information and distrust content without it. But metadata can be stripped,
    watermarks can potentially be removed or spoofed, and AI-generated content without credentials can still circulate.
    The race between authentication and forgery capability is uncertain, but authentication provides one of the few
    technical defenses against the coming flood of synthetic content.
  tags:
    - deepfakes
    - digital-evidence
    - verification
    - watermarking
    - trust
  lastUpdated: 2025-12
- id: coordination-tech
  type: intervention
  title: Coordination Technologies
  customFields:
    - label: Maturity
      value: Emerging; active development
    - label: Key Strength
      value: Addresses collective action failures
    - label: Key Challenge
      value: Bootstrapping trust and adoption
    - label: Key Domains
      value: AI governance, epistemic defense, international cooperation
  relatedEntries:
    - id: racing-dynamics
      type: risk
    - id: multipolar-trap
      type: risk
    - id: flash-dynamics
      type: risk
    - id: proliferation
      type: risk
  sources:
    - title: The Strategy of Conflict
      author: Thomas Schelling
      date: '1960'
    - title: Governing the Commons
      author: Elinor Ostrom
      date: '1990'
    - title: GovAI Research
      url: https://www.governance.ai/
    - title: Computing Power and the Governance of AI
      url: https://arxiv.org/abs/2402.08797
      date: '2024'
  description: >
    Coordination technologies are tools and mechanisms that enable actors to cooperate on collective challenges when
    individual incentives favor defection. For AI safety, these technologies address the fundamental problem that racing
    to develop AI faster may be individually rational but collectively catastrophic. For epistemic security, they help
    coordinate defensive responses to disinformation.


    These technologies draw on mechanism design, game theory, and institutional economics. Examples include:
    verification protocols that allow actors to confirm others' compliance with agreements (critical for AI safety
    treaties); commitment devices that make defection from cooperative arrangements costly; signaling mechanisms that
    allow actors to credibly communicate intentions; and platforms that make coordination focal points more visible.


    For AI governance specifically, coordination technologies might include compute monitoring systems that verify
    compliance with training restrictions, international registries of advanced AI systems, and mechanisms for sharing
    safety research while protecting commercial interests. The fundamental insight from Elinor Ostrom's work is that
    collective action problems are not unsolvable - but they require deliberate institutional design. The urgency of AI
    risk makes developing effective coordination mechanisms for this domain a priority.
  tags:
    - game-theory
    - governance
    - international-cooperation
    - mechanism-design
    - verification
  lastUpdated: 2025-12
- id: deliberation
  type: intervention
  title: AI-Assisted Deliberation
  customFields:
    - label: Maturity
      value: Emerging; promising pilots
    - label: Key Strength
      value: Scales genuine dialogue, not just voting
    - label: Key Challenge
      value: Adoption and integration with governance
    - label: Key Players
      value: Polis, Anthropic (Collective Constitutional AI), Taiwan vTaiwan
  sources:
    - title: Polis
      url: https://pol.is/
    - title: Collective Constitutional AI
      url: https://www.anthropic.com/news/collective-constitutional-ai-aligning-a-language-model-with-public-input
      author: Anthropic
      date: '2023'
    - title: Stanford Deliberative Democracy Lab
      url: https://deliberation.stanford.edu/
    - title: Democracy When the People Are Thinking
      author: James Fishkin
      date: '2018'
    - title: vTaiwan
      url: https://info.vtaiwan.tw/
  description: >
    AI-assisted deliberation uses AI to scale meaningful democratic dialogue beyond the constraints of traditional town
    halls and focus groups. Rather than replacing human deliberation with AI decisions, these tools use AI to
    facilitate, synthesize, and scale genuine human discussion - enabling thousands or millions of people to engage in
    deliberative processes that traditionally require small groups.


    Pioneering systems like Polis cluster participant opinions to surface areas of consensus and reveal the structure of
    disagreement. Taiwan's vTaiwan platform has used these tools to engage citizens in policy development on contentious
    issues. Anthropic's Collective Constitutional AI experiment used similar methods to gather public input on how AI
    systems should behave. The core insight is that AI can help identify common ground, summarize diverse viewpoints,
    and translate between different perspectives at scales previously impossible.


    For AI governance, these tools offer a path to democratically legitimate AI policy. Rather than leaving AI
    development decisions to companies or technical elites, deliberation platforms could engage broader publics in
    decisions about how AI should be developed and deployed. For epistemic security, deliberative processes can help
    societies navigate contested questions by surfacing genuine consensus where it exists and clarifying the structure
    of genuine disagreement where it doesn't.
  tags:
    - democratic-innovation
    - collective-intelligence
    - governance
    - participatory-democracy
    - consensus-building
  lastUpdated: 2025-12
- id: epistemic-infrastructure
  type: intervention
  title: Epistemic Infrastructure
  customFields:
    - label: Maturity
      value: Conceptual; partial implementations
    - label: Key Insight
      value: Knowledge systems need deliberate design
    - label: Key Challenge
      value: Coordination, funding, governance
    - label: Key Examples
      value: Wikipedia, Semantic Scholar, fact-checking networks
  relatedEntries:
    - id: trust-decline
      type: risk
    - id: epistemic-collapse
      type: risk
    - id: knowledge-monopoly
      type: risk
    - id: scientific-corruption
      type: risk
    - id: historical-revisionism
      type: risk
  sources:
    - title: Wikimedia Foundation
      url: https://wikimediafoundation.org/
    - title: Internet Archive
      url: https://archive.org/
    - title: Semantic Scholar
      url: https://www.semanticscholar.org/
    - title: International Fact-Checking Network
      url: https://www.poynter.org/ifcn/
  description: >
    Epistemic infrastructure refers to the foundational systems that societies depend on for creating, verifying,
    preserving, and accessing knowledge. Just as physical infrastructure (roads, power grids) underlies economic
    activity, epistemic infrastructure (archives, scientific publishing, fact-checking networks, educational
    institutions) underlies society's capacity to know things collectively. This infrastructure is under stress and
    requires deliberate investment.


    Current epistemic infrastructure includes elements like Wikipedia (the largest attempt at collaborative knowledge
    creation), the Internet Archive (preserving digital history), academic peer review (verifying scientific claims),
    journalism (investigating and reporting events), and educational systems (transmitting knowledge across
    generations). Each of these faces AI-related threats: Wikipedia can be corrupted with AI-generated misinformation,
    archives struggle to authenticate materials, peer review cannot keep pace with AI-generated fraud, and journalism is
    economically threatened.


    Strengthening epistemic infrastructure requires treating it as a public good deserving of investment. This might
    include: funding for fact-checking organizations and investigative journalism, technical infrastructure for content
    authentication, archives designed for an AI-generated-content world, AI systems explicitly designed to support human
    knowledge creation rather than replace it, and educational programs that teach critical evaluation in an AI context.
    The alternative - letting epistemic infrastructure decay while AI advances - leads to knowledge monopolies, trust
    collapse, and reality fragmentation.
  tags:
    - knowledge-management
    - public-goods
    - information-infrastructure
    - verification
    - ai-for-good
  lastUpdated: 2025-12
- id: hybrid-systems
  type: intervention
  title: AI-Human Hybrid Systems
  customFields:
    - label: Maturity
      value: Emerging field; active research
    - label: Key Strength
      value: Combines AI scale with human robustness
    - label: Key Challenge
      value: Avoiding the worst of both
    - label: Related Fields
      value: HITL, human-computer interaction, AI safety
  relatedEntries:
    - id: automation-bias
      type: risk
    - id: erosion-of-agency
      type: risk
    - id: enfeeblement
      type: risk
    - id: learned-helplessness
      type: risk
    - id: expertise-atrophy
      type: risk
  sources:
    - title: 'Humans and Automation: Use, Misuse, Disuse, Abuse'
      author: Parasuraman & Riley
      date: '1997'
    - title: 'High-Performance Medicine: Convergence of AI and Human Expertise'
      url: https://www.nature.com/articles/s41591-018-0300-7
      author: Eric Topol
      date: '2019'
    - title: Stanford HAI
      url: https://hai.stanford.edu/
    - title: Redwood Research
      url: https://www.redwoodresearch.org/
  description: >
    AI-human hybrid systems are designs that deliberately combine AI capabilities with human judgment to achieve
    outcomes better than either could produce alone. Rather than full automation or human-only processes, hybrid systems
    aim to capture the benefits of AI (scale, speed, consistency, pattern recognition) while preserving the benefits of
    human judgment (contextual understanding, values, robustness to novel situations).


    Effective hybrid systems require careful design to avoid the pathologies of both pure automation and nominal human
    oversight. Automation bias leads humans to defer to AI even when AI is wrong. Rubber-stamp oversight gives an
    illusion of human control without substance. The challenge is creating systems where humans genuinely contribute and
    AI genuinely assists, rather than one side dominating or the partnership failing.


    Examples of promising hybrid approaches include: AI systems that flag decisions for human review based on
    uncertainty or stakes, rather than automating all decisions; human-in-the-loop systems where AI drafts and humans
    edit; collaborative intelligence systems where AI and humans have complementary roles; and AI tutoring systems that
    guide rather than replace learning. For AI safety, hybrid systems represent a middle ground between naive confidence
    in human oversight and resignation to full AI autonomy.
  tags:
    - human-ai-interaction
    - ai-control
    - decision-making
    - automation-bias
    - ai-safety
  lastUpdated: 2025-12
- id: prediction-markets
  type: intervention
  title: Prediction Markets
  customFields:
    - label: Maturity
      value: Growing adoption; proven concept
    - label: Key Strength
      value: Incentive-aligned information aggregation
    - label: Key Limitation
      value: Liquidity, legal barriers, manipulation risk
    - label: Key Players
      value: Polymarket, Metaculus, Manifold, Kalshi
  relatedEntries:
    - id: flash-dynamics
      type: risk
    - id: racing-dynamics
      type: risk
    - id: consensus-manufacturing
      type: risk
  sources:
    - title: Prediction Markets
      url: https://www.aeaweb.org/articles?id=10.1257/0895330041371321
      author: Wolfers & Zitzewitz
      date: '2004'
    - title: Superforecasting
      author: Philip Tetlock
      date: '2015'
    - title: 'Futarchy: Vote Values, Bet Beliefs'
      url: https://mason.gmu.edu/~rhanson/futarchy.html
      author: Robin Hanson
    - title: Metaculus
      url: https://www.metaculus.com/
    - title: Good Judgment Project
      url: https://goodjudgment.com/
  description: >
    Prediction markets use market mechanisms to aggregate beliefs about future events, producing probability estimates
    that reflect the collective knowledge of participants. Unlike polls or expert surveys, prediction markets create
    incentives for truthful revelation of beliefs - participants profit by being right, not by appearing smart or
    conforming to social expectations. This makes them resistant to many of the biases that afflict other forecasting
    methods.


    Empirically, prediction markets have strong track records. They consistently outperform expert panels on questions
    with clear resolution criteria. Platforms like Polymarket, Metaculus, and Manifold generate forecasts on AI
    development, geopolitical events, and scientific questions that often prove more accurate than institutional
    predictions. The Good Judgment Project demonstrated that carefully selected forecasters using prediction market-like
    mechanisms could outperform intelligence analysts with access to classified information.


    For AI governance and epistemic security, prediction markets offer several valuable functions. They can provide
    credible forecasts of AI capability development, helping policymakers time interventions appropriately. They can
    surface genuine expert consensus (or lack thereof) on contested questions. They can create accountability for AI
    labs' claims about safety and timelines. And they can provide a coordination mechanism for collective knowledge that
    is resistant to the manipulation that undermines traditional media and expert systems.
  tags:
    - forecasting
    - information-aggregation
    - mechanism-design
    - collective-intelligence
    - decision-making
  lastUpdated: 2025-12
- id: value-learning
  type: safety-agenda
  title: Value Learning
  description: >-
    Research agenda focused on AI systems learning human values from data, behavior, or feedback rather than explicit
    specification.
  status: stub
  relatedEntries:
    - id: rlhf
      type: intervention
    - id: reward-hacking
      type: risk
  tags:
    - alignment
    - values
    - learning
  lastUpdated: 2025-12
- id: prosaic-alignment
  type: safety-agenda
  title: Prosaic Alignment
  description: >-
    Approach to AI alignment that doesn't require fundamental theoretical breakthroughs, focusing on scaling current
    techniques.
  status: stub
  tags:
    - alignment
    - research-agenda
  lastUpdated: 2025-12
- id: ai-executive-order
  type: policy
  title: Biden AI Executive Order
  description: >-
    Executive Order 14110 on AI safety signed by President Biden in October 2023, establishing AI safety reporting
    requirements.
  status: stub
  relatedEntries:
    - id: us-aisi
      type: organization
  tags:
    - policy
    - us-government
    - regulation
  lastUpdated: 2025-12
